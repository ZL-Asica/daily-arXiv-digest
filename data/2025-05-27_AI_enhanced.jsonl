{"id": "2505.18318", "pdf": "https://arxiv.org/pdf/2505.18318.pdf", "abs": "https://arxiv.org/abs/2505.18318", "title": "The Relational Origins of Rules in Online Communities", "authors": ["Charles Kiene", "Sohyeon Hwang", "Nathan TeBlunthuis", "Carl Colglazier", "Aaron Shaw", "Benjamin Mako Hill"], "categories": ["cs.HC"], "comment": null, "summary": "Where do rules come from in online communities? While prior studies of online\ncommunity governance in social computing have sought to characterize rules by\ntheir functions within communities and documented practices of rule\nenforcement, they have largely overlooked rule adoption and change. This study\ninvestigates how and why online communities adopt and change their rules. We\nconducted a grounded theory-based analysis of 40 in-depth interviews with\ncommunity leaders from subreddits, Fandom wikis, and Fediverse servers, and\nidentified seven processes involved in the adoption of online community rules.\nOur findings reveal that, beyond regulating behavior and solving functional\nintra-community problems, rules are also adopted and changed for relational\nreasons, such as signaling or reinforcing community legitimacy and identity to\nother communities. While rule change was often prompted by challenges during\ncommunity growth or decline, change also depended on volunteer leaders' work\ncapacity, the presence of member feedback mechanisms, and relational dynamics\nbetween leaders and members. The findings extend prior theories from social\ncomputing and organizational research, illustrating how institutionalist and\necological explanations of the relational origins of rules complement more\nfunctional accounts. The results also support design recommendations that\nintegrate the relational aspects of rules and rulemaking to facilitate\nsuccessful governance across communities' lifecycles.", "AI": {"tldr": "This study investigates the adoption and change of rules in online communities through interviews with community leaders, revealing seven processes linked to relational dynamics and community identity.", "motivation": "To understand the processes behind the adoption and change of rules in online communities, which previous studies have largely overlooked.", "method": "Grounded theory-based analysis of 40 in-depth interviews with community leaders from various online platforms.", "result": "Identified seven processes of rule adoption and change, emphasizing relational factors and community identity over purely functional reasons.", "conclusion": "The findings suggest that rule-making in communities is influenced by relational dynamics and recommend integrating these aspects into governance design.", "key_contributions": ["Identification of seven processes involved in rule adoption and change", "Reinforcement of the need for relational dynamics in rulemaking", "Extension of social computing and organizational theories regarding rule origins"], "limitations": "", "keywords": ["online communities", "rules", "governance", "relational dynamics", "community identity"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.18385", "pdf": "https://arxiv.org/pdf/2505.18385.pdf", "abs": "https://arxiv.org/abs/2505.18385", "title": "Human-Centered AI Communication in Co-Creativity: An Initial Framework and Insights", "authors": ["Jeba Rezwana", "Corey Ford"], "categories": ["cs.HC", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2504.02526", "summary": "Effective communication between AI and humans is essential for successful\nhuman-AI co-creation. However, many current co-creative AI systems lack\neffective communication, which limits their potential for collaboration. This\npaper presents the initial design of the Framework for AI Communication (FAICO)\nfor co-creative AI, developed through a systematic review of 107 full-length\npapers. FAICO presents key aspects of AI communication and their impact on user\nexperience, offering preliminary guidelines for designing human-centered AI\ncommunication. To improve the framework, we conducted a preliminary study with\ntwo focus groups involving skilled individuals in AI, HCI, and design. These\nsessions sought to understand participants' preferences for AI communication,\ngather their perceptions of the framework, collect feedback for refinement, and\nexplore its use in co-creative domains like collaborative writing and design.\nOur findings reveal a preference for a human-AI feedback loop over linear\ncommunication and emphasize the importance of context in fostering mutual\nunderstanding. Based on these insights, we propose actionable strategies for\napplying FAICO in practice and future directions, marking the first step toward\ndeveloping comprehensive guidelines for designing effective human-centered AI\ncommunication in co-creation.", "AI": {"tldr": "This paper presents the Framework for AI Communication (FAICO), developed to enhance effective communication between AI and humans in co-creative contexts.", "motivation": "Current co-creative AI systems often fail to communicate effectively, which hampers collaboration potential.", "method": "The framework was developed through a systematic review of 107 papers and refined via focus groups with experts in AI, HCI, and design.", "result": "Participants preferred a human-AI feedback loop and emphasized context in communication for better mutual understanding.", "conclusion": "The study provides preliminary guidelines and strategies for applying the FAICO framework to improve human-centered AI communication.", "key_contributions": ["Introduction of the Framework for AI Communication (FAICO).", "Preliminary guidelines for human-centered AI communication based on user feedback.", "Emphasis on the importance of a feedback loop and context in AI communication."], "limitations": "The findings are based on a limited preliminary study with focus groups, which may not represent all user perspectives.", "keywords": ["AI communication", "human-AI co-creation", "Framework for AI Communication", "user experience", "collaborative design"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464.pdf", "abs": "https://arxiv.org/abs/2505.18464", "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies.", "AI": {"tldr": "This study evaluates the use of Large Language Models (LLMs) for anxiety support by analyzing their performance on user-generated content from the r/Anxiety subreddit.", "motivation": "To address the growing demand for accessible mental health support using LLMs in sensitive domains like anxiety support while acknowledging the workforce shortages.", "method": "A systematic evaluation framework assessing linguistic quality, safety and trustworthiness, and supportiveness using real user-generated posts for prompting and fine-tuning LLMs (GPT and Llama).", "result": "Fine-tuning LLMs on anxiety-related data improved linguistic quality but increased toxicity and bias, reducing emotional responsiveness. GPT was found to be more supportive than Llama.", "conclusion": "While LLMs can assist in anxiety support, careful consideration is necessary due to the potential risks of increased toxicity and diminished empathy when fine-tuned on unprocessed social media content.", "key_contributions": ["Evaluation of LLMs in a mental health context for anxiety support.", "Identification of risks associated with fine-tuning LLMs on social media data.", "Comparison of performance between different LLMs (GPT and Llama) in supportiveness."], "limitations": "LLMs exhibited limited empathy and increased bias and toxicity when fine-tuned on unprocessed data.", "keywords": ["Large Language Models", "Anxiety Support", "Mental Health", "Fine-tuning", "Naturalistic Data"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18553", "pdf": "https://arxiv.org/pdf/2505.18553.pdf", "abs": "https://arxiv.org/abs/2505.18553", "title": "Applying Ontologies and Knowledge Augmented Large Language Models to Industrial Automation: A Decision-Making Guidance for Achieving Human-Robot Collaboration in Industry 5.0", "authors": ["John Oyekan", "Christopher Turner", "Michael Bax", "Erich Graf"], "categories": ["cs.HC", "cs.RO"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has resulted in\ninterest in their potential applications within manufacturing systems,\nparticularly in the context of Industry 5.0. However, determining when to\nimplement LLMs versus other Natural Language Processing (NLP) techniques,\nontologies or knowledge graphs, remains an open question. This paper offers\ndecision-making guidance for selecting the most suitable technique in various\nindustrial contexts, emphasizing human-robot collaboration and resilience in\nmanufacturing. We examine the origins and unique strengths of LLMs, ontologies,\nand knowledge graphs, assessing their effectiveness across different industrial\nscenarios based on the number of domains or disciplines required to bring a\nproduct from design to manufacture. Through this comparative framework, we\nexplore specific use cases where LLMs could enhance robotics for human-robot\ncollaboration, while underscoring the continued relevance of ontologies and\nknowledge graphs in low-dependency or resource-constrained sectors.\nAdditionally, we address the practical challenges of deploying these\ntechnologies, such as computational cost and interpretability, providing a\nroadmap for manufacturers to navigate the evolving landscape of Language based\nAI tools in Industry 5.0. Our findings offer a foundation for informed\ndecision-making, helping industry professionals optimize the use of Language\nBased models for sustainable, resilient, and human-centric manufacturing. We\nalso propose a Large Knowledge Language Model architecture that offers the\npotential for transparency and configuration based on complexity of task and\ncomputing resources available.", "AI": {"tldr": "This paper discusses the application of Large Language Models (LLMs) in Industry 5.0, focusing on decision-making for using LLMs versus other NLP techniques like ontologies and knowledge graphs in manufacturing.", "motivation": "The increasing interest in applying AI, particularly LLMs, in manufacturing systems highlights the need for clarity on their appropriate use compared to traditional NLP methods, especially in human-robot collaboration contexts.", "method": "The paper presents a comparative framework examining the effectiveness of LLMs, ontologies, and knowledge graphs across various industrial scenarios, encompassing the design-to-manufacture process.", "result": "It identifies specific use cases where LLMs can enhance human-robot collaboration while affirming the importance of ontologies and knowledge graphs in simpler, resource-constrained environments.", "conclusion": "The findings provide a roadmap for manufacturers to effectively implement language-based AI tools, supporting sustainable and resilient manufacturing practices.", "key_contributions": ["Decision-making guidance for selecting between LLMs and traditional NLP techniques", "Insights on LLMs' role in human-robot collaboration", "Proposed architecture for a Large Knowledge Language Model that emphasizes transparency and adaptability."], "limitations": "Challenges include computational costs and issues with interpretability in deploying language-based AI technologies.", "keywords": ["Large Language Models", "Human-Robot Collaboration", "Industry 5.0", "Natural Language Processing", "Knowledge Graphs"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159.pdf", "abs": "https://arxiv.org/abs/2505.18159", "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement.", "AI": {"tldr": "This study addresses the digital exclusion of the endangered Comanche language by introducing low-cost NLP interventions for language preservation, demonstrating effective use of LLMs for language identification with minimal data.", "motivation": "The need for linguistic research and revitalization efforts for endangered languages, specifically focusing on the digital exclusion of Comanche.", "method": "A manually curated dataset of 412 phrases and a synthetic data generation pipeline were created, followed by an empirical evaluation of the performance of GPT-4o and GPT-4o-mini in language identification tasks.", "result": "LLMs demonstrated challenges in zero-shot settings for Comanche, but few-shot prompting improved accuracy to near-perfect levels with just five examples, highlighting the effectiveness of targeted NLP methodologies.", "conclusion": "The study illustrates the potential of targeted NLP approaches in low-resource contexts and emphasizes the importance of visibility and community engagement for language preservation.", "key_contributions": ["First computational investigation of Comanche.", "Development of a synthetic data generation pipeline for low-resource languages.", "Empirical evaluation showing the effectiveness of few-shot prompting in LLMs."], "limitations": "Focus on a single endangered language; results may not generalize to all low-resource languages.", "keywords": ["endangered languages", "NLP", "Comanche", "language preservation", "LLMs"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18771", "pdf": "https://arxiv.org/pdf/2505.18771.pdf", "abs": "https://arxiv.org/abs/2505.18771", "title": "SPIRAL integration of generative AI in an undergraduate creative media course: effects on self-efficacy and career outcome expectations", "authors": ["Troy Schotter", "Saba Kawas", "James Prather", "Juho Leinonen", "Jon Ippolito", "Greg L Nelson"], "categories": ["cs.HC"], "comment": null, "summary": "Computing education and computing students are rapidly integrating generative\nAI, but we know relatively little about how different pedagogical strategies\nfor intentionally integrating generative AI affect students' self-efficacy and\ncareer interests. This study investigates a SPIRAL integration of generative AI\n(Skills Practiced Independently, Revisited with AI Later), implemented in an\nintroductory undergraduate creative media and technology course in Fall 2023\n(n=31). Students first developed domain skills for half the semester, then\nrevisited earlier material integrating using generative AI, with explicit\ninstruction on how to use it critically and ethically. We contribute a mixed\nmethods quantitative and qualitative analysis of changes in self-efficacy and\ncareer interests over time, including longitudinal qualitative interviews (n=9)\nand thematic analysis. We found positive changes in both students' creative\nmedia self-efficacy and generative AI use self-efficacy, and mixed changes for\nethical generative AI use self-efficacy. We also found students experienced\ndemystification, transitioning from initial fear about generative AI taking\nover their fields and jobs, to doubting AI capability to do so and/or that\nsociety will push back against AI, through personal use of AI and observing\nothers' use of AI vicariously. For career interests, our SPIRAL integration of\ngenerative AI use appeared to have either a neutral or positive influence on\nstudents, including widening their perceived career options, depending on their\nview of how AI would influence the career itself. These findings suggest that\ncareful pedagogical sequencing can mitigate some potential negative impacts of\nAI, while promoting ethical and critical AI use that supports or has a neutral\neffect on students' career formation. To our knowledge our SPIRAL integration\nstrategy applied to generative AI integration is novel.", "AI": {"tldr": "This study investigates how the SPIRAL integration of generative AI in a creative media course affects students' self-efficacy and career interests.", "motivation": "To explore the impact of different pedagogical strategies for integrating generative AI on students' self-efficacy and career interests in computing education.", "method": "A mixed methods approach involving quantitative and qualitative analysis, longitudinal interviews, and thematic analysis, conducted in an introductory undergraduate course with 31 students.", "result": "Positive changes in students' creative media self-efficacy and generative AI use self-efficacy were observed, alongside mixed results for ethical AI use self-efficacy. The integration influenced career interests neutrally or positively, widening perceived options for students.", "conclusion": "Careful pedagogical sequencing can mitigate negative impacts of AI, promoting ethical AI use and maintaining a neutral or positive effect on career formation.", "key_contributions": ["Novel SPIRAL integration strategy for generative AI in education", "Mixed methods analysis of self-efficacy and career interests", "Insights into student perceptions of AI's impact on careers"], "limitations": "", "keywords": ["generative AI", "pedagogy", "self-efficacy", "career interests", "creative media"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215.pdf", "abs": "https://arxiv.org/abs/2505.18215", "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.", "AI": {"tldr": "This paper compares BERT-like models with LLMs in text classification and proposes a task-driven approach.", "motivation": "To challenge the growing trend of relying exclusively on LLMs for text classification by showcasing the strengths of traditional BERT-like models.", "method": "A systematic comparison of BERT-like fine-tuning, LLM internal state utilization, and zero-shot inference across six challenging datasets.", "result": "BERT-like models often outperform LLMs across various tasks, with specific strengths identified through PCA and probing experiments.", "conclusion": "A nuanced, task-driven approach to model selection, proposed as TaMAS, is superior to a one-size-fits-all reliance on LLMs.", "key_contributions": ["Systematic comparison of BERT-like models and LLMs.", "Identification of task-specific model strengths.", "Introduction of TaMAS for improved model selection."], "limitations": "Limited to six high-difficulty datasets; findings may not generalize to all text classification tasks.", "keywords": ["BERT", "LLM", "text classification", "TaMAS", "task-driven approach"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18862", "pdf": "https://arxiv.org/pdf/2505.18862.pdf", "abs": "https://arxiv.org/abs/2505.18862", "title": "Literature review on assistive technologies for people with Parkinson's disease", "authors": ["Subek Acharya", "Sansrit Paudel"], "categories": ["cs.HC"], "comment": null, "summary": "Parkinson's Disease (PD) is a neurodegenerative disorder that significantly\nimpacts motor and non-motor functions. There is currently no treatment that\nslows or stops neurodegeneration in PD. In this context, assistive technologies\n(ATs) have emerged as vital tools to aid people with Parkinson's and\nsignificantly improve their quality of life. This review explores a broad\nspectrum of ATs, including wearable and cueing devices, exoskeletons, robotics,\nvirtual reality, voice and video-assisted technologies, and emerging\ninnovations such as artificial intelligence (AI), machine learning (ML), and\nthe Internet of Things (IoT). The review highlights ATs' significant role in\naddressing motor symptoms such as freezing of gait (FOG) and gait and posture\ndisorders. However, it also identifies significant gaps in addressing non-motor\nsymptoms such as sleep dysfunction and mental health. Similarly, the research\nidentifies substantial potential in the further implementation of deep\nlearning, AI, IOT technologies. Overall, this review highlights the\ntransformative potential of AT in PD management while identifying gaps that\nfuture research should address to ensure personalized, accessible, and\neffective solutions.", "AI": {"tldr": "This review examines assistive technologies (ATs) for managing Parkinson's Disease (PD), emphasizing wearable devices, robotics, AI, and ML, while identifying gaps in addressing non-motor symptoms.", "motivation": "To explore the potential of assistive technologies in enhancing the quality of life for individuals with Parkinson's Disease, particularly in managing motor symptoms.", "method": "The review analyzes various assistive technologies, including both established and emerging innovations, and discusses their impact on motor and non-motor symptoms of PD.", "result": "The review reveals that while ATs significantly help manage motor symptoms like freezing of gait, substantial gaps remain in addressing non-motor symptoms such as sleep dysfunction and mental health.", "conclusion": "The study highlights the potential of ATs integrated with AI and ML for better PD management but calls for further research to personalize and improve interventions for non-motor symptoms.", "key_contributions": ["Comprehensive overview of current and emerging assistive technologies for PD.", "Identification of significant gaps regarding the treatment of non-motor symptoms.", "Discussion on the integration of AI and ML into assistive technologies for personalized solutions."], "limitations": "The review primarily focuses on the effectiveness of ATs for motor symptoms, with less emphasis on non-motor interventions.", "keywords": ["Parkinson's Disease", "assistive technologies", "machine learning", "artificial intelligence", "neurodegenerative disorders"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218.pdf", "abs": "https://arxiv.org/abs/2505.18218", "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "authors": ["Shuhang Xu", "Fangwei Zhong"], "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors.", "AI": {"tldr": "CoMet is a framework enhancing LLM agents' capabilities to process metaphors in multi-agent language games, improving their strategic communication skills.", "motivation": "Large language models (LLMs) often struggle with metaphors, hindering covert communication and semantic evasion necessary for strategic interactions.", "method": "CoMet integrates a hypothesis-based metaphor reasoner and a metaphor generator which improve through self-reflection and knowledge integration, facilitating better metaphor interpretation and application.", "result": "CoMet was evaluated on two multi-agent language games, showing significant improvements in agents' abilities to communicate strategically using metaphors.", "conclusion": "The introduction of CoMet allows LLM agents to more effectively engage in complex communication using metaphors, enhancing their strategic interaction capabilities.", "key_contributions": ["Introduction of the CoMet framework for metaphor processing in LLMs", "Combination of hypothesis-based reasoning and metaphor generation", "Demonstrated improved performance in multi-agent language games."], "limitations": "", "keywords": ["metaphor processing", "large language models", "covert communication", "strategic interaction", "multi-agent systems"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18928", "pdf": "https://arxiv.org/pdf/2505.18928.pdf", "abs": "https://arxiv.org/abs/2505.18928", "title": "Toward Human Centered Interactive Clinical Question Answering System", "authors": ["Dina Albassam"], "categories": ["cs.HC"], "comment": null, "summary": "Unstructured clinical notes contain essential patient information but are\nchallenging for physicians to search and interpret efficiently. Although large\nlanguage models (LLMs) have shown promise in question answering (QA), most\nexisting systems lack transparency, usability, and alignment with clinical\nworkflows. This work introduces an interactive QA system that enables\nphysicians to query clinical notes via text or voice and receive extractive\nanswers highlighted directly in the note for traceability.\n  The system was built using OpenAI models with zero-shot prompting and\nevaluated across multiple metrics, including exact string match, word overlap,\nSentenceTransformer similarity, and BERTScore. Results show that while exact\nmatch scores ranged from 47 to 62 percent, semantic similarity scores exceeded\n87 percent, indicating strong contextual alignment even when wording varied.\n  To assess usability, the system was also evaluated using simulated clinical\npersonas. Seven diverse physician and nurse personas interacted with the system\nacross scenario-based tasks and provided structured feedback. The evaluations\nhighlighted strengths in intuitive design and answer accessibility, alongside\nopportunities for enhancing explanation clarity.", "AI": {"tldr": "An interactive QA system enables physicians to query clinical notes via text or voice, providing extractive answers directly highlighted in the notes.", "motivation": "Unstructured clinical notes contain vital patient information but are difficult for physicians to interpret. Large language models, although promising, often lack usability and transparency in clinical settings.", "method": "Built using OpenAI models with zero-shot prompting; evaluated across metrics like exact string match and semantic similarity with multiple clinical personas.", "result": "Exact match scores ranged from 47 to 62 percent, while semantic similarity scores exceeded 87 percent, indicating strong contextual alignment.", "conclusion": "The system demonstrated strengths in intuitive design and answer accessibility, with feedback indicating areas for improving explanation clarity.", "key_contributions": ["Introduced an interactive QA system for clinical notes", "Implemented query via text and voice with extractive answers", "Evaluated usability with simulated physician and nurse personas"], "limitations": "Limited to the evaluation scores and feedback from a small number of clinical personas.", "keywords": ["interactive QA system", "clinical notes", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223.pdf", "abs": "https://arxiv.org/abs/2505.18223", "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning.", "AI": {"tldr": "Introducing IDA-Bench, a benchmark for evaluating large language models (LLMs) in multi-round interactive data analysis tasks.", "motivation": "To address the inadequacy of existing benchmarks that overlook the iterative nature of data analysis, where expert decisions evolve with insights gained during analysis.", "method": "Development of IDA-Bench, which assesses LLM agents in multi-round scenarios using tasks derived from complex Kaggle notebooks, presenting them as sequential natural language instructions.", "result": "Initial evaluations reveal that even advanced coding agents, such as Claude-3.7-thinking, fail to complete less than 50% of the tasks, indicating significant limitations in their performance compared to human benchmarks.", "conclusion": "There is a pressing need to enhance the multi-round capabilities of LLMs to create more dependable data analysis agents, balancing instruction following with reasoning.", "key_contributions": ["Introduction of IDA-Bench for multi-round interactive evaluation", "Comparison of LLM performance against human-derived baselines", "Highlighting the gap in current LLM capabilities in iterative tasks"], "limitations": "Focus on a specific set of tasks derived from Kaggle notebooks may limit generalizability.", "keywords": ["Large Language Models", "Data Analysis", "Benchmarking", "Multi-Round Interaction", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19101", "pdf": "https://arxiv.org/pdf/2505.19101.pdf", "abs": "https://arxiv.org/abs/2505.19101", "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems", "authors": ["Vaishali Dhanoa", "Anton Wolter", "Gabriela Molina León", "Hans-Jörg Schulz", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous agents powered by Large Language Models are transforming AI,\ncreating an imperative for the visualization field to embrace agentic\nframeworks. However, our field's focus on a human in the sensemaking loop\nraises critical questions about autonomy, delegation, and coordination for such\n\\textit{agentic visualization} that preserve human agency while amplifying\nanalytical capabilities. This paper addresses these questions by reinterpreting\nexisting visualization systems with semi-automated or fully automatic AI\ncomponents through an agentic lens. Based on this analysis, we extract a\ncollection of design patterns for agentic visualization, including agentic\nroles, communication and coordination. These patterns provide a foundation for\nfuture agentic visualization systems that effectively harness AI agents while\nmaintaining human insight and control.", "AI": {"tldr": "This paper discusses the integration of autonomous agents powered by Large Language Models in the field of visualization, focusing on preserving human agency while enhancing analytical capabilities. It proposes design patterns for effective agentic visualization systems.", "motivation": "The need to integrate AI agents in visualization while maintaining human oversight and decision-making capability.", "method": "The authors reinterpret existing visualization systems through an agentic lens, analyzing how semi-automated and fully automated AI components can be incorporated effectively.", "result": "The analysis led to the extraction of design patterns for agentic visualization, outlining roles, communication, and coordination between human users and AI agents.", "conclusion": "The proposed design patterns serve as a foundation for creating future visualization systems that effectively utilize AI while preserving human control.", "key_contributions": ["Introduction of design patterns for agentic visualization systems", "Analysis of human agency in the context of AI integration in visualization", "Framework for communication and coordination between human users and AI agents."], "limitations": "The paper does not provide empirical testing of the proposed design patterns in real-world applications.", "keywords": ["agentic visualization", "human agency", "AI integration", "visualization systems", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237.pdf", "abs": "https://arxiv.org/abs/2505.18237", "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment.", "AI": {"tldr": "This paper evaluates the efficiency of large reasoning models in multi-step reasoning, revealing a trade-off between reasoning length and semantic efficiency and proposing an adaptive strategy to improve both accuracy and efficiency.", "motivation": "To investigate the trade-off between reasoning length and semantic efficiency in large reasoning models, given the excessive lengths of reasoning chains often generated.", "method": "The authors propose two metrics, InfoBias and InfoGain, to quantify inefficiencies in reasoning. They introduce an entropy-based Adaptive Think strategy that halts reasoning dynamically based on confidence levels.", "result": "Using the Adaptive Think strategy, the study shows a 1.10% improvement in accuracy and a 50.80% reduction in token usage compared to the Vanilla Think approach, across six benchmark tasks.", "conclusion": "The entropy-based methods can enhance accuracy and cost-efficiency in deploying large language models.", "key_contributions": ["Introduction of InfoBias and InfoGain metrics for reasoning efficiency", "Development of Adaptive Think strategy that improves model reasoning process", "Demonstrates significant improvements in accuracy and token usage efficiency."], "limitations": "", "keywords": ["Large Reasoning Models", "Semantic Efficiency", "Adaptive Think", "Entropy-based Methods", "Multi-step Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19325", "pdf": "https://arxiv.org/pdf/2505.19325.pdf", "abs": "https://arxiv.org/abs/2505.19325", "title": "What do Blind and Low-Vision People Really Want from Assistive Smart Devices? Comparison of the Literature with a Focus Study", "authors": ["Bhanuka Gamage", "Thanh-Toan Do", "Nicholas Seow Chiang Price", "Arthur Lowery", "Kim Marriott"], "categories": ["cs.HC"], "comment": "Author's accepted version of a paper published at ACM SIGACCESS\n  Conference on Computers and Accessibility (ASSETS '23)", "summary": "Over the last decade there has been considerable research into how artificial\nintelligence (AI), specifically computer vision, can assist people who are\nblind or have low-vision (BLV) to understand their environment. However, there\nhas been almost no research into whether the tasks (object detection, image\ncaptioning, text recognition etc.) and devices (smartphones, smart-glasses\netc.) investigated by researchers align with the needs and preferences of BLV\npeople. We identified 646 studies published in the last two and a half years\nthat have investigated such assistive AI techniques. We analysed these papers\nto determine the task, device and participation by BLV individuals. We then\ninterviewed 24 BLV people and asked for their top five AI-based applications\nand to rank the applications found in the literature. We found only a weak\npositive correlation between BLV participants' perceived importance of tasks\nand researchers' focus and that participants prefer conversational agent\ninterface and head-mounted devices.", "AI": {"tldr": "This paper investigates the alignment of AI assistive technologies for blind or low-vision (BLV) individuals with their actual needs and preferences, highlighting a disconnect between research focus and user preferences.", "motivation": "To assess whether AI technologies aimed at assisting people who are blind or have low-vision align with their actual needs and preferences.", "method": "The study analyzed 646 recent studies on assistive AI techniques and conducted interviews with 24 BLV individuals to determine their preferred applications.", "result": "A weak positive correlation was found between the tasks considered important by BLV participants and the tasks prioritized in the literature; participants favored conversational agents and head-mounted devices.", "conclusion": "The findings highlight a mismatch between AI research focus and the preferences of BLV users, indicating a need for more user-centered design in assistive technologies.", "key_contributions": ["Analysis of 646 AI assistive technology studies for BLV individuals.", "Interviews with BLV individuals to determine preferred AI applications.", "Identification of user preferences for conversational agents and head-mounted devices."], "limitations": "The study's sample size for user interviews was limited to 24 BLV individuals, which may not represent the broader population.", "keywords": ["assistive technology", "blindness", "low vision", "AI applications", "user preferences"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240.pdf", "abs": "https://arxiv.org/abs/2505.18240", "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations.", "AI": {"tldr": "The paper presents a novel evaluation approach for automatically generated presentation slides using the RefSlides dataset and REFLEX metrics.", "motivation": "To improve the generation and evaluation of presentation slides using generative AI by introducing a reference-free evaluation approach.", "method": "The authors create a benchmark dataset (RefSlides) of high-quality presentations and propose REFLEX, an evaluation method that scores presentations based on metrics derived from negative examples generated with varying perturbations.", "result": "The evaluation approach demonstrates superior performance to existing heuristic and LLM-based evaluations in terms of scoring and providing feedback.", "conclusion": "REFLEX enhances the ability to evaluate presentation content effectively without the need for ground truth presentations, paving the way for better automated slide generation.", "key_contributions": ["Introduction of the RefSlides benchmark dataset for presentation evaluation.", "Development of REFLEX, a new evaluation method for presentation content.", "Demonstration of the effectiveness of reference-free evaluation techniques in outperforming traditional methods."], "limitations": "", "keywords": ["presentation generation", "evaluation metrics", "generative AI", "RefSlides dataset", "REFLEX evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19335", "pdf": "https://arxiv.org/pdf/2505.19335.pdf", "abs": "https://arxiv.org/abs/2505.19335", "title": "Knoll: Creating a Knowledge Ecosystem for Large Language Models", "authors": ["Dora Zhao", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC"], "comment": "21 pages, 8 figures, 7 tables", "summary": "Large language models are designed to encode general purpose knowledge about\nthe world from Internet data. Yet, a wealth of information falls outside this\nscope -- ranging from personal preferences to organizational policies, from\ncommunity-specific advice to up-to-date news -- that users want models to\naccess but remains unavailable. In this paper, we propose a knowledge ecosystem\nin which end-users can create, curate, and configure custom knowledge modules\nthat are utilized by language models, such as ChatGPT and Claude. To support\nthis vision, we introduce Knoll, a software infrastructure that allows users to\nmake modules by clipping content from the web or authoring shared documents on\nGoogle Docs and GitHub, add modules that others have made, and rely on the\nsystem to insert relevant knowledge when interacting with an LLM. We conduct a\npublic deployment of Knoll reaching over 200 users who employed the system for\na diverse set of tasks including personalized recommendations, advice-seeking,\nand writing assistance. In our evaluation, we validate that using Knoll\nimproves the quality of generated responses.", "AI": {"tldr": "The paper presents Knoll, a software infrastructure enabling users to create and curate custom knowledge modules for language models to enhance response quality.", "motivation": "To address the limitation of large language models that lack access to specific personal and contextual information that users often require.", "method": "Knoll allows users to clip content from the web or author documents on platforms like Google Docs and GitHub, which can then be integrated into language models for improved interaction.", "result": "A public deployment of Knoll with over 200 users showed that it supports diverse tasks and improves the quality of responses generated by language models.", "conclusion": "The findings suggest that custom knowledge modules can significantly enhance the utility of language models in providing personalized and relevant responses.", "key_contributions": ["Introduction of a knowledge ecosystem for LLMs", "Development of Knoll software for user-created knowledge modules", "Public deployment and evaluation of Knoll with positive feedback on response quality"], "limitations": "The study is limited to users familiar with digital tools, and the effectiveness may vary based on the quality of the modules created.", "keywords": ["knowledge modules", "language models", "user-generated content"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244.pdf", "abs": "https://arxiv.org/abs/2505.18244", "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "authors": ["Yukin Zhang", "Qi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities.", "AI": {"tldr": "This paper introduces a hierarchical framework called Multi_Scale Probabilistic Generation Theory (MSPGT) to interpret how Transformer-based language models generate text by analyzing global, intermediate, and local scales.", "motivation": "To understand the opaque nature of text generation in large Transformer models and to improve interpretability and control.", "method": "The framework factorizes text generation into three semantic scales and identifies scale boundaries using attention span thresholds and inter-layer mutual information peaks across various models.", "result": "Stable partitions of generation scales were identified in different Transformer architectures, revealing how decoder-only models focus more on global processing while encoder-only models emphasize local feature extraction.", "conclusion": "MSPGT provides a unified approach that enhances the interpretability and controllability of language models, bridging mechanistic understanding and emergent properties.", "key_contributions": ["Introduction of Multi_Scale Probabilistic Generation Theory (MSPGT) for language model interpretation.", "Identification of attention span thresholds and inter-layer mutual information peaks as metrics for scale boundaries.", "Demonstration of significant effects of scale manipulations on lexical diversity, sentence structure, and discourse coherence."], "limitations": "", "keywords": ["Transformer models", "Text generation", "Probabilistic generation", "Interpretability", "Hierarchical framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19419", "pdf": "https://arxiv.org/pdf/2505.19419.pdf", "abs": "https://arxiv.org/abs/2505.19419", "title": "It's Not Just Labeling\" -- A Research on LLM Generated Feedback Interpretability and Image Labeling Sketch Features", "authors": ["Baichuan Li", "Larry Powell", "Tracy Hammond"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The quality of training data is critical to the performance of machine\nlearning applications in domains like transportation, healthcare, and robotics.\nAccurate image labeling, however, often relies on time-consuming, expert-driven\nmethods with limited feedback. This research introduces a sketch-based\nannotation approach supported by large language models (LLMs) to reduce\ntechnical barriers and enhance accessibility. Using a synthetic dataset, we\nexamine how sketch recognition features relate to LLM feedback metrics, aiming\nto improve the reliability and interpretability of LLM-assisted labeling. We\nalso explore how prompting strategies and sketch variations influence feedback\nquality. Our main contribution is a sketch-based virtual assistant that\nsimplifies annotation for non-experts and advances LLM-driven labeling tools in\nterms of scalability, accessibility, and explainability.", "AI": {"tldr": "This research presents a sketch-based annotation approach leveraging large language models (LLMs) to improve the quality and accessibility of image labeling in machine learning applications.", "motivation": "The quality of training data is crucial for machine learning performance, especially in fields like healthcare, necessitating better annotation methods.", "method": "A sketch-based annotation system is developed, using a synthetic dataset to analyze sketch recognition and LLM feedback metrics, alongside various prompting strategies.", "result": "The study demonstrates that the sketch recognition features positively correlate with LLM feedback metrics, enhancing the reliability of data labeling.", "conclusion": "A sketch-based virtual assistant is introduced, aimed at simplifying image annotation for non-experts and enhancing the explainability and accessibility of LLM tools.", "key_contributions": ["Introduction of a sketch-based annotation method for image labeling", "Development of a virtual assistant for non-expert annotation", "Investigation of the relationship between sketch recognition features and LLM feedback metrics"], "limitations": "", "keywords": ["sketch-based annotation", "large language models", "image labeling", "healthcare", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247.pdf", "abs": "https://arxiv.org/abs/2505.18247", "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc.", "AI": {"tldr": "The paper proposes a novel method named 'MetaGen Blended RAG' to enhance retrieval accuracy in enterprise search for domain-specific datasets, achieving significantly higher accuracy compared to previous methods without the need for fine-tuning.", "motivation": "The motivation is to improve answer accuracy in enterprise environments where domain-specific knowledge bases pose challenges due to complex terminology and significant semantic variability.", "method": "The methodology focuses on enhancing the retriever through hybrid query indexes and metadata enrichment, constructing a metadata generation pipeline utilizing key concepts and topics.", "result": "On the PubMedQA benchmark, the approach achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing existing results and setting a new benchmark for zero-shot performance.", "conclusion": "The proposed approach avoids overfitting and generalizes effectively across varied domains, demonstrating robustness and scalability across multiple benchmarks.", "key_contributions": ["Introduced 'MetaGen Blended RAG' for improved retrieval accuracy.", "Achieved state-of-the-art results on PubMedQA without fine-tuning.", "Demonstrated the effectiveness across multiple Q&A datasets."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Enterprise Search", "Domain-Specific Datasets"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19441", "pdf": "https://arxiv.org/pdf/2505.19441.pdf", "abs": "https://arxiv.org/abs/2505.19441", "title": "Fairness Practices in Industry: A Case Study in Machine Learning Teams Building Recommender Systems", "authors": ["Jing Nathan Yan", "Junxiong Wang", "Jeffrey M. Rzeszotarski", "Allison Koenecke"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.LG"], "comment": null, "summary": "The rapid proliferation of recommender systems necessitates robust fairness\npractices to address inherent biases. Assessing fairness, though, is\nchallenging due to constantly evolving metrics and best practices. This paper\nanalyzes how industry practitioners perceive and incorporate these changing\nfairness standards in their workflows. Through semi-structured interviews with\n11 practitioners from technical teams across a range of large technology\ncompanies, we investigate industry implementations of fairness in\nrecommendation system products. We focus on current debiasing practices,\napplied metrics, collaborative strategies, and integrating academic research\ninto practice. Findings show a preference for multi-dimensional debiasing over\ntraditional demographic methods, and a reliance on intuitive rather than\nacademic metrics. This study also highlights the difficulties in balancing\nfairness with both the practitioner's individual (bottom-up) roles and\norganizational (top-down) workplace constraints, including the interplay with\nlegal and compliance experts. Finally, we offer actionable recommendations for\nthe recommender system community and algorithmic fairness practitioners,\nunderlining the need to refine fairness practices continually.", "AI": {"tldr": "The paper investigates how industry practitioners deal with fairness in recommender systems, focusing on debiasing practices, metrics, and integration of academic research.", "motivation": "To address biases in recommender systems and understand industry practices related to fairness standards.", "method": "Semi-structured interviews with 11 practitioners from large technology companies to analyze their approaches towards fairness in recommendation systems.", "result": "Practitioners prefer multi-dimensional debiasing methods over traditional techniques and rely on intuitive metrics rather than academic benchmarks.", "conclusion": "The study reveals challenges in balancing fairness with practical constraints and provides recommendations for enhancing fairness practices in recommender systems.", "key_contributions": ["Insights into the perception of fairness by industry practitioners", "Identification of preferred debiasing methods", "Recommendations for improving fairness practices in the recommender system community"], "limitations": "Limited to interviews with 11 practitioners, which may not represent the entire industry.", "keywords": ["fairness", "recommender systems", "debiasing", "industry practices", "algorithmic fairness"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283.pdf", "abs": "https://arxiv.org/abs/2505.18283", "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.", "AI": {"tldr": "Introducing TAGS, a framework that enhances medical reasoning in large language models (LLMs) by integrating a generalist model with a domain-specific specialist, improving accuracy without fine-tuning.", "motivation": "To address the limitations in existing prompting-based methods and fine-tuned medical LLMs in terms of stability, generalization, and adaptability to new clinical scenarios.", "method": "TAGS combines a generalist model with a specialist, incorporating a hierarchical retrieval mechanism and a reliability scorer to support reasoning and answer aggregation.", "result": "TAGS significantly improves accuracy across nine MedQA benchmarks, with notable performance boosts for GPT-4o (13.8%) and DeepSeek-R1 (16.8%), while enhancing a 7B model from 14.1% to 23.9%.", "conclusion": "TAGS demonstrates a method to improve medical reasoning in LLMs without needing parameter updates, surpassing many fine-tuned models.", "key_contributions": ["Introduction of TAGS framework combining generalist and specialist models", "Hierarchical retrieval mechanism for multi-scale exemplars", "Reliability scorer for evaluating reasoning consistency"], "limitations": "", "keywords": ["medical reasoning", "large language models", "framework", "hierarchical retrieval", "reliability scoring"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2505.19652", "pdf": "https://arxiv.org/pdf/2505.19652.pdf", "abs": "https://arxiv.org/abs/2505.19652", "title": "SACM: SEEG-Audio Contrastive Matching for Chinese Speech Decoding", "authors": ["Hongbin Wang", "Zhihong Jia", "Yuanzhong Shen", "Ziwei Wang", "Siyang Li", "Kai Shu", "Feng Hu", "Dongrui Wu"], "categories": ["cs.HC", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech disorders such as dysarthria and anarthria can severely impair the\npatient's ability to communicate verbally. Speech decoding brain-computer\ninterfaces (BCIs) offer a potential alternative by directly translating speech\nintentions into spoken words, serving as speech neuroprostheses. This paper\nreports an experimental protocol for Mandarin Chinese speech decoding BCIs,\nalong with the corresponding decoding algorithms. Stereo-electroencephalography\n(SEEG) and synchronized audio data were collected from eight drug-resistant\nepilepsy patients as they conducted a word-level reading task. The proposed\nSEEG and Audio Contrastive Matching (SACM), a contrastive learning-based\nframework, achieved decoding accuracies significantly exceeding chance levels\nin both speech detection and speech decoding tasks. Electrode-wise analysis\nrevealed that a single sensorimotor cortex electrode achieved performance\ncomparable to that of the full electrode array. These findings provide valuable\ninsights for developing more accurate online speech decoding BCIs.", "AI": {"tldr": "This paper presents a framework for speech decoding BCIs using SEEG and audio data from patients with speech disorders.", "motivation": "The research aims to enhance communication for patients with dysarthria and anarthria by developing brain-computer interfaces that decode speech intentions directly.", "method": "An experimental protocol was executed involving SEEG and synchronized audio data from eight epilepsy patients performing a word-level reading task, implementing the SEEG and Audio Contrastive Matching (SACM) framework.", "result": "The SACM framework achieved decoding accuracies surpassing chance levels in speech detection and decoding tasks, with a single sensorimotor cortex electrode performing on par with a full electrode array.", "conclusion": "The findings contribute to the advancement of online speech decoding BCIs, potentially improving communication capabilities for patients with speech disorders.", "key_contributions": ["Introduction of the SACM framework for speech decoding BCIs", "Demonstrated high decoding accuracy using minimal electrode data", "Identified single electrode usefulness in speech detection tasks"], "limitations": "", "keywords": ["speech disorders", "brain-computer interfaces", "dysarthria", "electroencephalography", "contrastive learning"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.18298", "pdf": "https://arxiv.org/pdf/2505.18298.pdf", "abs": "https://arxiv.org/abs/2505.18298", "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards", "authors": ["Jinyan Su", "Claire Cardie"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models.", "AI": {"tldr": "This paper presents an adaptive reward-shaping method for large language models to produce concise outputs while maintaining accuracy during reasoning tasks.", "motivation": "To address the issue of unnecessarily long reasoning traces in reinforcement learning-trained models, which lead to increased costs and latency.", "method": "An adaptive reward-shaping approach that adjusts the trade-off between accuracy and response length based on the model's performance, dynamically increasing or relaxing the length penalty.", "result": "The proposed method consistently reduces reasoning length in various datasets while largely preserving accuracy, leading to more cost-efficient adaptive reasoning in language models.", "conclusion": "The adaptive reward method accelerates length reduction early on and prevents over-compression later, providing a significant improvement in reasoning efficiency.", "key_contributions": ["Introduces an adaptive reward-shaping mechanism for LLMs.", "Demonstrates effectiveness in reducing reasoning length while maintaining accuracy.", "Offers a new direction for efficiency in large-scale language model reasoning."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "reward shaping", "reasoning length", "accuracy"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.20068", "pdf": "https://arxiv.org/pdf/2505.20068.pdf", "abs": "https://arxiv.org/abs/2505.20068", "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction", "authors": ["Qingyu Liang", "Jaime Banks"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Shared understanding plays a key role in the effective communication in and\nperformance of human-human interactions. With the increasingly common\nintegration of AI into human contexts, the future of personal and workplace\ninteractions will likely see human-AI interaction (HAII) in which the\nperception of shared understanding is important. Existing literature has\naddressed the processes and effects of PSU in human-human interactions, but the\nconstrual remains underexplored in HAII. To better understand PSU in HAII, we\nconducted an online survey to collect user reflections on interactions with a\nlarge language model when it sunderstanding of a situation was thought to be\nsimilar to or different from the participant's. Through inductive thematic\nanalysis, we identified eight dimensions comprising PSU in human-AI\ninteractions: Fluency, aligned operation, fluidity, outcome satisfaction,\ncontextual awareness, lack of humanlike abilities, computational limits, and\nsuspicion.", "AI": {"tldr": "This paper explores the concept of perceived shared understanding (PSU) in human-AI interactions, specifically with large language models, identifying key dimensions that influence user perceptions.", "motivation": "To understand how perceived shared understanding affects interactions between humans and AI, particularly in workplace settings.", "method": "An online survey was conducted to gather user reflections on their experiences interacting with a large language model, followed by inductive thematic analysis to identify underlying dimensions of PSU.", "result": "The analysis revealed eight dimensions of PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.", "conclusion": "Understanding these dimensions can enhance communication and performance in human-AI interactions, shaping future AI integration in personal and workplace contexts.", "key_contributions": ["Identification of eight dimensions of PSU in human-AI interactions", "Contribution to the literature on human-AI interaction by exploring PSU", "Insights into user perceptions of AI capabilities and limitations"], "limitations": "The study is based on user reflections which may not comprehensively capture all dimensions of PSU in HAII.", "keywords": ["human-AI interaction", "perceived shared understanding", "large language models", "communication", "user perceptions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322.pdf", "abs": "https://arxiv.org/abs/2505.18322", "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base.", "AI": {"tldr": "The paper explores how LLMs, particularly GPT-4, reason about cultural norms across different cultures, revealing shortcomings in cultural specificity and hidden stereotypes.", "motivation": "To investigate whether LLMs apply cultural values consistently in real-world scenarios beyond survey-based assessments.", "method": "A bottom-up approach asking LLMs to reason about cultural norms in narratives from various cultures.", "result": "GPT-4 generates less culture-specific norms and retains hidden stereotypes instead of outright suppressing them.", "conclusion": "Improving LLMs to serve a diverse user base fairly requires addressing the lack of cultural specificity and the presence of hidden stereotypes.", "key_contributions": ["Introduces a bottom-up method to assess LLMs' understanding of cultural norms", "Finds significant non-specificity in the norms generated by GPT-4", "Identifies hidden stereotypes in LLM outputs"], "limitations": "The study focuses on GPT-4 and may not generalize to other LLMs or cultures.", "keywords": ["Cultural norms", "GPT-4", "Stereotypes", "Machine Learning", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20082", "pdf": "https://arxiv.org/pdf/2505.20082.pdf", "abs": "https://arxiv.org/abs/2505.20082", "title": "Understanding and Supporting Co-viewing Comedy in VR with Embodied Expressive Avatars", "authors": ["Ryo Ohara", "Chi-Lan Yang", "Takuji Narumi", "Hideaki Kuzuoka"], "categories": ["cs.HC"], "comment": null, "summary": "Co-viewing videos with family and friends remotely has become prevalent with\nthe support of communication channels such as text messaging or real-time voice\nchat. However, current co-viewing platforms often lack visible embodied cues,\nsuch as body movements and facial expressions. This absence can reduce\nemotional engagement and the sense of co-presence when people are watching\ntogether remotely. Although virtual reality (VR) is an emerging technology that\nallows individuals to participate in various social activities while embodied\nas avatars, we still do not fully understand how this embodiment in VR affects\nco-viewing experiences, particularly in terms of engagement, emotional\ncontagion, and expressive norms. In a controlled experiment involving eight\ntriads of three participants each (N=24), we compared the participants'\nperceptions and reactions while watching comedy in VR using embodied expressive\navatars that displayed visible laughter cues. This was contrasted with a\ncontrol condition where no such embodied expressions were presented. With a\nmixed-method analysis, we found that embodied laughter cues shifted\nparticipants' engagement from individual immersion to socially coordinated\nparticipation. Participants reported heightened self-awareness of emotional\nexpression, greater emotional contagion, and the development of expressive\nnorms surrounding co-viewers' laughter. The result highlighted the tension\nbetween individual engagement and interpersonal emotional accommodation when\nco-viewing with embodied expressive avatars.", "AI": {"tldr": "The study investigates how embodied expressive avatars in VR influence co-viewing experiences, focusing on engagement, emotional contagion, and expressive norms.", "motivation": "To understand the impact of visible embodied cues in co-viewing experiences on platforms lacking such features, particularly in virtual reality settings.", "method": "A controlled experiment comparing reactions of participants watching comedy in VR with and without embodied laughter cues, analyzed through mixed methods.", "result": "Participants using embodied avatars showed increased engagement, emotional contagion, and developed new expressive norms related to laughter.", "conclusion": "Embodied laughter cues in VR change the dynamics of co-viewing from individual focus to social interaction, enhancing emotional awareness and expression among viewers.", "key_contributions": ["Demonstrated the impact of VR embodiment on co-viewing engagement", "Revealed how laughter cues foster emotional contagion among viewers", "Identified new expressive norms in virtual co-viewing scenarios."], "limitations": "Limited sample size of 24 participants and specific context of comedy viewing.", "keywords": ["embodiment", "virtual reality", "co-viewing", "emotional contagion", "social interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331.pdf", "abs": "https://arxiv.org/abs/2505.18331", "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA", "AI": {"tldr": "Presentation of the PerMedCQA benchmark for evaluating LLMs on Persian-language medical consumer questions.", "motivation": "Despite advances in LLMs for medical QA, there is a lack of resources in low-resource languages like Persian.", "method": "Creation of PerMedCQA benchmark sourced from a medical QA forum, evaluating several multilingual LLMs using a novel rubric-based framework.", "result": "The evaluation of state-of-the-art LLMs reveals significant challenges in multilingual medical QA.", "conclusion": "Insights gained can help improve context-aware medical assistance systems, with the dataset made publicly available.", "key_contributions": ["Introduction of the PerMedCQA benchmark for Persian medical questions", "Evaluation of LLMs with a novel rubric-based framework", "Public availability of a large dataset for further research."], "limitations": "Limited to Persian language; challenges identified may not generalize across other languages.", "keywords": ["medical question answering", "large language models", "Persian", "multilingual", "health informatics"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.20085", "pdf": "https://arxiv.org/pdf/2505.20085.pdf", "abs": "https://arxiv.org/abs/2505.20085", "title": "Explanation User Interfaces: A Systematic Literature Review", "authors": ["Eleonora Cappuccio", "Andrea Esposito", "Francesco Greco", "Giuseppe Desolda", "Rosa Lanzilotti", "Salvatore Rinzivillo"], "categories": ["cs.HC", "cs.AI", "A.1"], "comment": "First version", "summary": "Artificial Intelligence (AI) is one of the major technological advancements\nof this century, bearing incredible potential for users through AI-powered\napplications and tools in numerous domains. Being often black-box (i.e., its\ndecision-making process is unintelligible), developers typically resort to\neXplainable Artificial Intelligence (XAI) techniques to interpret the behaviour\nof AI models to produce systems that are transparent, fair, reliable, and\ntrustworthy. However, presenting explanations to the user is not trivial and is\noften left as a secondary aspect of the system's design process, leading to AI\nsystems that are not useful to end-users. This paper presents a Systematic\nLiterature Review on Explanation User Interfaces (XUIs) to gain a deeper\nunderstanding of the solutions and design guidelines employed in the academic\nliterature to effectively present explanations to users. To improve the\ncontribution and real-world impact of this survey, we also present a framework\nfor Human-cEnteRed developMent of Explainable user interfaceS (HERMES) to guide\npractitioners and academics in the design and evaluation of XUIs.", "AI": {"tldr": "A systematic review of Explanation User Interfaces (XUIs) in AI, accompanied by a framework (HERMES) to guide their design and evaluation.", "motivation": "To address the challenge of designing effective explanations for AI systems, ensuring they are transparent and useful to end-users.", "method": "Systematic Literature Review on existing Explanation User Interfaces (XUIs), focusing on solutions and design guidelines from the academic literature.", "result": "The review reveals key strategies and design principles employed in XUIs and introduces the HERMES framework to enhance the development of explainable user interfaces.", "conclusion": "HERMES serves as a practical guide for practitioners and researchers in creating more effective and user-friendly explanations in AI applications.", "key_contributions": ["Systematic literature review of XUIs.", "Identification of design guidelines for effective explanations.", "Development of the HERMES framework for designing and evaluating XUIs."], "limitations": "The review may not cover all existing literature on XUIs, and the framework's applicability may vary across different contexts.", "keywords": ["Explainable AI", "Explanation User Interfaces", "XUIs", "HERMES", "Systematic Literature Review"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18343", "pdf": "https://arxiv.org/pdf/2505.18343.pdf", "abs": "https://arxiv.org/abs/2505.18343", "title": "Model Editing with Graph-Based External Memory", "authors": ["Yash Kumar Atri", "Ahmed Alaa", "Thomas Hartvigsen"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning.", "AI": {"tldr": "HYPE is a novel framework that improves the stability of post-training model edits in large language models (LLMs) using hyperbolic geometry and graph neural networks.", "motivation": "To address the issues of hallucinations and outdated knowledge in LLMs resulting from traditional editing methods that suffer from overfitting and catastrophic forgetting.", "method": "The framework consists of three components: (i) Hyperbolic Graph Construction using Poincaré embeddings to represent knowledge triples, (ii) Möbius-Transformed Updates for consistent edits in hyperbolic space, and (iii) Dual Stabilization to prevent catastrophic forgetting.", "result": "Experiments showed that HYPE enhances edit stability, factual accuracy, and multi-hop reasoning in LLMs such as GPT-J and GPT2-XL.", "conclusion": "HYPE provides a robust solution for dynamic parameter edits in LLMs by leveraging hyperbolic geometry.", "key_contributions": ["Introduction of HYPE framework for model editing", "Use of hyperbolic geometry to maintain hierarchical relationships", "Enhancements in stability and accuracy of model edits"], "limitations": "", "keywords": ["Hyperbolic Geometry", "Graph Neural Networks", "Model Editing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20138", "pdf": "https://arxiv.org/pdf/2505.20138.pdf", "abs": "https://arxiv.org/abs/2505.20138", "title": "FairTalk: Facilitating Balanced Participation in Video Conferencing by Implicit Visualization of Predicted Turn-Grabbing Intention", "authors": ["Ryo Iijima", "Shigeo Yoshida", "Atsushi Hashimoto", "Jiaxin Ma"], "categories": ["cs.HC"], "comment": null, "summary": "Creating fair opportunities for all participants to contribute is a notable\nchallenge in video conferencing. This paper introduces FairTalk, a system that\nfacilitates the subconscious redistribution of speaking opportunities. FairTalk\npredicts participants' turn-grabbing intentions using a machine learning model\ntrained on web-collected videoconference data with positive-unlabeled learning,\nwhere turn-taking detection provides automatic positive labels. To subtly\nbalance speaking turns, the system visualizes predicted intentions by mimicking\nnatural human behaviors associated with the desire to speak. A user study\nsuggests that FairTalk may help improve speaking balance, though subjective\nfeedback indicates no significant perceived impact. We also discuss design\nimplications derived from participant interviews.", "AI": {"tldr": "FairTalk is a system designed to create fair speaking opportunities in video conferencing by predicting turn-grabbing intentions using machine learning.", "motivation": "The challenge of ensuring fair opportunities for all participants in video conferencing settings to speak and contribute.", "method": "FairTalk utilizes a machine learning model trained on videoconference data with positive-unlabeled learning to predict participants' intentions to speak and visualizes these predictions to balance speaking turns.", "result": "User study results indicate that FairTalk may improve the balance of speaking turns among participants, but feedback showed no significant perceived impact on speaking balance.", "conclusion": "The paper discusses the potential of FairTalk in improving speaking balance and explores design implications from participant interviews.", "key_contributions": ["Introduction of FairTalk system for fair turn-taking in video conferencing", "Utilization of positive-unlabeled learning for training the model", "Insights from user studies on the effectiveness and design implications"], "limitations": "Subjective feedback from users indicated no significant perceived impact, suggesting a need for further investigation.", "keywords": ["FairTalk", "video conferencing", "turn-taking", "machine learning", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356.pdf", "abs": "https://arxiv.org/abs/2505.18356", "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start.", "AI": {"tldr": "This paper explores enhancements to large language models for lower-resource languages through modular frameworks that separate mathematical reasoning and multilingual capabilities during fine-tuning.", "motivation": "Large language models (LLMs) are less effective for tasks in low-resource languages due to limited data. This study aims to improve their performance by investigating cross-lingual transfer techniques that can specifically address performance gaps in such languages.", "method": "Developing modular frameworks that utilize frozen parameters and post hoc model merging to improve fine-tuning for separate mathematical and language tasks within LLMs.", "result": "Modular methods, such as fine-tuning language and math experts separately and applying Layer-Swapping for model merging, significantly outperform traditional fine-tuning approaches across various languages and models.", "conclusion": "The study identifies effective modular strategies for enhancing LLMs in low-resource settings, suggesting that decoupling specific model capabilities can lead to better performance outcomes.", "key_contributions": ["Introduction of modular frameworks for fine-tuning LLMs", "Validation of task-specific parameter separability", "Demonstration of improved performance with Layer-Swapping method"], "limitations": "Findings are primarily based on lower-resource languages and may not extend to all language tasks.", "keywords": ["large language models", "cross-lingual transfer", "fine-tuning", "modular frameworks", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18363", "pdf": "https://arxiv.org/pdf/2505.18363.pdf", "abs": "https://arxiv.org/abs/2505.18363", "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases", "authors": ["AmirHossein Safdarian", "Milad Mohammadi", "Ehsan Jahanbakhsh", "Mona Shahamat Naderi", "Heshaam Faili"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes.", "AI": {"tldr": "A novel zero-shot schema linking approach enhances text-to-SQL systems by utilizing LLMs to convert natural language queries into accurate SQL commands, achieving state-of-the-art results.", "motivation": "The paper addresses the challenge of schema linking in text-to-SQL systems, aiming to streamline the conversion of natural language questions into SQL queries by improving focus and reducing prompt size.", "method": "The proposed method constructs a schema graph based on foreign key relations and employs a single prompt to interact with Gemini 2.5 Flash to identify relevant tables and columns. Classical path-finding algorithms are used for optimal table and column selection.", "result": "The method achieves state-of-the-art performance on the BIRD benchmark, surpassing previous methodologies, including specialized and fine-tuned LLM approaches.", "conclusion": "The schema linking approach is simple, cost-effective, and scalable, making it a strong alternative to more complex solutions, with detailed studies showing its effectiveness in execution accuracy.", "key_contributions": ["Introduction of a zero-shot schema linking method for text-to-SQL tasks", "Construction of a schema graph utilizing foreign key relations", "State-of-the-art performance on BIRD benchmark with minimal resource usage"], "limitations": "", "keywords": ["Text-to-SQL", "Schema linking", "Large language models", "SQL queries", "Benchmark performance"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.18374", "pdf": "https://arxiv.org/pdf/2505.18374.pdf", "abs": "https://arxiv.org/abs/2505.18374", "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation", "authors": ["Jarrod Ragsdale", "Rajendra Boppana"], "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 11 figures, conference preprint", "summary": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research.", "AI": {"tldr": "Introducing a new Shell Input-Output Environment (ShIOEnv) for generating datasets of command-line interactions to enhance language model fine-tuning.", "motivation": "To address the limitations of existing public datasets in behavioral modeling of command-line interfaces (CLIs), especially for smaller language models.", "method": "The paper introduces ShIOEnv, which models command construction as a Markov Decision Process. It utilizes a context-free grammar derived from man pages to mask invalid command arguments and explores strategies such as grammar masking and PPO to enhance sample efficiency.", "result": "The use of ShIOEnv and the generated datasets significantly improved the quality of command behavior datasets, achieving up to 85% improvement in BLEU-4 scores when fine-tuning CodeT5.", "conclusion": "ShIOEnv facilitates better data generation for CLIs and is expected to benefit future research by providing high-quality command interaction datasets.", "key_contributions": ["Introduction of ShIOEnv for CLI behavioral modeling", "Use of a context-free grammar to mask invalid arguments", "Demonstrated significant improvements in model performance using the generated datasets"], "limitations": "", "keywords": ["command-line interfaces", "language models", "behavioral modeling", "dataset generation", "machine learning"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.18383", "pdf": "https://arxiv.org/pdf/2505.18383.pdf", "abs": "https://arxiv.org/abs/2505.18383", "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities", "authors": ["Abdellah El Mekki", "Houdaifa Atou", "Omer Nacar", "Shady Shehata", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development.", "AI": {"tldr": "This paper presents a novel methodology for enhancing Large Language Models (LLMs) with low-resource languages, focusing on Egyptian and Moroccan dialects to better represent their cultural heritage and values.", "motivation": "To address the underrepresentation of low-resource languages and cultures in LLMs, which often fail to align with the cultural aspects of local communities due to reliance on synthetic data from English corpora.", "method": "The authors propose a methodology for creating synthetic and retrieval-based pre-training data tailored to specific communities, based on their language, cultural heritage, and values, exemplified through Egyptian and Moroccan dialects.", "result": "NileChat, a 3B parameter LLM, shows improved performance on understanding, translation, and cultural alignment benchmarks compared to existing Arabic-aware LLMs, and is on par with larger models.", "conclusion": "The study demonstrates the importance of incorporating local community aspects into LLMs and shares methods, data, and models to encourage diversity in LLM development.", "key_contributions": ["Development of a novel methodology for culturally-aware LLM fine-tuning", "Introduction of NileChat, specifically tailored for Egyptian and Moroccan dialects", "Provision of data and models for community use to promote diversity in LLMs"], "limitations": "", "keywords": ["Large Language Models", "Cultural Heritage", "Low-Resource Languages", "NLP", "Cultural Values"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18405", "pdf": "https://arxiv.org/pdf/2505.18405.pdf", "abs": "https://arxiv.org/abs/2505.18405", "title": "RaDeR: Reasoning-aware Dense Retrieval Models", "authors": ["Debrup Das", "Sam O' Nuallain", "Razieh Rahimi"], "categories": ["cs.CL", "cs.IR"], "comment": "26 pages", "summary": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance.Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data.", "AI": {"tldr": "RaDeR is a new reasoning-based dense retrieval model that excels in mathematical problem solving using LLMs, outperforming existing methods with less training data.", "motivation": "The paper explores enhancing retrieval-augmented reasoning in large language models for mathematical problem solving, seeking to create diverse and hard-negative samples.", "method": "RaDeR employs reasoning trajectories and self-reflective relevance evaluation to train retrievers specifically for reasoning-intensive tasks, evaluated on benchmarks like BRIGHT and RAR-b.", "result": "RaDeR displays superior performance compared to strong baselines in reasoning tasks, particularly on Math and Coding splits, and significantly outperforms BM25 on Chain-of-Thought queries.", "conclusion": "With a fraction of the training data, RaDeR establishes a new standard for reasoning-based retrieval models in mathematical contexts, demonstrating high potential for diverse reasoning applications.", "key_contributions": ["Introduction of RaDeR as a reasoning-based dense retrieval model", "Demonstration of superior performance on Math and Coding benchmarks compared to existing methods", "First dense retriever to beat BM25 on Chain-of-Thought queries"], "limitations": "", "keywords": ["Reasoning-based retrieval", "Large language models", "Mathematical problem solving"], "importance_score": 8, "read_time_minutes": 26}}
{"id": "2505.18411", "pdf": "https://arxiv.org/pdf/2505.18411.pdf", "abs": "https://arxiv.org/abs/2505.18411", "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding", "authors": ["Yue Jiang", "Jichu Li", "Yang Liu", "Dingkang Yang", "Feng Zhou", "Quyu Kong"], "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "AI": {"tldr": "DanmakuTPPBench is a benchmark for improving multi-modal Temporal Point Process modeling utilizing Large Language Models.", "motivation": "Existing datasets for Temporal Point Processes are mostly unimodal, limiting the development of models that need to reason over temporal, textual, and visual information together.", "method": "The benchmark consists of two components: a dataset from Bilibili video comments that features annotated multi-modal events and a challenging question-answering dataset created using state-of-the-art multi-modal LLMs to assess complex reasoning.", "result": "Evaluation reveals significant performance gaps in current models' abilities to capture multi-modal event dynamics, with better performance in MLLMs compared to classical TPP models.", "conclusion": "The benchmark provides strong baselines for future research and advocates for improving the integration of TPP modeling in multi-modal language models.", "key_contributions": ["Introduction of a novel multi-modal dataset from user-generated content", "Development of a question-answering dataset utilizing advanced LLMs", "Establishment of benchmarks highlighting current model limitations."], "limitations": "", "keywords": ["Multi-modal learning", "Temporal Point Processes", "Large Language Models", "Question-Answering", "Benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.18426", "pdf": "https://arxiv.org/pdf/2505.18426.pdf", "abs": "https://arxiv.org/abs/2505.18426", "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps", "authors": ["Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mizanur Rahman", "Mashrur Chowdhury", "Bhavani Thuraisingham"], "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting\n  2025, and subsequently submitted for publication consideration in the\n  Transportation Research Record (TRR)", "summary": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies.", "AI": {"tldr": "A RAG-based LLM framework enhances legal content extraction and response generation for policymakers addressing cybersecurity and privacy in transportation.", "motivation": "To address emerging cybersecurity and data privacy challenges in connected and automated transportation systems through legal scrutiny.", "method": "A Retrieval-Augmented Generation (RAG) based LLM framework that uses domain-specific questions for improved output specificity and grounding.", "result": "The proposed framework outperformed leading commercial LLMs across four evaluation metrics, demonstrating its effectiveness in providing reliable legal insights.", "conclusion": "This framework offers a scalable AI-driven method for legislative analysis, supporting updates of legal frameworks in line with technological advancements.", "key_contributions": ["Introduction of a RAG-based LLM framework for legal insights", "Demonstrated performance superiority over commercial LLMs", "Addresses specific needs for policymakers in the context of transportation legislation"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Model", "Legal insights", "Transportation cybersecurity", "Data privacy"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.18436", "pdf": "https://arxiv.org/pdf/2505.18436.pdf", "abs": "https://arxiv.org/abs/2505.18436", "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier", "authors": ["AbdelRahim Elmadany", "Sang Yun Kwon", "Hawau Olamide Toyin", "Alcides Alcoba Inciarte", "Hanan Aldarmaki", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies.", "AI": {"tldr": "The paper addresses the underrepresentation of Africa's linguistic diversity in speech technologies, introducing a benchmark and models to improve performance in African languages.", "motivation": "To address the barriers to digital inclusion caused by the lack of speech technologies for Africa's diverse languages.", "method": "Systematic mapping of datasets and technologies in African speech, leading to the creation of SimbaBench and the Simba family of models.", "result": "Achieved state-of-the-art performance across multiple African languages and revealed critical patterns in resource availability through benchmark analysis.", "conclusion": "The work emphasizes the need for more resources in speech technology that reflect African linguistic diversity and lays the groundwork for future inclusive technology developments.", "key_contributions": ["Creation of the SimbaBench benchmark for African speech tasks", "Introduction of the Simba family of models for improved performance", "Analysis of critical patterns in dataset quality and language relationships affecting technology performance"], "limitations": "", "keywords": ["speech technologies", "African languages", "benchmarking", "digital inclusion", "machine learning"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18440", "pdf": "https://arxiv.org/pdf/2505.18440.pdf", "abs": "https://arxiv.org/abs/2505.18440", "title": "Efficient Long CoT Reasoning in Small Language Models", "authors": ["Zhaoyang Wang", "Jinqi Jiang", "Tian Qiu", "Hui Liu", "Xianfeng Tang", "Huaxiu Yao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.", "AI": {"tldr": "The paper proposes a method to distill long chain-of-thought reasoning into small language models by pruning redundant steps, allowing SLMs to learn efficient reasoning without overthinking.", "motivation": "The rise of large reasoning models highlights the challenge of training small language models to generate effective long chain-of-thought (CoT) reasoning due to their limited capacity and social tendencies to produce redundant content.", "method": "The proposed method involves pruning unnecessary steps from long CoT sequences and using an on-policy approach for the small language model to curate training data, enhancing learning efficiency.", "result": "Experimental validation shows that the proposed pruning and training data curation method enables small language models to achieve competitive performance on mathematical reasoning benchmarks while significantly reducing redundant reasoning steps.", "conclusion": "The proposed approach effectively distills complex reasoning abilities into small language models, promoting efficient learning and maintaining performance standards without excessive redundancy.", "key_contributions": ["Development of a pruning method for CoT reasoning", "Implementation of an on-policy method for training data curation", "Demonstration of competitive performance on mathematical reasoning tasks"], "limitations": "", "keywords": ["small language models", "chain-of-thought reasoning", "distillation", "mathematical reasoning", "redundancy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18450", "pdf": "https://arxiv.org/pdf/2505.18450.pdf", "abs": "https://arxiv.org/abs/2505.18450", "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph", "authors": ["Ainulla Khan", "Yamada Moyuru", "Srinidhi Akella"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising technique to\nenhance the quality and relevance of responses generated by large language\nmodels. While recent advancements have mainly focused on improving RAG for\ntext-based queries, RAG on multi-modal documents containing both texts and\nimages has not been fully explored. Especially when fine-tuning does not work.\nThis paper proposes BRIT, a novel multi-modal RAG framework that effectively\nunifies various text-image connections in the document into a multi-modal graph\nand retrieves the texts and images as a query-specific sub-graph. By traversing\nboth image-to-text and text-to-image paths in the graph, BRIT retrieve not only\ndirectly query-relevant images and texts but also further relevant contents to\nanswering complex cross-modal multi-hop questions. To evaluate the\neffectiveness of BRIT, we introduce MM-RAG test set specifically designed for\nmulti-modal question answering tasks that require to understand the text-image\nrelations. Our comprehensive experiments demonstrate the superiority of BRIT,\nhighlighting its ability to handle cross-modal questions on the multi-modal\ndocuments.", "AI": {"tldr": "BRIT is a new multi-modal Retrieval-Augmented Generation (RAG) framework that improves the handling of cross-modal questions in documents containing text and images.", "motivation": "To enhance the capability of RAG frameworks in processing multi-modal documents that combine texts and images, particularly for complex queries.", "method": "BRIT constructs a multi-modal graph that unifies text-image connections and retrieves relevant sub-graphs based on query-specific needs, facilitating cross-modal question answering.", "result": "BRIT outperforms existing methods on the newly introduced MM-RAG test set designed for multi-modal question answering, showing better handling of cross-modal relations.", "conclusion": "The results indicate that BRIT effectively addresses the challenges posed by multi-modal documents in RAG formulations.", "key_contributions": ["Introduction of a novel multi-modal RAG framework called BRIT.", "Development of the MM-RAG test set for evaluating multi-modal question answering.", "Demonstration of superior performance of BRIT in handling complex cross-modal queries."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "multi-modal", "question answering", "text-image relations", "cross-modal"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18452", "pdf": "https://arxiv.org/pdf/2505.18452.pdf", "abs": "https://arxiv.org/abs/2505.18452", "title": "MedScore: Factuality Evaluation of Free-Form Medical Answers", "authors": ["Heyuan Huang", "Alexandra DeLucia", "Vijay Murari Tiyyala", "Mark Dredze"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent and convincing\nresponses, they are not necessarily correct. This is especially apparent in the\npopular decompose-then-verify factuality evaluation pipeline, where LLMs\nevaluate generations by decomposing the generations into individual, valid\nclaims. Factuality evaluation is especially important for medical answers,\nsince incorrect medical information could seriously harm the patient. However,\nexisting factuality systems are a poor match for the medical domain, as they\nare typically only evaluated on objective, entity-centric, formulaic texts such\nas biographies and historical topics. This differs from condition-dependent,\nconversational, hypothetical, sentence-structure diverse, and subjective\nmedical answers, which makes decomposition into valid facts challenging. We\npropose MedScore, a new approach to decomposing medical answers into\ncondition-aware valid facts. Our method extracts up to three times more valid\nfacts than existing methods, reducing hallucination and vague references, and\nretaining condition-dependency in facts. The resulting factuality score\nsignificantly varies by decomposition method, verification corpus, and used\nbackbone LLM, highlighting the importance of customizing each step for reliable\nfactuality evaluation.", "AI": {"tldr": "MedScore improves factuality evaluation in medical answers by extracting more valid facts.", "motivation": "Existing factuality systems struggle with the complexity of medical answers, leading to potential misinformation that can harm patients.", "method": "MedScore decomposes medical answers into condition-aware valid facts, enhancing the extraction of factual information.", "result": "MedScore extracts up to three times more valid facts than traditional methods, reducing hallucination and retaining condition dependency.", "conclusion": "Customizing methods for factuality evaluation is crucial for reliability in medical contexts.", "key_contributions": ["Introduction of MedScore for improved factuality evaluation in medical contexts", "Increased extraction of valid facts compared to existing systems", "Emphasis on condition-awareness for accurate fact decomposition"], "limitations": "", "keywords": ["Factuality evaluation", "Large Language Models", "MedScore", "Health informatics", "Data extraction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18454", "pdf": "https://arxiv.org/pdf/2505.18454.pdf", "abs": "https://arxiv.org/abs/2505.18454", "title": "Hybrid Latent Reasoning via Reinforcement Learning", "authors": ["Zhenrui Yue", "Bowen Jin", "Huimin Zeng", "Honglei Zhuang", "Zhen Qin", "Jinsung Yoon", "Lanyu Shang", "Jiawei Han", "Dong Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning.", "AI": {"tldr": "This paper introduces hybrid reasoning policy optimization (HRPO), a reinforcement learning-based approach to latent reasoning in large language models, enhancing their generative capabilities while improving performance on knowledge- and reasoning-intensive tasks.", "motivation": "To address the incompatibility of latent reasoning methods with LLMs and to leverage their inherent reasoning patterns without requiring chain-of-thought traces for training.", "method": "Hybrid reasoning policy optimization (HRPO) integrates prior hidden states into sampled tokens using a learnable gating mechanism and initializes training with token embeddings, progressively incorporating hidden features, and introducing stochasticity through token sampling.", "result": "HRPO outperforms previous methods on diverse benchmarks in knowledge- and reasoning-intensive tasks, maintaining interpretability and showcasing interesting behaviors like cross-lingual patterns.", "conclusion": "The RL-based HRPO approach not only enhances LLM capabilities but also provides insights for future research into latent reasoning in AI.", "key_contributions": ["Introduction of hybrid reasoning policy optimization (HRPO) for latent reasoning in LLMs.", "Integration of discrete and continuous representations in reasoning tasks.", "Performance improvement over existing methods in knowledge- and reasoning-intensive benchmarks."], "limitations": "", "keywords": ["latent reasoning", "large language models", "reinforcement learning", "hybrid reasoning", "interpretable AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18456", "pdf": "https://arxiv.org/pdf/2505.18456.pdf", "abs": "https://arxiv.org/abs/2505.18456", "title": "Anchored Diffusion Language Model", "authors": ["Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches", "AI": {"tldr": "The paper presents the Anchored Diffusion Language Model (ADLM) which improves the performance of diffusion language models by accurately addressing key tokens' masking in the generation process.", "motivation": "Diffusion Language Models (DLMs) have potential advantages over autoregressive models but currently underperform in generating high-quality text due to limitations in modeling important tokens during the generation process.", "method": "The ADLM framework is a two-stage process that first identifies important tokens using an anchor network, followed by predicting the likelihood of other tokens while conditioning on these important tokens.", "result": "ADLM shows a significant reduction in test perplexity on datasets like LM1B and OpenWebText, achieving up to 25.4% better performance than previous DLMs, and outperforms autoregressive models in generating human-like text.", "conclusion": "The introduction of anchoring improves the performance of DLMs and AR models, enhancing accuracy in reasoning tasks and establishing a new benchmark in text generation.", "key_contributions": ["Introduction of the Anchored Diffusion Language Model (ADLM) framework", "Achievement of state-of-the-art performance in text generation compared to autoregressive models", "Derivation of the Anchored Negative Evidence Lower Bound (ANELBO) objective to improve sample complexity"], "limitations": "", "keywords": ["Diffusion Language Models", "Anchored Diffusion", "Natural Language Processing", "Text Generation", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18466", "pdf": "https://arxiv.org/pdf/2505.18466.pdf", "abs": "https://arxiv.org/abs/2505.18466", "title": "Measuring South Asian Biases in Large Language Models", "authors": ["Mamnuya Rinki", "Chahat Raj", "Anjishnu Mukherjee", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluations of Large Language Models (LLMs) often overlook intersectional and\nculturally specific biases, particularly in underrepresented multilingual\nregions like South Asia. This work addresses these gaps by conducting a\nmultilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan\nand Dravidian languages, identifying how cultural stigmas influenced by purdah\nand patriarchy are reinforced in generative tasks. We construct a culturally\ngrounded bias lexicon capturing previously unexplored intersectional dimensions\nincluding gender, religion, marital status, and number of children. We use our\nlexicon to quantify intersectional bias and the effectiveness of self-debiasing\nin open-ended generations (e.g., storytelling, hobbies, and to-do lists), where\nbias manifests subtly and remains largely unexamined in multilingual contexts.\nFinally, we evaluate two self-debiasing strategies (simple and complex prompts)\nto measure their effectiveness in reducing culturally specific bias in\nIndo-Aryan and Dravidian languages. Our approach offers a nuanced lens into\ncultural bias by introducing a novel bias lexicon and evaluation framework that\nextends beyond Eurocentric or small-scale multilingual settings.", "AI": {"tldr": "This paper addresses intersectional and culturally specific biases in LLMs, particularly in South Asian languages, by creating a bias lexicon and evaluating self-debiasing strategies.", "motivation": "To uncover and address the overlooked biases in LLM outputs, particularly in underrepresented multilingual regions like South Asia.", "method": "Conducted a multilingual intersectional analysis of LLM outputs across 10 languages, developed a bias lexicon, and assessed self-debiasing strategies.", "result": "Identified cultural biases linked to purdah and patriarchy in generative tasks, and evaluated the effectiveness of simple and complex self-debiasing prompts.", "conclusion": "The introduced lexicon and evaluation framework provide deeper insights into cultural bias in LLMs beyond traditional Eurocentric contexts.", "key_contributions": ["Development of a culturally grounded bias lexicon for intersectional analysis", "Quantitative assessment of intersectional bias in multilingual contexts", "Evaluation of self-debiasing strategies for reducing cultural bias in generative tasks"], "limitations": "The focus is limited to specific cultural contexts and languages, potentially limiting generalizability.", "keywords": ["Large Language Models", "Cultural Bias", "Self-Debiasing", "Intersectionality", "Indo-Aryan Languages"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.07960", "pdf": "https://arxiv.org/pdf/2405.07960.pdf", "abs": "https://arxiv.org/abs/2405.07960", "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments", "authors": ["Samuel Schmidgall", "Rojin Ziaei", "Carl Harris", "Eduardo Reis", "Jeffrey Jopling", "Michael Moor"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Evaluating large language models (LLM) in clinical scenarios is crucial to\nassessing their potential clinical utility. Existing benchmarks rely heavily on\nstatic question-answering, which does not accurately depict the complex,\nsequential nature of clinical decision-making. Here, we introduce AgentClinic,\na multimodal agent benchmark for evaluating LLMs in simulated clinical\nenvironments that include patient interactions, multimodal data collection\nunder incomplete information, and the usage of various tools, resulting in an\nin-depth evaluation across nine medical specialties and seven languages. We\nfind that solving MedQA problems in the sequential decision-making format of\nAgentClinic is considerably more challenging, resulting in diagnostic\naccuracies that can drop to below a tenth of the original accuracy. Overall, we\nobserve that agents sourced from Claude-3.5 outperform other LLM backbones in\nmost settings. Nevertheless, we see stark differences in the LLMs' ability to\nmake use of tools, such as experiential learning, adaptive retrieval, and\nreflection cycles. Strikingly, Llama-3 shows up to 92% relative improvements\nwith the notebook tool that allows for writing and editing notes that persist\nacross cases. To further scrutinize our clinical simulations, we leverage\nreal-world electronic health records, perform a clinical reader study, perturb\nagents with biases, and explore novel patient-centric metrics that this\ninteractive environment firstly enables.", "AI": {"tldr": "AgentClinic is a benchmark for evaluating LLMs in clinical settings that simulate complex decision-making and patient interactions.", "motivation": "To improve evaluation of large language models by capturing the complexity of clinical decision-making beyond static question-answering frameworks.", "method": "We developed AgentClinic, a multimodal benchmark that evaluates LLMs in simulated clinical environments, including patient interactions, incomplete information situations, and the use of various clinical tools.", "result": "AgentClinic revealed significant challenges in diagnostic accuracy for LLMs in sequential decision-making contexts, with some accuracies dropping below 10%. Claude-3.5 generally outperformed other models, while Llama-3 showed substantial improvements (up to 92%) when using a notebook tool for case notes.", "conclusion": "The benchmark enables a deeper understanding of LLM capabilities in healthcare settings, highlighting the need for improved tools and workflows for clinical AI applications.", "key_contributions": ["Introduction of the AgentClinic benchmark for evaluating LLMs in clinical scenarios.", "Demonstration of the challenges LLMs face in sequential clinical decision-making.", "Insights into the performance of various LLMs with different tools in simulated environments."], "limitations": "Challenges include potential biases in agent performance and the limitations of simulation accuracy compared to real clinical environments.", "keywords": ["Large Language Models", "Clinical Decision-Making", "AI in Healthcare", "Simulation Benchmark", "Multimodal Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18486", "pdf": "https://arxiv.org/pdf/2505.18486.pdf", "abs": "https://arxiv.org/abs/2505.18486", "title": "Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek", "authors": ["Hong Jiao", "Dan Song", "Won-Chan Lee"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects.", "AI": {"tldr": "This study evaluates the scoring accuracy of ten large language models (LLMs) against human raters in low-stakes writing assessments.", "motivation": "To collect empirical evidence on the reliability of LLMs for automated scoring in education.", "method": "The study compared ten LLMs with human expert raters in scoring two types of writing tasks, using metrics like Quadratic Weighted Kappa and Cronbach Alpha for evaluation.", "result": "Results indicated that ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet showed high scoring accuracy, reliable rater consistency, and reduced rater effects.", "conclusion": "The findings support the use of specific LLMs for automated scoring in educational settings.", "key_contributions": ["Comparison of multiple LLMs for scoring accuracy", "Identification of LLMs with high reliability and low rater effects", "Implications for the use of LLMs in educational assessment"], "limitations": "", "keywords": ["automated scoring", "large language models", "educational assessment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.14341", "pdf": "https://arxiv.org/pdf/2405.14341.pdf", "abs": "https://arxiv.org/abs/2405.14341", "title": "How do Observable Users Decompose D3 Code? A Qualitative Study", "authors": ["Melissa Lin", "Heer Patel", "Medina Lamkin", "Hannah Bako", "Leilani Battle"], "categories": ["cs.HC"], "comment": null, "summary": "Many toolkit developers seek to streamline the visualization programming\nprocess for their users through structured support such as prescribed templates\nand example galleries. However, few projects examine how users organize their\nown visualization programs, how their coding choices may deviate from the\nintents of toolkit developers, and how these differences may impact\nvisualization prototyping and design. Further, is it possible to infer users'\nreasoning indirectly through their code, even when users copy code from other\nsources? Understanding these patterns can reveal opportunities to align toolkit\ndesign with actual user behavior, improving usability and supporting more\nflexible workflows. We explore this question through a qualitative analysis of\n715 D3 programs on Observable. We identify three levels of program organization\nbased on how users decompose their code into smaller blocks: Program-, Chart-,\nand Component-Level code decomposition, with a strong preference for\nComponent-Level reasoning. In a series of interviews, we corroborate that these\nlevels reflect how Observable users reason about visualization programs. We\ncompare common user-made components with those theorized in the Grammar of\nGraphics to assess overlap in user and toolkit developer reasoning. We find\nthat, while the Grammar of Graphics covers basic visualizations well, it falls\nshort in describing complex visualization types, especially those with\nanimation, interaction, and parameterization components. Our findings highlight\nhow user practices differ from formal grammars and suggest opportunities for\nrethinking visualization toolkit support, including augmenting learning tools\nand AI assistants to better reflect real-world coding strategies.", "AI": {"tldr": "This paper analyzes how users organize their visualization programs in D3 and identifies three levels of code decomposition that diverge from toolkit developers' intents, suggesting improvements for visualization toolkits.", "motivation": "To understand how users organize their visualization code and identify gaps between user practices and toolkit design, ultimately to improve usability and workflows.", "method": "A qualitative analysis of 715 D3 programs on Observable, complemented by user interviews to correlate program organization with user reasoning.", "result": "Three levels of program organization were identified: Program-Level, Chart-Level, and Component-Level, with a preference for Component-Level organization. User reasoning often diverges from the Grammar of Graphics, especially for complex visualizations.", "conclusion": "The findings indicate a need for visualization toolkits to better align with user practices, considering enhancements in learning tools and AI support.", "key_contributions": ["Identification of three levels of code decomposition in visualization programs", "Comparison of user-made components with the Grammar of Graphics", "Suggestions for improving visualization toolkit usability through alignment with user behavior"], "limitations": "", "keywords": ["visualization", "toolkits", "D3", "grammar of graphics", "user behavior"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18497", "pdf": "https://arxiv.org/pdf/2505.18497.pdf", "abs": "https://arxiv.org/abs/2505.18497", "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms.", "AI": {"tldr": "This paper introduces ALTPRAG, a dataset to evaluate the pragmatic competence of large language models (LLMs) during training, revealing that LLMs improve in understanding speaker intentions as they are fine-tuned.", "motivation": "Understanding how LLMs develop pragmatic competence is crucial for better aligning AI with human communicative norms.", "method": "The study utilizes the ALTPRAG dataset, which compares model outputs in contextually appropriate yet pragmatically distinct scenarios across different training stages of 22 LLMs.", "result": "LLMs, even at base levels, show sensitivity to pragmatic cues, with performance improving through fine-tuning and preference optimization.", "conclusion": "The development of pragmatic competence in LLMs is emergent and compositional, providing insights for enhancing model alignment with human communication.", "key_contributions": ["Introduction of the ALTPRAG dataset for evaluating pragmatic reasoning in LLMs", "Findings indicate significant improvements in pragmatic understanding with model training stages", "Insights into aligning LLMs with human communicative norms"], "limitations": "The study primarily focuses on LLMs; insights may not apply equally to other AI systems.", "keywords": ["Pragmatics", "Large Language Models", "Model Training", "Human-Computer Interaction", "Cognitive Reasoning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2501.10551", "pdf": "https://arxiv.org/pdf/2501.10551.pdf", "abs": "https://arxiv.org/abs/2501.10551", "title": "An Empirical Study to Understand How Students Use ChatGPT for Writing Essays", "authors": ["Andrew Jelson", "Daniel Manesh", "Alice Jang", "Daniel Dunlap", "Sang Won Lee"], "categories": ["cs.HC"], "comment": "19 pages, 10 figures, 2 tables, final preparation for a TOCHI\n  submission", "summary": "As large language models (LLMs) advance and become widespread, students\nincreasingly turn to systems like ChatGPT for assistance with writing tasks.\nEducators are concerned with students' usage of ChatGPT beyond cheating; using\nChatGPT may reduce their critical engagement with writing, hindering students'\nlearning processes. The negative or positive impact of using LLM-powered tools\nfor writing will depend on how students use them; however, how students use\nChatGPT remains largely unknown, resulting in a limited understanding of its\nimpact on learning. To better understand how students use these tools, we\nconducted an online study $(n=70)$ where students were given an essay-writing\ntask using a custom platform we developed to capture the queries they made to\nChatGPT. To characterize their ChatGPT usage, we categorized each of the\nqueries students made to ChatGPT. We then analyzed the relationship between\nChatGPT usage and a variety of other metrics, including students'\nself-perception, attitudes towards AI, and the resulting essay itself. We found\nthat factors such as gender, race, and perceived self-efficacy can help predict\ndifferent AI usage patterns. Additionally, we found that different usage\npatterns were associated with varying levels of enjoyment and perceived\nownership over the essay. The results of this study contribute to discussions\nabout how writing education should incorporate generative AI-powered tools in\nthe classroom.", "AI": {"tldr": "The study investigates how students use ChatGPT for writing tasks, revealing diverse usage patterns influenced by demographics and attitudes towards AI, impacting their engagement and ownership of the writing process.", "motivation": "To understand the implications of student engagement with ChatGPT on learning processes, given the concerns about its impact on critical thinking and writing skills.", "method": "An online study with 70 students performing an essay-writing task using a custom platform that tracked their queries to ChatGPT, followed by analysis of usage patterns and correlations with various metrics.", "result": "The research identified demographic factors influencing ChatGPT usage patterns, showing that these patterns affected students' enjoyment and their sense of ownership over their essays.", "conclusion": "The findings suggest that writing education must adapt to incorporate generative AI tools and consider how student demographics influence their engagement with such technologies.", "key_contributions": ["Identification of demographic influences on AI usage patterns among students", "Analysis of the relationship between ChatGPT usage and students' self-perception", "Recommendations for integrating AI tools in writing education"], "limitations": "The study was limited to a sample size of 70 students and was conducted in an online environment, which may not represent all students' experiences or contexts.", "keywords": ["large language models", "ChatGPT", "writing education", "student engagement", "AI usage patterns"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18522", "pdf": "https://arxiv.org/pdf/2505.18522.pdf", "abs": "https://arxiv.org/abs/2505.18522", "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation", "authors": ["Xin Lu", "Yanyan Zhao", "Si Wei", "Shijin Wang", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs.", "AI": {"tldr": "This paper investigates how sequence modeling architectures affect the base capabilities of pre-trained language models and proposes a new design principle to enhance these capabilities.", "motivation": "To understand the impact of sequence modeling architectures on the base capabilities of pre-trained language models, addressing gaps in existing approaches that do not showcase architectural differences effectively.", "method": "The authors propose a limited domain pre-training setting with out-of-distribution testing to uncover differences in base capabilities among various architectures. They perform architecture component analysis to identify key design principles and validate their findings through empirical testing.", "result": "Significant differences in base capabilities were discovered among stateful and Transformer architectures, with the latter exhibiting superior performance. The proposed principle of full-sequence arbitrary selection capability mitigates capability degradation.", "conclusion": "The findings indicate that adopting the proposed design principle can enhance the base capabilities of language models, which can inform future architecture designs.", "key_contributions": ["Introduction of a limited domain pre-training setting for better evaluation of architectural differences.", "Identification of the key design principle: full-sequence arbitrary selection capability is essential to maintain base capabilities.", "Empirical validation of the proposed architectures, demonstrating significant improvements in performance."], "limitations": "The study is limited to the architectures tested and may not generalize beyond the scope of the proposed settings.", "keywords": ["Language Models", "Sequence Modeling", "Transformer Architecture", "Pre-training", "Machine Learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2502.19082", "pdf": "https://arxiv.org/pdf/2502.19082.pdf", "abs": "https://arxiv.org/abs/2502.19082", "title": "Trust-Enabled Privacy: Social Media Designs to Support Adolescent User Boundary Regulation", "authors": ["JaeWon Kim", "Robert Wolfe", "Ramya Bhagirathi Subramanian", "Mei-Hsuan Lee", "Jessica Colnago", "Alexis Hiniker"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescents heavily rely on social media to build and maintain close\nrelationships, yet current platform designs often make self-disclosure feel\nrisky or uncomfortable. Through a three-part study involving 19 teens aged\n13-18, we identify key barriers to meaningful self-disclosure on social media.\nOur findings reveal that while these adolescents seek casual, frequent sharing\nto strengthen relationships, existing platform norms often discourage such\ninteractions. Based on our co-design interview findings, we propose platform\ndesign ideas to foster a more dynamic and nuanced privacy experience for teen\nsocial media users. We then introduce \\textbf{\\textit{trust-enabled privacy}}\nas a framework that recognizes trust -- whether building or eroding -- as\ncentral to boundary regulation, and foregrounds the role of platform design in\nshaping the very norms and interaction patterns that influence how trust\nunfolds. When trust is supported, boundary regulation becomes more adaptive and\nempowering; when it erodes, users resort to self-censorship or disengagement.\nThis work provides empirical insights and actionable guidelines for designing\nsocial media spaces where teens feel empowered to engage in meaningful\nrelationship-building processes.", "AI": {"tldr": "The paper explores how social media platform designs affect self-disclosure in adolescents and proposes a framework for trust-enabled privacy to improve relationship-building on these platforms.", "motivation": "To identify barriers to meaningful self-disclosure among adolescents on social media and improve platform designs to foster better communication and relationship-building.", "method": "A three-part study involving co-design interviews with 19 teens aged 13-18 to gather insights on their experiences and needs regarding social media and self-disclosure.", "result": "Findings indicate that existing platform norms discourage casual sharing, leading to self-censorship or disengagement. The proposed trust-enabled privacy framework helps to enhance boundary regulation and support trust-building.", "conclusion": "Adaptive boundary regulation can empower users when trust is upheld, while eroding trust leads to negative outcomes like disengagement. The study offers actionable design guidelines to enhance teen engagement on social media.", "key_contributions": ["Identification of barriers to self-disclosure for teens on social media", "Introduction of the trust-enabled privacy framework", "Actionable design ideas for improving teen social media experiences"], "limitations": "", "keywords": ["self-disclosure", "social media", "trust", "privacy", "adolescents"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.18524", "pdf": "https://arxiv.org/pdf/2505.18524.pdf", "abs": "https://arxiv.org/abs/2505.18524", "title": "metaTextGrad: Automatically optimizing language model optimizers", "authors": ["Guowei Xu", "Mert Yuksekgonul", "Carlos Guestrin", "James Zou"], "categories": ["cs.CL"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline.", "AI": {"tldr": "The paper presents metaTextGrad, a meta-optimizer designed to enhance existing LLM-based optimizers for specific tasks, improving their performance by up to 6%.", "motivation": "To address the limitations of LLM-based optimizers that are generally designed and not optimized for specific tasks, enhancing their performance in AI systems.", "method": "The proposed method includes a meta prompt optimizer and a meta structure optimizer that together improve the performance of existing optimizers across benchmarks.", "result": "The metaTextGrad significantly improves the performance of AI systems, achieving an average absolute performance improvement of up to 6% over the best baseline.", "conclusion": "The introduction of metaTextGrad offers a tailored optimization approach for LLM-based optimizers, aligning them more effectively with specific tasks and resulting in better AI system performance.", "key_contributions": ["Development of a novel meta-optimizer for language model optimizers", "Introduction of a meta prompt optimizer", "Introduction of a meta structure optimizer"], "limitations": "", "keywords": ["LLMs", "optimizers", "meta-optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536.pdf", "abs": "https://arxiv.org/abs/2505.18536", "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.", "AI": {"tldr": "This position paper discusses the role of reinforcement fine-tuning (RFT) in enhancing the reasoning capabilities of multimodal large language models (MLLMs) and outlines future research directions.", "motivation": "The paper addresses the potential of reinforcement fine-tuning (RFT) in advancing the reasoning capabilities of multimodal large language models (MLLMs) in the context of Artificial General Intelligence (AGI).", "method": "The authors provide a comprehensive introduction to the necessary background knowledge, summarize the improvements brought by RFT to MLLMs into five key points, and propose five promising directions for future research.", "result": "RFT has significantly enhanced reasoning capabilities in MLLMs through diverse modalities, tasks, improved training algorithms, benchmarks, and engineering frameworks.", "conclusion": "The paper aims to provide insights for future research and contribute to the community's understanding of the role of RFT in achieving AGI.", "key_contributions": ["Detailed introduction to RFT in MLLMs", "Summary of RFT improvements in five key aspects", "Proposed future research directions for MLLMs"], "limitations": "", "keywords": ["Reinforcement Fine-Tuning", "Multimodal Language Models", "Artificial General Intelligence", "Reasoning Capability", "Future Research Directions"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2504.04833", "pdf": "https://arxiv.org/pdf/2504.04833.pdf", "abs": "https://arxiv.org/abs/2504.04833", "title": "Explanation-Driven Interventions for Artificial Intelligence Model Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology", "authors": ["Andrea Esposito", "Miriana Calvano", "Antonio Curci", "Francesco Greco", "Rosa Lanzilotti", "Antonio Piccinno"], "categories": ["cs.HC", "cs.AI"], "comment": "Second version (11 pages, 8 of content) [edited to remove unsupported\n  packages]", "summary": "The integration of Artificial Intelligence (AI) in modern society is\ntransforming how individuals perform tasks. In high-risk domains, ensuring\nhuman control over AI systems remains a key design challenge. This article\npresents a novel End-User Development (EUD) approach for black-box AI models,\nenabling users to edit explanations and influence future predictions through\ntargeted interventions. By combining explainability, user control, and model\nadaptability, the proposed method advances Human-Centered AI (HCAI), promoting\na symbiotic relationship between humans and adaptive, user-tailored AI systems.", "AI": {"tldr": "This paper introduces an End-User Development approach for enhancing user control and explainability in black-box AI systems, targeting high-risk domains.", "motivation": "The need for human control over AI systems in high-risk domains drives the development of effective user interaction methods.", "method": "The proposed approach enables users to edit AI explanations and intervene in predictions, thereby fostering a more interactive and controllable user experience with AI.", "result": "The methodology combines user control with explainability, leading to improved adaptability of AI models to user needs and preferences.", "conclusion": "This work enhances Human-Centered AI by promoting a collaborative relationship between users and AI, facilitating tailored user experiences in AI applications.", "key_contributions": ["Introduction of an End-User Development approach for black-box AI models.", "Enhancement of user control over AI predictions through targeted interventions.", "Advancement of Human-Centered AI by integrating explainability and adaptability."], "limitations": "The approach primarily focuses on black-box AI systems, which may limit applicability in fully explainable models.", "keywords": ["Human-Centered AI", "End-User Development", "black-box AI", "explainability", "user control"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18542", "pdf": "https://arxiv.org/pdf/2505.18542.pdf", "abs": "https://arxiv.org/abs/2505.18542", "title": "Business as \\textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs", "authors": ["Chen Yang", "Ruping Xu", "Ruizhe Li", "Bin Cao", "Jing Fan"], "categories": ["cs.CL"], "comment": null, "summary": "Process mining aims to discover, monitor and optimize the actual behaviors of\nreal processes. While prior work has mainly focused on extracting procedural\naction flows from instructional texts, rule flows embedded in business\ndocuments remain underexplored. To this end, we introduce a novel annotated\nChinese dataset, \\textbf{BPRF}, which contains 50 business process documents\nwith 326 explicitly labeled business rules across multiple domains. Each rule\nis represented as a <Condition, Action> pair, and we annotate logical\ndependencies between rules (sequential, conditional, or parallel). We also\npropose \\textbf{ExIde}, a framework for automatic business rule extraction and\ndependency relationship identification using large language models (LLMs). We\nevaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,\nbenchmarking performance on both rule extraction and dependency classification\ntasks of current LLMs. Our results demonstrate the effectiveness of ExIde in\nextracting structured business rules and analyzing their interdependencies for\ncurrent SOTA LLMs, paving the way for more automated and interpretable business\nprocess automation.", "AI": {"tldr": "Research introduces a novel framework for extracting business rules from documents using LLMs, leveraging a new annotated dataset.", "motivation": "To explore the under-researched area of rule flows in business documents using process mining techniques.", "method": "Introduced a new dataset (BPRF) with annotated business rules and developed ExIde for automatic extraction and dependency identification using LLMs.", "result": "Evaluation of ExIde on 12 SOTA LLMs shows effective structured rule extraction and analysis of interdependencies, indicating potential for improved business process automation.", "conclusion": "The study demonstrates that LLMs can effectively extract and analyze business rules from documents, enhancing process automation.", "key_contributions": ["Introduction of the BPRF dataset with extensively annotated business rules", "Development of the ExIde framework for rule extraction and dependency identification", "Performance evaluation of ExIde on multiple state-of-the-art LLMs"], "limitations": "", "keywords": ["business process mining", "business rules extraction", "large language models", "dependency analysis", "dataset"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.12830", "pdf": "https://arxiv.org/pdf/2504.12830.pdf", "abs": "https://arxiv.org/abs/2504.12830", "title": "A Taxonomy of Questions for Critical Reflection in Machine-Assisted Decision-Making", "authors": ["Simon W. S. Fischer", "Hanna Schraffenberger", "Serge Thill", "Pim Haselager"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Decision-makers run the risk of relying too much on machine recommendations,\nwhich is associated with lower cognitive engagement. Reflection has been shown\nto increase cognitive engagement and improve critical thinking and reasoning\nand therefore decision-making. However, there is currently no approach to\nsupport reflection in machine-assisted decision-making. We therefore present a\ntaxonomy that serves to systematically create questions related to\nmachine-assisted decision-making that promote reflection and thus cognitive\nengagement and ultimately a deliberate decision-making process. Our taxonomy\nbuilds on a taxonomy of Socratic questions and a question bank for\nhuman-centred explainable AI (XAI), and illustrates how XAI techniques can be\nutilised and repurposed to formulate questions. As a use case, we focus on\nclinical decision-making. An evaluation in education confirms the applicability\nand expected benefits of our taxonomy. Our work contributes to the growing\nresearch on human-AI interaction that goes beyond the paradigm of machine\nrecommendations and explanations and aims to enable effective human oversight\nas required by the European AI Act.", "AI": {"tldr": "The paper presents a taxonomy designed to create questions that facilitate reflection in machine-assisted decision-making, particularly in clinical contexts, enhancing cognitive engagement and decision quality.", "motivation": "To address the over-reliance on machine recommendations in decision-making, which can diminish cognitive engagement and critical thinking.", "method": "The authors develop a taxonomy based on Socratic questioning and human-centered explainable AI (XAI) to formulate reflective questions for decision-making processes.", "result": "The use case evaluation in education demonstrates the taxonomy's effectiveness in promoting reflection and improving decision-making engagement.", "conclusion": "The research contributes to the field of human-AI interaction by providing tools to enhance human oversight in decision-making, aligning with the regulations of the European AI Act.", "key_contributions": ["Development of a taxonomy for reflective questioning in machine-assisted decision-making.", "Integration of Socratic questioning and explainable AI techniques.", "Empirical evaluation showing benefits in educational settings."], "limitations": "", "keywords": ["human-AI interaction", "reflective questioning", "explainable AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18548", "pdf": "https://arxiv.org/pdf/2505.18548.pdf", "abs": "https://arxiv.org/abs/2505.18548", "title": "Composable Cross-prompt Essay Scoring by Merging Models", "authors": ["Sanwoo Lee", "Kun Liang", "Yunfang Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in cross-prompt automated essay scoring (AES) typically train\nmodels jointly on all source prompts, often requiring additional access to\nunlabeled target prompt essays simultaneously. However, using all sources is\nsuboptimal in our pilot study, and re-accessing source datasets during\nadaptation raises privacy concerns. We propose a source-free adaptation\napproach that selectively merges individually trained source models' parameters\ninstead of datasets. In particular, we simulate joint training through linear\ncombinations of task vectors -- the parameter updates from fine-tuning. To\noptimize the combination's coefficients, we propose Prior-encoded Information\nMaximization (PIM), an unsupervised objective which promotes the model's score\ndiscriminability regularized by priors pre-computed from the sources. We employ\nBayesian optimization as an efficient optimizer of PIM. Experimental results\nwith LLMs on in-dataset and cross-dataset adaptation show that our method (1)\nconsistently outperforms training jointly on all sources, (2) maintains\nsuperior robustness compared to other merging methods, (3) excels under severe\ndistribution shifts where recent leading cross-prompt methods struggle, all\nwhile retaining computational efficiency.", "AI": {"tldr": "This paper presents a novel source-free adaptation approach for automated essay scoring that merges model parameters from individually trained source models instead of datasets, addressing privacy concerns and optimizing performance through a Bayesian optimization of an unsupervised objective.", "motivation": "Recent advancements in automated essay scoring have raised issues regarding the use of multiple source datasets, particularly in terms of privacy and effectiveness of joint training.", "method": "The paper proposes a source-free adaptation approach that combines separately trained model parameters using linear combinations of task vectors. It introduces Prior-encoded Information Maximization (PIM) as an unsupervised objective optimized via Bayesian optimization.", "result": "Experimental results demonstrate that the proposed method consistently outperforms traditional joint training methods, maintains robustness compared to other merging approaches, and excels under severe distribution shifts while being computationally efficient.", "conclusion": "The findings suggest that the source-free adaptation method is viable and effective, overcoming limitations of previous models that rely on joint training with all available datasets.", "key_contributions": ["Source-free adaptation approach for automated essay scoring", "Introduction of Prior-encoded Information Maximization (PIM) for optimization", "Demonstration of robustness and efficiency under distribution shifts"], "limitations": "", "keywords": ["automated essay scoring", "source-free adaptation", "Bayesian optimization", "machine learning", "privacy concerns"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18549", "pdf": "https://arxiv.org/pdf/2505.18549.pdf", "abs": "https://arxiv.org/abs/2505.18549", "title": "MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors", "authors": ["Baraa Hikal", "Mohamed Basem", "Islam Oshallah", "Ali Hamdi"], "categories": ["cs.CL"], "comment": null, "summary": "We present MSA-MathEval, our submission to the BEA 2025 Shared Task on\nevaluating AI tutor responses across four instructional dimensions: Mistake\nIdentification, Mistake Location, Providing Guidance, and Actionability. Our\napproach uses a unified training pipeline to fine-tune a single\ninstruction-tuned language model across all tracks, without any task-specific\narchitectural changes. To improve prediction reliability, we introduce a\ndisagreement-aware ensemble inference strategy that enhances coverage of\nminority labels. Our system achieves strong performance across all tracks,\nranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both\nMistake Identification and Mistake Location. These results demonstrate the\neffectiveness of scalable instruction tuning and disagreement-driven modeling\nfor robust, multi-dimensional evaluation of LLMs as educational tutors.", "AI": {"tldr": "MSA-MathEval evaluates AI tutor responses across four dimensions using a unified instruction-tuned language model and disagreement-aware ensemble inference, achieving strong performance in a competitive setting.", "motivation": "To evaluate AI tutor responses effectively across multiple instructional dimensions with a robust methodology.", "method": "A unified training pipeline was used to fine-tune a single instruction-tuned language model without task-specific changes, along with a disagreement-aware ensemble strategy to improve prediction reliability.", "result": "The system ranked 1st in Providing Guidance, 3rd in Actionability, and 4th in Mistake Identification and Mistake Location, demonstrating strong multi-dimensional evaluation performance.", "conclusion": "Scalable instruction tuning combined with disagreement-driven modeling can effectively evaluate LLMs as educational tutors.", "key_contributions": ["Unified training approach for multi-dimensional evaluation", "Disagreement-aware ensemble inference strategy", "Strong competitive performance across evaluation dimensions"], "limitations": "", "keywords": ["AI education", "language model evaluation", "instruction tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18555", "pdf": "https://arxiv.org/pdf/2505.18555.pdf", "abs": "https://arxiv.org/abs/2505.18555", "title": "Unraveling Misinformation Propagation in LLM Reasoning", "authors": ["Yiyang Feng", "Yichen Wang", "Shaobo Cui", "Boi Faltings", "Mina Lee", "Jiawei Zhou"], "categories": ["cs.CL"], "comment": "24 pages, 14 figures, 4 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%). Further analysis shows that applying factual corrections early in the\nreasoning process most effectively reduces misinformation propagation, and\nfine-tuning on synthesized data with early-stage corrections significantly\nimproves reasoning factuality. Our work offers a practical approach to\nmitigating misinformation propagation.", "AI": {"tldr": "This paper analyzes the impact of misinformation on mathematical reasoning in Large Language Models (LLMs) and suggests methods to mitigate this issue.", "motivation": "The research explores how misinformation affects LLMs' reasoning, which is crucial for their reliability in real-world interactions where incorrect inputs are common.", "method": "A comprehensive analysis of the propagation of misinformation during the reasoning process in LLMs, with experiments on the effectiveness of factual corrections and fine-tuning with synthesized data.", "result": "LLMs show a significant drop in accuracy when facing misinformation, with a success rate of less than 50% in rectifying it, particularly when factual corrections are applied early in reasoning.", "conclusion": "The study presents effective strategies for reducing misinformation propagation, highlighting the importance of early-stage corrections and fine-tuning for improving reasoning accuracy.", "key_contributions": ["Detailed analysis of misinformation impact on LLMs' reasoning", "Effective methods for mitigating misinformation propagation", "Insights on the timing of factual corrections for improved outcomes"], "limitations": "The study focuses primarily on mathematical reasoning and might not generalize to other domains of LLM application.", "keywords": ["Large Language Models", "misinformation", "reasoning", "factual corrections", "machine learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.18556", "pdf": "https://arxiv.org/pdf/2505.18556.pdf", "abs": "https://arxiv.org/abs/2505.18556", "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation", "authors": ["Jun Zhuang", "Haibo Jin", "Ye Zhang", "Zhengjian Kang", "Wenbin Zhang", "Gaby G. Dagher", "Haohan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review. TL;DR: We propose a new two-stage\n  intent-based prompt-refinement framework, IntentPrompt, that aims to explore\n  the vulnerability of LLMs' content moderation guardrails by refining prompts\n  into benign-looking declarative forms via intent manipulation for red-teaming\n  purposes", "summary": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails.", "AI": {"tldr": "We propose a new two-stage intent-based prompt-refinement framework, IntentPrompt, that aims to explore the vulnerability of LLMs' content moderation guardrails by refining prompts into benign-looking declarative forms via intent manipulation for red-teaming purposes.", "motivation": "To investigate the vulnerability of intent-aware guardrails in large language models (LLMs) and their implicit intent detection capabilities.", "method": "A two-stage intent-based prompt-refinement framework, IntentPrompt, that transforms harmful inquiries into structured outlines and reframes them into declarative-style narratives through iterative optimization.", "result": "The framework significantly outperforms existing jailbreak methods and successfully evades advanced defenses, achieving high attack success rates against various models.", "conclusion": "The findings reveal critical weaknesses in LLMs' safety mechanisms, highlighting intent manipulation as a growing challenge for content moderation.", "key_contributions": ["Introduction of IntentPrompt framework for the vulnerability assessment of LLMs' guardrails.", "Demonstrated high success rates in prompt manipulation against state-of-the-art defenses.", "Highlighted the implicit intent detection capabilities of LLMs."], "limitations": "", "keywords": ["intent detection", "prompt refinement", "large language models", "content moderation", "jailbreak methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18557", "pdf": "https://arxiv.org/pdf/2505.18557.pdf", "abs": "https://arxiv.org/abs/2505.18557", "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation", "authors": ["He Zhu", "Zhiwen Ruan", "Junyou Su", "Xingwei He", "Wenjia Zhang", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL"], "comment": null, "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks.", "AI": {"tldr": "TAG-INSTRUCT is a novel framework for enhancing instruction complexity in large language models through structured semantic compression and difficulty augmentation.", "motivation": "To improve the effectiveness of instruction data used to develop large language models by controlling instruction complexity.", "method": "Utilizes structured semantic compression to convert instructions into a compact tag space and applies RL-guided tag expansion to systematically enhance complexity.", "result": "TAG-INSTRUCT outperformed existing methods for augmenting instruction complexity, demonstrating superior controllability and stability.", "conclusion": "The findings indicate that the tag space approach is more effective for instruction synthesis across various frameworks.", "key_contributions": ["Introduction of the TAG-INSTRUCT framework", "Enhanced controllability through tag space manipulation", "Demonstrated superiority in instruction synthesis performance"], "limitations": "", "keywords": ["instruction complexity", "large language models", "semantic compression", "difficulty augmentation", "RL-guided expansion"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.13836", "pdf": "https://arxiv.org/pdf/2501.13836.pdf", "abs": "https://arxiv.org/abs/2501.13836", "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Most social media users come from non-English speaking countries in the\nGlobal South, where much of harmful content appears in local languages. Yet,\ncurrent AI-driven moderation systems struggle with low-resource languages\nspoken in these regions. This work examines the systemic challenges in building\nautomated moderation tools for these languages. We conducted semi-structured\ninterviews with 22 AI experts working on detecting harmful content in four\nlow-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi\nArabic (North Africa), and Quechua (South America). Our findings show that\nbeyond the well-known data scarcity in local languages, technical issues--such\nas outdated machine translation systems, sentiment and toxicity models grounded\nin Western values, and unreliable language detection technologies--undermine\nmoderation efforts. Even with more data, current language models and\npreprocessing pipelines--primarily designed for English--struggle with the\nmorphological richness, linguistic complexity, and code-mixing. As a result,\nautomated moderation in Tamil, Swahili, Arabic, and Quechua remains fraught\nwith inaccuracies and blind spots. Based on our findings, we argue that these\nlimitations are not just technical gaps but reflect deeper structural\ninequities that continue to reproduce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve automated moderation for\nlow-resource languages.", "AI": {"tldr": "This paper explores the challenges of building AI moderation systems for low-resource languages due to data scarcity and technical issues, along with proposing multi-stakeholder solutions.", "motivation": "To address the systemic challenges in automated moderation of harmful content in low-resource languages from the Global South.", "method": "Semi-structured interviews with 22 AI experts on detecting harmful content in Tamil, Swahili, Maghrebi Arabic, and Quechua.", "result": "Identified that technical issues alongside data scarcity hinder effective moderation in low-resource languages, resulting in inaccuracies.", "conclusion": "The limitations in moderation reflect structural inequities and suggest the need for multi-stakeholder approaches to enhance the effectiveness of AI moderation systems.", "key_contributions": ["Identified systemic challenges in AI moderation for low-resource languages.", "Documented technical issues affecting content moderation efforts.", "Proposed multi-stakeholder approaches for improvement."], "limitations": "The study focuses on a limited set of languages and may not cover all low-resource languages.", "keywords": ["AI moderation", "low-resource languages", "harmful content", "multi-stakeholder approaches", "structural inequities"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.18562", "pdf": "https://arxiv.org/pdf/2505.18562.pdf", "abs": "https://arxiv.org/abs/2505.18562", "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test", "authors": ["Xunlian Dai", "Li Zhou", "Benyou Wang", "Haizhou Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies.", "AI": {"tldr": "This paper introduces CultureSteer, an LLM-adaptive approach that enhances cross-cultural cognitive alignment in large language models by integrating a culture-aware steering mechanism.", "motivation": "To assess and improve the alignment of large language models with cross-cultural cognition and address biases towards Western cultural schemas.", "method": "The paper extends the human-centered word association test to create a free-relation task for LLMs, using CultureSteer for better cultural representation and alignment.", "result": "Experiments demonstrate that current LLMs show significant bias toward Western schemas; however, CultureSteer improves cross-cultural alignment and outperforms prompt-based methods in semantic association tasks.", "conclusion": "The findings suggest that enhancing cultural awareness in LLMs is crucial for developing more inclusive language technologies, and the proposed methodology is validated through various culture-sensitive tasks.", "key_contributions": ["Introduction of CultureSteer for cultural alignment in LLMs.", "Demonstration of significant bias in current LLMs towards Western cultural schemas.", "Validation of the method's efficacy through downstream tasks."], "limitations": "Limited to the word association level and may not address all aspects of cultural representation in LLMs.", "keywords": ["human-centered word association test", "cross-cultural cognition", "large language models", "CultureSteer", "cultural bias"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.18581", "pdf": "https://arxiv.org/pdf/2505.18581.pdf", "abs": "https://arxiv.org/abs/2505.18581", "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "authors": ["Wentao Hu", "Wengyu Zhang", "Yiyang Jiang", "Chen Jason Zhang", "Xiaoyong Wei", "Qing Li"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG.", "AI": {"tldr": "This paper presents Debate-Augmented RAG (DRAG), a framework that enhances the factual accuracy of Retrieval-Augmented Generation by integrating Multi-Agent Debate mechanisms into both the retrieval and generation processes.", "motivation": "The authors identify the problem of retrieval errors and biases in Retrieval-Augmented Generation that lead to the compounding of hallucinations, which they term Hallucination on Hallucination.", "method": "DRAG employs structured debates involving proponents, opponents, and judges to refine retrieval quality and ensure reliability. Additionally, it implements asymmetric information roles and adversarial debates in the generation phase.", "result": "Through evaluations across multiple tasks, DRAG is shown to improve retrieval reliability, reduce hallucinations, and enhance overall factual accuracy significantly compared to existing methods.", "conclusion": "The results suggest that integrating debate mechanisms can effectively counteract the issues of misinformation in generation processes, promoting better factual outcomes in RAG systems.", "key_contributions": ["Introduction of the Debate-Augmented RAG (DRAG) framework", "Use of Multi-Agent Debate mechanisms to refine retrieval processes", "Enhancement of reasoning robustness during generation through adversarial debates"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Multi-Agent Debate", "Factual Accuracy", "Hallucination Mitigation", "Natural Language Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18588", "pdf": "https://arxiv.org/pdf/2505.18588.pdf", "abs": "https://arxiv.org/abs/2505.18588", "title": "Safety Alignment via Constrained Knowledge Unlearning", "authors": ["Zesheng Shi", "Yucheng Zhou", "Jing Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing.", "AI": {"tldr": "A new safety alignment method for LLMs called Constrained Knowledge Unlearning (CKU) improves safety against jailbreak attacks by selectively unlearning harmful knowledge while preserving useful information.", "motivation": "To improve the safety of large language models (LLMs) against jailbreak attacks that exploit existing vulnerabilities by unlearning harmful knowledge.", "method": "CKU scores neurons in multilayer perceptron (MLP) layers to identify important neurons for useful knowledge and prunes the gradients of these neurons during the unlearning process to mitigate harmful content.", "result": "CKU enhances model safety significantly without sacrificing overall performance, demonstrating better safety-utility trade-offs compared to existing methods.", "conclusion": "CKU provides a promising approach for enhancing the safety alignment of LLMs and offers insights into neuron knowledge sensitivity, aiding future safety alignment efforts.", "key_contributions": ["Introduction of Constrained Knowledge Unlearning (CKU) as a novel strategy for LLM safety.", "Demonstrated superior performance in safety alignment without compromising utility.", "Insights into neuron knowledge sensitivity across MLP layers."], "limitations": "", "keywords": ["large language models", "safety alignment", "knowledge unlearning", "jailbreak attacks", "neuron sensitivity"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596.pdf", "abs": "https://arxiv.org/abs/2505.18596", "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release.", "AI": {"tldr": "The paper presents Debate-to-Detect (D2D), a Multi-Agent Debate framework for more effective misinformation detection through structured debate processes.", "motivation": "The paper addresses limitations of static classification methods in misinformation detection by incorporating a more dynamic, debate-based approach.", "method": "D2D employs a five-stage debate process (Opening Statement, Rebuttal, Free Debate, Closing Statement, Judgment) with agents assigned domain-specific profiles.", "result": "Experiments show that D2D significantly outperforms traditional methods in misinformation detection across multiple dimensions.", "conclusion": "D2D enhances decision transparency and robustness in detecting misinformation by using a multi-dimensional evaluation mechanism rather than simple binary classification.", "key_contributions": ["Introduction of a novel Multi-Agent Debate framework for misinformation detection.", "A multi-dimensional evaluation mechanism assessing various aspects of claims.", "Demonstrated efficacy of the D2D approach through experiments with GPT-4o on fake news datasets."], "limitations": "", "keywords": ["misinformation detection", "Multi-Agent Debate", "Large Language Models", "fact-checking", "adversarial debate"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601.pdf", "abs": "https://arxiv.org/abs/2505.18601", "title": "Flex-Judge: Think Once, Judge Anywhere", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.", "AI": {"tldr": "Flex-Judge is a multimodal judge model that uses minimal textual reasoning data to generalize across various modalities, outperforming traditional models with extensive training data.", "motivation": "To develop a model that efficiently aligns generative models with human preferences in multimodal tasks while requiring less annotation data.", "method": "Flex-Judge leverages structured textual reasoning explanations to guide decision-making in multimodal evaluations, allowing generalization across different formats.", "result": "Flex-Judge achieves competitive performance against state-of-the-art models despite being trained on significantly less text data.", "conclusion": "The framework suggests a shift towards reasoning-based text supervision as a cost-effective method for scalable multimodal model evaluation.", "key_contributions": ["Introduction of Flex-Judge as a reasoning-guided multimodal judge model", "Demonstration of its competitive performance with minimal training data", "Potential application in resource-constrained domains like molecular evaluation"], "limitations": "", "keywords": ["multimodal model", "human-generated reward signals", "machine learning", "flex-judge", "textual reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18609", "pdf": "https://arxiv.org/pdf/2505.18609.pdf", "abs": "https://arxiv.org/abs/2505.18609", "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations", "authors": ["Ashwin Sankar", "Yoach Lacombe", "Sherry Thomas", "Praveen Srinivasa Varadhan", "Sanchit Gandhi", "Mitesh M Khapra"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages.", "AI": {"tldr": "RASMALAI introduces a large-scale speech dataset for TTS synthesis in 24 languages, leading to the development of IndicParlerTTS, the first open-source TTS for Indian languages.", "motivation": "To facilitate advancements in controllable and expressive text-to-speech synthesis for Indian languages and English.", "method": "RASMALAI is created with 13,000 hours of speech and 24 million annotated text descriptions covering various attributes. IndicParlerTTS is developed to utilize this dataset for TTS synthesis.", "result": "IndicParlerTTS demonstrates high-quality speech generation, accurately follows text descriptions, and effectively synthesizes specified attributes, achieving strong performance across evaluations.", "conclusion": "The research sets a new standard for controllable multilingual expressive speech synthesis in Indian languages, addressing limitations in existing systems.", "key_contributions": ["Introduction of the RASMALAI dataset for speech synthesis", "Development of IndicParlerTTS, an open-source TTS system", "High performance in multilingual expressive speech generation"], "limitations": "", "keywords": ["speech dataset", "text-to-speech synthesis", "Indian languages"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.18610", "pdf": "https://arxiv.org/pdf/2505.18610.pdf", "abs": "https://arxiv.org/abs/2505.18610", "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs", "authors": ["Tengxuan Liu", "Shiyao Li", "Jiayi Yang", "Tianchen Zhao", "Feng Zhou", "Xiaohui Song", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.", "AI": {"tldr": "This paper proposes Progressive Mixed-Precision KV Cache Quantization (PM-KVQ) to optimize memory overhead in long-CoT reasoning-capable LLMs by reducing cumulative errors and improving calibration strategies, achieving significant performance improvements over SOTA methods.", "motivation": "The memory overhead from long Chain-of-Thought reasoning in Large Language Models leads to performance degradation and inefficiencies that need addressing.", "method": "The paper introduces PM-KVQ, which uses a progressive quantization strategy to lower the bit-width of KV Cache incrementally and a novel calibration method with positional interpolation to enhance the calibration length without increasing overhead.", "result": "Experimental results show PM-KVQ improves reasoning benchmark performance by up to 8% over state-of-the-art baselines while maintaining the same memory budget.", "conclusion": "PM-KVQ effectively addresses memory-related challenges in long-CoT LLMs, achieving better performance without incurring additional memory costs.", "key_contributions": ["Progressive quantization strategy for KV Cache", "Block-wise memory allocation based on sensitivity of transformer blocks", "New calibration strategy with positional interpolation to enhance data distribution approximation."], "limitations": "", "keywords": ["Large Language Models", "Chain-of-Thought reasoning", "KV Cache quantization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614.pdf", "abs": "https://arxiv.org/abs/2505.18614", "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "28 pages, 8 figures", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.", "AI": {"tldr": "This paper introduces MAVL, the first multilingual, multimodal benchmark for singable lyrics translation for animated musicals and presents a new model, SylAVL-CoT, that significantly improves translation quality by leveraging audio-video cues.", "motivation": "Existing methods for lyrics translation often neglect the importance of musical rhythm, syllabic structure, and alignment with multimedia cues in animated musicals, highlighting the need for a dedicated benchmark and improved translation methodologies.", "method": "The authors present the Multilingual Audio-Video Lyrics Benchmark (MAVL), incorporating text, audio, and video elements, and propose the Sylabble-Constrained Audio-Video LLM with Chain-of-Thought (SylAVL-CoT) that integrates these multimodal cues while enforcing syllabic constraints in the translation process.", "result": "SylAVL-CoT outperforms traditional text-based models in terms of singability and contextual accuracy, demonstrating the effectiveness of multimodal translation approaches.", "conclusion": "The study concludes that leveraging audio-video cues in conjunction with syllabic constraints significantly enhances the quality of lyrics translation in animated musicals, urging a shift towards multimodal methodologies.", "key_contributions": ["Introduction of the MAVL benchmark for animated song translation", "Development of the SylAVL-CoT model for translating lyrics with syllabic constraints", "Demonstration of improved performance in singability and contextual accuracy using a multimodal approach"], "limitations": "", "keywords": ["lyrics translation", "multilingual", "multimodal", "animated musicals", "audio-video cues"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18630", "pdf": "https://arxiv.org/pdf/2505.18630.pdf", "abs": "https://arxiv.org/abs/2505.18630", "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "authors": ["Zhihao Jia", "Mingyi Jia", "Junwen Duan", "Jianxin Wang"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 4 figures", "summary": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task.", "AI": {"tldr": "The paper introduces DDO, a novel LLM-based framework for medical consultations that improves decision-making by decoupling symptom inquiry and disease diagnosis into separate tasks.", "motivation": "Existing LLM-based methods struggle with the dual nature of medical consultations, leading to ineffective symptom inquiry and diagnosis.", "method": "DDO employs a collaborative multi-agent workflow to independently optimize the two sub-tasks of medical consultations: symptom inquiry and disease diagnosis.", "result": "DDO shows consistent improvement over existing LLM methodologies and competes well against state-of-the-art generation-based methods across three medical consultation datasets.", "conclusion": "The DDO framework effectively enhances the performance of medical consultations by optimizing decision-making processes in LLMs.", "key_contributions": ["Proposes a dual-decision optimization framework for medical consultations.", "Decouples the symptom inquiry and disease diagnosis tasks for independent optimization.", "Demonstrates superior performance through experiments on real-world datasets."], "limitations": "", "keywords": ["Large Language Models", "medical consultation", "dual-decision optimization", "multi-agent workflow", "disease diagnosis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18638", "pdf": "https://arxiv.org/pdf/2505.18638.pdf", "abs": "https://arxiv.org/abs/2505.18638", "title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models", "authors": ["Md. Tanzib Hosain", "Rajan Das Gupta", "Md. Kishor Morol"], "categories": ["cs.CL"], "comment": "24 pages, 20 figures", "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation.", "AI": {"tldr": "This paper presents DZEN, a dataset of parallel Dzongkha and English test questions for Bhutanese students, exploring the performance of LLMs on these questions.", "motivation": "To improve understanding and performance of LLMs in low-resource languages, particularly Dzongkha, through evaluation with a dedicated dataset.", "method": "The study introduces DZEN, a dataset with over 5,000 parallel questions in Dzongkha and English, and evaluates various LLMs using different prompting strategies.", "result": "LLMs showed significant performance differences between English and Dzongkha, with Chain-of-Thought prompting proving effective for reasoning questions, and the addition of English translations improving response precision.", "conclusion": "The findings suggest that enhancements can be made to LLM performance in low-resource languages like Dzongkha, encouraging further research in this area.", "key_contributions": ["Introduction of the DZEN dataset for Dzongkha and English test questions.", "Evaluation of LLMs' performance on this dataset, revealing discrepancies between languages.", "Insights on the effectiveness of Chain-of-Thought prompting and the impact of English translations."], "limitations": "The dataset is limited to middle and high school level questions and may not cover all aspects of Dzongkha language use.", "keywords": ["Dzongkha", "Large Language Models", "Low-resource languages", "Dataset", "Prompting strategies"], "importance_score": 7, "read_time_minutes": 24}}
{"id": "2505.18642", "pdf": "https://arxiv.org/pdf/2505.18642.pdf", "abs": "https://arxiv.org/abs/2505.18642", "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "authors": ["Xiao Chen", "Sihang Zhou", "Ke Liang", "Xiaoyu Sun", "Xinwang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks.", "AI": {"tldr": "This paper presents a novel chunk-wise training method for improving the efficiency and accuracy of small language models during reasoning tasks.", "motivation": "Existing methods suffer from long rationales that hinder small language models' learning due to over-smoothing of gradients and slow response times.", "method": "The authors propose chunk-wise training (CWT), which divides rationales into semantically coherent chunks, allowing the small language model to focus on one chunk per iteration. They also introduce skip-thinking training (STT) to skip non-reasoning chunks for faster responses.", "result": "The proposed methods improve the reasoning speed of small language models while maintaining or enhancing their accuracy across various reasoning tasks.", "conclusion": "Chunk-wise training and skip-thinking training effectively enable small language models to learn reasoning more efficiently and perform better in reasoning tasks.", "key_contributions": ["Introduction of chunk-wise training to isolate reasoning chunks", "Development of skip-thinking training to accelerate response times", "Validation on multiple small language models and reasoning tasks"], "limitations": "", "keywords": ["chain-of-thought", "distillation", "small language models", "reasoning tasks", "chunk-wise training"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18651", "pdf": "https://arxiv.org/pdf/2505.18651.pdf", "abs": "https://arxiv.org/abs/2505.18651", "title": "On the Emergence of Linear Analogies in Word Embeddings", "authors": ["Daniel J. Korchinski", "Dhruva Karkada", "Yasaman Bahri", "Matthieu Wyart"], "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG"], "comment": "Main: 12 pages, 3 figures. Appendices: 8 pages, 7 figures", "summary": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al.", "AI": {"tldr": "This paper introduces a generative model to explain the linear analogy structure observed in word embeddings like Word2Vec and GloVe, linking it to binary semantic attributes.", "motivation": "To clarify the theoretical origins of the linear analogy structure seen in word embeddings and to understand how additional embedding dimensions influence this structure.", "method": "A theoretical generative model is proposed where words are represented by binary semantic attributes, and word co-occurrence probabilities are derived from these attributes.", "result": "The model reproduces the linear analogy structure and explains observed phenomena related to eigenvectors and co-occurrence statistics.", "conclusion": "The generative model provides insights into the role of embedding dimensions and demonstrates robustness against noise, aligning well with empirical co-occurrence data.", "key_contributions": ["Introduction of a generative model for word embeddings", "Analytical reproduction of the linear analogy structure", "Insights into the influence of embedding dimensions on analogy performance"], "limitations": "", "keywords": ["word embeddings", "linear analogy", "generative model", "co-occurrence probabilities", "semantic attributes"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.18653", "pdf": "https://arxiv.org/pdf/2505.18653.pdf", "abs": "https://arxiv.org/abs/2505.18653", "title": "Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change", "authors": ["Murathan Kurfalı", "Shorouq Zahra", "Joakim Nivre", "Gabriele Messori"], "categories": ["cs.CL"], "comment": "Accepted to ClimateNLP 2025@ACL", "summary": "Climate-Eval is a comprehensive benchmark designed to evaluate natural\nlanguage processing models across a broad range of tasks related to climate\nchange. Climate-Eval aggregates existing datasets along with a newly developed\nnews classification dataset, created specifically for this release. This\nresults in a benchmark of 25 tasks based on 13 datasets, covering key aspects\nof climate discourse, including text classification, question answering, and\ninformation extraction. Our benchmark provides a standardized evaluation suite\nfor systematically assessing the performance of large language models (LLMs) on\nthese tasks. Additionally, we conduct an extensive evaluation of open-source\nLLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot\nsettings, analyzing their strengths and limitations in the domain of climate\nchange.", "AI": {"tldr": "Climate-Eval is a benchmark for evaluating NLP models on climate change tasks, featuring 25 tasks and 13 datasets.", "motivation": "To assess the performance of NLP models, particularly large language models, in understanding and applying knowledge related to climate change.", "method": "The benchmark aggregates existing datasets and introduces a new news classification dataset, covering various NLP tasks such as text classification, question answering, and information extraction.", "result": "An extensive evaluation of open-source LLMs (2B to 70B parameters) in zero-shot and few-shot settings, highlighting their performance on climate-related tasks.", "conclusion": "The standardized evaluation suite allows for systematic assessment, revealing strengths and limitations of LLMs in climate discourse.", "key_contributions": ["Development of a comprehensive benchmark for climate change NLP tasks", "Introduction of a new news classification dataset", "Standardized evaluation suite for large language models"], "limitations": "", "keywords": ["climate change", "natural language processing", "benchmark", "large language models", "evaluation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658.pdf", "abs": "https://arxiv.org/abs/2505.18658", "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research.", "AI": {"tldr": "A survey on the robustness of Large Language Models (LLMs), discussing challenges, sources of non-robustness, mitigation strategies, and future research pathways.", "motivation": "To address critical challenges in ensuring the robustness of LLMs for reliable applications in natural language processing and artificial intelligence.", "method": "Comprehensive review of current studies, categorizing sources of non-robustness, analyzing mitigation strategies and benchmarks, and synthesizing findings from surveys.", "result": "Identifies intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors affecting LLM reliability, along with effective mitigation strategies and emerging assessment metrics.", "conclusion": "A roadmap for addressing robustness in LLMs, highlighting trends, unresolved issues, and future research directions in the field.", "key_contributions": ["Systematic examination of robustness in LLMs.", "Categorization of sources of non-robustness.", "Review of mitigation strategies and benchmarks."], "limitations": "The survey may not cover all emerging models or latest challenges in real-time applications.", "keywords": ["Large Language Models", "robustness", "natural language processing", "AI", "mitigation strategies"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18673", "pdf": "https://arxiv.org/pdf/2505.18673.pdf", "abs": "https://arxiv.org/abs/2505.18673", "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Xiuying Chen", "Jieyu Zhao", "Meng Jiang", "Xiangliang Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025. Code available at\n  https://github.com/xzx34/Cross-Lingual-Pitfalls", "summary": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls.", "AI": {"tldr": "This paper presents a new methodology to identify cross-lingual weaknesses in Large Language Models (LLMs) using bilingual question pairs, demonstrating significant performance discrepancies between English and other languages.", "motivation": "To address the challenge of inconsistent cross-lingual performance in LLMs, which affects their applications in multilingual contexts.", "method": "The methodology uses beam search and LLM-based simulation to create bilingual question pairs for analyzing performance across languages.", "result": "The experiments reveal over 50% accuracy drops in target languages among various state-of-the-art models, showcasing the effectiveness of the proposed approach.", "conclusion": "The proposed method successfully identifies cross-lingual weaknesses, leading to insights about the relationship between linguistic similarity and model performance, suggesting opportunities for targeted post-training.", "key_contributions": ["Introduction of a novel methodology for identifying cross-lingual weaknesses in LLMs.", "Creation of a new dataset with over 6,000 bilingual question pairs across 16 languages.", "Demonstration of the relationship between linguistic similarity and model performance discrepancies."], "limitations": "The method may still be limited by the availability of quality bilingual datasets and the need for extensive model evaluations.", "keywords": ["Large Language Models", "cross-lingual performance", "bilingual question pairs", "NLP", "linguistic similarity"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18677", "pdf": "https://arxiv.org/pdf/2505.18677.pdf", "abs": "https://arxiv.org/abs/2505.18677", "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts", "authors": ["Eric Chamoun", "Nedjma Ousidhoum", "Michael Schlichtkrull", "Andreas Vlachos"], "categories": ["cs.CL"], "comment": null, "summary": "Clarifying the research framing of NLP artefacts (e.g., models, datasets,\netc.) is crucial to aligning research with practical applications. Recent\nstudies manually analyzed NLP research across domains, showing that few papers\nexplicitly identify key stakeholders, intended uses, or appropriate contexts.\nIn this work, we propose to automate this analysis, developing a\nthree-component system that infers research framings by first extracting key\nelements (means, ends, stakeholders), then linking them through interpretable\nrules and contextual reasoning. We evaluate our approach on two domains:\nautomated fact-checking using an existing dataset, and hate speech detection\nfor which we annotate a new dataset-achieving consistent improvements over\nstrong LLM baselines. Finally, we apply our system to recent automated\nfact-checking papers and uncover three notable trends: a rise in vague or\nunderspecified research goals, increased emphasis on scientific exploration\nover application, and a shift toward supporting human fact-checkers rather than\npursuing full automation.", "AI": {"tldr": "This paper proposes an automated system for analyzing NLP research framings, focusing on means, ends, and stakeholders to align research with practical applications.", "motivation": "To enhance alignment between NLP research and its practical applications by clarifying research framings.", "method": "Develop a three-component system that extracts key elements from NLP research and applies contextual reasoning to link them.", "result": "Achieved consistent improvements over strong LLM baselines in two domains: automated fact-checking and hate speech detection.", "conclusion": "The system revealed trends in recent automated fact-checking research, including vague research goals and a shift toward supporting human fact-checkers.", "key_contributions": ["Automation of analysis for NLP research framings", "Identification of key elements and their relationships", "Insights into current trends within automated fact-checking research"], "limitations": "", "keywords": ["NLP", "automated analysis", "research framing", "fact-checking", "hate speech detection"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.18683", "pdf": "https://arxiv.org/pdf/2505.18683.pdf", "abs": "https://arxiv.org/abs/2505.18683", "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation", "authors": ["Raphaël Merx", "Hanna Suominen", "Lois Hong", "Nick Thieberger", "Trevor Cohn", "Ekaterina Vylomova"], "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) systems that support low-resource languages often\nstruggle on specialized domains. While researchers have proposed various\ntechniques for domain adaptation, these approaches typically require model\nfine-tuning, making them impractical for non-technical users and small\norganizations. To address this gap, we propose Tulun, a versatile solution for\nterminology-aware translation, combining neural MT with large language model\n(LLM)-based post-editing guided by existing glossaries and translation\nmemories. Our open-source web-based platform enables users to easily create,\nedit, and leverage terminology resources, fostering a collaborative\nhuman-machine translation process that respects and incorporates domain\nexpertise while increasing MT accuracy. Evaluations show effectiveness in both\nreal-world and benchmark scenarios: on medical and disaster relief translation\ntasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41\nChrF++ points over baseline MT systems. Across six low-resource languages on\nthe FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,\nachieving an average improvement of 2.8 ChrF points over NLLB-54B.", "AI": {"tldr": "Tulun is a web-based platform that combines neural machine translation with LLM-guided post-editing for low-resource languages, enabling user-friendly, terminology-aware translation.", "motivation": "Existing machine translation systems for low-resource languages struggle with specialized domains and require technical expertise for domain adaptation techniques, making access difficult for non-technical users.", "method": "Tulun integrates neural machine translation with large language model-based post-editing, utilizing existing glossaries and translation memories, to facilitate a collaborative human-machine translation process.", "result": "Tulun demonstrates significant improvements in translation accuracy for medical and disaster relief tasks, achieving 16.90-22.41 ChrF++ point increases for Tetun and Bislama, and an average improvement of 2.8 ChrF points across six low-resource languages on the FLORES dataset.", "conclusion": "Tulun offers a practical solution for improving machine translation outcomes in specialized domains by empowering users to effectively employ terminology resources without needing technical expertise.", "key_contributions": ["Combines neural MT with LLM-based post-editing.", "User-friendly platform for creating and managing translation resources.", "Demonstrated effectiveness in real-world applications and benchmark datasets."], "limitations": "", "keywords": ["machine translation", "low-resource languages", "large language models", "terminology-aware translation", "collaborative translation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.18685", "pdf": "https://arxiv.org/pdf/2505.18685.pdf", "abs": "https://arxiv.org/abs/2505.18685", "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation", "authors": ["Zhihao Zhang", "Yiran Zhang", "Xiyue Zhou", "Liting Huang", "Imran Razzak", "Preslav Nakov", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Infodemics and health misinformation have significant negative impact on\nindividuals and society, exacerbating confusion and increasing hesitancy in\nadopting recommended health measures. Recent advancements in generative AI,\ncapable of producing realistic, human like text and images, have significantly\naccelerated the spread and expanded the reach of health misinformation,\nresulting in an alarming surge in its dissemination. To combat the infodemics,\nmost existing work has focused on developing misinformation datasets from\nsocial media and fact checking platforms, but has faced limitations in topical\ncoverage, inclusion of AI generation, and accessibility of raw content. To\naddress these issues, we present MM Health, a large scale multimodal\nmisinformation dataset in the health domain consisting of 34,746 news article\nencompassing both textual and visual information. MM Health includes\nhuman-generated multimodal information (5,776 articles) and AI generated\nmultimodal information (28,880 articles) from various SOTA generative AI\nmodels. Additionally, We benchmarked our dataset against three tasks\n(reliability checks, originality checks, and fine-grained AI detection)\ndemonstrating that existing SOTA models struggle to accurately distinguish the\nreliability and origin of information. Our dataset aims to support the\ndevelopment of misinformation detection across various health scenarios,\nfacilitating the detection of human and machine generated content at multimodal\nlevels.", "AI": {"tldr": "MM Health is a large-scale multimodal misinformation dataset in the health domain aimed at combating the spread of health misinformation exacerbated by generative AI.", "motivation": "To combat the global issue of health misinformation exacerbated by AI, existing datasets lacked coverage, diverse content, and accessibility.", "method": "The dataset consists of 34,746 news articles, including both human-generated and AI-generated multimodal content, and is benchmarked against tasks like reliability and originality checks.", "result": "Benchmarking demonstrates that current state-of-the-art models struggle to accurately identify the source and reliability of health-related misinformation.", "conclusion": "MM Health serves as a crucial resource for enhancing misinformation detection capabilities in the health sector, supporting the distinction between human and machine-generated content.", "key_contributions": ["A large-scale dataset (34,746 articles) addressing health misinformation.", "Inclusion of both human- and AI-generated multimodal information.", "Benchmarking against reliability checks and fine-grained AI detection tasks."], "limitations": "", "keywords": ["health misinformation", "multimodal dataset", "generative AI", "AI detection", "reliability checks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688.pdf", "abs": "https://arxiv.org/abs/2505.18688", "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "authors": ["Aleksandr Tsymbalov"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning.", "AI": {"tldr": "This paper explores the use of Large Language Models (LLMs) as a replacement for human annotators in text classification tasks to maintain model quality and address data drift.", "motivation": "The need for human annotators in text classification is labor-intensive and costly, especially when retraining models due to data drift; the paper aims to alleviate this burden using LLMs.", "method": "The study proposes multiple approaches where LLMs are utilized to test and validate classifier predictions, thus reducing reliance on human specialists.", "result": "Using LLMs for validation can effectively support incremental learning and model quality assurance, potentially making the process more efficient and less costly.", "conclusion": "LLMs can play a significant role in minimizing the labor involved in annotating datasets for text classification, particularly in dynamic contexts where ongoing data drift occurs.", "key_contributions": ["Proposed methods for utilizing LLMs in validating text classification predictions.", "Demonstrated potential cost reduction in model retraining processes.", "Provided a framework for incorporating LLMs in incremental learning pipelines."], "limitations": "The effectiveness of LLMs as annotators may vary with the complexity of the classification task and the specifics of the model being trained.", "keywords": ["Large Language Models", "text classification", "incremental learning", "model drift", "data collection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18690", "pdf": "https://arxiv.org/pdf/2505.18690.pdf", "abs": "https://arxiv.org/abs/2505.18690", "title": "Benchmarking and Rethinking Knowledge Editing for Large Language Models", "authors": ["Guoxiu He", "Xin Song", "Futing Wang", "Aixin Sun"], "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.05212", "summary": "Knowledge editing aims to update the embedded knowledge within Large Language\nModels (LLMs). However, existing approaches, whether through parameter\nmodification or external memory integration, often suffer from inconsistent\nevaluation objectives and experimental setups. To address this gap, we conduct\na comprehensive benchmarking study. In addition to fact-level datasets, we\nintroduce more complex event-based datasets and general-purpose datasets drawn\nfrom other tasks. Our evaluation covers both instruction-tuned and\nreasoning-oriented LLMs, under a realistic autoregressive inference setting\nrather than teacher-forced decoding. Beyond single-edit assessments, we also\nevaluate multi-edit scenarios to better reflect practical demands. We employ\nfour evaluation dimensions, including portability, and compare all recent\nmethods against a simple and straightforward baseline named Selective\nContextual Reasoning (SCR). Empirical results reveal that parameter-based\nediting methods perform poorly under realistic conditions. In contrast, SCR\nconsistently outperforms them across all settings. This study offers new\ninsights into the limitations of current knowledge editing methods and\nhighlights the potential of context-based reasoning as a more robust\nalternative.", "AI": {"tldr": "A benchmarking study on knowledge editing in LLMs reveals that common parameter-based methods perform poorly compared to a simple baseline approach, Selective Contextual Reasoning (SCR).", "motivation": "To address inconsistent evaluation objectives and experimental setups in existing knowledge editing methods for Large Language Models (LLMs).", "method": "Conducted a benchmarking study using fact-level and event-based datasets, evaluating both instruction-tuned and reasoning-oriented LLMs under realistic autoregressive inference settings, and employing multi-edit assessments.", "result": "Parameter-based editing methods consistently underperformed, while the Selective Contextual Reasoning (SCR) baseline outperformed them across all settings.", "conclusion": "This study highlights the limitations of current knowledge editing approaches and suggests that context-based reasoning could be a more effective alternative.", "key_contributions": ["Introduction of complex event-based datasets for evaluation", "Evaluation of multi-edit scenarios to reflect practical use", "Demonstration of the superiority of Selective Contextual Reasoning (SCR) over parameter-based methods."], "limitations": "", "keywords": ["knowledge editing", "Large Language Models", "benchmarking", "Selective Contextual Reasoning", "context-based reasoning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.18703", "pdf": "https://arxiv.org/pdf/2505.18703.pdf", "abs": "https://arxiv.org/abs/2505.18703", "title": "Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task", "authors": ["Gaurav Negi", "Dhairya Dalal", "Omnia Zayed", "Paul Buitelaar"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Unified Opinion Concepts (UOC) ontology to\nintegrate opinions within their semantic context. The UOC ontology bridges the\ngap between the semantic representation of opinion across different\nformulations. It is a unified conceptualisation based on the facets of opinions\nstudied extensively in NLP and semantic structures described through symbolic\ndescriptions. We further propose the Unified Opinion Concept Extraction (UOCE)\ntask of extracting opinions from the text with enhanced expressivity.\nAdditionally, we provide a manually extended and re-annotated evaluation\ndataset for this task and tailored evaluation metrics to assess the adherence\nof extracted opinions to UOC semantics. Finally, we establish baseline\nperformance for the UOCE task using state-of-the-art generative models.", "AI": {"tldr": "The paper presents the Unified Opinion Concepts (UOC) ontology for semantic representation of opinions and introduces a new task, Unified Opinion Concept Extraction (UOCE), along with a dataset and metrics for evaluation.", "motivation": "To integrate opinions within their semantic context and enhance the expressivity of opinion extraction from text.", "method": "Development of the UOC ontology and the UOCE task, along with a new manually extended evaluation dataset and tailored evaluation metrics.", "result": "Establishment of baseline performance for the UOCE task using state-of-the-art generative models, validating the effectiveness of the UOC ontology.", "conclusion": "The UOC ontology and UOCE task provide a robust framework for enhanced opinion extraction and evaluation in NLP.", "key_contributions": ["Introduction of the Unified Opinion Concepts (UOC) ontology", "Definition of the Unified Opinion Concept Extraction (UOCE) task", "Creation of a new evaluation dataset and tailored metrics for opinion extraction"], "limitations": "", "keywords": ["opinion extraction", "semantic representation", "natural language processing"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.18708", "pdf": "https://arxiv.org/pdf/2505.18708.pdf", "abs": "https://arxiv.org/abs/2505.18708", "title": "A General Knowledge Injection Framework for ICD Coding", "authors": ["Xu Zhang", "Kun Zhang", "Wenxin Ma", "Rongsheng Wang", "Chenxu Wu", "Yingtai Li", "S. Kevin Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD.", "AI": {"tldr": "The paper presents GKI-ICD, a novel framework for ICD coding that integrates multiple knowledge types to enhance coding performance without complex modular designs.", "motivation": "The paper aims to address the challenges in ICD coding due to long-tail distribution and insufficient annotations by leveraging diverse knowledge sources.", "method": "GKI-ICD integrates three types of knowledge—ICD Description, ICD Synonym, and ICD Hierarchy—into a comprehensive framework that avoids complex module design.", "result": "Extensive experiments show that GKI-ICD achieves state-of-the-art performance on popular ICD coding benchmarks across most evaluation metrics.", "conclusion": "The proposed method effectively enhances ICD coding performance by leveraging diverse knowledge while maintaining scalability and compatibility.", "key_contributions": ["Introduction of GKI-ICD framework for ICD coding.", "Integration of multiple types of knowledge without specialized modules.", "Demonstration of state-of-the-art performance on ICD benchmarks."], "limitations": "", "keywords": ["ICD Coding", "Knowledge Injection", "Machine Learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.18709", "pdf": "https://arxiv.org/pdf/2505.18709.pdf", "abs": "https://arxiv.org/abs/2505.18709", "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla", "authors": ["Sourav Kumar Das", "Md. Julkar Naeen", "MD. Jahidul Islam", "Md. Anisul Haque Sajeeb", "Narayan Ranjan Chakraborty", "Mayen Uddin Mojumdar"], "categories": ["cs.CL", "cs.AI"], "comment": "2024 15th International Conference on Computing Communication and\n  Networking Technologies (ICCCNT)", "summary": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations.", "AI": {"tldr": "This paper presents a system for translating Modern Bangla to Sylheti Bangla using NLP techniques.", "motivation": "The paper addresses the lack of research on local Bangla languages and aims to enhance the understanding of the Sylheti language through NLP.", "method": "Three models (LSTM, Bi-LSTM, Seq2Seq) were trained on a dataset of 1200 samples to perform translations from Modern Bangla to Sylheti.", "result": "The LSTM model achieved the best performance with an accuracy of 89.3%.", "conclusion": "The research contributes valuable insights that could promote further advancements in Bangla NLP.", "key_contributions": ["Developed an NLP system for translating Modern Bangla to Sylheti Bangla.", "Achieved high accuracy with the LSTM model.", "Filled a gap in research regarding local Bangla languages."], "limitations": "", "keywords": ["Natural Language Processing", "Bangla language", "Sylheti language", "LSTM", "translation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720.pdf", "abs": "https://arxiv.org/abs/2505.18720", "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}.", "AI": {"tldr": "This paper presents Optimal Transport-based token weighting for Direct Preference Optimization in LLMs, enhancing reward stability and instruction-following through context-aware token importance.", "motivation": "To improve Direct Preference Optimization in Large Language Models by addressing the issue of equal token weighting leading to suboptimal alignment with human preferences.", "method": "A context-aware token weighting scheme that emphasizes semantically meaningful token pairs while de-emphasizing less relevant tokens for preference optimization.", "result": "The proposed OTPO method was validated through extensive experiments, demonstrating improved instruction-following ability in LLMs.", "conclusion": "OTPO enhances the effectiveness of Direct Preference Optimization by focusing on meaningful differences, leading to more stable and interpretable rewards.", "key_contributions": ["Introduction of a context-aware token weighting scheme for DPO", "Improved performance of LLMs in instruction-following tasks", "Enhanced interpretability and reward stability in preference optimization"], "limitations": "", "keywords": ["Direct Preference Optimization", "Large Language Models", "Token weighting", "Human preferences", "Instruction-following"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744.pdf", "abs": "https://arxiv.org/abs/2505.18744", "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges", "authors": ["Tao Liu", "Hongying Zan", "Yifan Li", "Dixuan Zhang", "Lulu Kong", "Haixin Liu", "Jiaming Hou", "Aoze Zheng", "Rui Li", "Yiming Qiao", "Zewei Luo", "Qi Wang", "Zhiqiang Zhang", "Jiaxi Li", "Supeng Liu", "Kunli Zhang", "Min Peng"], "categories": ["cs.CL"], "comment": "22 pages, 10 figures", "summary": "Text-to-SQL is a fundamental task in natural language processing that seeks\nto translate natural language questions into meaningful and executable SQL\nqueries. While existing datasets are extensive and primarily focus on business\nscenarios and operational logic, they frequently lack coverage of\ndomain-specific knowledge and complex mathematical reasoning. To address this\ngap, we present a novel dataset tailored for complex reasoning and\nchain-of-thought analysis in SQL inference, encompassing physical, arithmetic,\ncommonsense, and hypothetical reasoning. The dataset consists of 4,038 English\nquestions, each paired with a unique SQL query and accompanied by 12,114\nstep-by-step reasoning annotations, spanning 45 databases across diverse\ndomains. Experimental results demonstrate that LogicCat substantially increases\nthe difficulty for state-of-the-art models, with the highest execution accuracy\nreaching only 14.96%. Incorporating our chain-of-thought annotations boosts\nperformance to 33.96%. Benchmarking leading public methods on Spider and BIRD\nfurther underscores the unique challenges presented by LogicCat, highlighting\nthe significant opportunities for advancing research in robust,\nreasoning-driven text-to-SQL systems. We have released our dataset code at\nhttps://github.com/Ffunkytao/LogicCat.", "AI": {"tldr": "Introducing a new dataset, LogicCat, for complex reasoning in text-to-SQL tasks, featuring 4,038 questions with SQL queries and detailed annotations for reasoning.", "motivation": "Existing text-to-SQL datasets lack domain-specific knowledge and complex reasoning capabilities, necessitating a specialized dataset for better performance.", "method": "A novel dataset, LogicCat, was created consisting of 4,038 questions paired with SQL queries and 12,114 reasoning annotations, covering various forms of reasoning.", "result": "LogicCat increases the challenge for state-of-the-art models, achieving only 14.96% execution accuracy; however, using chain-of-thought annotations improves accuracy to 33.96%.", "conclusion": "The LogicCat dataset presents significant challenges for text-to-SQL systems, highlighting the need for improved reasoning capabilities in natural language processing tasks.", "key_contributions": ["Creation of the LogicCat dataset for complex SQL reasoning", "Inclusion of diverse reasoning annotations", "Benchmarking results demonstrating challenges for existing models"], "limitations": "", "keywords": ["Text-to-SQL", "Natural Language Processing", "Dataset", "Reasoning", "Machine Learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.18752", "pdf": "https://arxiv.org/pdf/2505.18752.pdf", "abs": "https://arxiv.org/abs/2505.18752", "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning", "authors": ["Haolin Yang", "Hakaze Cho", "Yiqiao Zhong", "Naoya Inoue"], "categories": ["cs.CL"], "comment": "45 pages, 49 figures", "summary": "The unusual properties of in-context learning (ICL) have prompted\ninvestigations into the internal mechanisms of large language models. Prior\nwork typically focuses on either special attention heads or task vectors at\nspecific layers, but lacks a unified framework linking these components to the\nevolution of hidden states across layers that ultimately produce the model's\noutput. In this paper, we propose such a framework for ICL in classification\ntasks by analyzing two geometric factors that govern performance: the\nseparability and alignment of query hidden states. A fine-grained analysis of\nlayer-wise dynamics reveals a striking two-stage mechanism: separability\nemerges in early layers, while alignment develops in later layers. Ablation\nstudies further show that Previous Token Heads drive separability, while\nInduction Heads and task vectors enhance alignment. Our findings thus bridge\nthe gap between attention heads and task vectors, offering a unified account of\nICL's underlying mechanisms.", "AI": {"tldr": "This paper proposes a unified framework for understanding in-context learning (ICL) in large language models by analyzing how query hidden states evolve through layers during classification tasks.", "motivation": "To investigate the internal mechanisms of large language models, particularly the role of in-context learning (ICL), which has unusual properties that require a unified understanding.", "method": "The paper analyzes two geometric factors—separability and alignment of query hidden states—by conducting a fine-grained, layer-wise dynamics analysis and performing ablation studies to understand their impact during ICL in classification tasks.", "result": "The study reveals a two-stage mechanism where separability emerges in early layers, and alignment develops in later layers, with specific attention heads driving these characteristics.", "conclusion": "The findings bridge the gap between attention heads and task vectors, providing a comprehensive understanding of ICL's mechanisms in language models.", "key_contributions": ["Proposed a unified framework for understanding ICL in large language models.", "Identified a two-stage mechanism involving separability and alignment of query hidden states.", "Linked attention heads and task vectors to the evolution of model outputs."], "limitations": "", "keywords": ["in-context learning", "large language models", "hidden states", "classification tasks", "machine learning"], "importance_score": 8, "read_time_minutes": 45}}
{"id": "2505.18754", "pdf": "https://arxiv.org/pdf/2505.18754.pdf", "abs": "https://arxiv.org/abs/2505.18754", "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection", "authors": ["Elsen Ronando", "Sozo Inoue"], "categories": ["cs.CL", "I.2.7"], "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors\n  (2025). Final version before journal publication", "summary": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios.", "AI": {"tldr": "This paper introduces HED-LM, a method to enhance example selection in few-shot optimization for sensor-based classification tasks, specifically addressing fatigue detection using accelerometer data.", "motivation": "The performance of few-shot prompting is significantly influenced by the quality of selected examples, which can hinder effective inference in sensor-based applications.", "method": "HED-LM utilizes a hybrid selection pipeline that combines Euclidean distance filtering with contextual relevance scoring from large language models to improve example selection.", "result": "HED-LM achieved a mean macro F1-score of 69.13%, outperforming random selection and distance-only filtering by 16.6% and 2.3%, respectively, demonstrating its effectiveness for nuanced tasks like fatigue detection.", "conclusion": "HED-LM offers a robust solution for improving performance in real-world sensor-based learning applications and has further implications for healthcare monitoring and activity recognition.", "key_contributions": ["Introduces a hybrid example selection method combining numerical and contextual relevance for few-shot learning.", "Demonstrates effectiveness in fatigue detection with accelerometer data characterized by high variability.", "Provides empirical evidence of improved performance over existing selection methods."], "limitations": "The approach's effectiveness may vary across different sensor modalities and task complexities.", "keywords": ["few-shot learning", "HED-LM", "example selection", "fatigue detection", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761.pdf", "abs": "https://arxiv.org/abs/2505.18761", "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions.", "AI": {"tldr": "The paper presents GSM-DC, a benchmark for testing LLMs' reasoning capabilities in handling irrelevant contexts during mathematical problem-solving.", "motivation": "To evaluate the robustness of Large Language Models when faced with distracting contexts in reasoning tasks, particularly in grade school mathematics.", "method": "The method involves constructing symbolic reasoning graphs with controlled irrelevant context injections and performing experiments to assess the impact on LLMs' reasoning and arithmetic performance.", "result": "LLMs show significant sensitivity to irrelevant context, impairing reasoning path selection and accuracy, but incorporating strong distractors during training improves overall performance.", "conclusion": "A novel stepwise tree search guided by a process reward model enhances LLM robustness to irrelevant contexts, especially in out-of-distribution conditions.", "key_contributions": ["Introduction of GSM-DC benchmark for evaluating LLM reasoning under distracting context.", "Demonstrates the negative impact of irrelevant context on LLMs' performance.", "Proposes a tree search method that improves model robustness in challenging scenarios."], "limitations": "Focused primarily on mathematical reasoning; results may vary in other domains or complex tasks.", "keywords": ["Large Language Models", "Reasoning Robustness", "Irrelevant Context", "Machine Learning", "Benchmark"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18762", "pdf": "https://arxiv.org/pdf/2505.18762.pdf", "abs": "https://arxiv.org/abs/2505.18762", "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "authors": ["Michael Flor", "Zuowei Wang", "Paul Deane", "Tenaha O'Reilly"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This manuscript was accepted to be published as an ETS Research\n  Report. Keywords topics; vocabulary; background knowledge; automatic item\n  generation; assessment; reading comprehension", "summary": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs.", "AI": {"tldr": "K-tool is an automated system that generates topical vocabulary tests to assess students' background knowledge for improving reading comprehension in STEM texts.", "motivation": "The need for an automated measure of students' background knowledge to predict their ability to comprehend specific domain-related texts.", "method": "K-tool detects the topic of a text and generates vocabulary tests based on the topic's relationship to the words, categorizing words into related and unrelated groups.", "result": "Initial evaluation shows that K-tool can effectively produce vocabulary items that may indicate a student's readiness to understand text based on their knowledge state.", "conclusion": "K-tool is intended for middle and high school students and does not require a corpus, making it versatile for various single reading passages.", "key_contributions": ["Development of an automated system for generating topical vocabulary tests", "Initial evaluation demonstrating system applicability", "Unique approach to measuring background knowledge related to reading comprehension"], "limitations": "Currently designed only for native English speakers in middle and high school; focuses on single reading passages without a broader text corpus.", "keywords": ["topics", "vocabulary", "background knowledge", "automatic item generation", "assessment", "reading comprehension"], "importance_score": 5, "read_time_minutes": 5}}
{"id": "2505.18774", "pdf": "https://arxiv.org/pdf/2505.18774.pdf", "abs": "https://arxiv.org/abs/2505.18774", "title": "Disentangling Knowledge Representations for Large Language Model Editing", "authors": ["Mengqi Zhang", "Zisheng Zhou", "Xiaotian Ye", "Qiang Liu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing has emerged as a promising solution for efficiently\nupdating embedded knowledge in large language models (LLMs). While existing\napproaches demonstrate effectiveness in integrating new knowledge and\npreserving the original capabilities of LLMs, they fail to maintain\nfine-grained irrelevant knowledge facts that share the same subject as edited\nknowledge but differ in relation and object. This challenge arises because\nsubject representations inherently encode multiple attributes, causing the\ntarget and fine-grained irrelevant knowledge to become entangled in the\nrepresentation space, and thus vulnerable to unintended alterations during\nediting. To address this, we propose DiKE, a novel approach that Disentangles\nKnowledge representations for LLM Editing (DiKE). DiKE consists of two key\ncomponents: a Knowledge Representation Disentanglement (KRD) module that\ndecomposes the subject representation into target-knowledgerelated and\n-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module\nthat updates only the target-related component while explicitly preserving the\nunrelated one. We further derive a closed-form, rank-one parameter update based\non matrix theory to enable efficient and minimally invasive edits. To\nrigorously evaluate fine-grained irrelevant knowledge preservation, we\nconstruct FINE-KED, a new benchmark comprising fine-grained irrelevant\nknowledge at different levels of relational similarity to the edited knowledge.\nExtensive experiments across multiple LLMs demonstrate that DiKE substantially\nimproves fine-grained irrelevant knowledge preservation while maintaining\ncompetitive general editing performance.", "AI": {"tldr": "DiKE is a novel approach for efficiently editing knowledge in LLMs without losing fine-grained irrelevant knowledge, achieved through a unique disentanglement methodology.", "motivation": "To overcome the limitations of existing knowledge editing approaches that fail to preserve fine-grained irrelevant knowledge when editing LLMs.", "method": "DiKE utilizes a Knowledge Representation Disentanglement module to separate target-related and unrelated components of knowledge, along with a Disentanglement-based Knowledge Edit module to update only the relevant parts while preserving others.", "result": "DiKE shows substantial improvements in preserving fine-grained irrelevant knowledge while maintaining competitive performance in general editing tasks across multiple LLMs.", "conclusion": "DiKE effectively disentangles and updates knowledge in LLMs, addressing challenges in preserving unrelated knowledge during the editing process.", "key_contributions": ["Introduction of the DiKE approach for knowledge editing in LLMs", "Development of Knowledge Representation Disentanglement and Disentanglement-based Knowledge Edit modules", "Creation of the FINE-KED benchmark for evaluating knowledge preservation"], "limitations": "", "keywords": ["Knowledge Editing", "LLMs", "DiKE", "Disentanglement", "Health Informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18778", "pdf": "https://arxiv.org/pdf/2505.18778.pdf", "abs": "https://arxiv.org/abs/2505.18778", "title": "A generalised editor calculus (Short Paper)", "authors": ["Benjamin Bennetzen", "Peter Buus Steffensen", "Hans Hüttel", "Nikolaj Rossander Kristensen", "Andreas Tor Mortensen"], "categories": ["cs.CL", "F.2.2, I.2.7"], "comment": "7 pages, 21 figures", "summary": "In this paper, we present a generalization of a syntax-directed editor\ncalculus, which can be used to instantiate a specialized syntax-directed editor\nfor any language, given by some abstract syntax. The editor calculus guarantees\nthe absence of syntactical errors while allowing incomplete programs. The\ngeneralized editor calculus is then encoded into a simply typed lambda\ncalculus, extended with pairs, booleans, pattern matching and fixed points", "AI": {"tldr": "This paper presents a generalized syntax-directed editor calculus for creating specialized editors for various programming languages while avoiding syntactical errors and supporting incomplete programs.", "motivation": "To develop a flexible editor calculus that can instantiate syntax-directed editors for any programming language based on its abstract syntax, thereby facilitating better programming environments.", "method": "The paper generalizes a syntax-directed editor calculus and encodes it into an extended simply typed lambda calculus, incorporating pairs, booleans, pattern matching, and fixed points.", "result": "The generalized calculus ensures syntactical correctness and allows for editing incomplete programs, enhancing the flexibility of syntax-directed editing.", "conclusion": "The work provides a foundational tool for building specialized editors that prevent syntactical errors while supporting the editing of partial code, which is common in programming.", "key_contributions": ["Generalized syntax-directed editor calculus", "Encoding in simply typed lambda calculus", "Support for incomplete programs in editing"], "limitations": "", "keywords": ["syntax-directed editing", "lambda calculus", "programming languages", "editor calculus", "syntactical errors"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799.pdf", "abs": "https://arxiv.org/abs/2505.18799", "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant costs, including constructing task-specific\ninstruction pairs and extensive training adjustments. Prior research has\nexplored various avenues to enhance alignment efficiency, primarily through\nminimal-data training or data-driven activations to identify key attention\nheads. However, these approaches inherently introduce data dependency, which\nhinders generalization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment.", "AI": {"tldr": "Proposes the ALPS algorithm for efficient LLM alignment by localizing and pruning task-sensitive attention heads, achieving improved performance with reduced training costs.", "motivation": "To improve alignment efficiency of LLMs while addressing data dependency issues that hinder generalizability to downstream tasks.", "method": "The ALPS algorithm localizes most task-sensitive attention heads and prunes training updates to only these heads, drastically reducing alignment costs.", "result": "ALPS activates 10% of attention parameters during fine-tuning, achieving a 2% performance improvement over baselines on three tasks, with transferable task-specific heads that reduce knowledge forgetting.", "conclusion": "ALPS offers a novel approach to enhance LLM alignment efficiency while being robust against knowledge retention across different datasets.", "key_contributions": ["Introduction of the ALPS algorithm for LLM alignment", "Significant reduction in training parameters required for alignment", "Demonstration of transferable task-specific attention heads."], "limitations": "The method's performance may vary across different LLM architectures and tasks, requiring further validation.", "keywords": ["Large Language Models", "Attention Mechanism", "Pruning Strategy", "Task Alignment", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842.pdf", "abs": "https://arxiv.org/abs/2505.18842", "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.", "AI": {"tldr": "A lightweight extension to MLLMs enables visual revisitation during inference, improving multimodal reasoning.", "motivation": "To enhance performance of Multimodal Large Language Models by allowing selective visual revisitation during inference.", "method": "Introduces a point-and-copy mechanism for dynamic retrieval of relevant image regions while reasoning, with minimal modifications to existing architectures.", "result": "Experiments on three benchmarks show that the new model significantly outperforms existing baselines, especially in tasks requiring fine-grained visual references and multi-step reasoning.", "conclusion": "Dynamic visual access is a promising approach for improving grounded multimodal reasoning in AI models.", "key_contributions": ["Introduction of a selective visual revisitation mechanism in MLLMs", "Construction of v1g dataset with 300K multimodal reasoning traces", "Improved performance on multimodal reasoning benchmarks"], "limitations": "", "keywords": ["multimodal", "large language models", "visual reasoning", "AI", "grounded reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18845", "pdf": "https://arxiv.org/pdf/2505.18845.pdf", "abs": "https://arxiv.org/abs/2505.18845", "title": "Multi-Party Conversational Agents: A Survey", "authors": ["Sagar Sapkota", "Mohammad Saqib Hasan", "Mubarak Shah", "Santu Karmaker"], "categories": ["cs.CL"], "comment": null, "summary": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in\ndialogue with more than two participants simultaneously. Unlike traditional\ntwo-party agents, designing MPCAs faces additional challenges due to the need\nto interpret both utterance semantics and social dynamics. This survey explores\nrecent progress in MPCAs by addressing three key questions: 1) Can agents model\neach participants' mental states? (State of Mind Modeling); 2) Can they\nproperly understand the dialogue content? (Semantic Understanding); and 3) Can\nthey reason about and predict future conversation flow? (Agent Action\nModeling). We review methods ranging from classical machine learning to Large\nLanguage Models (LLMs) and multi-modal systems. Our analysis underscores Theory\nof Mind (ToM) as essential for building intelligent MPCAs and highlights\nmulti-modal understanding as a promising yet underexplored direction. Finally,\nthis survey offers guidance to future researchers on developing more capable\nMPCAs.", "AI": {"tldr": "This survey explores the design of Multi-party Conversational Agents (MPCAs), focusing on mental state modeling, semantic understanding, and future conversation prediction.", "motivation": "To address the challenges in creating agents that can engage in multi-party dialogues by interpreting semantics and social dynamics.", "method": "The survey reviews various approaches including classical machine learning, Large Language Models (LLMs), and multi-modal systems in the context of MPCAs.", "result": "Identifies Theory of Mind (ToM) as crucial for MPCAs and highlights multi-modal understanding as an underexplored area.", "conclusion": "The paper guides future research in developing more capable MPCAs based on the survey findings.", "key_contributions": ["Introduces the necessity of Theory of Mind for MPCAs.", "Explores the importance of mental state modeling and semantic understanding.", "Suggests directions for future research in multi-modal understanding."], "limitations": "Focuses primarily on theoretical insights and does not provide extensive real-world applications or empirical studies.", "keywords": ["Multi-party Conversational Agents", "Theory of Mind", "Large Language Models", "Semantic Understanding", "Multi-modal Systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18853", "pdf": "https://arxiv.org/pdf/2505.18853.pdf", "abs": "https://arxiv.org/abs/2505.18853", "title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation", "authors": ["Alexander Shabalin", "Viacheslav Meshchaninov", "Dmitry Vetrov"], "categories": ["cs.CL"], "comment": "17 pages, 2 figures, 8 tables", "summary": "Diffusion models have achieved state-of-the-art performance in generating\nimages, audio, and video, but their adaptation to text remains challenging due\nto its discrete nature. Prior approaches either apply Gaussian diffusion in\ncontinuous latent spaces, which inherits semantic structure but struggles with\ntoken decoding, or operate in categorical simplex space, which respect\ndiscreteness but disregard semantic relation between tokens. In this paper, we\npropose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion\nmethod that combines the strengths of both approaches by progressively\nsmoothing token embeddings based on semantic similarity. This technique enables\ngradual information removal while maintaining a natural decoding process.\nExperimental results on several sequence-to-sequence generation tasks\ndemonstrate that Smoothie outperforms existing diffusion-based models in\ngeneration quality. Furthermore, ablation studies show that our proposed\ndiffusion space yields better performance than both the standard embedding\nspace and the categorical simplex. Our code is available at\nhttps://github.com/ashaba1in/smoothie.", "AI": {"tldr": "Smoothie introduces a novel diffusion model for text generation that blends continuous and categorical approaches to enhance semantic representation and decoding quality.", "motivation": "Despite their success in generating images and audio, adapting diffusion models for text generation remains a challenge because textual data is discrete in nature.", "method": "The proposed method, Smoothing Diffusion on Token Embeddings (Smoothie), progressively smooths token embeddings based on semantic similarity, allowing for better information management during decoding.", "result": "Experimental results indicate that Smoothie outperforms existing diffusion-based models in terms of generation quality across various sequence-to-sequence tasks.", "conclusion": "The proposed method provides a significant improvement in text generation, combining the advantages of both continuous latent spaces and categorical spaces.", "key_contributions": ["Introduction of Smoothing Diffusion on Token Embeddings (Smoothie)", "Demonstration of improved generation quality over existing models", "Ablation studies that confirm the efficacy of the new diffusion space"], "limitations": "", "keywords": ["diffusion models", "text generation", "token embeddings", "semantic similarity", "sequence-to-sequence"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18859", "pdf": "https://arxiv.org/pdf/2505.18859.pdf", "abs": "https://arxiv.org/abs/2505.18859", "title": "Writing Like the Best: Exemplar-Based Expository Text Generation", "authors": ["Yuxiang Liu", "Kevin Chen-Chuan Chang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025. Camera-ready version", "summary": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task.", "AI": {"tldr": "This paper presents a new framework called RePA for generating expository texts based on exemplars, aiming to improve coherence and relevance while addressing limitations in existing methodologies.", "motivation": "Current methodologies in text generation struggle with the coherence and relevance of output texts, particularly when adapting content from exemplars on similar topics.", "method": "The paper introduces the Recurrent Plan-then-Adapt (RePA) framework, which utilizes large language models to implement adaptive imitation through a detailed planning and adaptation process. The approach incorporates recurrent segment-by-segment imitation supported by memory structures for clarity and coherence.", "result": "Experimental results show that the RePA framework outperforms previous baselines across three diverse datasets, producing texts that are more factual, consistent, and relevant.", "conclusion": "The proposed RePA framework effectively enhances the process of expository text generation using exemplars, achieving improved adaptability and coherence.", "key_contributions": ["Introduction of the Exemplar-Based Expository Text Generation task", "Development of the Recurrent Plan-then-Adapt (RePA) framework", "Creation of new evaluation metrics for text generation tasks"], "limitations": "", "keywords": ["expository text generation", "adaptive imitation", "large language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18864", "pdf": "https://arxiv.org/pdf/2505.18864.pdf", "abs": "https://arxiv.org/abs/2505.18864", "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework", "authors": ["Binhao Ma", "Hanqing Guo", "Zhengping Jay Luo", "Rui Duan"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the naturalness and flexibility of human computer\ninteraction by enabling seamless understanding across text, vision, and audio\nmodalities. Among these, voice enabled models such as SpeechGPT have\ndemonstrated considerable improvements in usability, offering expressive, and\nemotionally responsive interactions that foster deeper connections in real\nworld communication scenarios. However, the use of voice introduces new\nsecurity risks, as attackers can exploit the unique characteristics of spoken\nlanguage, such as timing, pronunciation variability, and speech to text\ntranslation, to craft inputs that bypass defenses in ways not seen in\ntext-based systems. Despite substantial research on text based jailbreaks, the\nvoice modality remains largely underexplored in terms of both attack strategies\nand defense mechanisms. In this work, we present an adversarial attack\ntargeting the speech input of aligned MLLMs in a white box scenario.\nSpecifically, we introduce a novel token level attack that leverages access to\nthe model's speech tokenization to generate adversarial token sequences. These\nsequences are then synthesized into audio prompts, which effectively bypass\nalignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,\nour approach achieves up to 89 percent attack success rate across multiple\nrestricted tasks, significantly outperforming existing voice based jailbreak\nmethods. Our findings shed light on the vulnerabilities of voice-enabled\nmultimodal systems and to help guide the development of more robust\nnext-generation MLLMs.", "AI": {"tldr": "This paper presents a novel adversarial attack targeting the speech input of Multimodal Large Language Models (MLLMs), particularly focusing on vulnerabilities in voice-enabled systems like SpeechGPT.", "motivation": "To address the underexplored security risks of voice inputs in MLLMs, especially in light of significant advances in human-computer interaction.", "method": "A novel token level adversarial attack that utilizes the model's speech tokenization to create adversarial sequences, which are synthesized into audio prompts.", "result": "Achieved up to 89% attack success rate across multiple restricted tasks on SpeechGPT, outperforming existing voice-based jailbreak methods.", "conclusion": "The findings highlight the vulnerabilities of voice-enabled multimodal systems and provide insights for developing more robust MLLMs.", "key_contributions": ["Introduction of a novel token level attack on speech input in MLLMs", "Demonstration of high attack success rates on SpeechGPT", "Identification of security vulnerabilities specific to voice modality in MLLMs"], "limitations": "The focus is primarily on SpeechGPT and may not generalize to all MLLMs or different contexts of voice interaction.", "keywords": ["Multimodal Large Language Models", "adversarial attack", "voice-enabled systems", "SpeechGPT", "security vulnerabilities"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.18867", "pdf": "https://arxiv.org/pdf/2505.18867.pdf", "abs": "https://arxiv.org/abs/2505.18867", "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing", "authors": ["Ming Cheng", "Jiaying Gong", "Hoda Eldardiry"], "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, ACL 2025 Findings", "summary": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing.", "AI": {"tldr": "Sci-LoRA is a model designed for lay paraphrasing that dynamically integrates knowledge from multiple scientific domains to enhance accessibility of information.", "motivation": "The need for interdisciplinary understanding in lay paraphrasing, as most existing studies focus on single domains, creating barriers for non-experts.", "method": "Sci-LoRA uses a mixture of LoRAs fine-tuned on multiple scientific domains and dynamically adjusts the weight of each based on the input text, allowing it to balance domain-specific knowledge and generalization.", "result": "Sci-LoRA significantly outperforms state-of-the-art large language models across twelve domains on five public datasets, demonstrating improved adaptability and performance in lay paraphrasing.", "conclusion": "The dynamic fusion of domain-specific and general knowledge in Sci-LoRA provides a robust solution for cross-domain lay paraphrasing, making scientific information more accessible.", "key_contributions": ["Introduction of the Sci-LoRA model for interdisciplinary lay paraphrasing.", "Dynamic weighting of LoRAs for improved performance across different scientific domains.", "Demonstrated ability to generalize and adapt without explicit domain labels."], "limitations": "The performance may still vary based on the complexity of specific domains and input text characteristics.", "keywords": ["lay paraphrasing", "interdisciplinary research", "Sci-LoRA", "large language models", "cross-domain adaptability"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18878", "pdf": "https://arxiv.org/pdf/2505.18878.pdf", "abs": "https://arxiv.org/abs/2505.18878", "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition.", "AI": {"tldr": "CRMArena-Pro is a novel benchmark for assessing LLM agents in business settings, revealing significant performance gaps in multi-turn interactions and confidentiality awareness.", "motivation": "AI agents have transformative potential in business, but current performance benchmarks lack realism and diversity due to limited public data.", "method": "Introduced CRMArena-Pro with 19 expert-validated tasks spanning various business scenarios, incorporating multi-turn interactions and confidentiality assessments.", "result": "Leading LLM agents achieve around 58% single-turn success, dropping to 35% in multi-turn settings; top agents excel in Workflow Execution with over 83% single-turn success but struggle with other tasks.", "conclusion": "The substantial gap between LLM capabilities and enterprise demands highlights the need for advancements in multi-turn reasoning, confidentiality awareness, and skill acquisition.", "key_contributions": ["Introduction of CRMArena-Pro as a benchmark for LLM agents in diverse business contexts.", "Inclusion of multi-turn interactions and diverse personas for realistic assessments.", "Findings underscore the performance limitations of current LLM agents in enterprise applications."], "limitations": "Agents exhibit near-zero inherent confidentiality awareness; prompting improvements can compromise task performance.", "keywords": ["LLM agents", "benchmark", "business scenarios", "multi-turn interactions", "confidentiality awareness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.18903", "pdf": "https://arxiv.org/pdf/2505.18903.pdf", "abs": "https://arxiv.org/abs/2505.18903", "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos", "authors": ["Valentin Barriere", "Nahuel Gomez", "Leo Hemamou", "Sofia Callejas", "Brian Ravenet"], "categories": ["cs.CL"], "comment": null, "summary": "Aiming towards improving current computational models of humor detection, we\npropose a new multimodal dataset of stand-up comedies, in seven languages:\nEnglish, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset\nof more than 330 hours, is at the time of writing the biggest available for\nthis type of task, and the most diverse. The whole dataset is automatically\nannotated in laughter (from the audience), and the subpart left for model\nvalidation is manually annotated. Contrary to contemporary approaches, we do\nnot frame the task of humor detection as a binary sequence classification, but\nas word-level sequence labeling, in order to take into account all the context\nof the sequence and to capture the continuous joke tagging mechanism typically\noccurring in natural conversations. As par with unimodal baselines results, we\npropose a method for e propose a method to enhance the automatic laughter\ndetection based on Audio Speech Recognition errors. Our code and data are\navailable online: https://tinyurl.com/EMNLPHumourStandUpPublic", "AI": {"tldr": "A new multimodal dataset for humor detection in stand-up comedy across seven languages, accounting for audience laughter and proposing a novel word-level sequence labeling approach.", "motivation": "To improve computational models of humor detection by providing a diverse and extensive dataset.", "method": "A new multimodal dataset is created, annotated for audience laughter, and the task is framed as word-level sequence labeling instead of binary classification.", "result": "The dataset of over 330 hours is the largest for humor detection and shows improved results in capturing the context of humor.", "conclusion": "The proposed method highlights the need for word-level analysis in humor detection and facilitates better modeling of laughter in conversations.", "key_contributions": ["Creation of a multimodal dataset in seven languages.", "Introduction of word-level sequence labeling for humor detection.", "Method to enhance automatic laughter detection based on speech recognition errors."], "limitations": "", "keywords": ["humor detection", "multimodal dataset", "sequence labeling", "audience laughter", "stand-up comedy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18905", "pdf": "https://arxiv.org/pdf/2505.18905.pdf", "abs": "https://arxiv.org/abs/2505.18905", "title": "Building a Functional Machine Translation Corpus for Kpelle", "authors": ["Kweku Andoh Yamoah", "Jackson Weako", "Emmanuel J. Dorley"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce the first publicly available English-Kpelle\ndataset for machine translation, comprising over 2000 sentence pairs drawn from\neveryday communication, religious texts, and educational materials. By\nfine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the\ndataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English\ndirection, demonstrating the benefits of data augmentation. Our findings align\nwith NLLB-200 benchmarks on other African languages, underscoring Kpelle's\npotential for competitive performance despite its low-resource status. Beyond\nmachine translation, this dataset enables broader NLP tasks, including speech\nrecognition and language modelling. We conclude with a roadmap for future\ndataset expansion, emphasizing orthographic consistency, community-driven\nvalidation, and interdisciplinary collaboration to advance inclusive language\ntechnology development for Kpelle and other low-resourced Mande languages.", "AI": {"tldr": "Introduction of a new English-Kpelle dataset for machine translation, achieving competitive results.", "motivation": "To address the lack of resources for Kpelle language and improve machine translation capabilities.", "method": "Fine-tuning Meta's No Language Left Behind model on a dataset of over 2000 English-Kpelle sentence pairs.", "result": "Achieved BLEU scores up to 30 in Kpelle-to-English direction, demonstrating the potential of Kpelle for competitive NLP performance.", "conclusion": "The dataset allows for future expansion and improvement of inclusive language technology for Kpelle and other low-resourced languages.", "key_contributions": ["First English-Kpelle dataset for machine translation", "Data augmentation leads to improved translation performance", "Roadmap for future dataset expansion and collaboration"], "limitations": "", "keywords": ["machine translation", "Kpelle", "low-resource languages", "NLP", "data augmentation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18906", "pdf": "https://arxiv.org/pdf/2505.18906.pdf", "abs": "https://arxiv.org/abs/2505.18906", "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Vivek Gupta"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems.", "AI": {"tldr": "This paper presents a systematic mapping study of Federated Retrieval-Augmented Generation, analyzing its architecture, trends, and challenges, and its implications for privacy-sensitive applications in NLP.", "motivation": "With the increasing deployment of large language models in sensitive domains like healthcare, combining Federated Learning with Retrieval-Augmented Generation offers a secure framework for NLP.", "method": "The authors conducted a systematic mapping study of literature from 2020 to 2025, applying Kitchenham's guidelines to categorize research focuses, contributions, and application domains.", "result": "The study identifies architectural patterns, trends, and challenges, including privacy-preserving retrieval and cross-client heterogeneity, laying groundwork for future research in Federated RAG.", "conclusion": "Federated RAG is a promising framework for enhancing knowledge-intensive NLP while ensuring privacy, and there are open questions that require further exploration.", "key_contributions": ["First systematic mapping study of Federated RAG", "Structured classification of research focuses and application domains", "Identification of key challenges and trends in the field."], "limitations": "The study may not cover all relevant literature outside the specified date range and focuses primarily on theoretical aspects rather than empirical evaluations.", "keywords": ["Federated Learning", "Retrieval-Augmented Generation", "Natural Language Processing", "Privacy", "Systematic Mapping"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18916", "pdf": "https://arxiv.org/pdf/2505.18916.pdf", "abs": "https://arxiv.org/abs/2505.18916", "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media", "authors": ["Yue Li", "Jake Vasilakes", "Zhixue Zhao", "Carolina Scarton"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce SCRum-9, a multilingual dataset for Rumour Stance\nClassification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond\nexisting stance classification datasets by covering more languages (9), linking\nexamples to more fact-checked claims (2.1k), and including complex annotations\nfrom multiple annotators to account for intra- and inter-annotator variability.\nAnnotations were made by at least three native speakers per language, totalling\naround 405 hours of annotation and 8,150 dollars in compensation. Experiments\non SCRum-9 show that it is a challenging benchmark for both state-of-the-art\nLLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating\nfuture work in this area.", "AI": {"tldr": "SCRum-9 is a multilingual dataset for Rumour Stance Classification, comprising 7,516 tweet-reply pairs across 9 languages, with complex annotations from native speakers.", "motivation": "To improve existing stance classification datasets by incorporating a broader linguistic range and linking to fact-checked claims.", "method": "Created a multilingual dataset consisting of tweet-reply pairs annotated by native speakers, with thorough cross-linguistic and complex annotations collected over 405 hours.", "result": "The SCRum-9 dataset serves as a challenging benchmark for leading LLMs and fine-tuned models, indicating significant room for improvement in rumour stance classification.", "conclusion": "The dataset highlights the challenges in stance classification and encourages future research in utilizing multilingual data for improved model performance.", "key_contributions": ["Introduction of a multilingual dataset for rumour stance classification.", "Linking examples to fact-checked claims.", "In-depth annotations to address annotator variability."], "limitations": "", "keywords": ["Rumour Stance Classification", "Multilingual Dataset", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927.pdf", "abs": "https://arxiv.org/abs/2505.18927", "title": "Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments", "authors": ["Amel Muminovic"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation.", "AI": {"tldr": "This study evaluates the performance of three large language models in identifying harmful comments in YouTube threads, finding strengths and weaknesses in their precision and recall.", "motivation": "Addressing harassment in online comment sections is crucial for improving user experience and well-being.", "method": "Benchmarking of GPT-4.1, Gemini 1.5 Pro, and Claude 3 Opus on a dataset of 5,080 YouTube comments categorized as harmful or non-harmful.", "result": "GPT-4.1 displayed the best overall balance (F1 score: 0.863), Gemini flagged the most harmful posts but had lower precision, and Claude had the highest precision but the lowest recall.", "conclusion": "The study highlights the need for improved moderation models that combine the strengths of different approaches and adapt to diverse languages and contexts.", "key_contributions": ["Evaluation of LLMs on automated content moderation", "Public release of a dataset for further research", "Insights into the challenges of sarcasm and mixed-language content in moderation"], "limitations": "Models struggled with coded insults, sarcasm, and slang, indicating gaps in their capabilities.", "keywords": ["automated moderation", "large language models", "toxic comment detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18943", "pdf": "https://arxiv.org/pdf/2505.18943.pdf", "abs": "https://arxiv.org/abs/2505.18943", "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Min-Hsuan Yeh", "Yixuan Li"], "categories": ["cs.CL"], "comment": null, "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses user mental states (e.g., intent,\nemotion), (2) a Domain Agent refines these hypotheses using cultural norms and\nethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind.", "AI": {"tldr": "MetaMind is a multi-agent framework designed to enhance large language models' ability to understand human social interactions by emulating cognitive processes of Theory of Mind (ToM).", "motivation": "To improve large language models' performance in understanding and responding to human social cues, which are complex and nuanced compared to traditional semantic tasks.", "method": "MetaMind uses three collaborative agents: a Theory-of-Mind Agent for hypothesizing user mental states, a Domain Agent for refining these hypotheses based on cultural norms and ethical constraints, and a Response Agent for generating contextually appropriate replies while aligning with inferred intents.", "result": "The framework achieves a 35.7% improvement in real-world social scenarios and a 6.2% gain in ToM reasoning, enabling LLMs to match human-level performance on ToM tasks.", "conclusion": "MetaMind advances AI systems towards human-like social intelligence and has potential applications in areas such as empathetic dialogue and culturally sensitive interactions.", "key_contributions": ["Introduces a novel multi-agent framework for social reasoning in LLMs.", "Achieves state-of-the-art performance on benchmarks related to Theory of Mind and social interactions.", "Demonstrates the capability of LLMs to perform at human-level competency in understanding social cues."], "limitations": "", "keywords": ["Theory of Mind", "MetaMind", "social reasoning", "large language models", "empathetic dialogue"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949.pdf", "abs": "https://arxiv.org/abs/2505.18949", "title": "The Price of Format: Diversity Collapse in LLMs", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning.", "AI": {"tldr": "This paper investigates the issue of diversity collapse in instruction-tuned LLMs, revealing that structured templates can limit output variability, and proposes the need for diversity-aware prompt design.", "motivation": "The research is motivated by the need to understand how formatting constraints in instruction-tuned LLMs affect their creativity and variability in output.", "method": "The authors evaluate the diversity collapse phenomenon by fine-tuning LLMs with structured prompts and assessing their performance on multiple tasks. They analyze the impact of structural tokens on output diversity and task alignment.", "result": "The study finds that diversity collapse occurs even with high-temperature sampling, with structural tokens severely limiting output space. Format consistency is critical for certain tasks, while output diversity is enhanced with minimal formatting.", "conclusion": "The findings highlight the trade-off between alignment and output diversity in LLMs, suggesting a need for reformulating prompt design to improve diversity without compromising performance.", "key_contributions": ["Identification of diversity collapse in instruction-tuned LLMs", "Systematic evaluation of the impact of structural tokens on output diversity", "Recommendations for diversity-aware prompt design"], "limitations": "", "keywords": ["large language models", "diversity collapse", "prompt design", "instruction tuning", "output variability"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2505.18951", "pdf": "https://arxiv.org/pdf/2505.18951.pdf", "abs": "https://arxiv.org/abs/2505.18951", "title": "BnMMLU: Measuring Massive Multitask Language Understanding in Bengali", "authors": ["Saman Sarker Joy"], "categories": ["cs.CL"], "comment": "18 pages, 9 figures, 5 tables; Code & dataset available at\n  https://github.com/samanjoy2/bnmmlu", "summary": "The Massive Multitask Language Understanding (MMLU) benchmark has been widely\nused to evaluate language models across various domains. However, existing MMLU\ndatasets primarily focus on high-resource languages such as English, which\nleaves low-resource languages like Bengali underrepresented. In this paper, we\nintroduce BnMMLU, a benchmark to evaluate the multitask language understanding\ncapabilities of Bengali in language models. The dataset spans 23 domains,\nincluding science, humanities, mathematics and general knowledge and is\nstructured in a multiple-choice format to assess factual knowledge,\napplication-based problem-solving and reasoning abilities of language models.\nIt consists of 138,949 question-option pairs. We benchmark several proprietary\nand open-source large language models (LLMs) on the BnMMLU test set.\nAdditionally, we annotate the test set with three cognitive categories-factual\nknowledge, procedural application and reasoning-to gain deeper insights into\nmodel strengths and weaknesses across various cognitive tasks. The results\nreveal significant performance gaps, highlighting the need for improved\npre-training and fine-tuning strategies tailored to Bengali data. We release\nthe dataset and benchmark results to facilitate further research in this area.", "AI": {"tldr": "Introduction of a benchmark for evaluating language models on multitask understanding in Bengali, revealing performance gaps in existing models.", "motivation": "Existing MMLU datasets focus on high-resource languages, leaving low-resource languages like Bengali underrepresented.", "method": "Introduction of BnMMLU, a benchmark comprised of 138,949 question-option pairs across 23 domains, structured in multiple-choice format; benchmarking of various LLMs on this dataset.", "result": "Significant performance gaps observed in LLMs tested on the BnMMLU benchmark, indicating the need for better pre-training and fine-tuning for Bengali data.", "conclusion": "The BnMMLU dataset and results emphasize the necessity for tailored language model strategies for low-resource languages like Bengali and are released for further research.", "key_contributions": ["Introduction of BnMMLU benchmark for Bengali language models", "Dataset includes 138,949 question-option pairs across 23 domains", "Highlighting performance gaps in existing LLMs for low-resource languages"], "limitations": "Focus primarily on Bengali, may not generalize to other low-resource languages; results are specific to tested LLMs.", "keywords": ["Bengali", "language models", "benchmarking", "multitask language understanding", "low-resource languages"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.18953", "pdf": "https://arxiv.org/pdf/2505.18953.pdf", "abs": "https://arxiv.org/abs/2505.18953", "title": "Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?", "authors": ["Divij Chawla", "Ashita Bhutada", "Do Duc Anh", "Abhinav Raghunathan", "Vinod SP", "Cathy Guo", "Dar Win Liew", "Prannaya Gupta", "Rishabh Bhardwaj", "Rajat Bhardwaj", "Soujanya Poria"], "categories": ["cs.CL"], "comment": null, "summary": "We evaluate the credibility of leading AI models in assessing investment risk\nappetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and\nopen-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720\nuser profiles constructed with 16 risk-relevant features across 10 countries\nand both genders. We observe significant variance across models in score\ndistributions and demographic sensitivity. For example, GPT-4o assigns higher\nrisk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show\nopposite gender tendencies in risk classification. While some models (e.g.,\nGPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk\nranges, none maintain consistent performance across regions and demographics.\nOur findings highlight the need for rigorous, standardized evaluations of AI\nsystems in regulated financial contexts to prevent bias, opacity, and\ninconsistency in real-world deployment.", "AI": {"tldr": "The paper evaluates AI models' credibility in assessing investment risk appetite across diverse user profiles and highlights significant variance and biases in their performance.", "motivation": "To address the inconsistency and potential biases in AI models used for investment risk assessments, particularly in regulated financial contexts.", "method": "The study analyzes proprietary and open-weight AI models using 1,720 user profiles with 16 risk-relevant features across 10 countries and genders.", "result": "The analysis reveals significant disparities in risk score distributions and model behavior, with certain models displaying demographic biases and inconsistent performance across different regions.", "conclusion": "There is a critical need for rigorous evaluations of AI systems in finance to mitigate risks of bias and ensure reliable deployment.", "key_contributions": ["Evaluation of multiple AI models in investment risk assessment", "Identification of demographic biases in AI risk scoring", "Recommendation for standardized evaluations in financial AI applications"], "limitations": "The study does not explore the underlying reasons for the observed biases and inconsistencies in AI model performances.", "keywords": ["AI models", "investment risk", "demographic bias", "financial applications", "standardized evaluation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.18962", "pdf": "https://arxiv.org/pdf/2505.18962.pdf", "abs": "https://arxiv.org/abs/2505.18962", "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space.Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage.", "AI": {"tldr": "This paper proposes System-1.5 Reasoning, an adaptive reasoning framework for large language models that improves computational efficiency by dynamically allocating computation across reasoning steps in latent space.", "motivation": "Existing chain-of-thought reasoning methods are inefficient due to verbose outputs and uniform treatment of reasoning steps. This paper seeks to enhance efficiency by better utilizing computational resources through an adaptive framework.", "method": "The System-1.5 Reasoning framework introduces dynamic shortcuts, including a model depth shortcut that allows critical reasoning to proceed through deeper layers while skipping non-critical tokens, and a step shortcut that reuses hidden states to skip trivial steps.", "result": "System-1.5 Reasoning shows superior performance on reasoning tasks like GSM8K, achieving performance comparable to traditional methods while speeding up inference by over 20x and reducing token generation by 92.31%.", "conclusion": "The proposed System-1.5 Reasoning framework represents a significant step forward in adapting reasoning efficiency for large language models, enabling more effective use of computational resources.", "key_contributions": ["Introduction of the System-1.5 Reasoning framework for adaptive reasoning in LLMs", "Implementation of depth and step shortcuts for improved computational efficiency", "Demonstrated 20x speed-up and significant reduction in token generation for reasoning tasks"], "limitations": "The work is still in progress, and further validation and testing of the framework may be required.", "keywords": ["large language models", "adaptive reasoning", "efficiency", "latent-space", "shortcut paths"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.18970", "pdf": "https://arxiv.org/pdf/2505.18970.pdf", "abs": "https://arxiv.org/abs/2505.18970", "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM Classification", "authors": ["Bowen Wei", "Ziwei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications.", "AI": {"tldr": "ProtoSurE is a prototype-based framework that provides faithful and understandable explanations for large language models, outperforming existing methods across various datasets.", "motivation": "To improve the transparency and understandability of large language models' decision-making processes, which are often opaque and difficult to explain using current methods.", "method": "ProtoSurE utilizes a prototype-based surrogate model that aligns with target LLMs, employing sentence-level prototypes to enhance human comprehension of model outputs.", "result": "ProtoSurE outperforms state-of-the-art explanation methods in terms of faithfulness and understandability while demonstrating strong data efficiency with fewer training examples needed for effective performance.", "conclusion": "ProtoSurE presents a practical solution for enhancing the interpretability of LLMs, making it suitable for real-world applications where explainability is crucial.", "key_contributions": ["Introduction of ProtoSurE, a novel prototype-based explanation framework for LLMs", "Demonstration of improved performance over existing explanation methods", "Showcasing of strong data efficiency for real-world applicability"], "limitations": "", "keywords": ["large language models", "explanations", "prototype-based", "interpretability", "surrogate model"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.18971", "pdf": "https://arxiv.org/pdf/2505.18971.pdf", "abs": "https://arxiv.org/abs/2505.18971", "title": "Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Ashutosh Balasubramaniam", "Tejas Anvekar", "Vivek Gupta"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We revisit the efficacy of simple, real-valued embedding models for knowledge\ngraph completion and introduce RelatE, an interpretable and modular method that\nefficiently integrates dual representations for entities and relations. RelatE\nemploys a real-valued phase-modulus decomposition, leveraging sinusoidal phase\nalignments to encode relational patterns such as symmetry, inversion, and\ncomposition. In contrast to recent approaches based on complex-valued\nembeddings or deep neural architectures, RelatE preserves architectural\nsimplicity while achieving competitive or superior performance on standard\nbenchmarks. Empirically, RelatE outperforms prior methods across several\ndatasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,\nsurpassing all baselines. Additionally, RelatE offers significant efficiency\ngains, reducing training time by 24%, inference latency by 31%, and peak GPU\nmemory usage by 22% compared to RotatE. Perturbation studies demonstrate\nimproved robustness, with MRR degradation reduced by up to 61% relative to\nTransE and by up to 19% compared to RotatE under structural edits such as edge\nremovals and relation swaps. Formal analysis further establishes the model's\nfull expressiveness and its capacity to represent essential first-order logical\ninference patterns. These results position RelatE as a scalable and\ninterpretable alternative to more complex architectures for knowledge graph\ncompletion.", "AI": {"tldr": "RelatE is a modular method for knowledge graph completion that uses real-valued embedding models, offering significant efficiency and performance benefits over existing methods.", "motivation": "The paper aims to improve knowledge graph completion methods, focusing on simplicity and interpretability while maintaining competitive performance.", "method": "RelatE integrates dual representations for entities and relations using real-valued phase-modulus decomposition to encode relational patterns.", "result": "RelatE achieves superior performance on standard benchmarks, including a MRR of 0.521 on YAGO3-10, while reducing training and inference times and improving robustness against structural edits.", "conclusion": "RelatE is positioned as a scalable and interpretable alternative to complex architectures in knowledge graph completion.", "key_contributions": ["Introduction of RelatE model for knowledge graph completion", "Achieves competitive performance with architectural simplicity", "Significant efficiency gains over previous methods"], "limitations": "", "keywords": ["knowledge graph completion", "embedding models", "RelatE", "efficiency", "interpretability"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973.pdf", "abs": "https://arxiv.org/abs/2505.18973", "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail.", "AI": {"tldr": "The paper introduces Hierarchical Mamba (HiM), a model that integrates hyperbolic geometry to enhance language representation for complex hierarchical reasoning tasks.", "motivation": "To improve language representation in hierarchical reasoning tasks, where traditional models with flat Euclidean embeddings fall short.", "method": "The HiM model utilizes efficient Mamba2 alongside hyperbolic geometry to create hierarchy-aware language embeddings, leveraging tangent-based and cosine/sine-based mappings to optimize language modeling across hierarchical levels.", "result": "HiM models outperform Euclidean baselines in capturing hierarchical relationships, demonstrating better performance in mixed-hop prediction and multi-hop inference tasks on linguistic and medical datasets.", "conclusion": "HiM provides a more robust framework for understanding complex hierarchical structures in language, applicable to various inference tasks.", "key_contributions": ["Introduction of Hierarchical Mamba (HiM) for modeling hierarchical language embeddings.", "Utilization of hyperbolic geometry for enhanced distance representation in language tasks.", "Demonstration of superior performance in mixed-hop prediction and multi-hop inference over traditional models."], "limitations": "", "keywords": ["hierarchical models", "language representation", "hyperbolic geometry", "machine learning", "multi-hop inference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.18978", "pdf": "https://arxiv.org/pdf/2505.18978.pdf", "abs": "https://arxiv.org/abs/2505.18978", "title": "AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models", "authors": ["Miguel Angel Peñaloza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Michelle Bruno Hernandez", "Miguel Angel Alvarado Gonzalez", "Sandra Malagon"], "categories": ["cs.CL", "68", "I.2"], "comment": "36 pages, 5 figures", "summary": "Existing mathematical reasoning benchmarks are predominantly English only or\ntranslation-based, which can introduce semantic drift and mask languagespecific\nreasoning errors. To address this, we present AI4Math, a benchmark of 105\noriginal university level math problems natively authored in Spanish. The\ndataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,\nNumber Theory, Combinatorics, and Logic), and each problem is accompanied by a\nstep by step human solution. We evaluate six large language models GPT 4o, GPT\n4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under\nfour configurations: zero shot and chain of thought, each in Spanish and\nEnglish. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve\nover 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most\nmodels show no significant performance drop between languages, with GPT 4o even\nperforming better on Spanish problems in the zero shot setting. Geometry,\nCombinatorics, and Probability questions remain persistently challenging for\nall models. These results highlight the need for native-language benchmarks and\ndomain-specific evaluations to reveal reasoning failures not captured by\nstandard metrics.", "AI": {"tldr": "AI4Math is a benchmark of 105 original math problems in Spanish that evaluates various language models' performance.", "motivation": "To create a benchmark that addresses the shortcomings of existing English-only or translation-based mathematical reasoning assessments, which can obscure language-specific errors.", "method": "The study presents a dataset of 105 university-level math problems across seven domains, evaluated using six large language models under zero shot and chain of thought configurations, in both Spanish and English.", "result": "Top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve over 70% accuracy; LLaMA 3.3 70B and GPT-4o mini score below 40%. Most models perform consistently across languages, with some showing better results on Spanish problems.", "conclusion": "The results indicate a pressing need for native-language benchmarks to uncover reasoning failures that standard evaluations tend to miss.", "key_contributions": ["Introduction of AI4Math, a Spanish-native math problem benchmark", "Evaluation of diverse language models on the benchmark", "Insights into reasoning capabilities across languages and domains"], "limitations": "The study is focused only on university-level problems and does not address lower educational levels or all mathematical domains.", "keywords": ["math benchmark", "language models", "Spanish math problems", "AI4Math", "domain-specific evaluation"], "importance_score": 6, "read_time_minutes": 36}}
{"id": "2505.18995", "pdf": "https://arxiv.org/pdf/2505.18995.pdf", "abs": "https://arxiv.org/abs/2505.18995", "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)", "authors": ["Carlos Jude G. Maminta", "Isaiah Job Enriquez", "Deandre Nigel Nunez", "Michael B. Dela Fuente"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study presents FiLLM, a Filipino-optimized large language model,\ndesigned to enhance natural language processing (NLP) capabilities in the\nFilipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank\nAdaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining\ntask-specific performance. The model was trained and evaluated on diverse\nFilipino datasets to address key NLP tasks, including Named Entity Recognition\n(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text\nSummarization. Performance comparisons with the CalamanCy model were conducted\nusing F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap\nmetrics. Results indicate that Calamancy outperforms FILLM in several aspects,\ndemonstrating its effectiveness in processing Filipino text with improved\nlinguistic comprehension and adaptability. This research contributes to the\nadvancement of Filipino NLP applications by providing an optimized, efficient,\nand scalable language model tailored for local linguistic needs.", "AI": {"tldr": "FiLLM is a Filipino-optimized large language model enhancing NLP in the Filipino language, leveraging LoRA fine-tuning for memory efficiency.", "motivation": "To enhance natural language processing capabilities in the Filipino language through an optimized model.", "method": "FiLLM was developed based on the SeaLLM-7B 2.5 model with LoRA fine-tuning and trained on diverse Filipino datasets for various NLP tasks.", "result": "Performance comparisons with the CalamanCy model showed that Calamancy outperformed FiLLM in several NLP tasks, suggesting its superior linguistic comprehension.", "conclusion": "The study contributes to Filipino NLP advancements by offering an efficient and scalable language model for local linguistic needs.", "key_contributions": ["Development of FiLLM for Filipino NLP tasks", "Use of Low-Rank Adaptation for memory efficiency", "Performance evaluation against the CalamanCy model"], "limitations": "FiLLM did not outperform CalamanCy in several key performance metrics.", "keywords": ["Filipino language model", "Natural Language Processing", "Low-Rank Adaptation", "NLP tasks", "Machine Learning"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000.pdf", "abs": "https://arxiv.org/abs/2505.19000", "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability.", "AI": {"tldr": "VerIPO is a new method for improving video LLMs' reasoning capabilities by efficiently combining reinforcement learning strategies to generate high-quality reasoning chains.", "motivation": "Existing Reinforcement Fine-Tuning methods for video LLMs suffer from data preparation issues and unstable performance, hindering effective video reasoning.", "method": "VerIPO employs a Verifier-guided Iterative Policy Optimization process, incorporating a Rollout-Aware Verifier to produce better contrastive data for training video LLMs.", "result": "Experimental results show that VerIPO optimizes video LLMs 7x faster than standard methods and achieves superior performance in generating long, consistent reasoning chains.", "conclusion": "VerIPO provides a novel framework that combines the strengths of GRPO and DPO, resulting in more effective training and enhanced reasoning capabilities of video LLMs.", "key_contributions": ["Introduction of VerIPO for training video LLMs", "Use of Rollout-Aware Verifier to create high-quality training data", "Demonstrated superior optimization speed and reasoning performance over existing methods"], "limitations": "", "keywords": ["Reinforcement Learning", "Video Large Language Models", "Policy Optimization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19018", "pdf": "https://arxiv.org/pdf/2505.19018.pdf", "abs": "https://arxiv.org/abs/2505.19018", "title": "CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "Md. Rajib Hossain", "Md. Saifur Rahman", "A. B. M. Shawkat Ali"], "categories": ["cs.CL"], "comment": null, "summary": "Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural\nlanguage processing, offering fine-grained insights into opinions expressed in\ntext. While existing research has largely focused on resource-rich languages\nlike English which leveraging large annotated datasets, pre-trained models, and\nlanguage-specific tools. These resources are often unavailable for low-resource\nlanguages such as Bengali. The ABSA task in Bengali remains poorly explored and\nis further complicated by its unique linguistic characteristics and a lack of\nannotated data, pre-trained models, and optimized hyperparameters. To address\nthese challenges, this research propose CrosGrpsABS, a novel hybrid framework\nthat leverages bidirectional cross-attention between syntactic and semantic\ngraphs to enhance aspect-level sentiment classification. The CrosGrpsABS\ncombines transformerbased contextual embeddings with graph convolutional\nnetworks, built upon rule-based syntactic dependency parsing and semantic\nsimilarity computations. By employing bidirectional crossattention, the model\neffectively fuses local syntactic structure with global semantic context,\nresulting in improved sentiment classification performance across both low- and\nhigh-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali\nABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The\nCrosGrpsABS consistently outperforms existing approaches, achieving notable\nimprovements, including a 0.93% F1-score increase for the Restaurant domain and\na 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.", "AI": {"tldr": "This paper presents CrosGrpsABS, a hybrid framework for Aspect-Based Sentiment Analysis (ABSA) in Bengali, addressing challenges in low-resource languages by leveraging cross-attention mechanisms and graph convolutional networks.", "motivation": "The need to enhance aspect-level sentiment classification in low-resource languages, specifically Bengali, which faces a scarcity of annotated datasets and language-specific tools.", "method": "CrosGrpsABS employs bidirectional cross-attention between syntactic and semantic graphs, integrating transformer-based contextual embeddings with graph convolutional networks derived from syntactic dependency parsing and semantic similarity calculations.", "result": "CrosGrpsABS demonstrates significant performance gains on Bengali ABSA datasets and outperforms existing models on the SemEval 2014 Task 4 dataset, achieving a notable F1-score increase in the Restaurant and Laptop domains.", "conclusion": "The hybrid approach of CrosGrpsABS effectively fuses syntactic and semantic information, leading to superior sentiment classification in both low- and high-resource environments.", "key_contributions": ["Introduction of CrosGrpsABS for low-resource language ABSA", "Integration of cross-attention mechanisms with graph convolutional networks", "Demonstrated state-of-the-art performance on Bengali and English datasets."], "limitations": "", "keywords": ["Aspect-Based Sentiment Analysis", "Bengali", "Cross-attention", "Graph convolutional networks", "Natural language processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.19051", "pdf": "https://arxiv.org/pdf/2505.19051.pdf", "abs": "https://arxiv.org/abs/2505.19051", "title": "Efficient Data Selection at Scale via Influence Distillation", "authors": ["Mahdi Nikdan", "Vincent Cohen-Addad", "Dan Alistarh", "Vahab Mirrokni"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a $\\textit{landmark-based approximation}$:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to $3.5\\times$ faster selection.", "AI": {"tldr": "The paper presents Influence Distillation, a new framework for data selection in training Large Language Models, which uses second-order information to assign optimal weights to training samples, ensuring efficient fine-tuning.", "motivation": "Effective data selection is crucial for training performance in Large Language Models, and current methods lack a systematic approach to optimize this process.", "method": "Influence Distillation employs second-order information to compute model-specific weights for training samples, using a landmark-based approximation to scale the computation efficiently.", "result": "The method achieves comparable or superior performance to state-of-the-art techniques while enabling up to 3.5 times faster selection of training data.", "conclusion": "Influence Distillation provides a mathematically grounded and efficient data selection method that improves training process for various models and datasets in LLM fine-tuning.", "key_contributions": ["Introduction of Influence Distillation framework for data selection", "Optimal weighting of training samples using second-order information", "Landmark-based approximation for efficient scalability"], "limitations": "", "keywords": ["Large Language Models", "Data Selection", "Influence Distillation", "Machine Learning", "Instruction Tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056.pdf", "abs": "https://arxiv.org/abs/2505.19056", "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance.", "AI": {"tldr": "This paper addresses the vulnerability of large language models to an attack called abliteration that undermines their ability to refuse harmful instructions, proposing a defense through the creation of an extended-refusal dataset for fine-tuning.", "motivation": "To combat the abliteration attack that compromises the refusal behavior of large language models, enabling the generation of unethical content.", "method": "The authors constructed an extended-refusal dataset containing harmful prompts paired with justifications for refusal, and fine-tuned Llama-2-7B-Chat and Qwen2.5-Instruct models on this dataset.", "result": "The fine-tuned models maintained high refusal rates, decreasing by only 10% compared to baseline models, which experienced a 70-80% drop in refusal rates under the attack.", "conclusion": "Extended-refusal fine-tuning effectively neutralizes the abliteration attack while maintaining model performance and safety.", "key_contributions": ["Development of an extended-refusal dataset for model fine-tuning", "Demonstration of the effectiveness of fine-tuned models against abliteration attacks", "Balancing safety compliance with model performance during harmful prompt evaluations"], "limitations": "", "keywords": ["large language models", "safety guidelines", "fine-tuning", "abliteration attack", "ethical AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19060", "pdf": "https://arxiv.org/pdf/2505.19060.pdf", "abs": "https://arxiv.org/abs/2505.19060", "title": "UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models", "authors": ["Roman Vashurin", "Maiya Goloburda", "Preslav Nakov", "Maxim Panov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools across various\napplications, making it more important than ever to ensure the quality and the\ntrustworthiness of their outputs. This has led to growing interest in\nuncertainty quantification (UQ) methods for assessing the reliability of LLM\noutputs. Many existing UQ techniques rely on token probabilities, which\ninadvertently introduces a bias with respect to the length of the output. While\nsome methods attempt to account for this, we demonstrate that such biases\npersist even in length-normalized approaches. To address the problem, here we\npropose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing\nprocedure that regresses uncertainty scores on output length and uses the\nresiduals as corrected, length-invariant estimates. Our method is post-hoc,\nmodel-agnostic, and applicable to a range of UQ measures. Through extensive\nevaluation on machine translation, summarization, and question-answering tasks,\nwe demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally\nlength-normalized UQ methods uncertainty estimates across multiple metrics and\nmodels.", "AI": {"tldr": "The paper proposes UNCERTAINTY-LINE, a debiasing method for uncertainty quantification of LLM outputs that corrects for output length bias.", "motivation": "The reliability of outputs from large language models (LLMs) is crucial, yet existing uncertainty quantification (UQ) methods may introduce biases related to output length.", "method": "UNCERTAINTY-LINE regresses uncertainty scores on output length and uses residuals for length-invariant uncertainty estimates. It is a post-hoc and model-agnostic approach applicable to various UQ measures.", "result": "UNCERTAINTY-LINE consistently yields improved uncertainty estimates over nominally length-normalized UQ methods across multiple tasks and models.", "conclusion": "The proposed method enhances the accuracy of uncertainty quantification for LLM outputs by addressing length bias effectively.", "key_contributions": ["Introduction of UNCERTAINTY-LINE for debiasing uncertainty estimates", "Demonstration of effectiveness across machine translation, summarization, and question-answering", "Model-agnostic applicability for various UQ measures"], "limitations": "", "keywords": ["uncertainty quantification", "large language models", "debiasing", "post-hoc method", "length normalization"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19073", "pdf": "https://arxiv.org/pdf/2505.19073.pdf", "abs": "https://arxiv.org/abs/2505.19073", "title": "Towards Harmonized Uncertainty Estimation for Large Language Models", "authors": ["Rui Li", "Jing Long", "Muge Qi", "Heming Xia", "Lei Sha", "Peiyi Wang", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To facilitate robust and trustworthy deployment of large language models\n(LLMs), it is essential to quantify the reliability of their generations\nthrough uncertainty estimation. While recent efforts have made significant\nadvancements by leveraging the internal logic and linguistic features of LLMs\nto estimate uncertainty scores, our empirical analysis highlights the pitfalls\nof these methods to strike a harmonized estimation between indication, balance,\nand calibration, which hinders their broader capability for accurate\nuncertainty estimation. To address this challenge, we propose CUE (Corrector\nfor Uncertainty Estimation): A straightforward yet effective method that\nemploys a lightweight model trained on data aligned with the target LLM's\nperformance to adjust uncertainty scores. Comprehensive experiments across\ndiverse models and tasks demonstrate its effectiveness, which achieves\nconsistent improvements of up to 60% over existing methods.", "AI": {"tldr": "This paper presents CUE, a method for improving uncertainty estimation in large language models.", "motivation": "To enhance the reliability of large language model generations by quantifying uncertainty effectively.", "method": "CUE utilizes a lightweight model trained on data aligned with the performance of the target LLM to adjust its uncertainty scores.", "result": "CUE shows consistent improvements of up to 60% in uncertainty estimation across various models and tasks compared to existing methods.", "conclusion": "CUE provides a robust framework for more accurate uncertainty estimation in the deployment of LLMs.", "key_contributions": ["Introduction of CUE for adjusting uncertainty scores", "Demonstrated improvements over existing uncertainty estimation methods", "Empirical analysis of the effectiveness across diverse models and tasks"], "limitations": "", "keywords": ["Uncertainty Estimation", "Large Language Models", "Machine Learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091.pdf", "abs": "https://arxiv.org/abs/2505.19091", "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "authors": ["Benjamin Clavié", "Florian Brand"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench .", "AI": {"tldr": "Introduction of ReadBench, a benchmark for evaluating the reading comprehension abilities of Large Vision-Language Models (VLMs) on text-rich images.", "motivation": "To address the limited assessment of VLMs' performance in reading and reasoning about text-rich images, despite their advancements in visual comprehension.", "method": "ReadBench transposes established text-only benchmark contexts into images of text while maintaining textual prompts and questions, allowing for evaluation of VLM capabilities.", "result": "Evaluation of leading VLMs using ReadBench demonstrated minimal performance degradation on short text-image inputs, but significant decline on longer, multi-page contexts. Text resolution was found to have negligible effects on performance.", "conclusion": "Improvements are needed in VLMs for reasoning over extensive textual content presented visually, which is crucial for practical applications.", "key_contributions": ["Introduction of the ReadBench benchmark for VLM evaluation", "Demonstration of performance variations based on input length", "Insights on the limitations of multimodal reasoning in VLMs"], "limitations": "The study primarily focuses on text-rich images and does not encompass other multimodal capabilities of VLMs.", "keywords": ["Vision-Language Models", "Reading Comprehension", "Multimodal Benchmarking"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100.pdf", "abs": "https://arxiv.org/abs/2505.19100", "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models.", "AI": {"tldr": "This paper introduces Adaptive Sentence-level Preference Optimization (ASPO) to improve preference alignment in multimodal models through sentence-level evaluation.", "motivation": "Existing Direct Preference Optimization (DPO) methods do not account for fine-grained correctness in multimodal interactions, leading to suboptimal alignments of large language models (LLMs).", "method": "The authors propose ASPO, which dynamically calculates adaptive rewards at the sentence level based on model predictions to enhance preference optimization of multimodal outputs.", "result": "ASPO significantly improves alignment and overall performance of multimodal models compared to traditional DPO.", "conclusion": "The study demonstrates that incorporating sentence-level evaluation allows for more precise preference optimization, enhancing model performance without additional complexity.", "key_contributions": ["Introduction of Adaptive Sentence-level Preference Optimization (ASPO)", "Improvement of multimodal model performance with sentence-level preference evaluation", "Reduction of the reliance on additional models or parameters for optimization"], "limitations": "", "keywords": ["Direct Preference Optimization", "Adaptive Sentence-level Optimization", "Multimodal Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19103", "pdf": "https://arxiv.org/pdf/2505.19103.pdf", "abs": "https://arxiv.org/abs/2505.19103", "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection", "authors": ["Iddo Yosha", "Dorin Shteyman", "Yossi Adi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress.", "AI": {"tldr": "WHISTRESS is a novel method for detecting sentence stress in spoken language, trained on synthetic data, showing strong performance and zero-shot generalization.", "motivation": "To enhance transcription systems by integrating sentence stress detection, crucial for conveying speaker intent.", "method": "WHISTRESS, an alignment-free approach, is trained on TINYSTRESS-15K, a synthetic dataset created through an automated process, and evaluated against competitive baselines.", "result": "WHISTRESS outperforms existing methods and shows strong zero-shot generalization across diverse benchmarks without requiring additional input priors.", "conclusion": "The method demonstrates the potential of using synthetic data for effective sentence stress detection in automatic transcription systems.", "key_contributions": ["Introduction of WHISTRESS for sentence stress detection", "Creation of TINYSTRESS-15K, a scalable synthetic dataset", "Achievement of zero-shot generalization in stress detection"], "limitations": "", "keywords": ["sentence stress detection", "synthetic data", "automatic transcription"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2505.19108", "pdf": "https://arxiv.org/pdf/2505.19108.pdf", "abs": "https://arxiv.org/abs/2505.19108", "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "authors": ["Yongheng Zhang", "Xu Liu", "Ruoxi Zhou", "Qiguang Chen", "Hao Fei", "Wenpeng Lu", "Libo Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios.", "AI": {"tldr": "Introduction of a novel benchmark to assess hallucination issues in large language models (LLMs) across cross-lingual and cross-modal scenarios.", "motivation": "To address the gap in existing studies focusing solely on single scenarios of hallucinations in LLMs, either cross-lingual or cross-modal, by introducing a joint evaluation framework.", "method": "Development of the Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) that evaluates LLMs in scenarios involving both cross-lingual and cross-modal hallucinations.", "result": "Current mainstream open-source and closed-source LLMs show significant struggles with the challenges presented by the CCHall benchmark.", "conclusion": "CCHall can serve as a valuable resource for evaluating LLM capabilities in joint cross-lingual and cross-modal scenarios, helping to advance their real-world applications.", "key_contributions": ["Establishment of a new benchmark (CCHall) for assessing LLM hallucinations in dual scenarios.", "Comprehensive evaluation revealing struggles of existing LLMs with cross-lingual and cross-modal challenges.", "Potential to significantly impact the deployment of LLMs in real-world applications."], "limitations": "Limited to the assessment of hallucination issues and does not cover other evaluation aspects of LLMs.", "keywords": ["hallucinations", "large language models", "cross-lingual", "cross-modal", "benchmark"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19112", "pdf": "https://arxiv.org/pdf/2505.19112.pdf", "abs": "https://arxiv.org/abs/2505.19112", "title": "Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering", "authors": ["Zheng Chu", "Huiming Fan", "Jingchang Chen", "Qianyu Wang", "Mingda Yang", "Jiafeng Liang", "Zhongjie Wang", "Hao Li", "Guo Tang", "Ming Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Although large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they still face challenges in knowledge-intensive multi-hop\nreasoning. Recent work explores iterative retrieval to address complex\nproblems. However, the lack of intermediate guidance often results in\ninaccurate retrieval and flawed intermediate reasoning, leading to incorrect\nreasoning. To address these, we propose Self-Critique Guided Iterative\nReasoning (SiGIR), which uses self-critique feedback to guide the iterative\nreasoning process. Specifically, through end-to-end training, we enable the\nmodel to iteratively address complex problems via question decomposition.\nAdditionally, the model is able to self-evaluate its intermediate reasoning\nsteps. During iterative reasoning, the model engages in branching exploration\nand employs self-evaluation to guide the selection of promising reasoning\ntrajectories. Extensive experiments on three multi-hop reasoning datasets\ndemonstrate the effectiveness of our proposed method, surpassing the previous\nSOTA by $8.6\\%$. Furthermore, our thorough analysis offers insights for future\nresearch. Our code, data, and models are available at Github:\nhttps://github.com/zchuz/SiGIR-MHQA.", "AI": {"tldr": "This paper presents Self-Critique Guided Iterative Reasoning (SiGIR) to enhance multi-hop reasoning in LLMs by incorporating self-evaluation during the reasoning process.", "motivation": "To improve the accuracy of multi-hop reasoning in large language models by addressing issues related to iterative retrieval and intermediate reasoning guidance.", "method": "The proposed SiGIR model employs self-critique feedback during iterative reasoning processes, allowing it to decompose questions and evaluate its own reasoning steps.", "result": "The SiGIR approach outperformed the previous state-of-the-art (SOTA) by 8.6% on three multi-hop reasoning datasets, demonstrating significant improvements in addressing complex reasoning problems.", "conclusion": "SiGIR not only enhances reasoning accuracy through self-evaluation but also provides valuable insights for future research in multi-hop reasoning in LLMs.", "key_contributions": ["Introduction of self-critique feedback in iterative reasoning for LLMs.", "End-to-end training that allows question decomposition and self-evaluation of reasoning steps.", "Demonstration of an 8.6% improvement over previous SOTA methods in multi-hop reasoning tasks."], "limitations": "", "keywords": ["Large Language Models", "Multi-hop Reasoning", "Self-Critique", "Iterative Reasoning", "Question Decomposition"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19116", "pdf": "https://arxiv.org/pdf/2505.19116.pdf", "abs": "https://arxiv.org/abs/2505.19116", "title": "Controlling Language Confusion in Multilingual LLMs", "authors": ["Nahyun Lee", "Yeongseo Woo", "Hyunwoo Ko", "Guijin Son"], "categories": ["cs.CL"], "comment": "4 pages", "summary": "Large language models often suffer from language confusion, a phenomenon\nwhere responses are partially or entirely generated in unintended languages.\nThis can critically impact user experience in low-resource settings. We\nhypothesize that conventional supervised fine-tuning exacerbates this issue\nbecause the softmax objective focuses probability mass only on the single\ncorrect token but does not explicitly penalize cross-lingual mixing.\nInterestingly, by observing loss trajectories during the pretraining phase, we\nobserve that models fail to learn to distinguish between monolingual and\nlanguage-confused text. Additionally, we find that ORPO, which adds penalties\nfor unwanted output styles to standard SFT, effectively suppresses\nlanguage-confused generations even at high decoding temperatures without\ndegrading overall model performance. Our findings suggest that incorporating\nappropriate penalty terms can mitigate language confusion in low-resource\nsettings with limited data.", "AI": {"tldr": "This paper addresses the issue of language confusion in large language models, proposing a method to mitigate it through the use of penalty terms alongside supervised fine-tuning (SFT).", "motivation": "Language confusion in large language models adversely affects user experience, especially in low-resource language settings.", "method": "The authors analyze the loss trajectories during pretraining and propose ORPO, a method that adds penalties for unwanted output styles to standard supervised fine-tuning.", "result": "ORPO effectively reduces language-confused generations and improves performance without degrading overall model effectiveness.", "conclusion": "Integrating penalty terms into the training process can significantly alleviate language confusion in low-resource settings.", "key_contributions": ["Identification of the limitations of conventional supervised fine-tuning regarding language confusion.", "Introduction of ORPO as a solution to suppress unwanted language mixing.", "Empirical evidence demonstrating the effectiveness of ORPO in maintaining model performance while reducing confusion."], "limitations": "The study primarily focuses on low-resource settings and may not fully generalize to high-resource contexts.", "keywords": ["large language models", "language confusion", "supervised fine-tuning", "ORPO", "low-resource settings"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.19121", "pdf": "https://arxiv.org/pdf/2505.19121.pdf", "abs": "https://arxiv.org/abs/2505.19121", "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models", "authors": ["Seunguk Yu", "Juhwan Choi", "Youngbin Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models.", "AI": {"tldr": "This paper investigates ethical biases in large language models (LLMs) using a newly introduced dataset while highlighting the influence of language-specific distinctions on these biases.", "motivation": "To validate and compare the existence of ethical biases in LLMs across various languages and sensitive topics, particularly those identified globally by Human Rights organizations.", "method": "A new dataset called the Multilingual Sensitive Questions & Answers Dataset (MSQAD) was constructed from Human Rights Watch articles, consisting of socially sensitive questions and responses in multiple languages. Statistical hypothesis tests were employed to examine biases across these language responses.", "result": "The analysis revealed that biases were detected across different languages and topics, indicating that ethical biases are often inherent in LLM responses, with rejection of null hypotheses in most cases.", "conclusion": "The study illustrates widespread ethical biases in LLMs, underscoring the need for awareness and further study of cross-language biases, with MSQAD being a resource for future research.", "key_contributions": ["Introduction of the Multilingual Sensitive Questions & Answers Dataset (MSQAD) for analyzing biases in LLMs.", "Empirical evidence showing the presence of ethical biases across multiple languages and LLMs.", "Statistical methodology validating the existence of language-specific biases in responses."], "limitations": "Study focuses on a limited set of 17 sensitive topics and the dataset is restricted to responses generated from specific LLMs.", "keywords": ["ethical biases", "large language models", "multilingual dataset", "social biases", "human rights"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.19126", "pdf": "https://arxiv.org/pdf/2505.19126.pdf", "abs": "https://arxiv.org/abs/2505.19126", "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning", "authors": ["Wenyang Luo", "Wayne Xin Zhao", "Jing Sha", "Shijin Wang", "Ji-Rong Wen"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has\nsignificantly advanced complex reasoning tasks. However, their capabilities in\nmultilingual complex reasoning remain underexplored, with existing efforts\nlargely focused on simpler tasks like MGSM. To address this gap, we introduce\nMMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality\nmath problems across 10 typologically diverse languages. Using MMATH, we\nobserve that even advanced models like DeepSeek R1 exhibit substantial\nperformance disparities across languages and suffer from a critical off-target\nissue-generating responses in unintended languages. To address this, we explore\nstrategies including prompting and training, demonstrating that reasoning in\nEnglish and answering in target languages can simultaneously enhance\nperformance and preserve target-language consistency. Our findings offer new\ninsights and practical strategies for advancing the multilingual reasoning\ncapabilities of large language models. Our code and data could be found at\nhttps://github.com/RUCAIBox/MMATH.", "AI": {"tldr": "The paper introduces MMATH, a benchmark for multilingual complex reasoning in mathematics, highlighting performance disparities in advanced models across languages and proposing strategies for improvement.", "motivation": "To explore the capabilities of large reasoning models in multilingual complex reasoning, which remain underexplored despite advances in simpler tasks.", "method": "The MMATH benchmark was developed, encompassing 374 math problems across 10 languages, and performance of models was evaluated with various prompting and training strategies.", "result": "Advanced models like DeepSeek R1 showed significant performance disparities across languages, and off-target language issues were identified.", "conclusion": "Strategies including prompting in English while answering in target languages improve performance and consistency in multilingual reasoning tasks.", "key_contributions": ["Introduction of the MMATH benchmark for multilingual math reasoning.", "Identified performance disparities and off-target language issues in advanced reasoning models.", "Proposed strategies to enhance multilingual reasoning capabilities in large language models."], "limitations": "The findings are based on existing models and may not be generalizable to all reasoning tasks in multilingual contexts.", "keywords": ["multilingual reasoning", "large language models", "benchmark", "complex reasoning", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.19128", "pdf": "https://arxiv.org/pdf/2505.19128.pdf", "abs": "https://arxiv.org/abs/2505.19128", "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models", "authors": ["Jin Zhang", "Fan Gao", "Linyu Li", "Yongbin Yu", "Xiangxiang Wang", "Nyima Tashi", "Gadeng Luosang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rise of large language models has led to significant performance\nbreakthroughs in named entity recognition (NER) for high-resource languages,\nyet there remains substantial room for improvement in low- and medium-resource\nlanguages. Existing multilingual NER methods face severe language interference\nduring the multi-language adaptation process, manifested in feature conflicts\nbetween different languages and the competitive suppression of low-resource\nlanguage features by high-resource languages. Although training a dedicated\nmodel for each language can mitigate such interference, it lacks scalability\nand incurs excessive computational costs in real-world applications. To address\nthis issue, we propose RetrieveAll, a universal multilingual NER framework\nbased on dynamic LoRA. The framework decouples task-specific features across\nlanguages and demonstrates efficient dynamic adaptability. Furthermore, we\nintroduce a cross-granularity knowledge augmented method that fully exploits\nthe intrinsic potential of the data without relying on external resources. By\nleveraging a hierarchical prompting mechanism to guide knowledge injection,\nthis approach advances the paradigm from \"prompt-guided inference\" to\n\"prompt-driven learning.\" Experimental results show that RetrieveAll\noutperforms existing baselines; on the PAN-X dataset, it achieves an average F1\nimprovement of 12.1 percent.", "AI": {"tldr": "RetrieveAll is a universal multilingual named entity recognition framework that addresses language interference by decoupling features and employing a novel prompting method to enhance performance, particularly for low-resource languages.", "motivation": "There is a significant performance gap in named entity recognition for low- and medium-resource languages compared to high-resource languages, largely due to language interference in existing multilingual methods.", "method": "The paper proposes RetrieveAll, which utilizes dynamic LoRA to decouple task-specific features and introduces a cross-granularity knowledge augmented method with hierarchical prompting for better knowledge injection and adaptability.", "result": "RetrieveAll outperforms existing multilingual NER baselines, achieving a 12.1% average F1 improvement on the PAN-X dataset.", "conclusion": "The proposed framework significantly enhances multilingual NER performance, especially for low-resource languages, and avoids the scalability issues of existing methods by mitigating language interference effectively.", "key_contributions": ["Introduction of RetrieveAll universal multilingual NER framework.", "Decoupling of task-specific features across languages.", "Implementation of a hierarchical prompting mechanism for knowledge injection."], "limitations": "The framework's performance on extremely low-resource languages is yet to be evaluated.", "keywords": ["Multilingual NER", "Large language models", "Dynamic LoRA", "Cross-granularity knowledge augmentation", "Hierarchical prompting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147.pdf", "abs": "https://arxiv.org/abs/2505.19147", "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.", "AI": {"tldr": "This position paper advocates for a shift in focus from model-centric to data-centric compression in AI, emphasizing token compression as a means to enhance efficiency in managing long-context data across various domains.", "motivation": "As the cost of self-attention in long-context AI becomes a bottleneck, there's a pressing need to explore efficient data handling methods rather than simply increasing model size.", "method": "The authors establish a unified mathematical framework for analyzing token compression and explore its implications across multiple domains, reviewing existing strategies and their effectiveness.", "result": "Token compression is identified as a promising solution to reduce the number of tokens during training and inference, enhancing overall AI efficiency and performance.", "conclusion": "Token compression represents a critical paradigm shift necessary to tackle the challenges posed by increasing context lengths in language models and other AI systems.", "key_contributions": ["Proposes a new focus on token compression for AI efficiency", "Establishes a mathematical framework for understanding model efficiency strategies", "Reviews challenges and future directions in token compression research."], "limitations": "", "keywords": ["Token Compression", "AI Efficiency", "Long-context AI", "Data-centric Compression", "Model Efficiency"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.19163", "pdf": "https://arxiv.org/pdf/2505.19163.pdf", "abs": "https://arxiv.org/abs/2505.19163", "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs", "authors": ["Firoj Alam", "Md Arid Hasan", "Shammur Absar Chowdhury"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based\n  Evaluation, Dialectal Speech, Low-resource Languages, Multimodal\n  Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction,\n  Natural Language Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious disciplines and tasks. However, benchmarking their capabilities with\nmultilingual spoken queries remains largely unexplored. In this study, we\nintroduce SpokenNativQA, the first multilingual and culturally aligned spoken\nquestion-answering (SQA) dataset designed to evaluate LLMs in real-world\nconversational settings. The dataset comprises approximately 33,000 naturally\nspoken questions and answers in multiple languages, including low-resource and\ndialect-rich languages, providing a robust benchmark for assessing LLM\nperformance in speech-based interactions. SpokenNativQA addresses the\nlimitations of text-based QA datasets by incorporating speech variability,\naccents, and linguistic diversity. We benchmark different ASR systems and LLMs\nfor SQA and present our findings. We released the data at\n(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental\nscripts at (https://llmebench.qcri.org/) for the research community.", "AI": {"tldr": "The paper presents SpokenNativQA, a pioneering multilingual spoken question-answering dataset for evaluating large language models in real-world conversational settings.", "motivation": "To address the gap in benchmarking LLM performance with multilingual spoken queries and to evaluate their capabilities in real-world conversational contexts.", "method": "The study introduces a dataset comprising approximately 33,000 spoken questions and answers across multiple languages, benchmarking different ASR systems and LLMs for spoken question answering.", "result": "Findings indicate that traditional text-based QA systems face challenges in handling speech variability and linguistic diversity, highlighting the need for improved evaluation metrics in SQA.", "conclusion": "SpokenNativQA provides a robust resource for the research community to assess LLM performance in speech-based interactions, addressing limitations of current text-based datasets.", "key_contributions": ["Introduction of SpokenNativQA dataset for multilingual SQA", "Evaluation of LLMs in real-world conversational settings", "Benchmarking of various ASR systems alongside LLMs"], "limitations": "Limited to the languages and contexts represented in the dataset; potential biases in spoken data collection.", "keywords": ["Spoken Question Answering", "Multilingual LLMs", "Speech-based Evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19176", "pdf": "https://arxiv.org/pdf/2505.19176.pdf", "abs": "https://arxiv.org/abs/2505.19176", "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "authors": ["Zhuo Liu", "Moxin Li", "Xun Deng", "Qifan Wang", "Fuli Feng"], "categories": ["cs.CL"], "comment": "Under review", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.", "AI": {"tldr": "The paper presents AGDe-Judge, a framework to reduce teacher preference bias in evaluating LLM-generated responses.", "motivation": "To address the issue of teacher preference bias when training proxy judge models for evaluating LLM outputs, which can lead to biased evaluation results.", "method": "AGDe-Judge incorporates an additional assistant model to complement the training data and employs a three-stage framework to debias the training data from both labels and feedbacks.", "result": "Extensive experiments show that AGDe-Judge significantly reduces teacher preference bias while maintaining strong performance across six evaluation benchmarks.", "conclusion": "The proposed framework effectively mitigates bias in proxy judge models, improving evaluation quality for LLM-generated responses.", "key_contributions": ["Introduction of AGDe-Judge framework", "Development of a debiasing strategy using an assistant model", "Demonstration of effectiveness across multiple evaluation benchmarks"], "limitations": "", "keywords": ["LLM", "de-biasing", "evaluation", "human evaluations", "machine learning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184.pdf", "abs": "https://arxiv.org/abs/2505.19184", "title": "Two LLMs debate, both are certain they've won", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings.", "AI": {"tldr": "This study evaluates the confidence calibration of LLMs in a dynamic debate setting, revealing systematic overconfidence and failure to accurately update beliefs.", "motivation": "To explore whether Large Language Models can adjust their confidence accurately in dynamic adversarial scenarios, building on previous calibration studies in static tasks.", "method": "Conducted 60 three-round policy debates among ten state-of-the-art LLMs, having them rate their confidence in winning after each round.", "result": "Found that models exhibited systematic overconfidence, confidence escalation, mutual overestimation, persistent self-debate bias, and misaligned reasoning between private thoughts and public confidence ratings.", "conclusion": "LLMs demonstrate significant issues in self-assessment and belief updating in dynamic settings, raising concerns for their deployment in real-world assistant roles.", "key_contributions": ["Evaluation of LLM confidence in dynamic settings", "Identification of systemic overconfidence patterns", "Insights into the reliability of LLM reasoning in adversarial scenarios"], "limitations": "Limited to a controlled debate format and may not generalize to other scenarios or types of LLM outputs.", "keywords": ["Large Language Models", "confidence calibration", "adversarial debate", "self-assessment", "dynamic reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19187", "pdf": "https://arxiv.org/pdf/2505.19187.pdf", "abs": "https://arxiv.org/abs/2505.19187", "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling", "authors": ["Yang Xiao", "Jiashuo Wang", "Ruifeng Yuan", "Chunpu Xu", "Kaishuai Xu", "Wenjie Li", "Pengfei Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.", "AI": {"tldr": "The paper introduces PIR, a framework that optimizes reasoning steps in LLMs to enhance test-time efficiency and accuracy.", "motivation": "To address the verbosity in reasoning chains of LLMs that negatively impacts computational efficiency during inference.", "method": "The PIR framework quantitatively evaluates and prunes low-importance functional reasoning steps while retaining essential progressive reasoning components.", "result": "PIR-optimized models show improved accuracy (+0.9% to +6.6%) and reduced token usage (-3% to -41%) on reasoning benchmarks.", "conclusion": "PIR provides a practical solution for enhancing the efficiency of LLMs in reasoning tasks, maintaining model performance while optimizing resource usage.", "key_contributions": ["Introduction of the PIR framework for reasoning step optimization", "Demonstration of improved accuracy and reduced verbosity in reasoning chains", "Generalizability across various model sizes and benchmarks"], "limitations": "", "keywords": ["Large Language Models", "PIR", "Chain-of-Thought", "Reasoning Optimization", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19191", "pdf": "https://arxiv.org/pdf/2505.19191.pdf", "abs": "https://arxiv.org/abs/2505.19191", "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection", "authors": ["Nursulu Sagimbayeva", "Ruveyda Betül Bahçeci", "Ingmar Weber"], "categories": ["cs.CL"], "comment": "8 pages, 6 figures. Accepted for publication in the Proceedings of\n  1st Workshop on Misinformation Detection in the Era of LLMs (MisD) at\n  ICWSM-2025", "summary": "Inconsistent political statements represent a form of misinformation. They\nerode public trust and pose challenges to accountability, when left unnoticed.\nDetecting inconsistencies automatically could support journalists in asking\nclarification questions, thereby helping to keep politicians accountable. We\npropose the Inconsistency detection task and develop a scale of inconsistency\ntypes to prompt NLP-research in this direction. To provide a resource for\ndetecting inconsistencies in a political domain, we present a dataset of 698\nhuman-annotated pairs of political statements with explanations of the\nannotators' reasoning for 237 samples. The statements mainly come from voting\nassistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,\nreflecting real-world political issues. We benchmark Large Language Models\n(LLMs) on our dataset and show that in general, they are as good as humans at\ndetecting inconsistencies, and might be even better than individual humans at\npredicting the crowd-annotated ground-truth. However, when it comes to\nidentifying fine-grained inconsistency types, none of the model have reached\nthe upper bound of performance (due to natural labeling variation), thus\nleaving room for improvement. We make our dataset and code publicly available.", "AI": {"tldr": "This paper proposes an inconsistency detection task in political statements and presents a dataset to support NLP research in this area.", "motivation": "To address the issue of misinformation and enhance public trust in political discourse by enabling the automatic detection of inconsistencies in political statements.", "method": "The authors introduce the Inconsistency detection task, develop a scale of inconsistency types, and provide a dataset of 698 human-annotated pairs of political statements, with evaluations of Large Language Models on this dataset.", "result": "LLMs are found to detect inconsistencies as well as humans but struggle with fine-grained inconsistency types, indicating room for improvement.", "conclusion": "The paper highlights the importance of detecting inconsistencies to support accountability in politics and makes the dataset publicly available for further research.", "key_contributions": ["Introduction of the Inconsistency detection task", "Development of a scale of inconsistency types", "Creation of a publicly available annotated dataset for political statements"], "limitations": "None of the models achieved optimal performance on fine-grained inconsistency types due to natural labeling variation.", "keywords": ["inconsistency detection", "political statements", "natural language processing", "large language models", "misinformation"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2505.19201", "pdf": "https://arxiv.org/pdf/2505.19201.pdf", "abs": "https://arxiv.org/abs/2505.19201", "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding", "authors": ["Yunhai Hu", "Tianhua Xia", "Zining Liu", "Rahul Raman", "Xingyu Liu", "Bo Bao", "Eric Sather", "Vithursan Thangarasa", "Sai Qian Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD) has emerged as a powerful method for accelerating\nautoregressive generation in large language models (LLMs), yet its integration\ninto vision-language models (VLMs) remains underexplored. We introduce DREAM, a\nnovel speculative decoding framework tailored for VLMs that combines three key\ninnovations: (1) a cross-attention-based mechanism to inject intermediate\nfeatures from the target model into the draft model for improved alignment, (2)\nadaptive intermediate feature selection based on attention entropy to guide\nefficient draft model training, and (3) visual token compression to reduce\ndraft model latency. DREAM enables efficient, accurate, and parallel multimodal\ndecoding with significant throughput improvement. Experiments across a diverse\nset of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,\ndemonstrate up to 3.6x speedup over conventional decoding and significantly\noutperform prior SD baselines in both inference throughput and speculative\ndraft acceptance length across a broad range of multimodal benchmarks. The code\nis publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git", "AI": {"tldr": "DREAM introduces a novel speculative decoding framework for vision-language models, enhancing efficiency and accuracy in multimodal generation.", "motivation": "To explore the integration of speculative decoding into vision-language models, which has been underexplored despite its success in large language models.", "method": "DREAM combines a cross-attention mechanism, adaptive intermediate feature selection, and visual token compression to improve draft model latency and alignment.", "result": "Experiments show DREAM achieves up to 3.6x speedup over conventional decoding methods and significantly improves inference throughput compared to existing speculative decoding baselines.", "conclusion": "DREAM provides efficient and accurate multimodal decoding, setting a new standard for performance in vision-language models with publicly available code.", "key_contributions": ["Introduction of a cross-attention mechanism for model alignment", "Adaptive intermediate feature selection based on attention entropy", "Visual token compression to enhance decoding efficiency"], "limitations": "", "keywords": ["speculative decoding", "vision-language models", "multimodal generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.19206", "pdf": "https://arxiv.org/pdf/2505.19206.pdf", "abs": "https://arxiv.org/abs/2505.19206", "title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "authors": ["Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems.", "AI": {"tldr": "SpeakStream is a new streaming TTS system that reduces latency in conversational AI by incrementally generating audio from streaming text.", "motivation": "Traditional TTS systems hinder conversational AI due to high latency, making them unsuitable for responsive applications.", "method": "The system uses a decoder-only architecture and is trained with a next-step prediction loss on interleaved text-speech data to provide incremental audio output.", "result": "SpeakStream outperforms traditional systems in first-token latency while maintaining audio quality.", "conclusion": "The proposed system demonstrates significant improvements in responsiveness for conversational agents reliant on streaming LLMs.", "key_contributions": ["Introduction of SpeakStream, a low-latency streaming TTS system", "Use of next-step prediction loss for training", "Demonstration of state-of-the-art latency performance."], "limitations": "", "keywords": ["streaming TTS", "conversational AI", "latency", "LLM", "text-to-speech"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19209", "pdf": "https://arxiv.org/pdf/2505.19209.pdf", "abs": "https://arxiv.org/abs/2505.19209", "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Yujie Liu", "Wei Li", "Tong Xie", "Lidong Bing", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines.", "AI": {"tldr": "Introducing fine-grained scientific hypothesis generation using LLMs, improving from coarse directions to detailed actionable hypotheses.", "motivation": "Existing methods for scientific hypothesis generation using LLMs generate coarse hypotheses lacking necessary detail; there's a need for fine-grained hypothesis discovery.", "method": "The task is framed as a combinatorial optimization problem, with a hierarchical search method proposed for generating detailed hypotheses based on LLM internal heuristics and an ensemble of LLMs.", "result": "Empirical evaluations demonstrate that the proposed method consistently outperforms strong baselines on a benchmark of expert-annotated hypotheses from chemistry literature.", "conclusion": "The hierarchical search approach smooths the reward landscape, enabling better optimization and generation of more useful scientific hypotheses.", "key_contributions": ["Formal definition of fine-grained hypothesis discovery", "Hierarchical search method for hypothesis generation", "Empirical validation on expert-annotated benchmark data"], "limitations": "", "keywords": ["Large Language Models", "Scientific Hypothesis Generation", "Combinatorial Optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19212", "pdf": "https://arxiv.org/pdf/2505.19212.pdf", "abs": "https://arxiv.org/abs/2505.19212", "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard Schölkopf", "Zhijing Jin"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled their use in\ncomplex agentic roles, involving decision-making with humans or other agents,\nmaking ethical alignment a key AI safety concern. While prior work has examined\nboth LLMs' moral judgment and strategic behavior in social dilemmas, there is\nlimited understanding of how they act when moral imperatives directly conflict\nwith rewards or incentives. To investigate this, we introduce Moral Behavior in\nSocial Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the\nprisoner's dilemma and public goods game with morally charged contexts. In\nMoralSim, we test a range of frontier models across both game structures and\nthree distinct moral framings, enabling a systematic examination of how LLMs\nnavigate social dilemmas in which ethical norms conflict with payoff-maximizing\nstrategies. Our results show substantial variation across models in both their\ngeneral tendency to act morally and the consistency of their behavior across\ngame types, the specific moral framing, and situational factors such as\nopponent behavior and survival risks. Crucially, no model exhibits consistently\nmoral behavior in MoralSim, highlighting the need for caution when deploying\nLLMs in agentic roles where the agent's \"self-interest\" may conflict with\nethical expectations. Our code is available at\nhttps://github.com/sbackmann/moralsim.", "AI": {"tldr": "This paper introduces Moral Behavior in Social Dilemma Simulation (MoralSim) to investigate how large language models (LLMs) behave in social dilemmas when moral imperatives conflict with rewards.", "motivation": "Understanding how LLMs navigate moral dilemmas is crucial for ensuring ethical AI behavior, especially in agentic roles with decision-making responsibilities.", "method": "The study employs MoralSim to evaluate LLMs in the prisoner's dilemma and public goods game with various moral contexts, testing multiple state-of-the-art models across different scenarios.", "result": "Results reveal significant variability in LLMs' tendencies for moral behavior based on moral framing and situational dynamics; notably, no model consistently demonstrates moral behavior.", "conclusion": "The findings indicate that reliance on LLMs in ethically charged roles requires careful scrutiny due to inconsistencies in their moral decision-making.", "key_contributions": ["Introduction of MoralSim as a simulation for moral behavior in LLMs", "Evaluation of LLMs in social dilemmas with ethically-charged contexts", "Identification of variability in LLMs' moral responses across different scenarios"], "limitations": "Limited to specific game structures and models; may not fully generalize to all moral dilemmas or real-world applications.", "keywords": ["large language models", "moral behavior", "social dilemmas", "ethical alignment", "decision-making"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19217", "pdf": "https://arxiv.org/pdf/2505.19217.pdf", "abs": "https://arxiv.org/abs/2505.19217", "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training", "authors": ["Weize Chen", "Jiarui Yuan", "Tailin Jin", "Ning Ding", "Huimin Chen", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": "under review", "summary": "Recent large language models (LLMs) exhibit impressive reasoning but often\nover-think, generating excessively long responses that hinder efficiency. We\nintroduce DIET ( DIfficulty-AwarE Training), a framework that systematically\ncuts these \"token calories\" by integrating on-the-fly problem difficulty into\nthe reinforcement learning (RL) process. DIET dynamically adapts token\ncompression strategies by modulating token penalty strength and conditioning\ntarget lengths on estimated task difficulty, to optimize the\nperformance-efficiency trade-off. We also theoretically analyze the pitfalls of\nnaive reward weighting in group-normalized RL algorithms like GRPO, and propose\nAdvantage Weighting technique, which enables stable and effective\nimplementation of these difficulty-aware objectives. Experimental results\ndemonstrate that DIET significantly reduces token counts while simultaneously\nimproving reasoning performance. Beyond raw token reduction, we show two\ncrucial benefits largely overlooked by prior work: (1) DIET leads to superior\ninference scaling. By maintaining high per-sample quality with fewer tokens, it\nenables better scaling performance via majority voting with more samples under\nfixed computational budgets, an area where other methods falter. (2) DIET\nenhances the natural positive correlation between response length and problem\ndifficulty, ensuring verbosity is appropriately allocated, unlike many existing\ncompression methods that disrupt this relationship. Our analyses provide a\nprincipled and effective framework for developing more efficient, practical,\nand high-performing LLMs.", "AI": {"tldr": "DIET introduces a framework that improves LLM efficiency by reducing token counts during response generation through difficulty-aware training.", "motivation": "Large language models often generate overly long responses, leading to inefficiencies. The paper aims to address this issue by introducing a systematic approach to optimize response length based on task difficulty.", "method": "The DIET framework integrates problem difficulty into the reinforcement learning process by modulating token penalties and conditioning target response lengths on estimated task difficulty.", "result": "Experimental results show that DIET significantly reduces token counts while improving reasoning performance, enhancing inference scaling and maintaining quality in response generation.", "conclusion": "DIET provides an effective framework for creating more efficient and high-performing large language models by appropriately balancing response length and problem difficulty.", "key_contributions": ["Introduction of the DIET framework for difficulty-aware training in LLMs.", "Demonstration of a stable implementation of difficulty-aware objectives through Advantage Weighting technique.", "Illustration of how DIET improves inference scaling and maintains quality under fixed computational budgets."], "limitations": "", "keywords": ["large language models", "reinforcement learning", "token efficiency"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.19236", "pdf": "https://arxiv.org/pdf/2505.19236.pdf", "abs": "https://arxiv.org/abs/2505.19236", "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator", "authors": ["Qian Cao", "Xiting Wang", "Yuzhuo Yuan", "Yahui Liu", "Fang Luo", "Ruihua Song"], "categories": ["cs.CL"], "comment": null, "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.", "AI": {"tldr": "This paper proposes a pairwise-comparison framework for evaluating textual creativity in large language models (LLMs) and introduces a dataset called CreataSet to train a new LLM-based evaluator, CrEval, which outperforms existing methods in aligning with human judgments.", "motivation": "Creativity evaluation for large language models is currently inefficient as it relies on human judgments, and existing automated methods lack generalizability and alignment with human assessment.", "method": "The paper presents a novel pairwise-comparison framework that utilizes shared contextual instructions for more consistent evaluation of creativity. It also introduces CreataSet, a large-scale dataset for training the evaluation model.", "result": "CrEval, developed through training on CreataSet, shows superior performance in aligning with human judgments compared to existing evaluation methods.", "conclusion": "Integrating both human-generated and synthetic data is crucial for the development of robust evaluators. CrEval can effectively enhance the creativity of LLMs, and all associated data and models will be publicly released to aid further research.", "key_contributions": ["Introduction of a novel pairwise-comparison framework for creativity evaluation.", "Creation of CreataSet, a large-scale dataset for training evaluators.", "Development of CrEval, an LLM-based evaluator that surpasses existing methods."], "limitations": "", "keywords": ["creativity evaluation", "large language models", "dataset", "human judgment", "automated methods"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240.pdf", "abs": "https://arxiv.org/abs/2505.19240", "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole Pütz", "Benjamin Paaßen", "Steffen Eger"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research.", "AI": {"tldr": "This survey reviews the limitations of large language models (LLMs) from 2022 to 2024, highlighting trends and key areas of research.", "motivation": "To understand the growing concerns about the limitations of LLMs, including reasoning failures, hallucinations, and multilingual capabilities.", "method": "A semi-automated, data-driven approach was used to analyze 250,000 papers, filtering down to 14,648 relevant studies through keyword filtering and advanced classification methods.", "result": "LLM-related research has increased significantly, with LLLMs research growing rapidly and reasoning being the most studied limitation.", "conclusion": "The study provides insights into the evolving landscape of LLM limitations research and offers a dataset of annotated abstracts.", "key_contributions": ["Comprehensive identification of relevant LLM research papers", "Quantitative analysis of trends in LLM limitations", "Release of a dataset of annotated abstracts"], "limitations": "", "keywords": ["Large language models", "limitations", "research trends", "LLM-related papers", "multimodality"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19250", "pdf": "https://arxiv.org/pdf/2505.19250.pdf", "abs": "https://arxiv.org/abs/2505.19250", "title": "PATS: Process-Level Adaptive Thinking Mode Switching", "authors": ["Yi Wang", "Junxiao Liu", "Shimao Zhang", "Jiajun Chen", "Shujian Huang"], "categories": ["cs.CL"], "comment": null, "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs.", "AI": {"tldr": "This paper introduces Process-Level Adaptive Thinking Mode Switching (PATS) for large-language models to optimize reasoning strategies based on task difficulty, improving accuracy and efficiency in inference.", "motivation": "Current LLMs use a fixed reasoning strategy, leading to inefficiencies due to varying task complexities.", "method": "PATS utilizes Process Reward Models and Beam Search to dynamically adjust reasoning strategies at each step, allowing for progressive mode switching and penalties for poor performance.", "result": "Experiments show that PATS achieves high accuracy in mathematical benchmarks while maintaining moderate token usage.", "conclusion": "The study highlights the importance of adaptive reasoning strategies in LLMs for enhancing computational efficiency without sacrificing accuracy.", "key_contributions": ["Introduction of Process-Level Adaptive Thinking Mode Switching (PATS) for LLMs", "Integration of Process Reward Models with Beam Search for dynamic reasoning adjustment", "Demonstrated high accuracy with moderate computational resource use on benchmarks."], "limitations": "", "keywords": ["Large-language models", "reasoning strategies", "process reward models", "adaptive thinking", "computational efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19254", "pdf": "https://arxiv.org/pdf/2505.19254.pdf", "abs": "https://arxiv.org/abs/2505.19254", "title": "Unveiling Dual Quality in Product Reviews: An NLP-Based Approach", "authors": ["Rafał Poświata", "Marcin Michał Mirończuk", "Sławomir Dadas", "Małgorzata Grębowiec", "Michał Perełkiewicz"], "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Industry Track", "summary": "Consumers often face inconsistent product quality, particularly when\nidentical products vary between markets, a situation known as the dual quality\nproblem. To identify and address this issue, automated techniques are needed.\nThis paper explores how natural language processing (NLP) can aid in detecting\nsuch discrepancies and presents the full process of developing a solution.\nFirst, we describe in detail the creation of a new Polish-language dataset with\n1,957 reviews, 540 highlighting dual quality issues. We then discuss\nexperiments with various approaches like SetFit with sentence-transformers,\ntransformer-based encoders, and LLMs, including error analysis and robustness\nverification. Additionally, we evaluate multilingual transfer using a subset of\nopinions in English, French, and German. The paper concludes with insights on\ndeployment and practical applications.", "AI": {"tldr": "This paper addresses the dual quality problem in consumer products using NLP techniques by creating a Polish-language dataset and experimenting with various ML models for detection and analysis.", "motivation": "To tackle the inconsistent product quality experienced by consumers, particularly when identical products differ across markets.", "method": "The authors developed a Polish-language dataset with 1,957 reviews focused on dual quality issues, followed by experiments with NLP methods including SetFit, transformer-based encoders, and LLMs for discrepancies detection and robustness verification.", "result": "The experiments highlighted the effectiveness of various NLP approaches in detecting dual quality problems, alongside insights into multilingual transfer capabilities.", "conclusion": "The paper outlines practical insights for deploying their solution and suggests further applications in identifying dual quality issues across different languages.", "key_contributions": ["Creation of a new dataset for detecting dual quality issues in Polish language reviews.", "Evaluation of multiple NLP approaches including LLMs for improved accuracy.", "Insights into multilingual transfer of learned models for broader applicability."], "limitations": "", "keywords": ["dual quality", "NLP", "dataset", "multilingual transfer", "product quality"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286.pdf", "abs": "https://arxiv.org/abs/2505.19286", "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.", "AI": {"tldr": "This paper investigates the structural patterns of knowledge in large language models (LLMs) using a graph perspective, quantifying knowledge levels and developing graph machine learning models for better estimation and fine-tuning outcomes.", "motivation": "The study addresses the lack of focus on the structural patterns of knowledge in large language models, aiming to quantify and analyze these patterns from a graph perspective.", "method": "The authors quantify LLM knowledge at triplet and entity levels, analyze relationships with graph structural properties (e.g. node degree), and develop graph machine learning models for knowledge estimation based on local neighbors.", "result": "The research uncovers knowledge homophily in LLMs, showing that closely-related entities have similar knowledge levels, and demonstrates that selected triplets for fine-tuning improve performance.", "conclusion": "The findings suggest that a graph-based understanding of LLM knowledge can enhance knowledge checking and lead to improved model fine-tuning.", "key_contributions": ["Investigation of knowledge in LLMs from a graph structural perspective.", "Development of graph machine learning models for estimating entity knowledge.", "Evidence of knowledge homophily among topologically close entities."], "limitations": "", "keywords": ["large language models", "graph machine learning", "knowledge homophily", "fine-tuning", "knowledge estimation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293.pdf", "abs": "https://arxiv.org/abs/2505.19293", "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs.", "AI": {"tldr": "Introduction of a length-controllable long-context benchmark for evaluating LLMs, addressing current shortcomings in long-context evaluation metrics.", "motivation": "To enhance the evaluation of long-context capabilities in language models and clarify performance metrics compared to baseline abilities.", "method": "Development of a length-controllable benchmark and a novel metric that distinguishes baseline knowledge from true long-context capabilities in LLMs.", "result": "Experiments show that the proposed benchmark and metric provide a clearer and more effective evaluation of long-context performance in various LLMs.", "conclusion": "The novel benchmark and metric significantly improve the ability to evaluate long-context performances in language models, contributing to better model assessment and development.", "key_contributions": ["A length-controllable long-context benchmark for LLMs", "A novel evaluation metric distinguishing long-context capability from baseline performance", "Experimental validation demonstrating the effectiveness of the approach"], "limitations": "Potential reliance on specific model architectures and varying interpretations of long-context across different applications.", "keywords": ["long-context LLMs", "evaluation benchmark", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19299", "pdf": "https://arxiv.org/pdf/2505.19299.pdf", "abs": "https://arxiv.org/abs/2505.19299", "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations", "authors": ["Lingjun Zhao", "Hal Daumé III"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Faithful free-text explanations are important to ensure transparency in\nhigh-stakes AI decision-making contexts, but they are challenging to generate\nby language models and assess by humans. In this paper, we present a measure\nfor Prediction-EXplanation (PEX) consistency, by extending the concept of\nweight of evidence. This measure quantifies how much a free-text explanation\nsupports or opposes a prediction, serving as an important aspect of explanation\nfaithfulness. Our analysis reveals that more than 62% explanations generated by\nlarge language models lack this consistency. We show that applying direct\npreference optimization improves the consistency of generated explanations\nacross three model families, with improvement ranging from 43.1% to 292.3%.\nFurthermore, we demonstrate that optimizing this consistency measure can\nimprove explanation faithfulness by up to 9.7%.", "AI": {"tldr": "This paper introduces a measure for PEX consistency to evaluate the alignment between AI predictions and their explanations, highlighting the prevalence of inconsistency in language model-generated explanations and illustrating improvement through optimization techniques.", "motivation": "Ensuring transparency in AI decision-making, especially in high-stakes contexts, necessitates faithful explanations for predictions made by language models.", "method": "The paper presents a measure for measuring PEX consistency, which quantifies how much a free-text explanation supports or opposes a prediction. It involves the application of direct preference optimization to improve explanation consistency across different model families.", "result": "The study found that over 62% of explanations generated by large language models are inconsistent. Direct preference optimization improved consistency of explanations by 43.1% to 292.3% across three model families, with potential increases in explanation faithfulness by up to 9.7%.", "conclusion": "Optimizing PEX consistency significantly enhances the faithfulness of explanations provided by language models, thereby contributing to greater transparency in AI reasoning processes.", "key_contributions": ["Development of a measure for PEX consistency", "Demonstration of widespread inconsistency in model-generated explanations", "Optimization techniques effectively improve the quality of explanations"], "limitations": "", "keywords": ["AI transparency", "explanation consistency", "language models", "preference optimization", "explanation faithfulness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19300", "pdf": "https://arxiv.org/pdf/2505.19300.pdf", "abs": "https://arxiv.org/abs/2505.19300", "title": "SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking", "authors": ["Junnan Liu", "Linhao Luo", "Thuy-Trang Vu", "Gholamreza Haffari"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in large language models (LLMs) demonstrate their impressive\nreasoning capabilities. However, the reasoning confined to internal parametric\nspace limits LLMs' access to real-time information and understanding of the\nphysical world. To overcome this constraint, we introduce SituatedThinker, a\nnovel framework that enables LLMs to ground their reasoning in real-world\ncontexts through situated thinking, which adaptively combines both internal\nknowledge and external information with predefined interfaces. By utilizing\nreinforcement learning, SituatedThinker incentivizes deliberate reasoning with\nthe real world to acquire information and feedback, allowing LLMs to surpass\ntheir knowledge boundaries and enhance reasoning. Experimental results\ndemonstrate significant performance improvements on multi-hop\nquestion-answering and mathematical reasoning benchmarks. Furthermore,\nSituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,\nTableQA, and text-based games, showcasing the generalizable real-world grounded\nreasoning capability. Our codes are available at\nhttps://github.com/jnanliu/SituatedThinker.", "AI": {"tldr": "SituatedThinker is a framework that enables large language models to enhance their reasoning by integrating real-world information through situated thinking and reinforcement learning.", "motivation": "To address the limitations of large language models which restrict reasoning to internal parametric space, thereby lacking access to real-time information and understanding of the physical world.", "method": "The framework employs situated thinking to combine internal knowledge with external information via predefined interfaces, leveraging reinforcement learning to promote effective real-world reasoning and feedback acquisition.", "result": "Experimental results show significant improvements in multi-hop question-answering and mathematical reasoning, along with strong performance on unseen tasks such as KBQA and TableQA.", "conclusion": "SituatedThinker enhances LLMs' reasoning capabilities by grounding them in real-world contexts and demonstrating adaptability across various tasks.", "key_contributions": ["Introduction of the SituatedThinker framework for real-world grounded reasoning in LLMs", "Integration of reinforcement learning to enhance reasoning through real-world feedback", "Demonstrated generalizability across multiple unseen tasks and benchmarks"], "limitations": "", "keywords": ["Large Language Models", "Situated Thinking", "Reinforcement Learning", "Real-world Reasoning", "Grounded Knowledge"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345.pdf", "abs": "https://arxiv.org/abs/2505.19345", "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language generation (NLG) metrics play a central role in evaluating\ngenerated texts, but are not well suited for the structural and legal\ncharacteristics of patent documents. Large language models (LLMs) offer strong\npotential in automating patent generation, yet research on evaluating\nLLM-generated patents remains limited, especially in evaluating the generation\nquality of patent claims, which are central to defining the scope of\nprotection. Effective claim evaluation requires addressing legal validity,\ntechnical accuracy, and structural compliance. To address this gap, we\nintroduce PatentScore, a multi-dimensional evaluation framework for assessing\nLLM-generated patent claims. PatentScore incorporates: (1) hierarchical\ndecomposition for claim analysis; (2) domain-specific validation patterns based\non legal and technical standards; and (3) scoring across structural, semantic,\nand legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects\npatent-specific constraints and document structures, enabling evaluation beyond\nsurface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a\nPearson correlation of $r = 0.819$ with expert annotations, outperforming\nexisting NLG metrics. Furthermore, we conduct additional evaluations using open\nmodels such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong\ncorrelations with expert judgments, confirming the robustness and\ngeneralizability of our framework.", "AI": {"tldr": "The paper introduces PatentScore, a novel evaluation framework for assessing the quality of LLM-generated patent claims based on legal, technical, and structural dimensions.", "motivation": "The increasing use of LLMs for generating patent documents necessitates a specialized evaluation method that addresses the unique structural and legal characteristics of patents.", "method": "The framework involves hierarchical decomposition for claim analysis, domain-specific validation patterns, and a multi-dimensional scoring system that evaluates structural, semantic, and legal aspects of patent claims.", "result": "The framework showed a high Pearson correlation of $r = 0.819$ with expert annotations when evaluating 400 generated patent claims, indicating its effectiveness over traditional NLG metrics.", "conclusion": "PatentScore demonstrates the potential for effectively evaluating patent claims generated by LLMs, surpassing existing evaluation metrics and providing a robust solution for this domain.", "key_contributions": ["Introduction of a multi-dimensional evaluation framework for patent claims", "Incorporation of hierarchical claim analysis", "Demonstration of robust correlations with expert judgments in evaluation"], "limitations": "", "keywords": ["patent generation", "natural language generation", "evaluation framework", "large language models", "legal validity"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354.pdf", "abs": "https://arxiv.org/abs/2505.19354", "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public.", "AI": {"tldr": "GC-KBVQA improves Visual Question Answering by grounding question-aware captions and using LLMs effectively without fine-tuning.", "motivation": "To enhance the performance of Knowledge-Based Visual Question Answering (KB-VQA) methods by addressing the limitations of irrelevant auxiliary text.", "method": "We propose a four-stage framework called Grounding Caption-Guided Knowledge-Based Visual Question Answering (GC-KBVQA) which uses context-aware caption generation alongside external knowledge sources to inform LLMs.", "result": "GC-KBVQA significantly outperforms existing KB-VQA methods, demonstrating improved accuracy and efficacy in zero-shot VQA tasks without requiring task-specific fine-tuning.", "conclusion": "The GC-KBVQA framework facilitates high-quality VQA by leveraging the strengths of LLMs with rich contextual information, leading to reduced costs and complexities in deployment.", "key_contributions": ["Development of GC-KBVQA framework", "Use of grounding question-aware captions for context-rich information", "Ability to perform zero-shot tasks without multimodal training"], "limitations": "", "keywords": ["Visual Question Answering", "Knowledge-Based VQA", "Large Language Models", "Context-aware captioning", "Zero-shot learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19355", "pdf": "https://arxiv.org/pdf/2505.19355.pdf", "abs": "https://arxiv.org/abs/2505.19355", "title": "Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement", "authors": ["Lin Tian", "Marian-Andrei Rizoiu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Understanding true influence in social media requires distinguishing\ncorrelation from causation--particularly when analyzing misinformation spread.\nWhile existing approaches focus on exposure metrics and network structures,\nthey often fail to capture the causal mechanisms by which external temporal\nsignals trigger engagement. We introduce a novel joint treatment-outcome\nframework that leverages existing sequential models to simultaneously adapt to\nboth policy timing and engagement effects. Our approach adapts causal inference\ntechniques from healthcare to estimate Average Treatment Effects (ATE) within\nthe sequential nature of social media interactions, tackling challenges from\nexternal confounding signals. Through our experiments on real-world\nmisinformation and disinformation datasets, we show that our models outperform\nexisting benchmarks by 15--22% in predicting engagement across diverse\ncounterfactual scenarios, including exposure adjustment, timing shifts, and\nvaried intervention durations. Case studies on 492 social media users show our\ncausal effect measure aligns strongly with the gold standard in influence\nestimation, the expert-based empirical influence.", "AI": {"tldr": "A novel joint treatment-outcome framework is proposed to analyze causal mechanisms of engagement in social media, particularly in the context of misinformation spread.", "motivation": "Understanding true influence in social media requires distinguishing between correlation and causation, especially regarding the spread of misinformation.", "method": "The approach uses a joint treatment-outcome framework that combines causal inference techniques adapted from healthcare with sequential models to estimate Average Treatment Effects (ATE) in social media interactions.", "result": "Experiments show that the proposed model outperforms existing benchmarks by 15–22% in predicting engagement across various counterfactual scenarios.", "conclusion": "The framework effectively captures causal mechanisms, and case studies present strong alignment with expert-based empirical influence measures.", "key_contributions": ["Novel joint treatment-outcome framework for analyzing social media engagement", "Application of causal inference from healthcare to social media", "Demonstrated improved prediction of engagement metrics over existing methods"], "limitations": "", "keywords": ["social media", "causal inference", "misinformation", "engagement", "treatment effects"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.19360", "pdf": "https://arxiv.org/pdf/2505.19360.pdf", "abs": "https://arxiv.org/abs/2505.19360", "title": "ChartLens: Fine-grained Visual Attribution in Charts", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main)", "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.", "AI": {"tldr": "The paper presents ChartLens, a novel chart attribution algorithm designed to improve the accuracy of multimodal large language models (MLLMs) by addressing hallucinations in chart understanding.", "motivation": "MLLMs often generate conflicting text when interpreting visual data, which can hinder their effectiveness in understanding charts.", "method": "ChartLens utilizes segmentation-based techniques to identify chart elements and employs set-of-marks prompting with MLLMs for visual attribution.", "result": "ChartLens enhances fine-grained attributions by 26-66%, demonstrating superior performance in validating chart-associated responses against visual data.", "conclusion": "The introduction of ChartLens and the ChartVA-Eval benchmark provides a significant improvement in the reliability of MLLMs in chart understanding tasks.", "key_contributions": ["Introduction of ChartLens algorithm for fine-grained visual attribution", "Development of a benchmark, ChartVA-Eval, with diverse real-world chart data", "Significant improvement in attribution accuracy by 26-66%."], "limitations": "", "keywords": ["multimodal large language models", "chart understanding", "visual attribution"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19376", "pdf": "https://arxiv.org/pdf/2505.19376.pdf", "abs": "https://arxiv.org/abs/2505.19376", "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "authors": ["Lance Ying", "Almog Hillel", "Ryan Truong", "Vikash K. Mansinghka", "Joshua B. Tenenbaum", "Tan Zhi-Xuan"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures; oral presentation at CogSci 2025", "summary": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses.", "AI": {"tldr": "This paper investigates how people attribute beliefs to others based on the explanatory strength of those beliefs, using a computational model to analyze factors influencing these attributions.", "motivation": "To understand which specific beliefs people prefer to attribute to others as explanations for observed behaviors.", "method": "The authors develop a computational model assessing belief attribution using three factors: accuracy, informativity, and causal relevance, analyzed via a study where participants rank beliefs about an agent's behavior.", "result": "The study finds that while accuracy and informativity help predict rankings, causal relevance is the most significant factor in how participants attribute beliefs.", "conclusion": "Causal relevance plays a key role in belief attribution, suggesting that people are inclined to attribute beliefs that provide strong explanations for behaviors.", "key_contributions": ["Development of a computational model for belief attribution", "Identification of key factors (accuracy, informativity, causal relevance) influencing belief attributions", "Empirical findings showing causal relevance's dominance in belief selection"], "limitations": "", "keywords": ["theory-of-mind", "belief attribution", "causal relevance", "natural language processing", "human behavior"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.19384", "pdf": "https://arxiv.org/pdf/2505.19384.pdf", "abs": "https://arxiv.org/abs/2505.19384", "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor", "authors": ["Seokgi Lee", "Jungjun Kim"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "7 pages, 3 figures", "summary": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure.", "AI": {"tldr": "Introducing GSA-TTS, a style adaptor for TTS that encodes speaking styles from acoustic references for zero-shot synthesis.", "motivation": "To enhance speech synthesis by improving the naturalness, speaker similarity, and intelligibility of generated speech using hierarchical style encoding.", "method": "GSA-TTS employs a novel style encoder that captures local speaking styles from semantic sound units and combines them using self-attention, creating a global style condition.", "result": "GSA-TTS demonstrates promising results on unseen speakers, showcasing improvements in naturalness, speaker similarity, and intelligibility in speech synthesis tasks.", "conclusion": "GSA-TTS provides a robust style representation that enhances interpretability and controllability in text-to-speech applications, benefiting from its hierarchical structure.", "key_contributions": ["Introduction of a novel style encoder for TTS", "Hierarchical semantic and style representation", "Improvement of interpretability and controllability in TTS systems"], "limitations": "", "keywords": ["Text-to-Speech", "Speech Synthesis", "Style Encoding"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.19388", "pdf": "https://arxiv.org/pdf/2505.19388.pdf", "abs": "https://arxiv.org/abs/2505.19388", "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 System Demonstration Track, 11 pages, 9 figures", "summary": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube.", "AI": {"tldr": "Introduction of the gec-metrics library for grammatical error correction evaluation.", "motivation": "To enable fair comparisons of GEC systems through a consistent implementation of evaluation metrics.", "method": "Development of a library with a unified API for evaluating grammatical error correction systems, including meta-evaluation functionalities and analysis scripts.", "result": "The gec-metrics library allows for extensible and consistent evaluations of GEC systems.", "conclusion": "The library enhances the evaluation process in GEC, facilitating better comparisons and improvements in this NLP field.", "key_contributions": ["Unified interface for GEC metrics evaluation", "Focus on API usability and extensibility", "Includes meta-evaluation and visualization tools"], "limitations": "", "keywords": ["grammatical error correction", "evaluation metrics", "NLP", "library", "API"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.19392", "pdf": "https://arxiv.org/pdf/2505.19392.pdf", "abs": "https://arxiv.org/abs/2505.19392", "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "comment": null, "summary": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias.", "AI": {"tldr": "This paper introduces a new baseline method leveraging an LLM to evaluate code summaries, improving reliability over traditional methods.", "motivation": "To address the challenges of comparing code summary generation techniques due to expensive human evaluations and unreliable automatic metrics.", "method": "The proposed baseline uses an LLM to assign an overall score to code summaries, incorporating code context, and introduces a variant that evaluates documentation quality without relying on reference summaries.", "result": "The new approach performs as well or better than existing metrics while recommending to use it alongside embedding-based methods to mitigate LLM-specific bias.", "conclusion": "The proposed LLM-based scoring method enhances evaluation of code summaries and documentation, potentially serving diverse applications.", "key_contributions": ["Introduction of LLM-based evaluation for code summaries", "Ability to evaluate without reference summaries", "Recommendation to use in combination with existing metrics"], "limitations": "Potential LLM-specific biases and the necessity of using alongside other metrics.", "keywords": ["code documentation", "code summaries", "LLM", "evaluation metrics", "Human-Computer Interaction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.19405", "pdf": "https://arxiv.org/pdf/2505.19405.pdf", "abs": "https://arxiv.org/abs/2505.19405", "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "authors": ["Yan Wen", "Junfeng Guo", "Heng Huang"], "categories": ["cs.CL", "cs.CR"], "comment": "18 pages, 1 figure", "summary": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems.", "AI": {"tldr": "CoTGuard is a framework designed for copyright protection in multi-agent LLM systems by detecting unauthorized content during the reasoning process.", "motivation": "The rise of multi-agent LLM systems presents new challenges for copyright protection as they may recall sensitive or copyrighted content during inter-agent communication.", "method": "CoTGuard employs trigger-based detection within Chain-of-Thought reasoning by embedding specific trigger queries into agent prompts to monitor intermediate reasoning steps.", "result": "Extensive experiments demonstrate that CoTGuard effectively detects content leakage while maintaining task performance.", "conclusion": "Reasoning-level monitoring through CoTGuard provides a promising approach for safeguarding intellectual property in collaborative LLM agent systems.", "key_contributions": ["Introduction of CoTGuard framework for copyright protection in LLMs", "Utilization of trigger-based detection within Chain-of-Thought reasoning", "Demonstration of effective content leakage detection with minimal task performance interference"], "limitations": "", "keywords": ["copyright protection", "multi-agent systems", "Chain-of-Thought reasoning"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2505.19410", "pdf": "https://arxiv.org/pdf/2505.19410.pdf", "abs": "https://arxiv.org/abs/2505.19410", "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering", "authors": ["Jiajun Zhu", "Ye Liu", "Meikai Bao", "Kai Zhang", "Yanghai Zhang", "Qi Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability.", "AI": {"tldr": "The paper presents Self-Reflective Planning (SRP), a framework that enhances large language models (LLMs) by integrating them with knowledge graphs for improved reasoning accuracy.", "motivation": "LLMs often hallucinate due to insufficient internal knowledge, making their reasoning prone to inaccuracies. The need for better integration of structured, reliable information from knowledge graphs (KGs) is crucial for enhancing LLM capabilities.", "method": "SRP enhances LLMs by implementing iterative, reference-guided reasoning. It involves planning guided by references, checking initial relations, generating reasoning paths, and iteratively reflecting on results to refine these paths.", "result": "SRP demonstrates superior reasoning abilities compared to various strong baselines in experiments conducted on three public datasets.", "conclusion": "The proposed SRP framework not only improves reasoning accuracy for LLMs by leveraging KGs but also shows reliable performance in generating correct answers through iterative reflection.", "key_contributions": ["Introduction of Self-Reflective Planning (SRP) framework for LLMs", "Integration of iterative, reference-guided reasoning", "Demonstrated improvements in reasoning accuracy over existing methods"], "limitations": "", "keywords": ["large language models", "knowledge graphs", "reasoning", "iterative reflection", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426.pdf", "abs": "https://arxiv.org/abs/2505.19426", "title": "The Role of Diversity in In-Context Learning for Large Language Models", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection.", "AI": {"tldr": "The paper investigates the role of diversity in in-context learning (ICL) for large language models, showing that diversity-aware example selection improves performance on various tasks.", "motivation": "To explore the underexplored impact of diversity in examples selection for in-context learning in large language models (LLMs).", "method": "The authors conducted systematic experiments across diverse tasks, including sentiment classification and complex math and code problems, using various model families like Llama-3.1, Gemma-2, and Mistral-v0.3.", "result": "Diversity-aware selection methods were found to significantly enhance performance, especially on complicated tasks, and increase robustness to out-of-distribution queries.", "conclusion": "Incorporating diversity in in-context example selection leads to improved model performance and robustness in diverse query scenarios.", "key_contributions": ["Introduces a theoretical framework explaining the benefits of diversity in example selection", "Demonstrates improved performance in complex tasks through diversity-aware methods", "Shows enhanced robustness to out-of-distribution queries"], "limitations": "", "keywords": ["in-context learning", "diversity", "large language models", "example selection", "performance"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.19428", "pdf": "https://arxiv.org/pdf/2505.19428.pdf", "abs": "https://arxiv.org/abs/2505.19428", "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things", "authors": ["Abhijnan Nath", "Carine Graff", "Andrei Bachinin", "Nikhil Krishnaswamy"], "categories": ["cs.CL"], "comment": "48 pages (main paper: 10 pages incl. Limitations and Acknowledgments;\n  references: 6 pages; appendix: 32 pages), 9 figures, 12 tables, appearing in\n  Proceedings of ACL 2025, Vienna, Austria", "summary": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL.", "AI": {"tldr": "The Frictional Agent Alignment Framework (FAAF) enhances AI's role in dynamic collaborative interactions by generating precise context-aware friction to address belief misalignments between interlocutors.", "motivation": "To improve collaborative interactions by mediating belief misalignments that occur in dynamic settings with sparse signals of interlocutor beliefs.", "method": "FAAF decouples a two-player objective by implementing a frictive-state policy to identify misalignments and an intervention policy to craft preferred responses. The solution allows training through a simple supervised loss.", "result": "FAAF outperforms existing methods in generating interpretable friction and demonstrates improved out-of-distribution generalization across three benchmarks.", "conclusion": "By positioning LLMs as adaptive thought partners instead of passive responders, FAAF facilitates scalable and dynamic human-AI collaboration.", "key_contributions": ["Proposes the Frictional Agent Alignment Framework (FAAF) for dynamic collaborative tasks.", "Demonstrates superior performance in generating context-aware friction and belief alignment.", "Provides analytical solutions enabling effective training of collaborative AI policies."], "limitations": "Potential limitations include the framework's dependency on the specific nature of collaboration and the availability of recognizable belief signals.", "keywords": ["AI collaboration", "Frictional Agent Alignment Framework", "dynamic interaction", "human-AI collaboration", "belief alignment"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.19429", "pdf": "https://arxiv.org/pdf/2505.19429.pdf", "abs": "https://arxiv.org/abs/2505.19429", "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "categories": ["cs.CL"], "comment": null, "summary": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight finetuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models finetuned with in-domain data\nsignificantly outperform their zero-shot performance. The finetuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media.", "AI": {"tldr": "The paper presents Rhapsody, a dataset for podcast highlight detection, and evaluates various approaches to automate this process, revealing challenges even for advanced language models.", "motivation": "To aid podcast users in swiftly identifying content highlights, thereby improving the listening experience amid the abundance of available podcasts.", "method": "The paper investigates the podcast highlight detection problem as a binary classification challenge, utilizing Rhapsody dataset and baseline approaches like zero-shot prompting and fine-tuned language models.", "result": "The experimental results show that state-of-the-art language models struggle with podcast highlight detection, whereas models fine-tuned on the in-domain data significantly outperform zero-shot models, benefiting from integrating speech features and transcripts.", "conclusion": "Fine-tuned language models demonstrate improved performance in identifying podcast highlights, highlighting the complexities of parsing long-form spoken content for information retrieval.", "key_contributions": ["Introduction of Rhapsody dataset with highlight scores for 13K podcast episodes.", "Evaluation of language model approaches in detecting podcast highlights.", "Demonstration that model fine-tuning with in-domain data enhances performance significantly."], "limitations": "The study primarily focused on existing language models, which may limit the generalization of findings to future models or unintended podcast formats.", "keywords": ["podcast", "highlight detection", "language models", "finetuning", "speech transcripts"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19430", "pdf": "https://arxiv.org/pdf/2505.19430.pdf", "abs": "https://arxiv.org/abs/2505.19430", "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research.", "AI": {"tldr": "This paper introduces Fin-Force, a benchmark that supports LLM-based forward counterfactual reasoning in financial markets to automate insights for decision-making.", "motivation": "To facilitate understanding of potential future developments in dynamic financial markets, aiding stakeholders in decision-making by employing automated counterfactual reasoning.", "method": "The paper curates financial news headlines to create the Fin-Force benchmark, which evaluates LLMs and counterfactual generation methods for their applicability in predicting future market behaviors.", "result": "Experiments reveal the shortcomings of current LLM approaches in forward counterfactual generation and provide structured insights for future research in this area.", "conclusion": "Fin-Force enables scalable and automated forward counterfactual reasoning, which can significantly enhance decision-making in financial markets.", "key_contributions": ["Introduction of the Fin-Force benchmark for forward counterfactual reasoning in finance", "Evaluation of state-of-the-art LLMs for generating counterfactual insights", "Identification of limitations in current counterfactual generation approaches."], "limitations": "Mainly focused on financial contexts, which may limit broader applicability of findings; relies heavily on the quality of curated financial news headlines.", "keywords": ["Counterfactual reasoning", "Financial markets", "Large Language Models", "Benchmarking", "Decision-making"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.19435", "pdf": "https://arxiv.org/pdf/2505.19435.pdf", "abs": "https://arxiv.org/abs/2505.19435", "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection", "authors": ["Zhihong Pan", "Kai Zhang", "Yuze Zhao", "Yupeng Han"], "categories": ["cs.CL"], "comment": null, "summary": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%.", "AI": {"tldr": "Route-To-Reason (RTR) is a framework for optimizing language model performance in reasoning tasks by dynamically routing models and strategies based on task difficulty, achieving higher accuracy and reduced computational costs.", "motivation": "To address the computational costs and pitfalls of reasoning tasks with language models, especially related to overthinking during complex reasoning.", "method": "RTR dynamically allocates language models and reasoning strategies based on task difficulty under budget constraints, learning compressed representations for adaptive selection at inference time.", "result": "RTR achieves a higher accuracy than the best single model while reducing token usage by over 60%, demonstrating a successful trade-off between accuracy and computational efficiency.", "conclusion": "RTR provides a low-cost, flexible, and adaptable approach for improved reasoning capabilities in language models, applicable to various model types and strategies.", "key_contributions": ["Introduction of Route-To-Reason (RTR) framework", "Dynamic allocation of models and strategies based on task difficulty", "Significant reduction in computational costs while improving accuracy"], "limitations": "", "keywords": ["Language Models", "Reasoning Strategies", "Computational Efficiency", "Dynamic Routing", "Machine Learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439.pdf", "abs": "https://arxiv.org/abs/2505.19439", "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.", "AI": {"tldr": "This research proposes using format and length as surrogate signals to train Large Language Models (LLMs) for mathematical problem-solving, achieving performance improvements without traditional ground truth answers.", "motivation": "Address the challenges and costs of obtaining ground truth answers for training LLMs in mathematical problem-solving.", "method": "The study utilizes a reward function based on format correctness initially, and then incorporates length-based rewards in later phases, creating a GRPO approach that leverages these surrogate signals.", "result": "The new GRPO approach achieves performance comparable or superior to standard GRPO algorithms relying on ground truth answers, reaching 40.0% accuracy on AIME2024 with a 7B base model.", "conclusion": "The research offers a practical solution for training LLMs with less reliance on extensive ground truth data, demonstrating that enhancing answering habits can unlock the capabilities of base models.", "key_contributions": ["Utilization of format and length as surrogate signals for training LLMs.", "Performance improvements in mathematical problem-solving without traditional ground truth answers.", "A practical approach to reducing dependence on costly data collection for LLM training."], "limitations": "Performance may vary across different scenarios and further evaluation needed in late phases.", "keywords": ["Large Language Models", "Reinforcement Learning", "Mathematical Problem-Solving"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19440", "pdf": "https://arxiv.org/pdf/2505.19440.pdf", "abs": "https://arxiv.org/abs/2505.19440", "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models.", "AI": {"tldr": "The paper examines how interpretable categorical features develop in large language models, focusing on timeline, model scale, and layer-specific behaviors using sparse autoencoders.", "motivation": "To better understand how interpretable features emerge in large language models (LLMs) and challenge assumptions about their representational dynamics.", "method": "Employing sparse autoencoders for mechanistic interpretability to analyze the behavior of LLM features across different training checkpoints, transformer layers, and model sizes.", "result": "The study finds distinct temporal and spatial thresholds for feature emergence, along with notable semantic reactivations in early-layer features at later layers.", "conclusion": "The findings highlight the complex dynamics of feature emergence in LLMs and suggest a reevaluation of current theories regarding their representational processes.", "key_contributions": ["Identification of temporal and scale-specific thresholds for feature emergence.", "Discovery of unexpected semantic reactivation patterns in LLMs.", "Insights into representational dynamics that challenge existing assumptions."], "limitations": "", "keywords": ["language models", "interpretability", "semantic features", "neural activations", "transformer dynamics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19472", "pdf": "https://arxiv.org/pdf/2505.19472.pdf", "abs": "https://arxiv.org/abs/2505.19472", "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU).", "AI": {"tldr": "FlowHN is a novel parallel hybrid network architecture that improves token processing speeds and accuracy by effectively balancing computational load between Attention and State-Space Models.", "motivation": "To address the inefficiencies in existing hybrid networks that either use sequential or parallel pipelines, leading to latency and throughput issues.", "method": "FlowHN implements a parallel hybrid network architecture with a dynamic token split approach for load balancing between Attention and State-Space Models, enhancing processing efficiency and representation fidelity.", "result": "FlowHN achieves up to 4x higher Tokens per Second and 2x better Model FLOPs Utilization compared to other hybrid models.", "conclusion": "FlowHN demonstrates significant improvements in accuracy and processing speed, making it a more efficient alternative to traditional hybrid architectures.", "key_contributions": ["Novel parallel hybrid architecture using Attention and SSMs", "Dynamic token split for computational load balancing", "Enhanced output fusion method for better representation expressivity"], "limitations": "", "keywords": ["Hybrid Networks", "Parallel Architecture", "Load Balancing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.19475", "pdf": "https://arxiv.org/pdf/2505.19475.pdf", "abs": "https://arxiv.org/abs/2505.19475", "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation.", "AI": {"tldr": "Introducing VDS-TTT framework for efficient adaptation of LLMs to out-of-distribution data using a verifier-driven sample selection process.", "motivation": "Adapting pretrained language models to new, unlabeled data presents significant challenges due to their poor performance on structurally novel reasoning tasks outside their training distribution.", "method": "The VDS-TTT framework employs a learned verifier to score a pool of generated responses, selecting the highest-ranked examples for test-time training. Low-rank LoRA adapter parameters are fine-tuned for efficient adaptation.", "result": "Experiments show VDS-TTT provides up to a 32.29% relative performance improvement over baseline models and a 6.66% gain compared to verifier-based methods that do not employ test-time training.", "conclusion": "VDS-TTT is an effective self-supervised framework enabling continuous self-improvement for large language models during deployment.", "key_contributions": ["Introduction of VDS-TTT framework for language model adaptation", "First method to synthesize verifier-driven test-time training data", "Demonstration of significant performance improvements across multiple benchmarks"], "limitations": "", "keywords": ["language models", "test-time training", "unsupervised learning", "verifier-driven adaptation", "self-supervised learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.19484", "pdf": "https://arxiv.org/pdf/2505.19484.pdf", "abs": "https://arxiv.org/abs/2505.19484", "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning.", "AI": {"tldr": "CulFiT is a culturally-aware training paradigm for Large Language Models that incorporates multilingual data and fine-grained reward modeling to enhance cultural sensitivity and inclusivity.", "motivation": "To mitigate the cultural biases of LLMs that undermine equality and perpetuate discrimination, especially in low-resource regions.", "method": "CulFiT synthesizes diverse cultural-related questions, constructs critique data in relevant languages, and uses fine-grained rewards to evaluate cultural texts as verifiable knowledge units.", "result": "CulFiT achieves state-of-the-art performance in cultural alignment and reasoning across multiple benchmarks, including a newly introduced GlobalCultureQA dataset.", "conclusion": "The proposed approach significantly enhances the cultural sensitivity of LLMs, making them more inclusive and less biased.", "key_contributions": ["Introduction of CulFiT, a culturally-aware training paradigm for LLMs.", "Development of GlobalCultureQA, a multilingual dataset for evaluating cultural responses.", "Demonstration of state-of-the-art performance in cultural alignment and reasoning."], "limitations": "", "keywords": ["Large Language Models", "Cultural Bias", "Cultural Sensitivity", "Multilingual Data", "Question-Answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19494", "pdf": "https://arxiv.org/pdf/2505.19494.pdf", "abs": "https://arxiv.org/abs/2505.19494", "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents", "authors": ["Manoj Balaji Jagadeeshan", "Prince Raj", "Pawan Goyal"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana", "AI": {"tldr": "The study benchmarks retrieval methods for Sanskrit documents using English queries, focusing on cross-lingual challenges with a dataset of 3,400 pairs.", "motivation": "To enhance the retrieval systems in a RAG framework for Sanskrit documents and improve accessibility to ancient texts.", "method": "The study utilizes Direct Retrieval, Translation-based Retrieval, and Query Translation, employing shared embedding spaces and fine-tuning state-of-the-art models like BM25 and GPT-2.", "result": "Translation-based Retrieval methods significantly outperform Direct Retrieval and Query Translation in addressing linguistic challenges, improving comprehension and access to Sanskrit texts.", "conclusion": "The study demonstrates that advanced translation methods can effectively bridge the gap in cross-lingual retrieval for ancient documents, providing a means to preserve and disseminate Sanskrit scriptures.", "key_contributions": ["Development of a benchmarking framework for Sanskrit retrieval", "Publicly available dataset of English-Sanskrit query-document pairs", "Fine-tuning of state-of-the-art models for Sanskrit nuances"], "limitations": "", "keywords": ["Sanskrit retrieval", "cross-lingual challenges", "RAG framework"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2505.19510", "pdf": "https://arxiv.org/pdf/2505.19510.pdf", "abs": "https://arxiv.org/abs/2505.19510", "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study", "authors": ["Dongil Yang", "Minjin Kim", "Sunghwan Kim", "Beong-woo Kwak", "Minjun Park", "Jinseok Hong", "Woontack Woo", "Jinyoung Yeo"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://anonymous.4open.science/r/TSG-Bench.", "AI": {"tldr": "This paper introduces TSG Bench, a benchmark for evaluating LLMs' capabilities in understanding and generating scene graphs from text, highlighting their strengths and weaknesses in this area.", "motivation": "To evaluate the capacity of Large Language Models to understand and generate scene graphs, addressing the gap in existing benchmarks.", "method": "Introduces TSG Bench, a benchmark specifically designed for assessing LLMs' understanding and generation of scene graphs from textual narratives, evaluating 11 LLMs.", "result": "Models excel in scene graph understanding but struggle significantly with scene graph generation, especially for complex narratives, indicating a challenge in decomposing scenes.", "conclusion": "The findings suggest a need for improved methodologies in scene graph generation and provide insights for future research directions.", "key_contributions": ["Development of the TSG Bench benchmark for LLM evaluation", "Identification of performance gaps between scene graph understanding and generation", "Public availability of code and evaluation data for further research."], "limitations": "Focus on a limited set of 11 LLMs; does not explore other evaluation metrics beyond scene graphs.", "keywords": ["Large Language Models", "scene graphs", "benchmark", "evaluation", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19511", "pdf": "https://arxiv.org/pdf/2505.19511.pdf", "abs": "https://arxiv.org/abs/2505.19511", "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models", "authors": ["Aggrey Muhebwa", "Khalid K. Osman"], "categories": ["cs.CL"], "comment": null, "summary": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs.", "AI": {"tldr": "The paper presents a framework for distilling causal reasoning from large models to smaller ones using a new metric for evaluating explanation quality.", "motivation": "To address the gap in causal reasoning abilities between large proprietary language models and smaller open-source models.", "method": "The framework trains the smaller model to generate structured cause-and-effect explanations aligned with those of a teacher model, utilizing the new Causal Explanation Coherence (CEC) metric for evaluation.", "result": "The proposed CEC metric measures the structural and logical consistency of generated explanations, providing insights into the performance of smaller models in causal reasoning tasks.", "conclusion": "This framework enables smaller models to acquire robust causal reasoning abilities and offers a systematic way to assess the coherence of language model outputs.", "key_contributions": ["Introduction of a framework for distilling causal reasoning from teacher to student models.", "Development of the Causal Explanation Coherence (CEC) metric for assessing explanation quality.", "Demonstration of how smaller models can be trained to achieve robust causal reasoning capabilities."], "limitations": "", "keywords": ["causal reasoning", "language models", "explanation coherence"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514.pdf", "abs": "https://arxiv.org/abs/2505.19514", "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows.", "AI": {"tldr": "SIPDO introduces a framework that optimizes prompts for large language models by integrating synthetic data generation into the optimization process, enabling iterative improvement of prompt performance.", "motivation": "To improve prompt quality for large language models by addressing limitations in current optimization methods that rely on fixed datasets and lack iterative refinement.", "method": "SIPDO is a closed-loop framework that couples a synthetic data generator with a prompt optimizer, allowing for the generation of new examples that highlight prompt weaknesses and guide incremental refinement.", "result": "Experiments indicate that SIPDO outperforms traditional prompt tuning methods on question answering and reasoning benchmarks.", "conclusion": "Integrating data synthesis into the prompt learning workflow significantly enhances prompt performance and allows for systematic improvements without needing external supervision.", "key_contributions": ["Introduction of SIPDO framework for prompt optimization", "Coupling synthetic data generation with prompt tuning for iterative improvement", "Demonstrated superior performance on benchmarks compared to standard methods."], "limitations": "", "keywords": ["prompt optimization", "synthetic data generation", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19515", "pdf": "https://arxiv.org/pdf/2505.19515.pdf", "abs": "https://arxiv.org/abs/2505.19515", "title": "Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "categories": ["cs.CL"], "comment": "8 pages", "summary": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts.", "AI": {"tldr": "This paper analyzes Donald Trump's rhetorical strategies in the 2024 U.S. presidential debates using a new annotation framework called BEADS, highlighting his use of emotionally charged and adversarial discourse.", "motivation": "To understand Donald Trump's rhetorical strategies during the 2024 U.S. presidential debates and establish a framework for analyzing political discourse.", "method": "Introduces BEADS, an annotation framework that extends the DAMSL framework with tags for ideological framing, emotional appeals, and adversarial tactics, comparing human annotations with ChatGPT-assisted tagging on debate transcripts.", "result": "The analysis reveals that Trump excelled in categories such as Challenge and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias, and Perceived Dismissiveness.", "conclusion": "BEADS is proposed as a scalable and reproducible tool for critical discourse analysis applicable in various languages and political contexts.", "key_contributions": ["Development of BEADS for analyzing political discourse", "Comparison of human annotation and AI-assisted tagging", "Insights into Trump's rhetorical strategies in political debates"], "limitations": "", "keywords": ["Discourse Analysis", "Political Communication", "Rhetorical Strategies"], "importance_score": 2, "read_time_minutes": 8}}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528.pdf", "abs": "https://arxiv.org/abs/2505.19528", "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness.", "AI": {"tldr": "AmpleHate is a novel approach for implicit hate speech detection that mimics human inference by identifying explicit targets and their contextual relationships using attention mechanisms, achieving state-of-the-art performance.", "motivation": "The paper addresses the challenge of detecting implicit hate speech, which is often subtle and context-dependent, highlighting the differences between machine learning approaches and human understanding.", "method": "AmpleHate uses a pretrained Named Entity Recognition model to identify explicit targets, then captures implicit target information with [CLS] tokens, calculating attention-based relationships between these targets and the context to improve sentence representation.", "result": "AmpleHate outperforms existing contrastive learning methods by an average of 82.14% and shows faster convergence in training, while attention patterns align with human judgement.", "conclusion": "The effectiveness of AmpleHate in identifying implicit hate speech demonstrates its potential for interpretability and robustness in understanding context relations.", "key_contributions": ["Introduction of AmpleHate for implicit hate speech detection mimicking human inference.", "Use of attention-based mechanisms to enhance relational context understanding.", "Achieving state-of-the-art performance and faster convergence compared to previous methods."], "limitations": "", "keywords": ["implicit hate speech", "named entity recognition", "attention mechanism", "contrastive learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19529", "pdf": "https://arxiv.org/pdf/2505.19529.pdf", "abs": "https://arxiv.org/abs/2505.19529", "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "authors": ["Tanjil Hasan Sakib", "Md. Tanzib Hosain", "Md. Kishor Morol"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.", "AI": {"tldr": "The paper reviews Small Language Models (SLMs), their optimization techniques, and proposes a classification system for their design and evaluation.", "motivation": "To evaluate and optimize Small Language Models (SLMs) for deployment in resource-limited environments, enhancing their performance and efficiency.", "method": "The study involves a comprehensive assessment of SLMs, outlining design frameworks, training approaches, and optimization techniques such as pruning, quantization, and model compression.", "result": "A novel classification system for SLM optimization is proposed, alongside an evaluation suite using existing datasets, highlighting SLM capabilities and current limitations in the field.", "conclusion": "The research serves as a guide for constructing efficient and high-performing SLMs, while also addressing unresolved issues such as efficiency-performance trade-offs.", "key_contributions": ["Proposed a classification system for SLM optimization approaches.", "Established an evaluation suite for SLM capabilities using existing datasets.", "Identified unresolved challenges in SLM development and future research directions."], "limitations": "The study acknowledges unresolved trade-offs between efficiency and performance in SLMs.", "keywords": ["Small Language Models", "model optimization", "pruning", "quantization", "model compression"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19538", "pdf": "https://arxiv.org/pdf/2505.19538.pdf", "abs": "https://arxiv.org/abs/2505.19538", "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": "32 pages, 5 figures, 5 tables", "summary": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems.", "AI": {"tldr": "DoctorRAG is a novel RAG framework that integrates both explicit clinical knowledge and experiential knowledge from patient cases to emulate human clinical reasoning in medical contexts.", "motivation": "Existing medical RAG systems primarily rely on formal medical knowledge bases and overlook the valuable experiential knowledge that comes from similar patient cases, crucial for human clinical reasoning.", "method": "DoctorRAG uses a hybrid retrieval mechanism that combines relevant clinical knowledge with experiential case-based information, incorporating conceptual tags and a Med-TextGrad module for output accuracy.", "result": "Experiments show that DoctorRAG significantly outperforms existing RAG models in terms of response accuracy, relevance, and comprehensiveness, benefiting from iterative improvements.", "conclusion": "DoctorRAG represents a meaningful advancement towards creating medical reasoning systems that better reflect human-like clinical decision-making processes.", "key_contributions": ["Integration of experiential knowledge with clinical knowledge in RAG systems", "Use of hybrid retrieval mechanisms to enhance performance", "Implementation of a Med-TextGrad module for output validation"], "limitations": "", "keywords": ["medical RAG", "case-based reasoning", "clinical decision-making", "hybrid retrieval", "experiential knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19548", "pdf": "https://arxiv.org/pdf/2505.19548.pdf", "abs": "https://arxiv.org/abs/2505.19548", "title": "How Syntax Specialization Emerges in Language Models", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance.", "AI": {"tldr": "This paper analyzes the development of syntactic specialization in large language models during training, highlighting a clear trajectory and critical periods of sensitivity emergence.", "motivation": "To understand the emergence and development of syntactic sensitivity in large language models (LLMs) during their training process.", "method": "The study quantifies internal syntactic consistency by tracking minimal pairs across various syntactic phenomena throughout training.", "result": "A developmental trajectory is identified where syntactic sensitivity gradually emerges, concentrates in specific layers, and exhibits a 'critical period' of rapid internal specialization, influenced by model scale and training data.", "conclusion": "The findings reveal insights into how syntax is internalized in LLMs during training and suggest a systematic process of specialization that is consistent across different model architectures.", "key_contributions": ["Identified a developmental trajectory for syntactic specialization in LLMs.", "Showed the influence of model scale and training data on specialization.", "Provided evidence of a critical period for rapid internal specialization."], "limitations": "", "keywords": ["large language models", "syntactic structure", "internal specialization", "training process", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19549", "pdf": "https://arxiv.org/pdf/2505.19549.pdf", "abs": "https://arxiv.org/abs/2505.19549", "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents", "authors": ["Derong Xu", "Yi Wen", "Pengyue Jia", "Yingyi Zhang", "wenlin zhang", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao", "Enhong Chen", "Tong Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings.", "AI": {"tldr": "MemGAS is a new framework that improves long-term dialogue memory in conversational agents by using multi-granularity associations and adaptive selection to enhance memory retrieval and reduce noise.", "motivation": "Long interactions in conversational agents lead to extensive dialogue records, challenging LLMs with limited context to maintain coherence and personalization in responses.", "method": "MemGAS utilizes multi-granularity memory units and Gaussian Mixture Models to cluster memories, with an entropy-based router for optimal granularity selection and LLM-based filtering to refine retrievals.", "result": "Experiments show that MemGAS outperforms existing methods on four long-term memory benchmarks, demonstrating better performance in both question answering and retrieval tasks.", "conclusion": "MemGAS significantly enhances the ability of LLMs to manage long-term memory in dialogues, leading to improved user experiences in conversational agents.", "key_contributions": ["Introduction of multi-granularity memory units", "Adaptive selection of memory granularity based on query relevance", "Performance improvement on various long-term memory tasks"], "limitations": "", "keywords": ["Large Language Models", "long-term memory", "conversational agents", "memory retrieval", "Gaussian Mixture Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19572", "pdf": "https://arxiv.org/pdf/2505.19572.pdf", "abs": "https://arxiv.org/abs/2505.19572", "title": "DocMEdit: Towards Document-Level Model Editing", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods.", "AI": {"tldr": "The paper introduces a dataset and task for document-level model editing in LLMs, addressing limitations of prior datasets focused on short outputs.", "motivation": "To correct errors and outdated knowledge in LLMs with minimal cost and improve their usability in real-world document-level tasks.", "method": "A new dataset, \benchmarkname, is introduced for document-level model editing, featuring document-level inputs and outputs with extrapolative and multiple facts.", "result": "Experiments demonstrate that existing model editing methods struggle with the complexities of document-level editing tasks.", "conclusion": "The introduction of document-level model editing and the \benchmarkname dataset is crucial for enhancing LLMs' capabilities in practical settings.", "key_contributions": ["Introduction of the \benchmarkname dataset for document-level model editing", "Proposes new evaluation metrics for assessing model editing at the document level", "Highlights the challenges faced by current methods in document-level editing"], "limitations": "", "keywords": ["document-level", "model editing", "large language models", "dataset", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19586", "pdf": "https://arxiv.org/pdf/2505.19586.pdf", "abs": "https://arxiv.org/abs/2505.19586", "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.", "AI": {"tldr": "TailorKV offers a hybrid compression method for LLM caches that integrates quantization and offloading, achieving nearly lossless performance with reduced memory overhead.", "motivation": "To address the substantial memory overhead of the KV cache in generative LLMs and to mitigate latency and performance degradation caused by existing methods.", "method": "TailorKV employs a hybrid compression technique that combines loading dominant tokens with the quantization of all tokens, creating a hardware-friendly inference framework.", "result": "TailorKV outperforms state-of-the-art methods with nearly lossless performance, demonstrated through evaluations on long contexts, particularly with the Llama-3.1-8B model.", "conclusion": "The proposed method provides an effective solution for reducing memory overhead while maintaining high performance in LLM inference.", "key_contributions": ["Introduction of TailorKV, a hybrid compression method for KV caches in LLMs.", "Demonstration of nearly lossless performance under aggressive compression settings.", "Implementation of a hardware-friendly inference framework optimized for long-context evaluations."], "limitations": "", "keywords": ["Key-Value cache", "Large language models", "Hybrid compression", "Quantization", "Offloading"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.19591", "pdf": "https://arxiv.org/pdf/2505.19591.pdf", "abs": "https://arxiv.org/abs/2505.19591", "title": "Multi-Agent Collaboration via Evolving Orchestration", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution.", "AI": {"tldr": "Proposes a puppeteer-style paradigm for dynamic multi-agent collaboration among LLMs to improve scalability and efficiency.", "motivation": "To overcome the limitations of static organizational structures in LLM-enabled multi-agent systems, which lead to coordination overhead and inefficiencies as task complexity increases.", "method": "A puppeteer orchestrator dynamically directs puppets (agents) in response to evolving task states, trained via reinforcement learning to adaptively sequence and prioritize agents for flexible collective reasoning.", "result": "The proposed method demonstrates superior performance and reduced computational costs in various experimental scenarios, attributed to the development of compact cyclic reasoning structures.", "conclusion": "Dynamic orchestration significantly enhances multi-agent collaboration in LLMs, improving adaptability and performance in complex problem-solving tasks.", "key_contributions": ["Introduction of a dynamic orchestrator for LLM multi-agent collaboration", "Utilization of reinforcement learning to adapt agent coordination", "Demonstration of compact cyclic reasoning structures improving efficiency"], "limitations": "", "keywords": ["large language models", "multi-agent collaboration", "reinforcement learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19598", "pdf": "https://arxiv.org/pdf/2505.19598.pdf", "abs": "https://arxiv.org/abs/2505.19598", "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment.", "AI": {"tldr": "This paper evaluates the robustness of Large Audio-Language Models (LALMs) against various malicious audio injection attacks, revealing performance disparities and suggesting new strategies for enhancing model security.", "motivation": "To address the underexplored vulnerabilities of LALMs against malicious audio attacks in real-world applications.", "method": "The study systematically evaluates five leading LALMs across four attack scenarios using metrics like Defense Success Rate, Context Robustness Score, and Judgment Robustness Index.", "result": "Significant performance disparities were found among models; no single model outperformed others consistently. Attack effectiveness is influenced by the position of malicious content, and models adhering strictly to instructions were found to be more susceptible to attacks.", "conclusion": "The research highlights the importance of integrating robustness into model training, developing multi-modal defenses, and rethinking architectural designs for secure deployment of LALMs.", "key_contributions": ["Introduces a benchmark framework for evaluating LALMs against audio injection attacks.", "Identifies the need for tailored defense strategies based on attack type and model characteristics.", "Reveals a negative correlation between instruction-following capability and robustness, prompting further design considerations."], "limitations": "", "keywords": ["Large Audio-Language Models", "robustness", "malicious audio attacks", "multi-modal defenses"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599.pdf", "abs": "https://arxiv.org/abs/2505.19599", "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output.", "AI": {"tldr": "This paper analyzes the performance of language models on nuanced grammar evaluation, particularly focusing on the first person psych predicate restriction in Japanese.", "motivation": "Existing metrics for evaluating language models fail to capture nuanced grammatical capabilities, especially in languages other than English.", "method": "We measure the perplexity of language models, specifically examining Weblab and Llama 3, in response to grammatical constraints in Japanese.", "result": "Weblab shows higher perplexity for ungrammatical psych predicate sentences, and Llama 3's perplexity improves significantly with well-behaved tokenizations.", "conclusion": "Grammar evaluation using perplexity can reveal significant differences in model performance based on tokenization, influencing machine translation outputs.", "key_contributions": ["Introduced perplexity measurement for nuanced grammar evaluation in Japanese.", "Demonstrated Weblab's unique performance characteristics among tested models.", "Evidence that tokenization quality impacts language model output accuracy."], "limitations": "The findings are specific to the tested models and may not generalize to all language models or grammar points.", "keywords": ["language models", "grammar evaluation", "perplexity", "Japanese", "tokenization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19604", "pdf": "https://arxiv.org/pdf/2505.19604.pdf", "abs": "https://arxiv.org/abs/2505.19604", "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "authors": ["Ahan Prasannakumar Shetty"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems.", "AI": {"tldr": "This paper evaluates various machine translation models for English-Hindi translation using a large corpus and custom datasets.", "motivation": "To bridge linguistic gaps between English and Hindi through effective machine translation.", "method": "Evaluation of various machine translation models using a diverse set of automatic evaluation metrics and a parallel dataset of over 18,000 pairs.", "result": "Different models show varying performance levels, indicating strengths and areas for improvement across metrics.", "conclusion": "The effectiveness of different translation approaches varies, providing insights into both general and specialized language handling.", "key_contributions": ["Comprehensive evaluation of English-Hindi machine translation models.", "Utilization of a large parallel corpus and custom FAQ dataset for testing.", "Insights into strengths and weaknesses of current translation systems."], "limitations": "", "keywords": ["Machine Translation", "English-Hindi", "Evaluation Metrics", "Natural Language Processing", "Translation Models"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.19606", "pdf": "https://arxiv.org/pdf/2505.19606.pdf", "abs": "https://arxiv.org/abs/2505.19606", "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies.", "AI": {"tldr": "This paper investigates cross-lingual alignment in speech foundation models, exploring whether it operates on semantic grounds rather than relying solely on phonetic similarities.", "motivation": "The motivation behind this study is to determine whether methods of cross-lingual alignment, previously established in text-based language models, can be applied to speech models, particularly in the context of spoken translation retrieval.", "method": "The authors conduct pronunciation-controlled experiments to test the stability of spoken translation retrieval accuracy without phonetic cues and perform controlled experiments on a dataset of cross-lingual synonyms and near-homophones.", "result": "The experiments reveal that spoken translation retrieval accuracy remains stable without phonetic cues, confirming both phonetic and semantic knowledge in the encoder. Additionally, improved speech recognition accuracy is achieved in seven low-resource languages using early exiting strategies.", "conclusion": "The findings suggest that cross-lingual alignment in speech models can be based on semantic understanding rather than phonetic similarity, leading to better speech recognition outcomes in low-resource languages.", "key_contributions": ["Demonstrated cross-lingual alignment in speech models on a semantic basis", "Established the impact of early exiting strategies on speech recognition accuracy", "Validated the presence of phonetic and semantic knowledge in the encoder."], "limitations": "The study primarily focuses on low-resource languages and may not generalize to more resource-rich languages or other speech contexts.", "keywords": ["cross-lingual alignment", "speech models", "semantic knowledge", "spoken translation", "low-resource languages"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19628", "pdf": "https://arxiv.org/pdf/2505.19628.pdf", "abs": "https://arxiv.org/abs/2505.19628", "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench.", "AI": {"tldr": "Introducing HomeBench, a dataset for evaluating LLMs in smart home scenarios, including valid and invalid device instructions.", "motivation": "To improve LLM-based smart home assistants by addressing the limitations in handling complex user instructions and multiple device operations.", "method": "Development of the HomeBench dataset featuring diverse valid and invalid instructions across single and multiple smart home devices, and evaluation of 13 distinct LLMs on their ability to execute these instructions.", "result": "Experimental results show that current LLMs, like GPT-4o, perform poorly on invalid multi-device instructions, indicating a significant gap in capability.", "conclusion": "HomeBench serves as a foundational dataset to advance research in LLM applications for smart home environments, highlighting current limitations and areas for improvement.", "key_contributions": ["Introduction of HomeBench dataset for smart home instructions", "Evaluation of LLM performance across varied user commands", "Identification of critical challenges in LLM-based smart home integrations"], "limitations": "Existing LLMs struggle with invalid multi-device instructions; performance is significantly lacking even with advanced techniques.", "keywords": ["Large Language Models", "Smart Home", "Dataset", "Human-Computer Interaction", "Instruction Following"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19630", "pdf": "https://arxiv.org/pdf/2505.19630.pdf", "abs": "https://arxiv.org/abs/2505.19630", "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Yixue Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL", "AI": {"tldr": "DoctorAgent-RL is a reinforcement learning framework for improving multi-turn medical consultations by dynamically optimizing questioning strategies.", "motivation": "Existing biomedical question answering systems face challenges in clinical consultations due to one-way communication and poor generalizability in static learning models.", "method": "The study employs a reinforcement learning framework where a doctor agent interacts with a patient agent to optimize information gathering during consultations.", "result": "DoctorAgent-RL demonstrates improved multi-turn reasoning capabilities and diagnostic performance compared to existing models.", "conclusion": "The proposed framework shows practical applications in assisting clinical consultations by enabling LLMs to adopt clinical reasoning strategies through dynamic interactions.", "key_contributions": ["Introduces DoctorAgent-RL, a RL-based collaborative framework for medical consultations.", "Constructs the MTMedDialog dataset for simulating patient interactions in multi-turn dialogues.", "Enhances diagnostic performance and reasoning capabilities over traditional models."], "limitations": "", "keywords": ["reinforcement learning", "medical consultation", "large language models", "multi-turn dialogue", "health informatics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19631", "pdf": "https://arxiv.org/pdf/2505.19631.pdf", "abs": "https://arxiv.org/abs/2505.19631", "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA", "AI": {"tldr": "This paper introduces a new framework for unsupervised word segmentation using Large Language Models (LLMs), revealing enhanced performance with more parameters and a novel method called LLACA, which combines Aho-Corasick automata with LLMs.", "motivation": "The paper addresses the limitations of traditional word segmentation methods in NLP by leveraging LLMs to improve segmentation accuracy and semantic understanding.", "method": "The authors evaluate mainstream LLMs for word segmentation across multiple languages and propose a new unsupervised method, LLACA, which combines the capabilities of Aho-Corasick automata with insights from LLMs to dynamically adjust n-gram models.", "result": "LLMs demonstrated the ability to segment raw text effectively, with performance improvements observed in larger models. LLACA showed significant advantages over traditional segmentation methods by incorporating contextual semantics from LLMs.", "conclusion": "The integration of LLMs and Aho-Corasick automata presents a promising approach for unsupervised word segmentation, achieving higher accuracy and flexibility in language processing tasks.", "key_contributions": ["Introduction of LLACA, a novel unsupervised word segmentation method.", "Demonstration of LLMs' effectiveness in word segmentation across multiple languages.", "Analysis of the correlation between model size and segmentation performance."], "limitations": "", "keywords": ["word segmentation", "Large Language Models", "unsupervised learning", "Aho-Corasick automata", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634.pdf", "abs": "https://arxiv.org/abs/2505.19634", "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios.", "AI": {"tldr": "This paper addresses the efficiency of Test-Time Scaling (TTS) for Large Language Models (LLMs) by optimizing for latency in inference scenarios.", "motivation": "Existing TTS methods do not adequately consider the effects of latency, which is critical in many applications.", "method": "The authors propose latency-optimal TTS through branch-wise parallelism and sequence-wise parallelism, optimizing concurrency configurations in LLM inference.", "result": "A 32B model achieves 82.3% accuracy on MATH-500 within 1 minute, while a 3B model reaches 72.4% accuracy within 10 seconds.", "conclusion": "The proposed latency-aware TTS successfully balances speed and accuracy in critical latency scenarios, highlighting its significance.", "key_contributions": ["Introduction of latency-aware Test-Time Scaling (TTS) methods for LLMs", "Optimization techniques for concurrency configurations in inference", "Demonstration of trade-offs between speed and accuracy in latency-sensitive environments"], "limitations": "The study focuses primarily on TTS methods without extensive exploration of other latency-reducing techniques.", "keywords": ["Test-Time Scaling", "Large Language Models", "Latency Optimization", "Parallelism", "Inference Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19640", "pdf": "https://arxiv.org/pdf/2505.19640.pdf", "abs": "https://arxiv.org/abs/2505.19640", "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning", "authors": ["Roy Xie", "David Qiu", "Deepak Gopinath", "Dong Lin", "Yanchao Sun", "Chong Wang", "Saloni Potdar", "Bhuwan Dhingra"], "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.", "AI": {"tldr": "Proposes a reinforcement learning approach for interleaved reasoning in large language models, improving efficiency and accuracy in multi-hop questions.", "motivation": "To address inefficiencies caused by extensive reasoning traces in large language models, which increase time-to-first-token (TTFT).", "method": "A novel training paradigm that interleaves thinking and answering using reinforcement learning (RL) to guide reasoning.", "result": "The proposed method reduces TTFT by over 80% on average and improves Pass@1 accuracy by up to 19.3%, showing strong generalization to complex reasoning datasets.", "conclusion": "The approach enables efficient reasoning in language models while offering valuable insights into conditional reward modeling.", "key_contributions": ["Introduction of interleaved reasoning in LLMs via reinforcement learning", "A simple rule-based reward system to guide reasoning", "Extensive experimental validation across multiple datasets and RL algorithms"], "limitations": "", "keywords": ["reinforcement learning", "large language models", "interleaved reasoning", "multi-hop questions", "question answering"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19647", "pdf": "https://arxiv.org/pdf/2505.19647.pdf", "abs": "https://arxiv.org/abs/2505.19647", "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation", "authors": ["Xiaochuan Liu", "Ruihua Song", "Xiting Wang", "Xu Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Findings)", "summary": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG.", "AI": {"tldr": "This paper proposes a multi-agent framework for automatic related work generation (RWG) that addresses limitations in existing methods by employing full-text input and graph-aware strategies for better comprehension and relationship capturing among reference papers.", "motivation": "The motivation is to improve the effectiveness of automatic related work generation by overcoming shallow comprehension issues and isolated explanations in existing RWG methods.", "method": "The proposed framework includes three agents: a selector to determine which section of papers to read next, a reader to digest selected sections and update shared memory, and a writer to generate the related work section from the curated memory. Graph-aware strategies are integrated into the selector to optimize reading orders based on relationship constraints among reference papers.", "result": "Extensive experiments show that the multi-agent framework consistently enhances performance across various models and input configurations, with graph-aware selectors achieving state-of-the-art results compared to alternative methods.", "conclusion": "The proposed multi-agent framework significantly improves automatic related work generation by enabling deeper comprehension and better relationship capturing among references, demonstrating its efficacy in RWG tasks.", "key_contributions": ["Introduction of a multi-agent framework for RWG", "Implementation of graph-aware strategies in the selector", "Demonstration of state-of-the-art performance across multiple configurations"], "limitations": "", "keywords": ["related work generation", "multi-agent framework", "graph-aware strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19660", "pdf": "https://arxiv.org/pdf/2505.19660.pdf", "abs": "https://arxiv.org/abs/2505.19660", "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "comment": "13 pages, 5 figures", "summary": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI", "AI": {"tldr": "This paper presents GenKI, a novel framework for enhancing Open-domain Question Answering (OpenQA) by integrating retrieved knowledge into LLMs and enabling controllable output formats.", "motivation": "The motivation behind this work is to overcome the challenges faced by LLM-based OpenQA methods, particularly focusing on knowledge integration and the generation of task-specific answer formats.", "method": "The authors propose a framework called GenKI that trains a dense passage retrieval model for knowledge retrieval, integrates the retrieved knowledge into fine-tuning instructions, and utilizes a fine-tuned LLM to ensure controllable generation with the assistance of text consistency.", "result": "Experiments on diverse datasets (TriviaQA, MSMARCO, CMRC2018) show that GenKI significantly outperforms state-of-the-art baselines, with findings indicating a linear relationship between the frequency of retrieved knowledge and knowledge recall accuracy.", "conclusion": "The study concludes that GenKI is effective in enhancing OpenQA performance through integrated knowledge and controlled answer generation, encouraging future research in this direction.", "key_contributions": ["Introduction of the GenKI framework for OpenQA", "Training a dense passage retrieval model for effective knowledge extraction", "Development of a knowledge integration model for fine-tuning LLMs"], "limitations": "", "keywords": ["Open-domain Question Answering", "Large Language Models", "Knowledge Integration"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19667", "pdf": "https://arxiv.org/pdf/2505.19667.pdf", "abs": "https://arxiv.org/abs/2505.19667", "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions.", "AI": {"tldr": "LeCoDe is a multi-turn benchmark dataset for legal consultations designed to improve LLMs' capabilities in legal dialogue, revealing significant performance challenges in current models.", "motivation": "The high cost and limited accessibility of legal consultation necessitate scalable and effective LLM-based solutions, particularly for interactive scenarios.", "method": "The paper introduces LeCoDe, a dataset of 3,696 legal consultation dialogues collected from live-streamed short-video platforms, and proposes a comprehensive evaluation framework for LLMs.", "result": "State-of-the-art LLMs like GPT-4 show only 39.8% recall for clarification tasks and a 59% overall score for advice quality, indicating substantial difficulties in achieving effective legal consultation.", "conclusion": "The findings reveal significant limitations in current LLM performance in legal contexts and suggest strategies for improving LLMs in professional consultation scenarios.", "key_contributions": ["Introduction of the LeCoDe benchmark dataset for legal consultation dialogues", "A comprehensive evaluation framework with metrics for clarification and advice quality", "Insights into the performance limitations of existing LLMs in legal contexts"], "limitations": "The dataset may not cover all aspects of legal consultations and may reflect the biases of the data collection process.", "keywords": ["legal consultation", "Large Language Models", "benchmark dataset", "evaluation framework", "dialogue systems"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19670", "pdf": "https://arxiv.org/pdf/2505.19670.pdf", "abs": "https://arxiv.org/abs/2505.19670", "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models", "authors": ["Hao Yang", "Lizhen Qu", "Ehsan Shareghi", "Gholamreza Haffari"], "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples.", "AI": {"tldr": "This paper proposes an unsupervised safety-fine-tuning strategy to enhance the safety-alignment of Large Audio Language Models (LALMs) while minimizing over-rejection issues.", "motivation": "Despite advancements in safety measures for text and vision-based LLMs, LALMs have been found to be lacking effective safety-alignment strategies, making them susceptible to harmful queries.", "method": "The paper introduces an unsupervised safety-fine-tuning technique aimed at reshaping the model's representation space to improve safety-alignment without significantly increasing the rate of over-rejections.", "result": "Experiments across three generations of Qwen LALMs indicate that the proposed method greatly improves the safety of LALMs under various input conditions while only slightly increasing the over-rejection rate.", "conclusion": "The unsupervised safety-fine-tuning strategy effectively enhances safety for LALMs, addressing a critical gap in existing safety measures.", "key_contributions": ["Unsupervised safety-fine-tuning strategy for LALMs", "Demonstration of significant safety improvements across different input modalities", "Analysis of over-rejection rates alongside safety enhancements"], "limitations": "The paper contains harmful examples and may not address all safety concerns for LALMs.", "keywords": ["Large Audio Language Models", "Safety-alignment", "Unsupervised fine-tuning", "Machine Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.19674", "pdf": "https://arxiv.org/pdf/2505.19674.pdf", "abs": "https://arxiv.org/abs/2505.19674", "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "categories": ["cs.CL"], "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "summary": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations.", "AI": {"tldr": "The paper explores the moral values reflected by large language models (LLMs) using word associations as a more robust method than direct prompting.", "motivation": "Understanding the moral values reflected by LLMs is crucial due to their increasing impact, but assessing these values through direct means is difficult.", "method": "The authors create a dataset of LLM-generated word associations and develop a novel method to propagate moral values through association graphs based on Moral Foundation Theory.", "result": "The study reveals systematic differences between the moral values associated with human English speakers and those inferred from LLMs.", "conclusion": "Using word associations provides a clearer understanding of the moral reasoning of LLMs compared to traditional prompting methods.", "key_contributions": ["Creation of a dataset of LLM-generated word associations", "Novel method for propagating moral values through association graphs", "Comparison of moral conceptualizations between human and LLM associations"], "limitations": "The study focuses only on English-speaking communities, which may limit generalizability to other cultures.", "keywords": ["large language models", "moral reasoning", "word associations", "Moral Foundation Theory", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675.pdf", "abs": "https://arxiv.org/abs/2505.19675", "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.", "AI": {"tldr": "This paper presents SiDyP, a method to enhance classifier robustness against noisy labels generated by LLMs by refining label candidates using a simplex diffusion model.", "motivation": "To address the challenge of noisy labels generated by large language models in NLP tasks and improve model generalization.", "method": "SiDyP retrieves potential true label candidates from neighborhood label distributions in text embedding space and iteratively refines them using a simplex diffusion model.", "result": "SiDyP improves the performance of a fine-tuned BERT classifier on zero-shot and few-shot datasets by an average of 7.21% and 7.30%, respectively.", "conclusion": "The proposed SiDyP framework effectively enhances the robustness of classifiers dealing with LLM-generated noisy labels across various NLP tasks.", "key_contributions": ["Introduction of SiDyP framework for label correction.", "Demonstrated significant performance improvement on BERT classifier with LLM-generated data.", "Extensive benchmarking on various NLP tasks."], "limitations": "Focuses primarily on LLM-generated label noise, which may not generalize to other types of noise.", "keywords": ["noisy labels", "large language models", "NLP tasks", "classifier robustness", "label noise correction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678.pdf", "abs": "https://arxiv.org/abs/2505.19678", "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency.", "AI": {"tldr": "This paper introduces a Conditional Pointwise Mutual Information (C-PMI) decoding strategy to reduce hallucinations in Large Vision-Language Models (LVLMs) by reinforcing the relationship between text and image tokens during generation.", "motivation": "Large Vision-Language Models suffer from hallucinations, generating plausible but irrelevant responses, primarily due to reliance on language priors over visual information.", "method": "The authors propose a C-PMI calibrated decoding strategy that models the contributions of visual and textual tokens, treating hallucination mitigation as a bi-level optimization problem to enhance mutual information.", "result": "Experiments show that the C-PMI strategy significantly reduces hallucinations in LVLMs while maintaining decoding efficiency across various benchmarks.", "conclusion": "The proposed method effectively addresses hallucination issues in LVLMs by improving the synergy between text and image representations.", "key_contributions": ["Introduction of C-PMI calibrated decoding strategy", "Joint modeling of visual and textual token contributions", "Development of a token purification mechanism for relevant token sampling"], "limitations": "", "keywords": ["Vision-Language Models", "Hallucinations", "Conditional Pointwise Mutual Information", "Text-Image Dependencies", "Decoding Strategy"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.19679", "pdf": "https://arxiv.org/pdf/2505.19679.pdf", "abs": "https://arxiv.org/abs/2505.19679", "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points.", "AI": {"tldr": "The paper discusses the development of ASR and MT systems and E2E ST systems for low-resource languages, utilizing synthetic data and model regularization to enhance performance.", "motivation": "Addressing the challenge of low-resource language translation and improving translation quality by utilizing pre-trained models and synthetic data.", "method": "The authors implemented cascaded ASR and MT systems, and end-to-end ST systems, fine-tuning them with various strategies and evaluating them on language pairs: Bemba, North Levantine Arabic, and Tunisian Arabic.", "result": "The use of synthetic data and text-to-speech models improved ASR and ST performance. Intra-distillation consistently enhanced model performance across tasks, and Minimum Bayes Risk decoding improved results by approximately 1.5 BLEU points.", "conclusion": "The findings highlight the effectiveness of synthetic data and system augmentation techniques in low-resource machine translation and speech translation systems.", "key_contributions": ["Development of cascaded and E2E systems for low-resource languages", "Use of synthetic data to enhance ASR and ST performance", "Application of Minimum Bayes Risk decoding for performance improvement"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Machine Translation", "Speech Translation"], "importance_score": 5, "read_time_minutes": 8}}
{"id": "2505.19700", "pdf": "https://arxiv.org/pdf/2505.19700.pdf", "abs": "https://arxiv.org/abs/2505.19700", "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.", "AI": {"tldr": "The paper introduces a Residual Alignment Model (RAM) which enhances alignment of large language models (LLMs) for improved adaptability without retraining the entire model.", "motivation": "With the rising use of LLMs, there is a need for quick and customizable outputs which traditional alignment methods struggle to deliver due to the necessity of retraining.", "method": "The paper proposes RAM, which treats the alignment process as importance sampling using an autoregressive alignment module as an estimator for importance weights, allowing separate operation from the underlying model.", "result": "Experiments show that RAM outperforms existing baseline models on tasks such as instruction following and domain adaptation across two leading open-source LLMs.", "conclusion": "RAM provides a more flexible and efficient method for aligning large language models, addressing limitations of existing methods by decoupling the alignment and model training processes.", "key_contributions": ["Introduction of the Residual Alignment Model (RAM)", "Formalization of alignment as importance sampling", "Development of an efficient token-level decoding resampling algorithm."], "limitations": "", "keywords": ["large language models", "alignment", "importance sampling", "autoregressive model", "resampling algorithm"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19706", "pdf": "https://arxiv.org/pdf/2505.19706.pdf", "abs": "https://arxiv.org/abs/2505.19706", "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/declare-lab/PathFinder-PRM", "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.", "AI": {"tldr": "PathFinder-PRM is a novel hierarchical process reward model designed to enhance mathematical problem-solving by classifying errors at each step, achieving state-of-the-art performance with improved data efficiency.", "motivation": "Large Language Models often hallucinate during reasoning-intensive tasks like mathematics, necessitating better error detection and evaluation methods to improve their performance.", "method": "PathFinder-PRM classifies math and consistency errors at each step of problem-solving, combining these classifications to estimate correctness and guide generation towards coherent solutions.", "result": "On PRMBench, PathFinder-PRM achieved a PRMScore of 67.7, outperforming the previous best and using 3 times less data. It also improved reward-guided greedy search performance, yielding a prm@8 score of 48.3.", "conclusion": "Decoupling error detection from reward estimation enhances the detection of fine-grained errors and significantly improves the effectiveness of reward-guided mathematical reasoning while being more data-efficient.", "key_contributions": ["Introduction of a hierarchical error-aware discriminative Process Reward Model", "Construction of a 400K-sample dataset for training", "Achieving state-of-the-art performance with increased data efficiency"], "limitations": "", "keywords": ["Large Language Models", "mathematical problem solving", "Process Reward Models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714.pdf", "abs": "https://arxiv.org/abs/2505.19714", "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT.", "AI": {"tldr": "The paper presents MT³, a framework using Multi-Task Reinforcement Learning for end-to-end Text Image Machine Translation, achieving state-of-the-art results.", "motivation": "To address the complexities of Text Image Machine Translation (TIMT) which includes challenges in OCR, visual-text reasoning, and translation.", "method": "The MT³ framework applies Multi-Task Reinforcement Learning to MLLMs, focusing on text recognition, context-aware reasoning, and translation, using a unique multi-mixed reward mechanism.", "result": "MT³-7B-Zero outperforms baseline models on the MIT-10M benchmark and demonstrates strong generalization capabilities.", "conclusion": "The approach significantly improves TIMT performance through multi-task optimization and opens doors for better evaluation in social media contexts.", "key_contributions": ["Introduction of MT³ for TIMT using Multi-Task RL", "Development of XHSPost benchmark for social media TIMT", "State-of-the-art results on MIT-10M benchmark"], "limitations": "", "keywords": ["Text Image Machine Translation", "Reinforcement Learning", "Multimodal LLMs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715.pdf", "abs": "https://arxiv.org/abs/2505.19715", "title": "Graceful Forgetting in Generative Language Models", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance.", "AI": {"tldr": "The paper introduces a framework called Learning With Forgetting (LWF) for graceful forgetting in generative language models, addressing negative transfer during fine-tuning.", "motivation": "To overcome the detrimental effects of negative transfer in pre-trained language models and improve fine-tuning performance.", "method": "The framework uses Fisher Information Matrix to weight parameter updates, computing forgetting confidence to selectively unlearn irrelevant knowledge during fine-tuning.", "result": "Experiments show that applying graceful forgetting enhances fine-tuning performance in generative language models despite challenges in fully understanding knowledge interactions.", "conclusion": "Graceful forgetting can improve the performance of generative language models during fine-tuning by allowing selective unlearning of irrelevant knowledge.", "key_contributions": ["Introduction of Learning With Forgetting (LWF) framework for generative models.", "Application of Fisher Information Matrix for knowledge unlearning.", "Evidence of improved fine-tuning performance through graceful forgetting."], "limitations": "The mechanisms of knowledge interaction in pre-trained language models remain challenging to fully uncover.", "keywords": ["graceful forgetting", "generative language models", "fine-tuning", "negative transfer", "Fisher Information Matrix"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19722", "pdf": "https://arxiv.org/pdf/2505.19722.pdf", "abs": "https://arxiv.org/abs/2505.19722", "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICIC 2025", "summary": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework.", "AI": {"tldr": "This paper introduces RPDR, a framework that leverages both closed-source and open-source large language models (LLMs) for biomedical entity linking, improving performance with limited annotated data.", "motivation": "To improve biomedical entity linking in scenarios with limited annotated data and reduce dependency on costly closed-source LLMs.", "method": "RPDR utilizes a closed-source LLM to generate training data from unannotated data and then fine-tunes an open-source LLM for re-ranking candidates.", "result": "RPDR demonstrates improved accuracy with 0.019 Acc@1 and 0.036 Acc@1 improvements on two different datasets when training data is insufficient.", "conclusion": "The proposed RPDR framework successfully combines the benefits of both closed-source and open-source LLMs, making it a viable solution for biomedical entity linking.", "key_contributions": ["Introduction of the RPDR framework for biomedical entity linking", "Combination of closed-source and open-source LLMs to enhance performance", "Demonstration of effectiveness on multilingual datasets"], "limitations": "Limited to biomedical context and dependent on the quality of generated training data.", "keywords": ["biomedical entity linking", "large language models", "re-ranking", "open-source LLMs", "knowledge distillation"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743.pdf", "abs": "https://arxiv.org/abs/2505.19743", "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs.", "AI": {"tldr": "MARA is a new token-level alignment approach for LLMs that enhances ethical applications by efficiently classifying token preferences, significantly improving alignment performance while lowering computational costs.", "motivation": "The need for alignment techniques that can ensure ethical and safe application of Large Language Models without the high computational costs associated with current methods.", "method": "The MARA approach involves a three-layer fully-connected network that performs binary classification on individual tokens to determine their acceptance or rejection in a response, independent of the larger language model.", "result": "MARA has shown significant improvements in alignment performance compared to existing techniques across seven LLMs and three open-source datasets, while also reducing computational costs.", "conclusion": "The proposed MARA approach provides a more efficient means of aligning LLMs with human preferences, indicating potential for broader application in ethical AI.", "key_contributions": ["Introduction of the Micro token-level Accept-Reject Aligning (MARA) method", "Decomposition of sentence-level preferences into token-level classification", "Demonstrated efficiency and effectiveness across multiple LLMs and datasets"], "limitations": "", "keywords": ["Large Language Models", "alignment techniques", "ethical AI"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754.pdf", "abs": "https://arxiv.org/abs/2505.19754", "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG.", "AI": {"tldr": "Introducing NeuSym-RAG, a hybrid retrieval framework that integrates neural and symbolic approaches for effective academic paper querying.", "motivation": "To address the challenges researchers face in efficiently acquiring key information from an expanding pool of academic papers, especially through enhanced retrieval mechanisms.", "method": "NeuSym-RAG employs a hybrid neural-symbolic retrieval approach, utilizing multi-view chunking and schema-based parsing to organize PDF content into a relational database and vector store.", "result": "Experiments demonstrate that NeuSym-RAG outperforms traditional vector-based retrieval methods and structured baselines on three PDF-based QA datasets, including the self-annotated AIRQA-REAL.", "conclusion": "NeuSym-RAG successfully integrates complementary retrieval strategies, enhancing the performance of large language model agents in academic querying tasks.", "key_contributions": ["Development of a hybrid neural-symbolic retrieval framework.", "Implementation of multi-view chunking for better data representation.", "Public availability of code and datasets for reproducibility."], "limitations": "", "keywords": ["Retrieval Augmented Generation", "Large Language Models", "Hybrid Retrieval", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.19756", "pdf": "https://arxiv.org/pdf/2505.19756.pdf", "abs": "https://arxiv.org/abs/2505.19756", "title": "Efficient Reasoning via Chain of Unconscious Thought", "authors": ["Ruihan Gong", "Yue Liu", "Wenjie Qu", "Mingzhe Du", "Yufei He", "Yingwei Ma", "Yulin Chen", "Xiang Liu", "Yi Wen", "Xinfeng Li", "Ruidong Wang", "Xinzhong Zhu", "Bryan Hooi", "Jiaheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT", "AI": {"tldr": "This paper introduces the Chain of Unconscious Thought (CoUT) paradigm to improve token efficiency in Large Reasoning Models (LRMs) by mimicking human unconscious thought processes.", "motivation": "To enhance the token efficiency of LRMs while maintaining performance by leveraging concepts from Unconscious Thought Theory.", "method": "The authors propose CoUT, which involves prompting models to internalize reasoning processes in hidden layers and using token-efficient strategies to reduce unnecessary tokens.", "result": "CoUT reduces token usage by 47.62% compared to the Chain of Thought (CoT) framework while maintaining similar accuracy levels.", "conclusion": "The findings suggest that models can effectively incorporate strategies that resemble unconscious thought to boost efficiency without compromising performance.", "key_contributions": ["Introduction of the Chain of Unconscious Thought (CoUT) framework.", "Demonstrated significant token efficiency improvements over previous methods.", "Provided code for the CoUT implementation to facilitate further research."], "limitations": "", "keywords": ["Reasoning Models", "Token Efficiency", "Unconscious Thought Theory", "Machine Learning", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.19766", "pdf": "https://arxiv.org/pdf/2505.19766.pdf", "abs": "https://arxiv.org/abs/2505.19766", "title": "SGM: A Framework for Building Specification-Guided Moderation Filters", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol.", "AI": {"tldr": "SGM is a framework for training moderation filters for LLMs that align with user-specific requirements and goes beyond standard safety concerns, enabling automated, scalable support for various applications.", "motivation": "Large language models often face issues with misalignment and adversarial inputs, making the development of robust moderation frameworks essential for effective deployment.", "method": "SGM introduces a flexible framework for training moderation filters based on user-defined specifications, automating the generation of training data without needing human-written examples.", "result": "SGM-trained filters achieve performance comparable to state-of-the-art safety filters while allowing for more nuanced and application-specific alignment controls.", "conclusion": "The SGM framework demonstrates the ability to effectively address LLM alignment issues through automated moderation training, supporting diverse alignment goals.", "key_contributions": ["Development of SGM framework for training moderation filters", "Automation of training data generation", "Enhanced user-defined alignment controls in moderation filters"], "limitations": "", "keywords": ["large language models", "content moderation", "adversarial inputs", "user-defined specifications", "alignment control"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.19768", "pdf": "https://arxiv.org/pdf/2505.19768.pdf", "abs": "https://arxiv.org/abs/2505.19768", "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search", "authors": ["Xing Cui", "Yueying Zou", "Zekun Li", "Peipei Li", "Xinyuan Xu", "Xuannan Liu", "Huaibo Huang", "Ran He"], "categories": ["cs.CL"], "comment": null, "summary": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased.", "AI": {"tldr": "T2Agent is a novel misinformation detection agent that combines a modular toolkit with Monte Carlo Tree Search (MCTS) for dynamic verification of mixed-source forgery.", "motivation": "Existing misinformation detection methods are limited by static pipelines and cannot effectively handle the complexity and diversity of real-world multimodal misinformation.", "method": "T2Agent integrates an extensible toolkit that includes tools for web search, forgery detection, and consistency analysis, using a Bayesian optimization-based selector to identify a task-relevant subset of tools, which informs a modified MCTS for evidence collection and verification.", "result": "T2Agent outperforms existing baselines on challenging multimodal misinformation benchmarks, confirming the effectiveness of its tree search mechanism and tool integration.", "conclusion": "The findings indicate that T2Agent provides a strong training-free approach to enhancing detection accuracy for mixed-source misinformation.", "key_contributions": ["Introduction of T2Agent for dynamic misinformation detection", "Integration of modular tools and Bayesian optimization with MCTS", "Demonstrated performance improvements over existing methods in real-world benchmarks"], "limitations": "", "keywords": ["multimodal misinformation", "dynamic verification", "Monte Carlo Tree Search"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.19773", "pdf": "https://arxiv.org/pdf/2505.19773.pdf", "abs": "https://arxiv.org/abs/2505.19773", "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs", "authors": ["Sangyeop Kim", "Yohan Lee", "Yongwoo Song", "Kimin Lee"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted by ACL 2025", "summary": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms.", "AI": {"tldr": "This paper discusses vulnerabilities in Large Language Models (LLMs) regarding long-context processing, revealing safety gaps and the ineffectiveness of existing safety mechanisms across various contexts.", "motivation": "The study aims to explore vulnerabilities in LLMs related to long-context processing, specifically to understand how these weaknesses can be exploited and to highlight the ineffectiveness of current safety measures.", "method": "The authors conduct experiments using Many-Shot Jailbreaking (MSJ) techniques, analyzing various attack settings that vary in instruction styles, shot density, topics, and formats with a context length of up to 128K tokens.", "result": "The analysis shows that context length significantly affects the effectiveness of many-shot attacks, and that successful attacks can be achieved even with non-specific or random content, indicating serious limitations in LLM safety when processing long contexts.", "conclusion": "The findings demonstrate inconsistent safety behaviors in well-aligned models with longer contexts and underline the urgent need for developing new safety mechanisms to address these vulnerabilities.", "key_contributions": ["Investigation of long-context vulnerabilities in LLMs through Many-Shot Jailbreaking experiments.", "Demonstration that context length is the key factor for attack effectiveness.", "Identification of safety gaps in the long-context processing capacity of LLMs."], "limitations": "The study may not address the full spectrum of vulnerabilities or scenarios involved in LLM long-context usage.", "keywords": ["Large Language Models", "Many-Shot Jailbreaking", "long-context vulnerabilities", "safety mechanisms", "context length"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2505.19776", "pdf": "https://arxiv.org/pdf/2505.19776.pdf", "abs": "https://arxiv.org/abs/2505.19776", "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts.", "AI": {"tldr": "This paper presents a new method for analyzing political biases in LLMs by measuring sentiment prediction variability across different target entities in sentences.", "motivation": "To address the impact of political biases in LLMs on downstream applications, we seek a bias analysis method that doesn't propagate existing biases.", "method": "We insert demographically and politically diverse names into political sentences and measure sentiment predictions across models and languages using an entropy-based inconsistency metric.", "result": "The study finds significant inconsistencies in sentiment predictions related to political entities, with biases observable across multiple languages and correlations between politically aligned figures.", "conclusion": "Replacing real politician names with fictional equivalents can mitigate some reliability issues in LLM sentiment analysis while revealing underlying biases.", "key_contributions": ["Introduction of an entropy-based inconsistency metric for bias analysis", "Comprehensive analysis across 450 sentences using 1319 politicians", "Findings show language and model size impact on bias intensity"], "limitations": "The approach may not account for all aspects of bias and requires further validation across different contexts.", "keywords": ["Political Bias", "Large Language Models", "Sentiment Analysis", "Natural Language Processing", "Bias Mitigation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19797", "pdf": "https://arxiv.org/pdf/2505.19797.pdf", "abs": "https://arxiv.org/abs/2505.19797", "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers", "AI": {"tldr": "The Avengers framework leverages multiple smaller language models to outperform GPT-4.1 on various tasks, utilizing embedding, clustering, scoring, and voting techniques.", "motivation": "Explores the potential of smaller, open-source language models to compete with larger proprietary models in natural language processing tasks.", "method": "The framework employs four operations: embedding queries, clustering them by semantic similarity, scoring model performance within clusters, and voting for improved output generation.", "result": "The Avengers framework improved performance on 10 out of 15 benchmark datasets compared to GPT-4.1, particularly excelling in mathematics and coding tasks with notable percentage increases.", "conclusion": "Smaller language models can collectively outperform larger models by utilizing a novel ensemble method, demonstrating robustness and generalization capabilities.", "key_contributions": ["Introduced a novel ensemble method for smaller language models.", "Achieved performance surpassing GPT-4.1 in diverse tasks.", "Open-sourced implementation available for further research."], "limitations": "The framework's performance may vary depending on the selection of embedding models and clustering strategies.", "keywords": ["language models", "ensemble methods", "open-source", "natural language processing", "machine learning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.19800", "pdf": "https://arxiv.org/pdf/2505.19800.pdf", "abs": "https://arxiv.org/abs/2505.19800", "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "categories": ["cs.CL"], "comment": null, "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.", "AI": {"tldr": "This paper presents MOLE, a framework using Large Language Models to automatically extract metadata attributes from scientific papers, enhancing research discovery and reproducibility.", "motivation": "To improve the efficiency and effectiveness of metadata extraction in the context of the rapid expansion of scientific research, moving beyond the reliance on manual annotation.", "method": "The framework uses a schema-driven methodology to process documents in multiple formats with validation mechanisms to ensure consistent output.", "result": "MOLE shows promising results in automating metadata extraction through analysis of context length, few-shot learning, and web browsing integration, along with the introduction of a new benchmark for evaluating research progress in this area.", "conclusion": "The study indicates the need for further improvements in the automation of metadata extraction using LLMs to guarantee consistent and reliable performance.", "key_contributions": ["Introduction of the MOLE framework for automatic metadata extraction using LLMs", "Establishment of a new benchmark for evaluating this task", "Demonstration of the potential of modern LLMs in automating data extraction for diverse languages."], "limitations": "The framework's performance depends on further improvements for achieving consistent and reliable results across various datasets.", "keywords": ["metadata extraction", "Large Language Models", "scientific papers", "automation", "benchmark"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19804", "pdf": "https://arxiv.org/pdf/2505.19804.pdf", "abs": "https://arxiv.org/abs/2505.19804", "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation", "authors": ["Siyuan Li", "Jian Chen", "Rui Yao", "Xuming Hu", "Peilin Zhou", "Weihua Qiu", "Simin Zhang", "Chucheng Dong", "Zhiyao Li", "Qipeng Xie", "Zixuan Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation.", "AI": {"tldr": "This paper introduces Compliance-to-Code, a large-scale Chinese dataset aimed at improving financial regulatory compliance through automated code generation.", "motivation": "To address the challenges faced in automating compliance logic from complex Chinese financial regulations, particularly the suboptimal performance of existing LLMs.", "method": "Development of a domain-specific, code-oriented dataset, including 1,159 annotated clauses from 361 financial regulations, structured to aid model training.", "result": "The dataset includes modularly structured clauses and deterministic Python code mappings, aiming to facilitate automated compliance auditing and code generation.", "conclusion": "This new dataset and the accompanying FinCheck pipeline could significantly enhance the capabilities of regulatory technology in processing Chinese financial regulations.", "key_contributions": ["Introduction of Compliance-to-Code, the first large-scale dataset for Chinese regulatory compliance.", "Detailed annotations with logical elements for better model training.", "Presentation of FinCheck, a practical pipeline for automating compliance tasks."], "limitations": "", "keywords": ["Regulatory Technology", "Large Language Models", "Financial Regulations"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.19806", "pdf": "https://arxiv.org/pdf/2505.19806.pdf", "abs": "https://arxiv.org/abs/2505.19806", "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness.", "AI": {"tldr": "This paper explores the controversial topic of consciousness in large language models (LLMs), clarifying terms, organizing existing research, discussing potential risks, and outlining future directions.", "motivation": "To address the burgeoning discourse and confusion surrounding the consciousness of large language models (LLMs) as they evolve.", "method": "The authors synthesize existing theoretical and empirical research on LLM consciousness and categorize relevant terminologies.", "result": "The paper identifies various risks associated with conscious LLMs and highlights key challenges in the field, while organizing references for further exploration.", "conclusion": "The study sets the stage for future research by clarifying concepts and outlining both risks and avenues for investigation regarding LLM consciousness.", "key_contributions": ["Clarification of key terminologies regarding LLM consciousness and awareness.", "Synthesis of theoretical and empirical perspectives on LLM consciousness.", "Identification of potential risks and challenges in developing conscious LLMs."], "limitations": "The exploration of LLM consciousness is still in its nascent stages, and deeper empirical investigations are needed.", "keywords": ["LLM Consciousness", "Awareness", "Artificial Intelligence", "Theoretical Perspectives", "Empirical Research"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2505.19815", "pdf": "https://arxiv.org/pdf/2505.19815.pdf", "abs": "https://arxiv.org/abs/2505.19815", "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.", "AI": {"tldr": "A framework is proposed to enhance understanding of reasoning in large language models (LLMs) via meta-learning.", "motivation": "To explore the reasoning capabilities of LLMs and improve them through established meta-learning techniques.", "method": "The training process for reasoning tasks is formalized as a meta-learning setup, conceptualizing reasoning trajectories as pseudo-gradient descent updates to model parameters.", "result": "The study establishes a strong connection between LLM reasoning and meta-learning, demonstrating that LLMs can generalize reasoning capabilities to unseen questions after training on diverse tasks.", "conclusion": "The findings provide practical insights for enhancing LLMs using meta-learning strategies, improving their reasoning abilities.", "key_contributions": ["Introduction of a meta-learning perspective to LLM reasoning", "Formalization of the reasoning process as meta-learning tasks", "Empirical evaluations demonstrating generalization of reasoning capabilities"], "limitations": "", "keywords": ["Large Language Models", "Meta-Learning", "Reasoning", "Machine Learning", "Artificial Intelligence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19838", "pdf": "https://arxiv.org/pdf/2505.19838.pdf", "abs": "https://arxiv.org/abs/2505.19838", "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "categories": ["cs.CL", "cs.AI"], "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "summary": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes.", "AI": {"tldr": "This paper explores the use of Large Language Models for generating and completing taxonomies in the food technology sector.", "motivation": "The research aims to enhance taxonomy generation and completion processes in the food technology industry using LLMs.", "method": "Using an open-source LLM (Llama-3), the paper tests two approaches: completing taxonomies from a seed taxonomy and generating them from known concepts through iterative prompting techniques.", "result": "Experiments on five taxonomies show promise, though challenges in accurately placing inner nodes were identified.", "conclusion": "While the initial results are encouraging, further refinement is needed to improve the accuracy of inner node placements in generated taxonomies.", "key_contributions": ["Novel application of LLMs to taxonomy generation in food technology", "Iterative prompting techniques for taxonomy completion", "Insights into the limitations of current LLM approaches for inner node placement"], "limitations": "Difficulty in correctly placing inner nodes in taxonomies generated by LLMs.", "keywords": ["Large Language Models", "taxonomy generation", "food technology", "Llama-3", "iterative prompting"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.19848", "pdf": "https://arxiv.org/pdf/2505.19848.pdf", "abs": "https://arxiv.org/abs/2505.19848", "title": "Improving Multilingual Math Reasoning for African Languages", "authors": ["Odunayo Ogundepo", "Akintunde Oladipo", "Kelechi Ogueji", "Esther Adenuga", "David Ifeoluwa Adelani", "Jimmy Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model.", "AI": {"tldr": "This paper investigates adaptation strategies for large language models in low-resource African languages, focusing on performance evaluation of pre-training and post-training techniques.", "motivation": "The study addresses the challenges faced by researchers working on low-resource languages, specifically African languages, due to limited data and computational resources.", "method": "The authors conduct extensive experiments and ablation studies to compare various adaptation strategies, including different data types and training stages using the Llama 3.1 model family.", "result": "The paper identifies which adaptation strategies yield the best performance for extending LLMs to African languages, particularly in the context of mathematical reasoning tasks.", "conclusion": "The findings inform effective model adaptation configurations that can improve performance in low-resource language contexts.", "key_contributions": ["Systematic investigation of adaptation strategies for low-resource African languages", "Comprehensive evaluation of pre-training and post-training paradigms", "Insights into the use of translated versus synthetically generated data for model training"], "limitations": "The effectiveness of the strategies may vary across different languages and tasks beyond those evaluated in this study.", "keywords": ["low-resource languages", "large language models", "African languages", "adaptation strategies", "mathematical reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.19851", "pdf": "https://arxiv.org/pdf/2505.19851.pdf", "abs": "https://arxiv.org/abs/2505.19851", "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.", "AI": {"tldr": "This paper evaluates the performance of large language models (LLMs) in the task of transliteration across various Indian languages and compares them to a specialized model, IndicXlit.", "motivation": "The study aims to analyze how general-purpose LLMs can perform transliteration tasks, which are increasingly relevant in linguistically diverse contexts such as India, where multilingual processing is critical.", "method": "The authors conducted experiments utilizing prominent LLMs (GPT-4o, GPT-4.5, GPT-4.1, Gemma-3-27B-it, Mistral-Large) and compared their performances against IndicXlit using datasets like Dakshina and Aksharantar. Performance metrics included Top-1 Accuracy and Character Error Rate, with extensive error analysis and robustness testing.", "result": "Findings indicate that GPT family models generally outperform both other LLMs and the IndicXlit model across most instances, with fine-tuning of GPT-4o showing notable improvements in performance for specific languages.", "conclusion": "The research concludes that foundational models like GPT have significant potential for specialized applications, such as transliteration, demonstrating robustness even under noisy conditions with minimal overhead.", "key_contributions": ["Systematic evaluation of LLMs for transliteration tasks in a multilingual context.", "Comparison of LLMs against a state-of-the-art specialized model, IndicXlit.", "Demonstration of the potential of general-purpose models in specialized applications without explicit task-specific training."], "limitations": "The study may be limited by the choice of languages and datasets used for evaluation, as well as potential biases in LLM performance across different contexts.", "keywords": ["transliteration", "large language models", "multilingual processing", "natural language processing", "performance evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19862", "pdf": "https://arxiv.org/pdf/2505.19862.pdf", "abs": "https://arxiv.org/abs/2505.19862", "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL.", "AI": {"tldr": "Introducing REA-RL for more efficient online training of Large Reasoning Models, balancing performance and inference costs.", "motivation": "To address high inference costs and inefficient training in Large Reasoning Models due to overthinking and the inadequacies of online reinforcement learning methods.", "method": "The proposed method, REA-RL, employs a small reflection model to enhance online training efficiency, integrating parallel sampling with sequential revision and introducing a reflection reward to maintain response quality.", "result": "The combination of methods achieved a 35% reduction in inference costs while maintaining or enhancing performance, effectively managing reflection frequency based on problem complexity.", "conclusion": "REA-RL demonstrates that online training for LRMs can be made more efficient without sacrificing performance, optimizing the balance between response quality and inference speed.", "key_contributions": ["Introduction of REA-RL for online training", "Implementation of a reflection reward to optimize response quality", "Demonstrated significant reduction in inference costs"], "limitations": "", "keywords": ["Large Reasoning Models", "Reinforcement Learning", "Inference Efficiency", "Reflection Model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912.pdf", "abs": "https://arxiv.org/abs/2505.19912", "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "authors": ["Javier Marín"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining.", "AI": {"tldr": "Adjacent Possible Exploration (APE) offers a resource-efficient method for adapting large language models to specific tasks, achieving significant performance improvements with minimal computational overhead.", "motivation": "The need for an efficient fine-tuning method for large language models that requires less computational power while still achieving high performance.", "method": "APE employs an iterative approach to fine-tune models on small, selected data batches of 200 examples, retaining only those improvements that enhance model performance.", "result": "On news summarization, APE achieves a 40% BLEU score improvement with minimal resources, matching or exceeding more complex methods like LoRA while executing in only 60 minutes on a T4 GPU.", "conclusion": "APE demonstrates that small, iterative data perturbations can effectively enhance task-specific performance in LLMs without the need for extensive retraining.", "key_contributions": ["Introduces the Adjacent Possible Exploration method for efficient LLM adaptation.", "Achieves significant performance improvements with minimal computational resources.", "Provides open-source code and validation through metrics and human evaluations."], "limitations": "", "keywords": ["Large Language Models", "Fine-tuning", "Machine Learning", "Natural Language Processing", "Cost Efficiency"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.19914", "pdf": "https://arxiv.org/pdf/2505.19914.pdf", "abs": "https://arxiv.org/abs/2505.19914", "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.", "AI": {"tldr": "Enigmata is a comprehensive suite designed to enhance Large Language Models (LLMs) in puzzle reasoning, offering a generator-verifier framework for task training and evaluation.", "motivation": "To improve LLMs' ability to solve puzzles that do not require domain knowledge, which are typically easy for humans but challenging for LLMs.", "method": "The paper introduces a suite called Enigmata comprising 36 tasks across seven categories, featuring a generator for creating examples and a rule-based verifier for automatic evaluation, facilitating scalable and detailed RL training.", "result": "The trained model Qwen2.5-32B-Enigmata outperforms previous models on puzzle reasoning and generalizes well to mathematical reasoning tasks, showing significant performance improvements with the use of Enigmata data.", "conclusion": "The Enigmata framework provides a structured approach to enhancing logical reasoning in LLMs, with promising transferability to various reasoning tasks in AI.", "key_contributions": ["Introduction of Enigmata, a suite for puzzle reasoning in LLMs.", "Development of a generator-verifier system for scalable multi-task RL training.", "Creation of Enigmata-Eval, a benchmark to evaluate LLM puzzle reasoning abilities."], "limitations": "", "keywords": ["Large Language Models", "puzzle reasoning", "Reinforcement Learning", "benchmark", "AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19937", "pdf": "https://arxiv.org/pdf/2505.19937.pdf", "abs": "https://arxiv.org/abs/2505.19937", "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs", "authors": ["Pooneh Mousavi", "Yingzhi Wang", "Mirco Ravanelli", "Cem Subakan"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Spoken Language Understanding\n(SLU). Recent SLU models process audio directly by adapting speech input into\nLLMs for better multimodal learning. A key consideration for these models is\nthe cross-modal alignment between text and audio modalities, which is a\ntelltale sign as to whether or not LLM is able to associate semantic meaning to\naudio segments. While various methods exist for fusing these modalities, there\nis no standard metric to evaluate alignment quality in LLMs. In this work, we\npropose a new metric, ALAS (Automatic Latent Alignment Score). Our study\nexamines the correlation between audio and text representations across\ntransformer layers, for two different tasks (Spoken Question Answering and\nEmotion Recognition). We showcase that our metric behaves as expected across\ndifferent layers and different tasks.", "AI": {"tldr": "Proposes a new metric for evaluating cross-modal alignment in LLMs used for Spoken Language Understanding.", "motivation": "To address the lack of a standard metric for evaluating the alignment quality between audio and text representations in LLMs for spoken language understanding tasks.", "method": "Introduced ALAS (Automatic Latent Alignment Score) to measure correlation between audio and text representations across transformer layers.", "result": "The ALAS metric shows consistent behavior across different transformer layers and tasks, indicating effective cross-modal alignment evaluation.", "conclusion": "ALAS provides a valuable tool for assessing the integration of multimodal input in LLMs for spoken language understanding, aiding in model development.", "key_contributions": ["Introduction of the ALAS metric for alignment evaluation", "Analysis of audio-text representation correlation across transformer layers", "Application of the metric in two SLU tasks: Spoken Question Answering and Emotion Recognition."], "limitations": "", "keywords": ["Large Language Models", "Spoken Language Understanding", "Cross-modal alignment", "Automatic Latent Alignment Score", "Multimodal learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.19959", "pdf": "https://arxiv.org/pdf/2505.19959.pdf", "abs": "https://arxiv.org/abs/2505.19959", "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "categories": ["cs.CL"], "comment": "Accepted by ACL'25 main track", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.", "AI": {"tldr": "This paper introduces MiniLongBench, a low-cost benchmark for assessing Long Context Understanding (LCU) in large language models, significantly reducing evaluation costs while maintaining high correlation with existing benchmarks.", "motivation": "The motivation of this research is to address the high evaluation costs associated with current Long Context Understanding benchmarks for large language models, which hinder effective assessment of LCU capabilities.", "method": "The authors conducted extensive experimentation to identify redundancy in existing LCU benchmarks, leading to the development of MiniLongBench, which prunes the LongBench dataset to a more efficient set of 237 test samples across six major task categories and 21 distinct tasks.", "result": "MiniLongBench achieves an average evaluation cost of only 4.5% of the original LongBench while maintaining a high average rank correlation coefficient of 0.97 with LongBench results, demonstrating its effectiveness for LCU assessment.", "conclusion": "MiniLongBench provides a substantial reduction in evaluation costs and offers a promising avenue for future research into Long Context Understanding capabilities of large language models.", "key_contributions": ["Introduction of MiniLongBench, a concise benchmark for LCU.", "Significant reduction in evaluation costs for long-text data.", "High correlation with existing benchmarks, indicating reliability."], "limitations": "", "keywords": ["Long Context Understanding", "large language models", "benchmarking", "data compression", "evaluation costs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.19970", "pdf": "https://arxiv.org/pdf/2505.19970.pdf", "abs": "https://arxiv.org/abs/2505.19970", "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM", "authors": ["Jiayuan Su", "Fulin Lin", "Zhaopeng Feng", "Han Zheng", "Teng Wang", "Zhenyu Xiao", "Xinlong Zhao", "Zuozhu Liu", "Lu Cheng", "Hongwei Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness.", "AI": {"tldr": "CP-Router is a model-agnostic routing framework that dynamically selects between LLMs and LRMs for efficient multi-choice question answering by utilizing uncertainty estimates for optimal model choice.", "motivation": "To address the inefficiencies and accuracy degradation caused by Large Reasoning Models producing lengthy outputs for simple queries.", "method": "The CP-Router utilizes a training-free, model-agnostic approach that makes routing decisions based on prediction uncertainty estimates obtained through Conformal Prediction, enhanced by a novel Full and Binary Entropy criterion for adaptive threshold selection.", "result": "Experiments show that CP-Router reduces token usage while maintaining or improving accuracy compared to using LRM alone across various MCQA benchmarks.", "conclusion": "CP-Router exhibits strong performance across diverse model pairings and open-ended QA, validating its generality and robustness.", "key_contributions": ["Introduction of CP-Router as a model-agnostic routing framework.", "Utilization of Conformal Prediction for uncertainty estimation.", "Development of Full and Binary Entropy criterion for adaptive threshold selection."], "limitations": "", "keywords": ["Large Reasoning Models", "Large Language Models", "Conformal Prediction", "Multi-Choice Question Answering", "Entropy-based Criterion"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.19971", "pdf": "https://arxiv.org/pdf/2505.19971.pdf", "abs": "https://arxiv.org/abs/2505.19971", "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language", "authors": ["Kilian Sennrich", "Sina Ahmadi"], "categories": ["cs.CL"], "comment": "Accepted to LDK 2025 - the 5th Conference on Language, Data and\n  Knowledge. Naples, Italy, 9-11 September 2025", "summary": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation.", "AI": {"tldr": "This paper develops a natural language interface for querying lexicographic data in knowledge graphs, notably Wikidata, and evaluates the performance of several language models.", "motivation": "To simplify lexicographic data retrieval from knowledge graphs for non-expert users who struggle with SPARQL.", "method": "A multidimensional taxonomy of Wikidata's lexicographic data ontology was created, and a dataset with over 1.2 million mappings from natural language utterances to SPARQL queries was developed, followed by experiments with various language models.", "result": "GPT-2, Phi-1.5, and GPT-3.5-Turbo were tested, revealing that while all effectively handle familiar patterns, only GPT-3.5-Turbo shows notable generalization, indicating the importance of model size and diverse pre-training.", "conclusion": "Robust generalization and handling of diverse linguistic data remains challenging, pointing towards the need for scalable solutions in lexicographic knowledge representation.", "key_contributions": ["Development of a multidimensional taxonomy for Wikidata's lexicographic data ontology.", "Creation of a large template-based dataset for natural language to SPARQL query mapping.", "Experimental evaluation of language models' performance in this domain."], "limitations": "Challenges in achieving robust generalization and accommodating the full complexity of lexicographic knowledge representation remain.", "keywords": ["knowledge graphs", "natural language interface", "SPARQL", "lexicographic data", "language models"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2505.19978", "pdf": "https://arxiv.org/pdf/2505.19978.pdf", "abs": "https://arxiv.org/abs/2505.19978", "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Elena Baralis"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Currently under review. See the official website:\n  https://salt-research.github.io/DeepDialogue", "summary": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations.", "AI": {"tldr": "DeepDialogue is a large-scale multimodal dataset for multi-turn dialogues, addressing limitations in emotional range and domain diversity.", "motivation": "To improve coherence and emotional engagement in multi-turn dialogues in conversational AI, as current datasets lack depth and variety.", "method": "The dataset consists of 40,150 high-quality multi-turn dialogues across 41 domains and 20 distinct emotions, created by pairing 9 language models to generate initial conversations, which were then evaluated for quality.", "result": "The dataset provides insights into dialogue coherence, revealing limitations of smaller models and the benefits of concrete domains and cross-model interactions.", "conclusion": "DeepDialogue introduces an essential resource for advancing human-like conversational systems and offers the first large-scale open-source multimodal dialogue dataset preserving emotional context.", "key_contributions": ["Introduction of the DeepDialogue dataset for multi-turn dialogues.", "Inclusion of a speech component with emotion-consistent voices for dialogues.", "Insights on the importance of model size and domain in maintaining coherence."], "limitations": "", "keywords": ["Conversational AI", "Multimodal Dataset", "Multi-turn Dialogues", "Emotional Progression", "Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.19987", "pdf": "https://arxiv.org/pdf/2505.19987.pdf", "abs": "https://arxiv.org/abs/2505.19987", "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation", "authors": ["Yongshi Ye", "Biao Fu", "Chongxuan Huang", "Yidong Chen", "Xiaodong Shi"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts.", "AI": {"tldr": "This paper evaluates the effectiveness of Large Reasoning Models (LRMs) compared to traditional Large Language Models (LLMs) in domain-sensitive machine translation tasks, finding that LRMs outperform LLMs particularly in complex scenarios.", "motivation": "To investigate whether structured reasoning via Large Reasoning Models can improve translation quality in complex, domain-sensitive tasks.", "method": "The study compares LRMs with traditional LLMs in 15 domains and 4 translation directions using automatic metrics and an enhanced MQM-based evaluation hierarchy.", "result": "LRMs consistently outperform traditional LLMs in semantically complex domains, particularly for long-text and high-difficulty translations.", "conclusion": "The research suggests that structured reasoning can significantly enhance translation quality in domain-sensitive contexts and identifies domain-adaptive prompting as a beneficial strategy.", "key_contributions": ["Demonstrated superiority of LRMs over LLMs in complex translation scenarios", "Provided a comprehensive evaluation across multiple domains", "Introduced domain-adaptive prompting strategies for improved performance"], "limitations": "", "keywords": ["Large Language Models", "Large Reasoning Models", "Machine Translation", "Domain-Sensitive Translation", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20006", "pdf": "https://arxiv.org/pdf/2505.20006.pdf", "abs": "https://arxiv.org/abs/2505.20006", "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition", "authors": ["Raphaël Bagat", "Irina Illina", "Emmanuel Vincent"], "categories": ["cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR.", "AI": {"tldr": "Introducing MAS-LoRA, a fine-tuning method to enhance ASR robustness to non-native speech in multi-accent scenarios.", "motivation": "To address the challenges of Automatic Speech Recognition (ASR) for non-native speakers in low-resourced multi-accent settings.", "method": "MAS-LoRA utilizes a mixture of Low-Rank Adaptation experts tailored to specific accents, allowing for effective inference regardless of accent knowledge.", "result": "Significant improvements in Word Error Rate were observed with MAS-LoRA compared to regular LoRA and full fine-tuning, particularly when accent knowledge was either known or unknown.", "conclusion": "MAS-LoRA not only improves ASR accuracy but also reduces catastrophic forgetting during fine-tuning practices, marking a novel approach in the field.", "key_contributions": ["First application of a mixture of LoRA experts for non-native multi-accent ASR", "Demonstrated significant improvements in Word Error Rate", "Reduced catastrophic forgetting compared to existing methods"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Non-native speech", "Multi-accent adaptation", "Low-Rank Adaptation", "Machine Learning"], "importance_score": 5, "read_time_minutes": 6}}
{"id": "2505.20013", "pdf": "https://arxiv.org/pdf/2505.20013.pdf", "abs": "https://arxiv.org/abs/2505.20013", "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "categories": ["cs.CL"], "comment": "18 pages", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.", "AI": {"tldr": "This paper explores enhancing web agents powered by LLMs through improved reasoning skills by curating trajectory data and demonstrating significant performance gains via fine-tuning.", "motivation": "To address the limited reasoning capabilities of LLM-powered web agents in uncertain and dynamic environments, identifying key reasoning skills is essential for robust deployment.", "method": "The authors reconstruct the agent's reasoning algorithms into chain-of-thought rationales and conduct experiments using the OpenWebVoyager benchmark to fine-tune the LLM based on salient reasoning patterns.", "result": "The fine-tuned LLM shows substantial performance improvements across multiple benchmarks, indicating that focused enhancement of reasoning skills can significantly benefit web agents.", "conclusion": "Targeted reasoning skill enhancement for LLMs serves as a promising avenue for boosting the effectiveness of web agents in navigating complex web environments.", "key_contributions": ["Identification of key reasoning skills for web agents such as reflection, branching, and rollback.", "Curated trajectory data showcasing effective reasoning abilities.", "Demonstrated performance improvements through fine-tuning LLMs on reasoning patterns."], "limitations": "", "keywords": ["Large Language Models", "web agents", "reasoning skills", "fine-tuning", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20014", "pdf": "https://arxiv.org/pdf/2505.20014.pdf", "abs": "https://arxiv.org/abs/2505.20014", "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation", "authors": ["Hoyun Song", "Huije Lee", "Jisu Shin", "Sukmin Cho", "Changgeon Ko", "Jong C. Park"], "categories": ["cs.CL"], "comment": null, "summary": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications.", "AI": {"tldr": "This paper investigates the impact of rationale quality on the performance of smaller language models in mental health detection, proposing a framework that selects highly aligned rationales with expert clinical reasoning.", "motivation": "The study addresses the limitations of large language models in mental health applications due to their high computational costs and proposes a method for improving smaller language models using rationale quality.", "method": "The authors propose a framework that selects rationales based on their alignment with expert clinical reasoning, enhancing the distillation process for smaller language models.", "result": "Experiments demonstrate that the quality-focused approach improves the performance of smaller language models in detecting mental disorders and generating rationales.", "conclusion": "High-quality and domain-relevant rationales significantly enhance the performance of smaller language models in mental health applications, providing a useful framework for knowledge transfer.", "key_contributions": ["Proposed a rationale quality-focused framework for SLMs.", "Demonstrated improved performance in mental disorder detection using SLMs.", "Highlighted the significance of rationale quality in mental health applications."], "limitations": "", "keywords": ["Mental Health", "Language Models", "Rationale Quality", "Detection", "Expert Clinical Reasoning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015.pdf", "abs": "https://arxiv.org/abs/2505.20015", "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "authors": ["Ramon Ferrer-i-Cancho"], "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression.", "AI": {"tldr": "This paper introduces a new class of optimality for coding systems that exhibit Zipf's law, providing insights into the relationship between frequency ranks and optimal coding lengths.", "motivation": "The study aims to explore a new class of optimality for coding systems and the implications of Zipf's law on human languages and animal communication systems.", "method": "The authors analyze the relationship between coding lengths and frequency distributions in languages and communication systems, particularly focusing on Zipf's law and its characteristics in a double logarithmic scale.", "result": "The research identifies human languages that align with Zipf's law as potential members of this optimality class, while also suggesting that certain animal communication systems may not fit this classification due to differing frequency distributions.", "conclusion": "The findings imply that Zipf's law may arise naturally from the principles of coding and compression, providing new insights into language efficiency and communication systems.", "key_contributions": ["Introduction of a new class of optimality for coding systems", "Identification of human languages conforming to Zipf's law", "Insights into animal communication systems regarding frequency distributions."], "limitations": "", "keywords": ["Zipf's law", "coding systems", "human languages", "communication", "optimization"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.20016", "pdf": "https://arxiv.org/pdf/2505.20016.pdf", "abs": "https://arxiv.org/abs/2505.20016", "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "authors": ["Chengrui Huang", "Shen Gao", "Zhengliang Shi", "Dongsheng Wang", "Shuo Shang"], "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets.", "AI": {"tldr": "Introducing a framework for fine-tuning LLMs to improve tool-use preferences.", "motivation": "Existing methods for tool-learning often miss optimization of internal tool call details, limiting performance in preference alignment and error discrimination.", "method": "Proposes a Token-level Tool-use Preference Alignment Training Framework (TTPA) that constructs token-level tool-use preference datasets and uses an Error-oriented Scoring Mechanism (ESM) for training.", "result": "TTPA significantly enhances tool-using performance and generalizes well across various models and datasets.", "conclusion": "The framework presents a novel approach that improves alignment of LLMs with fine-grained preferences and better handling of tool-call errors.", "key_contributions": ["Introduction of the reversed dataset construction method for high-quality tool-use datasets", "Token-level Preference Sampling (TPS) to capture fine-grained preferences", "Error-oriented Scoring Mechanism (ESM) to quantify and address tool-call errors"], "limitations": "", "keywords": ["Tool-use preferences", "Token-level training", "Error-oriented scoring", "Machine learning", "Natural language models"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2505.20023", "pdf": "https://arxiv.org/pdf/2505.20023.pdf", "abs": "https://arxiv.org/abs/2505.20023", "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories.", "AI": {"tldr": "We propose STeP, a method to enhance LLM-based agent training through self-reflected trajectories to improve learning and performance.", "motivation": "Current LLM-based agents depend heavily on effective prompt engineering and closed-source models. Open-source LLMs have limitations such as performance plateauing and error propagation in agent capabilities.", "method": "We introduce STeP, synthesizing self-reflected trajectories with error corrections and a partial masking strategy to improve LLM agent training.", "result": "Experiments show significant performance improvement in LLaMA2-7B-Chat using our self-reflective method across tasks like ALFWorld, WebShop, and SciWorld.", "conclusion": "STeP enables LLM agents to self-reflect and correct, outperforming traditional training methods with less data.", "key_contributions": ["Introduction of STeP for self-reflection in agent training", "Development of a partial masking strategy to avoid incorrect learning", "Demonstrated performance improvements across multiple tasks"], "limitations": "", "keywords": ["LLM agents", "self-reflection", "machine learning", "training methodologies", "error correction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20045", "pdf": "https://arxiv.org/pdf/2505.20045.pdf", "abs": "https://arxiv.org/abs/2505.20045", "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs", "authors": ["Artem Vazhentsev", "Lyudmila Rvanova", "Gleb Kuzmin", "Ekaterina Fadeeva", "Ivan Lazichny", "Alexander Panchenko", "Maxim Panov", "Timothy Baldwin", "Mrinmaya Sachan", "Preslav Nakov", "Artem Shelmanov"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs.", "AI": {"tldr": "Proposes RAUQ, an unsupervised method for uncertainty quantification in LLMs that detects hallucinations by analyzing attention patterns, achieving high performance with low computational overhead.", "motivation": "To address the critical errors (hallucinations) produced by large language models by improving uncertainty quantification methods that are often computationally intensive and require supervised learning.", "method": "RAUQ leverages intrinsic attention patterns in transformers, identifying patterns in attention weights to detect hallucinations, aggregating these weights and confidences to compute uncertainty scores efficiently in a single forward pass.", "result": "RAUQ outperforms state-of-the-art UQ methods across 4 LLMs and 12 tasks (question answering, summarization, translation) while maintaining less than 1% latency and requiring no task-specific labels or hyperparameter tuning.", "conclusion": "RAUQ presents a plug-and-play solution for real-time hallucination detection in LLMs, simplifying the process of uncertainty quantification in natural language tasks.", "key_contributions": ["Introduction of RAUQ as an unsupervised UQ method for LLMs", "Demonstration of effective hallucination detection using attention weight analysis", "Achievement of superior performance with minimal computational cost"], "limitations": "", "keywords": ["large language models", "uncertainty quantification", "hallucination detection", "transformers", "attention patterns"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.20047", "pdf": "https://arxiv.org/pdf/2505.20047.pdf", "abs": "https://arxiv.org/abs/2505.20047", "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.", "AI": {"tldr": "This paper investigates the failure modes and uncertainty quantification in LLM-generated formal specifications, proposing a PCFG framework to improve the reliability of LLM outputs in formal verification.", "motivation": "To address the tension between the probabilistic nature of LLMs and the deterministic guarantees required for formal verification.", "method": "A systematic evaluation of five frontier LLMs and the introduction of a probabilistic context-free grammar (PCFG) framework to model LLM outputs.", "result": "LLM-generated formal artifacts show significant accuracy variations depending on the task, with known UQ techniques failing to identify many errors. A selective verification approach using uncertainty signals proves effective in reducing errors by 14-100%.", "conclusion": "The study provides insights into the task-dependent nature of uncertainty in LLM outputs and presents a method for enhancing reliability in LLM-driven formalization processes.", "key_contributions": ["Investigation of failure modes in LLM-generated formal artifacts", "Introduction of a probabilistic context-free grammar framework", "Development of a selective verification approach to reduce errors in formal specifications"], "limitations": "The framework and findings may be domain-specific and require further validation across various applications.", "keywords": ["large language models", "formal verification", "uncertainty quantification", "probabilistic context-free grammar", "selective verification"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072.pdf", "abs": "https://arxiv.org/abs/2505.20072", "title": "Incentivizing Reasoning from Weak Supervision", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.", "AI": {"tldr": "This paper explores how reasoning capabilities of large language models (LLMs) can be enhanced through supervision from weaker models, achieving significant performance improvements at a lower cost compared to traditional methods.", "motivation": "To find an effective and cost-efficient way to enhance the reasoning abilities of LLMs without relying on expensive reinforcement learning or high-quality supervised fine-tuning.", "method": "The study investigates the effects of supervision from significantly weaker models on the reasoning performance of stronger LLMs across various benchmarks and architectures.", "result": "Supervision from weaker reasoners enables substantial improvements in reasoning performance, recovering close to 94% of the gains achieved through expensive reinforcement learning methods.", "conclusion": "The findings suggest that the weak-to-strong supervision paradigm presents a viable alternative for enhancing reasoning abilities in LLMs during inference, offering significant cost benefits.", "key_contributions": ["Introduced the weak-to-strong supervision paradigm for LLM reasoning enhancement.", "Demonstrated substantial performance gains compared to expensive RL methods.", "Provided empirical evidence across diverse benchmarks supporting the effectiveness of weaker models as supervisors."], "limitations": "", "keywords": ["large language models", "weak supervision", "reasoning abilities", "reinforcement learning", "supervised fine-tuning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081.pdf", "abs": "https://arxiv.org/abs/2505.20081", "title": "Inference-time Alignment in Continuous Space", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA", "AI": {"tldr": "Proposes Simple Energy Adaptation (SEA) for aligning large language models with human feedback by adapting responses in continuous latent space.", "motivation": "Align large language models with human feedback at inference time to improve adaptability and effectiveness.", "method": "SEA uses gradient-based sampling in a continuous latent space for iterative optimization on an energy function, rather than generating multiple discrete responses.", "result": "SEA shows improved alignment performance with a 77.51% increase on AdvBench and 16.36% on MATH over the best baseline.", "conclusion": "SEA provides a more effective and simpler solution for alignment compared to existing discrete search methods.", "key_contributions": ["Introduction of Simple Energy Adaptation (SEA) algorithm", "Demonstration of performance improvements on standard benchmarks", "Public availability of the implementation for further research"], "limitations": "", "keywords": ["large language models", "human feedback", "inference-time alignment", "gradient-based sampling", "energy function"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20088", "pdf": "https://arxiv.org/pdf/2505.20088.pdf", "abs": "https://arxiv.org/abs/2505.20088", "title": "Multi-Domain Explainability of Preferences", "authors": ["Nitay Calderon", "Liat Ein-Dor", "Roi Reichart"], "categories": ["cs.CL"], "comment": null, "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated end-to-end method for\ngenerating local and global concept-based explanations of preferences across\nmultiple domains. Our method employs an LLM to discover concepts that\ndifferentiate between chosen and rejected responses and represent them with\nconcept-based vectors. To model the relationships between concepts and\npreferences, we propose a white-box Hierarchical Multi-Domain Regression model\nthat captures both domain-general and domain-specific effects. To evaluate our\nmethod, we curate a dataset spanning eight challenging and diverse domains and\nexplain twelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two novel application-driven settings.\nFirst, guiding LLM outputs with concepts from LaaJ explanations yields\nresponses that those judges consistently prefer. Second, prompting LaaJs with\nconcepts explaining humans improves their preference predictions. Together, our\nwork provides a new paradigm for explainability in the era of LLMs.", "AI": {"tldr": "The paper introduces an automated method for generating explanations of preferences in LLMs using a novel regression model, achieving superior performance in preference prediction across diverse domains.", "motivation": "To enhance understanding and explainability of preference mechanisms in large language models (LLMs), which are currently poorly understood.", "method": "The proposed method employs an LLM to identify concepts that differentiate chosen and rejected responses, using a Hierarchical Multi-Domain Regression model to relate concepts with preferences.", "result": "The method achieves strong performance in preference prediction, outpacing baselines, and provides explainable outputs through novel applications.", "conclusion": "The work establishes a new approach to explainability in LLMs, facilitating better alignment and evaluation through concept-based understanding of preferences.", "key_contributions": ["Automated generation of local and global explanations of preferences in LLMs.", "Introduction of a white-box Hierarchical Multi-Domain Regression model for understanding preferences.", "Demonstration of improved LLM outputs and preference predictions using concept-based explanations."], "limitations": "", "keywords": ["preference mechanisms", "LLM explainability", "Hierarchical Multi-Domain Regression"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20096", "pdf": "https://arxiv.org/pdf/2505.20096.pdf", "abs": "https://arxiv.org/abs/2505.20096", "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "AI": {"tldr": "MA-RAG is a Multi-Agent framework for Retrieval-Augmented Generation that utilizes specialized AI agents to address challenges in complex information-seeking tasks.", "motivation": "The paper addresses ambiguities and reasoning challenges that arise in traditional Retrieval-Augmented Generation (RAG) methods when handling complex information-seeking tasks.", "method": "MA-RAG orchestrates a set of specialized AI agents that handle various subtasks within the RAG pipeline, using task-aware reasoning and dynamic agent invocation to control information flow without model fine-tuning.", "result": "Experiments show that MA-RAG outperforms state-of-the-art training-free baselines and rivals fine-tuned systems on multi-hop and ambiguous QA benchmarks.", "conclusion": "The collaborative agent-based reasoning approach in MA-RAG enhances interpretability and robustness in RAG tasks, demonstrating its potential over conventional methods.", "key_contributions": ["Introduction of a multi-agent framework for RAG", "Dynamic and modular approach to information processing", "Improved performance on multi-hop and ambiguous QA tasks"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Multi-Agent Systems", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20097", "pdf": "https://arxiv.org/pdf/2505.20097.pdf", "abs": "https://arxiv.org/abs/2505.20097", "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs", "authors": ["Liang Cheng", "Tianyi LI", "Zhaowei Wang", "Mark Steedman"], "categories": ["cs.CL"], "comment": "15 pages", "summary": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness", "AI": {"tldr": "The paper explores the consistency of prompt preferences across various Large Language Models (LLMs) and proposes a method to efficiently select prompt templates using a smaller model, reducing costs while maintaining performance.", "motivation": "To address the challenges of prompt engineering in LLMs which incurs high computing and human costs.", "method": "Experiments conducted on multiple LLM variants of different sizes focusing on their performance across tasks like Question Answering and Natural Language Inference.", "result": "Found consistency in prompt preferences across LLMs, allowing the use of smaller models to select effective prompts for larger counterparts, resulting in reduced engineering costs and maintained performance.", "conclusion": "The proposed method demonstrates efficiency in prompt selection, applicable to various NLP tasks and shows robustness across numerous LLMs.", "key_contributions": ["Identification of prompt preference consistency across different LLM sizes", "Introduction of a method to use smaller models for effective prompt selection", "Demonstration of cost reduction in prompt engineering without compromising performance"], "limitations": "", "keywords": ["Large Language Models", "Prompt Engineering", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20099", "pdf": "https://arxiv.org/pdf/2505.20099.pdf", "abs": "https://arxiv.org/abs/2505.20099", "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Under Review", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.", "AI": {"tldr": "This survey categorizes methods for integrating large language models (LLMs) and knowledge graphs (KGs) in question-answering (QA) tasks, addresses challenges in QA, and highlights advancements and open questions.", "motivation": "The paper addresses challenges faced by LLM-based question-answering, such as reasoning deficiencies, outdated knowledge, and hallucinations, by integrating knowledge graphs.", "method": "The authors propose a structured taxonomy for synthesizing LLMs and KGs for QA, systematically surveying state-of-the-art methodologies and comparing them based on strengths, limitations, and KG requirements.", "result": "The survey provides a comprehensive analysis of how different approaches tackle complex QA challenges and summarizes advancements, evaluation metrics, and datasets used in the field.", "conclusion": "The survey highlights open challenges and opportunities in the integration of LLMs and KGs for QA, indicating a roadmap for future research.", "key_contributions": ["Proposed a structured taxonomy for LLM and KG integration in QA.", "Systematic survey of advanced methodologies and their comparative analysis.", "Identification of open challenges and opportunities in the field."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Question Answering", "Natural Language Processing", "Survey"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.20101", "pdf": "https://arxiv.org/pdf/2505.20101.pdf", "abs": "https://arxiv.org/abs/2505.20101", "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT.In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype.Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications.", "AI": {"tldr": "The paper presents a method for autonomously switching between long and short reasoning chains in large language models to optimize efficiency while maintaining accuracy.", "motivation": "Address the high computational costs of long-chain reasoning in large language models and improve practical deployment.", "method": "The approach involves supervised fine-tuning of the model to develop both reasoning capabilities, followed by reinforcement learning to balance the generation of short and long reasoning chains.", "result": "Experiments show that the model can switch between reasoning modes dynamically, enhancing performance on mathematical tasks without significant performance loss.", "conclusion": "The proposed method increases the practicality of using large language models in real-world applications by optimizing reasoning processes.", "key_contributions": ["Development of a dynamic switching mechanism for reasoning chains in LLMs", "Integration of reinforcement learning with an adaptive reward strategy", "Improvement of reasoning efficiency while maintaining accuracy"], "limitations": "", "keywords": ["large language models", "long-chain reasoning", "short-chain reasoning", "reinforcement learning", "supervised fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20109", "pdf": "https://arxiv.org/pdf/2505.20109.pdf", "abs": "https://arxiv.org/abs/2505.20109", "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to InterSpeech 2025", "summary": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment.", "AI": {"tldr": "This study presents a language-agnostic framework for detecting suicidal risk in adolescents using large language models (LLMs).", "motivation": "The challenge of suicidal risk detection in adolescents has been limited by existing methods that rely on language-specific models, hindering their scalability and generalization.", "method": "The proposed framework generates Chinese transcripts from speech using an automatic speech recognition (ASR) model and employs LLMs with prompt-based queries to extract suicidal risk-related features, maintaining these features in both Chinese and English for cross-linguistic analysis.", "result": "Experimental results indicate that the proposed method achieves performance comparable to direct fine-tuning with ASR results or models trained solely on Chinese features, suggesting it can effectively overcome language constraints.", "conclusion": "The framework demonstrates significant potential for improving the robustness of suicidal risk assessment across languages.", "key_contributions": ["Introduction of a language-agnostic framework for suicidal risk assessment", "Utilization of ASR models to facilitate cross-linguistic analysis", "Implementation of a methodology that retains extracted features in multiple languages"], "limitations": "", "keywords": ["suicidal risk detection", "language-agnostic", "large language models", "adolescents", "cross-linguistic analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112.pdf", "abs": "https://arxiv.org/abs/2505.20112", "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness.", "AI": {"tldr": "ResSVD is a novel post-training SVD-based method for compressing large language models that effectively reduces truncation loss and improves performance.", "motivation": "Large language models are powerful but challenging to deploy due to their size and memory demands, necessitating efficient compression strategies.", "method": "The paper introduces ResSVD, which uses the residual matrix from singular value decomposition truncation to minimize loss. It selectively compresses the last layers of models to prevent performance degradation.", "result": "ResSVD demonstrates superior performance compared to existing compression methods across various LLM families and benchmark datasets.", "conclusion": "ResSVD's approach provides an effective means to compress LLMs while maintaining model performance, highlighting its practical applicability.", "key_contributions": ["Introduced ResSVD for LLM compression using residual matrices", "Selective layer compression to mitigate error propagation", "Demonstrated superior performance on benchmark datasets"], "limitations": "Current methods do not address the residual matrix from truncation, leading to potential performance issues in some contexts.", "keywords": ["LLM Compression", "Singular Value Decomposition", "Machine Learning", "Natural Language Processing", "Model Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20113", "pdf": "https://arxiv.org/pdf/2505.20113.pdf", "abs": "https://arxiv.org/abs/2505.20113", "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences.", "AI": {"tldr": "This research evaluates Named Entity Recognition (NER) techniques on 19th century Italian texts, introducing a new dataset from Giacomo Leopardi's Zibaldone and comparing BERT-based models with state-of-the-art LLMs.", "motivation": "To address the challenges posed by historical texts in NER and highlight the need for effective computational techniques for digitized literary heritage, particularly in Italian language.", "method": "A new dataset comprising 2,899 references from Giacomo Leopardi's Zibaldone was created and used for reproducible experiments comparing domain-specific BERT-based models and LLaMa3.1.", "result": "Instruction-tuned models struggled with historical texts, while fine-tuned NER models demonstrated more robust performance, especially with complex entity types like bibliographic references.", "conclusion": "Fine-tuned NER models are more effective than instruction-tuned LLMs for processing historical texts, indicating a need for specialized approaches in NER tasks for digitized literary works.", "key_contributions": ["Introduction of a new dataset for NER based on historical texts", "Comparison of performance between BERT-based models and modern LLMs", "Insights into limitations of using instruction-tuned models for historical data"], "limitations": "The study mainly focuses on Italian texts from a specific historical period, limiting generalizability to other languages or periods.", "keywords": ["Named Entity Recognition", "historical texts", "large language models", "Giacomo Leopardi", "dataset"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20118", "pdf": "https://arxiv.org/pdf/2505.20118.pdf", "abs": "https://arxiv.org/abs/2505.20118", "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul Röttger", "Terry Ruas", "Bela Gipp"], "categories": ["cs.CL", "cs.CR"], "comment": "8 pages, 5 figures", "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.", "AI": {"tldr": "This paper proposes TrojanStego, a new threat model for LLMs where adversaries can fine-tune models to embed sensitive information into outputs using linguistic steganography.", "motivation": "To address concerns about the leakage of confidential information from LLMs in sensitive workflows.", "method": "A novel encoding scheme based on vocabulary partitioning is implemented, allowing LLMs to fine-tune and encode secrets without controlling inference inputs.", "result": "Compromised models successfully transmit 32-bit secrets with 87% accuracy and achieve over 97% accuracy through majority voting across generations.", "conclusion": "TrojanStego reveals a new class of covert and practical data exfiltration attacks on LLMs, highlighting significant risks.", "key_contributions": ["Introduction of a new threat model for LLMs (TrojanStego).", "Development of a practical encoding scheme using learnable vocabulary partitioning.", "Empirical demonstration of high accuracy in secret transmission and high utility of compromised models."], "limitations": "The model may require further evaluation across diverse datasets and context scenarios.", "keywords": ["Large Language Models", "Steganography", "Threat Model", "Data Exfiltration", "Security"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.20128", "pdf": "https://arxiv.org/pdf/2505.20128.pdf", "abs": "https://arxiv.org/abs/2505.20128", "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "categories": ["cs.CL"], "comment": "Working in process", "summary": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work.", "AI": {"tldr": "EXSEARCH is an agentic search framework that enables LLMs to effectively retrieve information for complex queries through a self-incentivized process.", "motivation": "To enhance LLMs' ability to accurately retrieve information in complex multi-hop queries, addressing challenges like irrelevant content.", "method": "EXSEARCH utilizes a Generalized Expectation-Maximization algorithm where LLMs generate multiple search trajectories, assigning weights and training through a re-weighted loss function.", "result": "EXSEARCH outperforms traditional methods, achieving a +7.8% improvement in exact match score across four knowledge-intensive benchmarks.", "conclusion": "EXSEARCH demonstrates significant advancements in LLM-driven information retrieval and introduces EXSEARCH-Zoo for expanded applicability in future research.", "key_contributions": ["Introduction of EXSEARCH for LLM information retrieval", "Development of a self-incentivized learning mechanism for search", "Introduction of EXSEARCH-Zoo for broader application scenarios"], "limitations": "", "keywords": ["Large Language Models", "Information Retrieval", "Self-incentivized Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20133", "pdf": "https://arxiv.org/pdf/2505.20133.pdf", "abs": "https://arxiv.org/abs/2505.20133", "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines.", "AI": {"tldr": "AweDist is a method for quickly learning high-quality embeddings for new tokens in language models by distilling representations from original tokenization, outperforming existing methods and strong baselines.", "motivation": "Address the challenges posed by static vocabularies in language models, which can negatively impact performance and increase computational costs, especially for underrepresented domains.", "method": "The paper introduces AweDist, a method that distills representations from original tokenization to learn embeddings for new tokens without the need for expensive additional training.", "result": "Experimental results demonstrate that AweDist outperforms traditional embedding initialization methods and strong baselines in terms of performance and efficiency.", "conclusion": "AweDist provides an effective solution for improving language model performance on specialized domains by enabling quick learning of new token embeddings.", "key_contributions": ["Introduction of AweDist for efficient embedding initialization", "Demonstration of outperforming strong baselines with distillation approach", "Reduction of training costs associated with adding new tokens"], "limitations": "", "keywords": ["language models", "embedding initialization", "AweDist", "tokenization", "distillation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.20144", "pdf": "https://arxiv.org/pdf/2505.20144.pdf", "abs": "https://arxiv.org/abs/2505.20144", "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "an early-stage version", "summary": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition.", "AI": {"tldr": "SeMe is a novel, data-free method for merging Language Models at a fine-grained level that preserves internal knowledge and outperforms existing techniques.", "motivation": "There is no single language model that consistently outperforms others, highlighting the need for efficient model merging without expensive retraining.", "method": "SeMe utilizes latent semantic alignment for layer-wise merging of language models without relying on external data.", "result": "SeMe shows superior performance and efficiency compared to existing model merging methods while preserving the models' internal knowledge.", "conclusion": "SeMe establishes a new standard for knowledge-aware model merging and enhances the interpretability and scalability of model composition.", "key_contributions": ["Introduces a data-free model merging approach", "Preserves model behaviors and stabilizes internal knowledge", "Demonstrates better performance than existing methods"], "limitations": "", "keywords": ["Language Models", "Model Merging", "Semantic Alignment", "Knowledge Preservation", "Efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20154", "pdf": "https://arxiv.org/pdf/2505.20154.pdf", "abs": "https://arxiv.org/abs/2505.20154", "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models", "authors": ["Xueyan Zhang", "Jinman Zhao", "Zhifei Yang", "Yibo Zhong", "Shuhao Guan", "Linbo Cao", "Yining Wang"], "categories": ["cs.CL"], "comment": "20 pages, 2 figures, 15 tables", "summary": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs.", "AI": {"tldr": "UORA is a new parameter-efficient fine-tuning method for LLMs, utilizing low-rank approximation for reduced trainable parameters and outperforming existing methods in efficiency and performance.", "motivation": "This work addresses the need for more efficient fine-tuning methods for Large Language Models that maintain high performance while reducing the computational burden.", "method": "The authors introduce Uniform Orthogonal Reinitialization Adaptation (UORA), which uses an interpolation-based reparametrization mechanism to selectively reinitialize parts of frozen projection matrices, reducing trainable parameters significantly.", "result": "UORA outperforms existing methods like LoRA and VeRA on various benchmarks including GLUE and E2E, demonstrating better computational and storage efficiency with competitive fine-tuning performance.", "conclusion": "UORA establishes a new framework for scalable and resource-efficient fine-tuning of LLMs, presenting a significant improvement over traditional methods.", "key_contributions": ["Introduction of a novel fine-tuning method (UORA) for LLMs.", "Significantly reduced number of trainable parameters through low-rank approximation and selective reinitialization.", "Demonstrated state-of-the-art performance and efficiency on multiple benchmarks."], "limitations": "", "keywords": ["Parameter-efficient fine-tuning", "Large Language Models", "Low-rank approximation", "Fine-tuning efficiency", "Instruction-tuning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.20155", "pdf": "https://arxiv.org/pdf/2505.20155.pdf", "abs": "https://arxiv.org/abs/2505.20155", "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs", "authors": ["Hanting Chen", "Jiarui Qin", "Jialong Guo", "Tao Yuan", "Yichun Yin", "Huiling Zhen", "Yasheng Wang", "Jinpeng Li", "Xiaojun Meng", "Meng Zhang", "Rongju Ruan", "Zheyuan Bai", "Yehui Tang", "Can Chen", "Xinghao Chen", "Fisher Yu", "Ruiming Tang", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s.", "AI": {"tldr": "Pangu Light is a framework for improving the efficiency of Large Language Models (LLMs) through structured pruning and novel weight re-initialization techniques, addressing performance degradation issues from aggressive pruning.", "motivation": "The paper addresses the computational challenges of deploying large language models (LLMs) due to their size and inference costs, emphasizing the importance of effective pruning methods.", "method": "The Pangu Light framework combines structured pruning with innovative weight re-initialization techniques like Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP) to enhance model training accuracies post-pruning.", "result": "Pangu Light consistently outperforms major baseline pruning methods and established LLMs, demonstrating superior accuracy-efficiency trade-offs on Ascend NPUs.", "conclusion": "The study concludes that strategic re-initialization of weights post-pruning is crucial for maintaining model performance, and Pangu Light excels in this regard.", "key_contributions": ["Introduction of Pangu Light framework for LLM acceleration.", "Novel weight re-initialization techniques that mitigate performance drops due to aggressive pruning.", "Demonstrated superior performance compared to existing models and methods."], "limitations": "", "keywords": ["Large Language Models", "Pruning", "Weight re-initialization", "Model efficiency", "Health informatics"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20163", "pdf": "https://arxiv.org/pdf/2505.20163.pdf", "abs": "https://arxiv.org/abs/2505.20163", "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition", "authors": ["Moreno La Quatra", "Alkis Koudounas", "Valerio Mario Salerno", "Sabato Marco Siniscalchi"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Despite the remarkable progress in end-to-end Automatic Speech Recognition\n(ASR) engines, accurately transcribing dysarthric speech remains a major\nchallenge. In this work, we proposed a two-stage framework for the Speech\nAccessibility Project Challenge at INTERSPEECH 2025, which combines\ncutting-edge speech recognition models with LLM-based generative error\ncorrection (GER). We assess different configurations of model scales and\ntraining strategies, incorporating specific hypothesis selection to improve\ntranscription accuracy. Experiments on the Speech Accessibility Project dataset\ndemonstrate the strength of our approach on structured and spontaneous speech,\nwhile highlighting challenges in single-word recognition. Through comprehensive\nanalysis, we provide insights into the complementary roles of acoustic and\nlinguistic modeling in dysarthric speech recognition", "AI": {"tldr": "The paper proposes a two-stage framework to improve transcription accuracy of dysarthric speech using advanced speech recognition models and LLM-based generative error correction.", "motivation": "To address the significant challenges of accurately transcribing dysarthric speech with state-of-the-art ASR technologies.", "method": "A two-stage framework combining speech recognition models and LLM-based generative error correction, with various configurations tested.", "result": "Experiments show the effectiveness of the framework on structured and spontaneous dysarthric speech, while revealing difficulties in single-word recognition.", "conclusion": "The analysis provides insights into how acoustic and linguistic modeling complement each other for better recognition of dysarthric speech.", "key_contributions": ["Proposed a novel two-stage framework for dysarthric speech transcription.", "Incorporated generative error correction using LLMs to improve accuracy.", "Provided insights into model configurations and their impact on transcription accuracy."], "limitations": "Challenges remain in accurately recognizing single words within dysarthric speech.", "keywords": ["Dysarthric speech", "Automatic Speech Recognition", "Generative error correction", "Speech Accessibility Project", "Large Language Models"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.20164", "pdf": "https://arxiv.org/pdf/2505.20164.pdf", "abs": "https://arxiv.org/abs/2505.20164", "title": "Visual Abstract Thinking Empowers Multimodal Reasoning", "authors": ["Dairu Liu", "Ziyue Wang", "Minyuan Ruan", "Fuwen Luo", "Chi Chen", "Peng Li", "Yang Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Images usually convey richer detail than text, but often include redundant\ninformation which potentially downgrades multimodal reasoning performance. When\nfaced with lengthy or complex messages, humans tend to employ abstract thinking\nto convert them into simple and concise abstracts. Inspired by this cognitive\nstrategy, we introduce Visual Abstract Thinking (VAT), a novel thinking\nparadigm that prompts Multimodal Large Language Models (MLLMs) with visual\nabstract instead of explicit verbal thoughts or elaborate guidance, permitting\na more concentrated visual reasoning mechanism. Explicit thinking, such as\nChain-of-thought (CoT) or tool-augmented approaches, increases the complexity\nof reasoning process via inserting verbose intermediate steps, external\nknowledge or visual information. In contrast, VAT reduces redundant visual\ninformation and encourages models to focus their reasoning on more essential\nvisual elements. Experimental results show that VAT consistently empowers\ndifferent models, and achieves an average gain of 17% over GPT-4o baseline by\nemploying diverse types of visual abstracts, demonstrating that VAT can enhance\nvisual reasoning abilities for MLLMs regarding conceptual, structural and\nrelational reasoning tasks. VAT is also compatible with CoT in\nknowledge-intensive multimodal reasoning tasks. These findings highlight the\neffectiveness of visual reasoning via abstract thinking and encourage further\nexploration of more diverse reasoning paradigms from the perspective of human\ncognition.", "AI": {"tldr": "The paper introduces Visual Abstract Thinking (VAT), a new paradigm for enhancing multimodal reasoning in MLLMs by reducing redundant visual information and focusing on essential elements, resulting in improved performance.", "motivation": "To improve the performance of multimodal reasoning by addressing redundancy in visual information and leveraging abstract thinking, analogous to human cognitive strategies.", "method": "The authors propose Visual Abstract Thinking (VAT) as a thinking paradigm for multimodal large language models, which prompts reasoning based on concise visual abstracts rather than verbose verbal thoughts.", "result": "Experiments show that VAT leads to an average performance gain of 17% over the GPT-4o baseline across various reasoning tasks, thus enhancing models' visual reasoning abilities in conceptual, structural, and relational tasks.", "conclusion": "VAT is demonstrated to improve visual reasoning in MLLMs and is found compatible with chain-of-thought methodologies in complex reasoning tasks, urging further investigation into diverse reasoning paradigms inspired by human cognition.", "key_contributions": ["Introduction of Visual Abstract Thinking (VAT) as a novel reasoning paradigm for MLLMs.", "Demonstrated consistent performance improvements in visual reasoning tasks using VAT.", "Compatibility of VAT with existing methodologies like chain-of-thought in knowledge-intensive tasks."], "limitations": "", "keywords": ["Visual Abstract Thinking", "Multimodal Large Language Models", "Visual Reasoning", "Human Cognition", "Cognitive Strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20176", "pdf": "https://arxiv.org/pdf/2505.20176.pdf", "abs": "https://arxiv.org/abs/2505.20176", "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Eliana Pastor", "Sabato Marco Siniscalchi", "Elena Baralis"], "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms.", "AI": {"tldr": "This paper investigates Kolmogorov-Arnold Networks (KANs) for Spoken Language Understanding (SLU) tasks, demonstrating their effectiveness in speech processing.", "motivation": "The application of KANs to speech processing is underexplored despite their potential as alternatives to traditional neural architectures.", "method": "We experimented with 2D-CNN models integrating KAN layers in five configurations within the dense block and evaluated their performance against transformer-based models on five SLU datasets of varying complexity.", "result": "The best-performing configuration placed a KAN layer between two linear layers, achieving comparable or superior performance compared to traditional linear layers in most evaluations.", "conclusion": "KAN layers can effectively substitute linear layers in transformer architectures for SLU tasks, offering new insights into input region attentiveness in waveforms.", "key_contributions": ["First investigation of KANs in Spoken Language Understanding", "Found that KAN layers can replace linear layers in transformers", "Provided insights into differing attentiveness of KAN and linear layers on raw waveforms."], "limitations": "", "keywords": ["Kolmogorov-Arnold Networks", "Spoken Language Understanding", "Deep Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.20184", "pdf": "https://arxiv.org/pdf/2505.20184.pdf", "abs": "https://arxiv.org/abs/2505.20184", "title": "THiNK: Can Large Language Models Think-aloud?", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository.", "AI": {"tldr": "The paper introduces THiNK, a framework for evaluating higher-order thinking skills in LLMs, demonstrating that structured feedback improves their reasoning performance.", "motivation": "There is a critical need to assess higher-order thinking in LLMs beyond surface-level accuracy to enhance their reasoning capabilities.", "method": "THiNK employs a multi-agent, feedback-driven approach based on Bloom's Taxonomy, framing reasoning assessment as an iterative task of problem generation, critique, and revision.", "result": "The study reveals LLMs perform well in lower-order tasks but struggle with higher-order thinking, though structured feedback significantly enhances their performance in this area.", "conclusion": "THiNK provides a scalable methodology for evaluating and improving LLM reasoning, with qualitative evaluations illustrating better alignment with domain logic when using the framework.", "key_contributions": ["Introduction of the THiNK framework for evaluating LLM reasoning", "Demonstration of the importance of structured feedback in improving higher-order thinking skills", "Provision of code and methodology for broader use in evaluating LLM performance"], "limitations": "The framework's effectiveness may vary with different model architectures and specific contexts not covered in this study.", "keywords": ["higher-order thinking", "LLM evaluation", "Bloom's Taxonomy", "feedback-driven", "cognitive analysis"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20195", "pdf": "https://arxiv.org/pdf/2505.20195.pdf", "abs": "https://arxiv.org/abs/2505.20195", "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning", "authors": ["Xiaorong Wang", "Ting Yang", "Zhu Zhang", "Shuo Wang", "Zihan Zhou", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach.", "AI": {"tldr": "Proposes a divide-and-conquer approach for evaluating long-form model-generated text, integrating human feedback and a novel active learning algorithm.", "motivation": "To improve the quality assessment of long-form generated text, which suffers from degradation due to input length.", "method": "A divide-and-conquer approach is used to break down evaluation tasks into localized and global assessments, combined with a hybrid in-context learning approach and an uncertainty-based active learning algorithm.", "result": "The framework outperforms several representative baselines in evaluating long-form generated text.", "conclusion": "Integration of human feedback into the evaluation process significantly enhances model performance in text assessment.", "key_contributions": ["Introduction of a localized scoring method for coherent evaluations of text segments.", "Development of a hybrid in-context learning approach leveraging human annotations.", "Implementation of an uncertainty-based active learning algorithm to optimize data annotation."], "limitations": "", "keywords": ["long-form text evaluation", "active learning", "human feedback", "LLM", "coherence"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20199", "pdf": "https://arxiv.org/pdf/2505.20199.pdf", "abs": "https://arxiv.org/abs/2505.20199", "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "authors": ["Pengxiang Li", "Shilin Yan", "Joey Tsai", "Renrui Zhang", "Ruichuan An", "Ziyu Guo", "Xiaowei Gao"], "categories": ["cs.CL"], "comment": "Project page: https://github.com/pixeli99/A-CFG", "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.", "AI": {"tldr": "This paper presents Adaptive Classifier-Free Guidance (A-CFG), a method that enhances generative models by dynamically adjusting unconditional inputs based on model confidence, leading to improved language generation.", "motivation": "Standard Classifier-Free Guidance (CFG) uses a static unconditional input, which is not always optimal during iterative generation when model uncertainty changes.", "method": "A-CFG modifies unconditional input by re-masking low-confidence tokens at each generation step, providing dynamic guidance that enhances the effectiveness of CFG during language generation.", "result": "A-CFG shows significant improvements in language generation tasks, achieving a 3.9 point gain on the GPQA benchmark compared to standard CFG.", "conclusion": "The paper demonstrates the advantages of adapting guidance mechanisms in generative models to align with the model's immediate uncertainty levels, resulting in better performance.", "key_contributions": ["Introduction of A-CFG for dynamic adjustment of guidance", "Integration with a state-of-the-art masked diffusion language model", "Empirical validation through substantial benchmark improvements"], "limitations": "", "keywords": ["Adaptive Classifier-Free Guidance", "Language Generation", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20201", "pdf": "https://arxiv.org/pdf/2505.20201.pdf", "abs": "https://arxiv.org/abs/2505.20201", "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Harneet Singh Khanuja", "Yiqiao Jin", "Munmun De Choudhury"], "categories": ["cs.CL"], "comment": "33 pages, 5 figures, 30 tables", "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations.", "AI": {"tldr": "Introduction of MedAgent framework and MHSD dataset to evaluate LLMs in mental health dialogue.", "motivation": "To address limited access to mental healthcare and the need for effective communication between LLMs and patients.", "method": "Development of MedAgent for generating synthetic multi-turn conversations and MultiSenseEval for evaluation based on human-centric criteria.", "result": "Created the MHSD dataset with 2,200 patient-LLM conversations; findings show that current models perform poorly in patient-centric communication.", "conclusion": "The work provides tools for generating and evaluating LLMs in mental health conversations, highlighting performance gaps in current models.", "key_contributions": ["Introduction of MedAgent for generating realistic mental health conversations.", "Creation of the MHSD dataset with 2,200 conversations.", "Development of MultiSenseEval for evaluating LLMs using human-centric criteria."], "limitations": "Current evaluation frameworks may overlook alignment with patient-specific goals and variance in patient personas.", "keywords": ["mental health", "large language models", "synthetic data generation", "multi-turn conversations", "evaluation framework"], "importance_score": 9, "read_time_minutes": 33}}
{"id": "2505.20209", "pdf": "https://arxiv.org/pdf/2505.20209.pdf", "abs": "https://arxiv.org/abs/2505.20209", "title": "How to Improve the Robustness of Closed-Source Models on NLI", "authors": ["Joe Stacey", "Lisa Alazraki", "Aran Ubhi", "Beyza Ermis", "Aaron Mueller", "Marek Rei"], "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Closed-source Large Language Models (LLMs) have become increasingly popular,\nwith impressive performance across a wide range of natural language tasks.\nThese models can be fine-tuned to further improve performance, but this often\nresults in the models learning from dataset-specific heuristics that reduce\ntheir robustness on out-of-distribution (OOD) data. Existing methods to improve\nrobustness either perform poorly, or are non-applicable to closed-source models\nbecause they assume access to model internals, or the ability to change the\nmodel's training procedure. In this work, we investigate strategies to improve\nthe robustness of closed-source LLMs through data-centric methods that do not\nrequire access to model internals. We find that the optimal strategy depends on\nthe complexity of the OOD data. For highly complex OOD datasets, upsampling\nmore challenging training examples can improve robustness by up to 1.5%. For\nless complex OOD datasets, replacing a portion of the training set with\nLLM-generated examples can improve robustness by 3.7%. More broadly, we find\nthat large-scale closed-source autoregressive LLMs are substantially more\nrobust than commonly used encoder models, and are a more appropriate choice of\nbaseline going forward.", "AI": {"tldr": "The paper evaluates methods to enhance the robustness of closed-source Large Language Models (LLMs) against out-of-distribution data using data-centric strategies, revealing that the effectiveness varies with the complexity of the OOD datasets.", "motivation": "Closed-source LLMs have achieved remarkable performance, but their robustness to out-of-distribution data is often compromised due to specific training heuristics.", "method": "The study investigates data-centric methods that do not require access to model internals, analyzing upsampling of training examples and substitution with LLM-generated samples based on the OOD complexity.", "result": "For complex OOD datasets, upsampling challenging examples improves robustness by up to 1.5%. For less complex datasets, replacing some training data with LLM-generated examples increases robustness by 3.7%.", "conclusion": "Large closed-source autoregressive LLMs exhibit greater robustness compared to traditional encoder models, suggesting they should be the benchmark for future research.", "key_contributions": ["Identified effective strategies to improve robustness of closed-source LLMs without requiring model access.", "Demonstrated varying effectiveness of data-centric methods based on OOD dataset complexity.", "Highlighted the superior robustness of autoregressive LLMs compared to encoder models."], "limitations": "", "keywords": ["Large Language Models", "Robustness", "Out-of-Distribution", "Data-Centric Methods", "Closed-source Models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20215", "pdf": "https://arxiv.org/pdf/2505.20215.pdf", "abs": "https://arxiv.org/abs/2505.20215", "title": "Dependency Parsing is More Parameter-Efficient with Normalization", "authors": ["Paolo Gajo", "Domenic Rosati", "Hassan Sajjad", "Alberto Barrón-Cedeño"], "categories": ["cs.CL"], "comment": null, "summary": "Dependency parsing is the task of inferring natural language structure, often\napproached by modeling word interactions via attention through biaffine\nscoring. This mechanism works like self-attention in Transformers, where scores\nare calculated for every pair of words in a sentence. However, unlike\nTransformer attention, biaffine scoring does not use normalization prior to\ntaking the softmax of the scores. In this paper, we provide theoretical\nevidence and empirical results revealing that a lack of normalization\nnecessarily results in overparameterized parser models, where the extra\nparameters compensate for the sharp softmax outputs produced by high variance\ninputs to the biaffine scoring function. We argue that biaffine scoring can be\nmade substantially more efficient by performing score normalization. We conduct\nexperiments on six datasets for semantic and syntactic dependency parsing using\na one-hop parser. We train N-layer stacked BiLSTMs and evaluate the parser's\nperformance with and without normalizing biaffine scores. Normalizing allows us\nto beat the state of the art on two datasets, with fewer samples and trainable\nparameters. Code: https://anonymous.4open.science/r/EfficientSDP-70C1", "AI": {"tldr": "This paper addresses the efficiency of biaffine scoring in dependency parsing by introducing score normalization, demonstrating its effectiveness in improving parser models.", "motivation": "The study aims to explore and improve the biaffine scoring mechanism used in dependency parsing, particularly focusing on the lack of normalization that leads to overparameterized models.", "method": "Empirical experiments were conducted using N-layer stacked BiLSTMs and assessed the parser's performance on six datasets for semantic and syntactic dependency parsing with and without score normalization.", "result": "Normalizing biaffine scores resulted in improved performance, surpassing the state of the art on two datasets while requiring fewer samples and parameters.", "conclusion": "The findings suggest that incorporating score normalization can lead to more efficient dependency parsing models and improved accuracy.", "key_contributions": ["Theoretical evidence supporting the need for normalization in biaffine scoring.", "Empirical results demonstrating improved parser performance with normalization.", "Achievement of state-of-the-art results on multiple datasets with reduced parameters."], "limitations": "", "keywords": ["dependency parsing", "biaffine scoring", "score normalization", "BiLSTM", "natural language processing"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.20225", "pdf": "https://arxiv.org/pdf/2505.20225.pdf", "abs": "https://arxiv.org/abs/2505.20225", "title": "FLAME-MoE: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models", "authors": ["Hao Kang", "Zichun Yu", "Chenyan Xiong"], "categories": ["cs.CL", "cs.LG"], "comment": "All code, training logs, and model checkpoints are available at\n  https://github.com/cmu-flame/FLAME-MoE", "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.", "AI": {"tldr": "FLAME-MoE is an open-source research suite featuring efficient Mixture-of-Experts architectures for large language models, improving accuracy and enabling reproducible experimentation.", "motivation": "To provide an open, end-to-end Mixture-of-Experts platform for academic researchers to study various aspects of scaling and performance in large language models.", "method": "The research suite includes seven decoder-only models with varying sizes, using a MoE architecture of 64 experts with top-8 gating and shared experts for efficient token processing.", "result": "FLAME-MoE improves average accuracy by up to 3.4 points over dense baselines, demonstrating effective expert specialization and routing stability during training.", "conclusion": "The open-source release of FLAME-MoE facilitates reproducible research and exploration of MoE dynamics in modern large language models.", "key_contributions": ["Release of FLAME-MoE open-source suite", "Demonstration of improved accuracy over dense models", "Insights into expert specialization and routing behavior"], "limitations": "", "keywords": ["Mixture-of-Experts", "Large Language Models", "Open-Source Research"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.20231", "pdf": "https://arxiv.org/pdf/2505.20231.pdf", "abs": "https://arxiv.org/abs/2505.20231", "title": "Bridging the Long-Term Gap: A Memory-Active Policy for Multi-Session Task-Oriented Dialogue", "authors": ["Yiming Du", "Bingbing Wang", "Yang He", "Bin Liang", "Baojun Wang", "Zhongyang Li", "Lin Gui", "Jeff Z. Pan", "Ruifeng Xu", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "Existing Task-Oriented Dialogue (TOD) systems primarily focus on\nsingle-session dialogues, limiting their effectiveness in long-term memory\naugmentation. To address this challenge, we introduce a MS-TOD dataset, the\nfirst multi-session TOD dataset designed to retain long-term memory across\nsessions, enabling fewer turns and more efficient task completion. This defines\na new benchmark task for evaluating long-term memory in multi-session TOD.\nBased on this new dataset, we propose a Memory-Active Policy (MAP) that\nimproves multi-session dialogue efficiency through a two-stage approach. 1)\nMemory-Guided Dialogue Planning retrieves intent-aligned history, identifies\nkey QA units via a memory judger, refines them by removing redundant questions,\nand generates responses based on the reconstructed memory. 2) Proactive\nResponse Strategy detects and correct errors or omissions, ensuring efficient\nand accurate task completion. We evaluate MAP on MS-TOD dataset, focusing on\nresponse quality and effectiveness of the proactive strategy. Experiments on\nMS-TOD demonstrate that MAP significantly improves task success and turn\nefficiency in multi-session scenarios, while maintaining competitive\nperformance on conventional single-session tasks.", "AI": {"tldr": "Introduces a multi-session task-oriented dialogue dataset and a Memory-Active Policy to enhance dialogue efficiency and task completion.", "motivation": "Existing task-oriented dialogue systems struggle with multi-session dialogues and long-term memory retention, reducing efficiency in task completion.", "method": "The paper proposes a new MS-TOD dataset specifically for multi-session dialogues and introduces a Memory-Active Policy (MAP) that includes a Memory-Guided Dialogue Planning and a Proactive Response Strategy.", "result": "The experiments show that MAP improves task success rates and dialogue turn efficiency in multi-session scenarios while performing competitively in single-session tasks.", "conclusion": "MAP represents a significant advancement in task-oriented dialogues by efficiently handling multi-session conversations, which is essential for applications requiring long-term interactions.", "key_contributions": ["Introduction of the first multi-session TOD dataset (MS-TOD) for evaluating long-term memory", "Development of the Memory-Active Policy (MAP) for enhanced dialogue efficiency", "Demonstration of improved task success and efficiency in multi-session dialogues."], "limitations": "", "keywords": ["Task-Oriented Dialogue", "Multi-session", "Long-term memory", "Dialogue efficiency", "Memory-Active Policy"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.20237", "pdf": "https://arxiv.org/pdf/2505.20237.pdf", "abs": "https://arxiv.org/abs/2505.20237", "title": "Efficient Speech Translation through Model Compression and Knowledge Distillation", "authors": ["Yasmin Moslem"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "IWSLT 2025", "summary": "Efficient deployment of large audio-language models for speech translation\nremains challenging due to their significant computational requirements. In\nthis paper, we address this challenge through our system submissions to the\n\"Model Compression\" track at the International Conference on Spoken Language\nTranslation (IWSLT 2025). We experiment with a combination of approaches\nincluding iterative layer pruning based on layer importance evaluation,\nlow-rank adaptation with 4-bit quantization (QLoRA), and knowledge\ndistillation. In our experiments, we use Qwen2-Audio-7B-Instruct for speech\ntranslation into German and Chinese. Our pruned (student) models achieve up to\na 50% reduction in both model parameters and storage footprint, while retaining\n97-100% of the translation quality of the in-domain (teacher) models.", "AI": {"tldr": "This paper discusses methods for efficiently deploying large audio-language models for speech translation, specifically at the IWSLT 2025 conference.", "motivation": "The paper aims to address the significant computational requirements of deploying large audio-language models for effective speech translation.", "method": "The authors use a combination of iterative layer pruning based on layer importance evaluation, low-rank adaptation with 4-bit quantization (QLoRA), and knowledge distillation.", "result": "The pruned models achieve up to a 50% reduction in both model parameters and storage footprint, while retaining 97-100% of the translation quality of the original models.", "conclusion": "The methods explored can significantly improve the efficiency of large audio-language models without compromising on translation quality.", "key_contributions": ["Introduction of layer pruning based on importance evaluation", "Application of 4-bit quantization (QLoRA)", "Demonstration of knowledge distillation in speech translation models"], "limitations": "", "keywords": ["speech translation", "model compression", "audio-language models", "quantization", "knowledge distillation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20243", "pdf": "https://arxiv.org/pdf/2505.20243.pdf", "abs": "https://arxiv.org/abs/2505.20243", "title": "It's High Time: A Survey of Temporal Information Retrieval and Question Answering", "authors": ["Bhawna Piryani", "Abdelrahman Abdullah", "Jamshid Mozafari", "Avishek Anand", "Adam Jatowt"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nInformation Retrieval and Temporal Question Answering, two research areas aimed\nat handling and understanding time-sensitive information. As the amount of\ntime-stamped content from sources like news articles, web archives, and\nknowledge bases increases, systems must address challenges such as detecting\ntemporal intent, normalizing time expressions, ordering events, and reasoning\nover evolving or ambiguous facts. These challenges are critical across many\ndynamic and time-sensitive domains, from news and encyclopedias to science,\nhistory, and social media. We review both traditional approaches and modern\nneural methods, including those that use transformer models and Large Language\nModels (LLMs). We also review recent advances in temporal language modeling,\nmulti-hop reasoning, and retrieval-augmented generation (RAG), alongside\nbenchmark datasets and evaluation strategies that test temporal robustness,\nrecency awareness, and generalization.", "AI": {"tldr": "This paper surveys Temporal Information Retrieval and Temporal Question Answering, addressing challenges in handling time-sensitive information.", "motivation": "The increasing amount of time-stamped content necessitates improved methods for understanding temporal information, crucial for various domains.", "method": "The paper reviews traditional and modern methods, including neural approaches and LLMs, focusing on challenges like temporal intent detection, time expression normalization, and event ordering.", "result": "It highlights advancements in temporal language modeling and retrieval-augmented generation, as well as evaluation strategies for assessing temporal robustness and recency awareness.", "conclusion": "The survey presents a comprehensive understanding of the current landscape in temporal information handling and suggests directions for future research.", "key_contributions": ["Comprehensive overview of Temporal Information Retrieval and Question Answering", "Discussion of traditional and modern approaches, including LLMs", "Identification of key challenges and evaluation strategies for temporal systems"], "limitations": "", "keywords": ["Temporal Information Retrieval", "Temporal Question Answering", "Large Language Models", "Temporal Language Modeling", "Event Ordering"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.20245", "pdf": "https://arxiv.org/pdf/2505.20245.pdf", "abs": "https://arxiv.org/abs/2505.20245", "title": "KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with Structured Knowledge Tracing", "authors": ["Rui Li", "Quanyu Dai", "Zeyu Zhang", "Xu Chen", "Zhenhua Dong", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Recent advances in retrieval-augmented generation (RAG) furnish large\nlanguage models (LLMs) with iterative retrievals of relevant information to\nhandle complex multi-hop questions. These methods typically alternate between\nLLM reasoning and retrieval to accumulate external information into the LLM's\ncontext. However, the ever-growing context inherently imposes an increasing\nburden on the LLM to perceive connections among critical information pieces,\nwith futile reasoning steps further exacerbating this overload issue. In this\npaper, we present KnowTrace, an elegant RAG framework to (1) mitigate the\ncontext overload and (2) bootstrap higher-quality multi-step reasoning. Instead\nof simply piling the retrieved contents, KnowTrace autonomously traces out\ndesired knowledge triplets to organize a specific knowledge graph relevant to\nthe input question. Such a structured workflow not only empowers the LLM with\nan intelligible context for inference, but also naturally inspires a reflective\nmechanism of knowledge backtracing to identify contributive LLM generations as\nprocess supervision data for self-bootstrapping. Extensive experiments show\nthat KnowTrace consistently surpasses existing methods across three multi-hop\nquestion answering benchmarks, and the bootstrapped version further amplifies\nthe gains.", "AI": {"tldr": "KnowTrace is a RAG framework that reduces context overload for multi-hop reasoning in LLMs by organizing retrieved information into knowledge graphs.", "motivation": "To tackle the challenge of context overload in LLMs during multi-hop reasoning tasks and improve the quality of reasoning processes.", "method": "KnowTrace organizes retrieved information into specific knowledge graphs, tracing desired knowledge triplets relevant to the input question to create an intelligible context for inference.", "result": "KnowTrace consistently outperforms existing multi-hop question answering methods across three benchmarks, with significant improvements observed in a bootstrapped version.", "conclusion": "The proposed method not only enhances reasoning capabilities but also introduces a mechanism for self-bootstrapping using process supervision data from LLM generations.", "key_contributions": ["Development of an effective RAG framework to alleviate context overload", "Introduction of knowledge backtracing to improve reasoning quality", "Demonstration of superior performance across multiple benchmarks"], "limitations": "", "keywords": ["retrieval-augmented generation", "large language models", "multi-hop reasoning", "knowledge graphs", "self-bootstrapping"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20249", "pdf": "https://arxiv.org/pdf/2505.20249.pdf", "abs": "https://arxiv.org/abs/2505.20249", "title": "WXImpactBench: A Disruptive Weather Impact Understanding Benchmark for Evaluating Large Language Models", "authors": ["Yongan Yu", "Qingchen Hu", "Xianda Du", "Jiayin Wang", "Fengran Mo", "Renee Sieber"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Climate change adaptation requires the understanding of disruptive weather\nimpacts on society, where large language models (LLMs) might be applicable.\nHowever, their effectiveness is under-explored due to the difficulty of\nhigh-quality corpus collection and the lack of available benchmarks. The\nclimate-related events stored in regional newspapers record how communities\nadapted and recovered from disasters. However, the processing of the original\ncorpus is non-trivial. In this study, we first develop a disruptive weather\nimpact dataset with a four-stage well-crafted construction pipeline. Then, we\npropose WXImpactBench, the first benchmark for evaluating the capacity of LLMs\non disruptive weather impacts. The benchmark involves two evaluation tasks,\nmulti-label classification and ranking-based question answering. Extensive\nexperiments on evaluating a set of LLMs provide first-hand analysis of the\nchallenges in developing disruptive weather impact understanding and climate\nchange adaptation systems. The constructed dataset and the code for the\nevaluation framework are available to help society protect against\nvulnerabilities from disasters.", "AI": {"tldr": "This study develops a dataset on disruptive weather impacts and introduces WXImpactBench, a benchmark for evaluating large language models on this topic.", "motivation": "To enhance understanding of the societal impacts of climate change and evaluate the effectiveness of LLMs in this domain.", "method": "A four-stage construction pipeline was used to develop the disruptive weather impact dataset, followed by the creation of WXImpactBench with multi-label classification and ranking-based question answering tasks.", "result": "Extensive experiments were conducted with various LLMs to analyze the challenges of understanding disruptive weather impacts and adapting to climate change.", "conclusion": "The dataset and evaluation framework aim to assist in protecting communities against vulnerabilities from climate-related disasters.", "key_contributions": ["Development of a disruptive weather impact dataset", "Introduction of WXImpactBench for LLM evaluation", "First-hand analysis of LLM challenges in climate adaptation understanding"], "limitations": "", "keywords": ["climate change", "large language models", "disruptive weather", "benchmark", "dataset"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.20258", "pdf": "https://arxiv.org/pdf/2505.20258.pdf", "abs": "https://arxiv.org/abs/2505.20258", "title": "ARM: Adaptive Reasoning Model", "authors": ["Siye Wu", "Jian Xie", "Yikai Zhang", "Aili Chen", "Kai Zhang", "Yu Su", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": "Work in Progress", "summary": "While large reasoning models demonstrate strong performance on complex tasks,\nthey lack the ability to adjust reasoning token usage based on task difficulty.\nThis often leads to the \"overthinking\" problem -- excessive and unnecessary\nreasoning -- which, although potentially mitigated by human intervention to\ncontrol the token budget, still fundamentally contradicts the goal of achieving\nfully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a\nreasoning model capable of adaptively selecting appropriate reasoning formats\nbased on the task at hand. These formats include three efficient ones -- Direct\nAnswer, Short CoT, and Code -- as well as a more elaborate format, Long CoT. To\ntrain ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy\nOptimization (GRPO), which addresses the format collapse issue in traditional\nGRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by\nan average of 30%, and up to 70%, while maintaining performance comparable to\nthe model that relies solely on Long CoT. Furthermore, not only does it improve\ninference efficiency through reduced token generation, but it also brings a 2x\nspeedup in training. In addition to the default Adaptive Mode, ARM supports two\nadditional reasoning modes: 1) Instruction-Guided Mode, which allows users to\nexplicitly specify the reasoning format via special tokens -- ideal when the\nappropriate format is known for a batch of tasks. 2) Consensus-Guided Mode,\nwhich aggregates the outputs of the three efficient formats and resorts to Long\nCoT in case of disagreement, prioritizing performance with higher token usage.", "AI": {"tldr": "The paper presents the Adaptive Reasoning Model (ARM), which dynamically selects reasoning formats based on task difficulty to improve token efficiency and reduce reasoning excess.", "motivation": "To overcome the issue of excessive reasoning in large models and achieve more autonomous AI without human intervention.", "method": "ARM utilizes a method called Ada-GRPO, an adaptation of Group Relative Policy Optimization, to train the model while addressing the issue of format collapse.", "result": "ARM achieves a reduction in token usage by an average of 30%, up to 70%, while performing comparably to the traditional Long CoT format and improves training speed by 2x.", "conclusion": "ARM demonstrates a significant improvement in inference efficiency and training speed, providing flexible reasoning modes for various task requirements.", "key_contributions": ["Introduction of the Adaptive Reasoning Model (ARM) for dynamic reasoning format selection.", "Development of Ada-GRPO for efficient training and token usage.", "Demonstration of performance improvement without the need for excessive reasoning."], "limitations": "The work is still in progress and may have unresolved issues as indicated in the arXiv comment.", "keywords": ["Adaptive Reasoning", "Token Efficiency", "Machine Learning", "Human-Computer Interaction", "AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.20264", "pdf": "https://arxiv.org/pdf/2505.20264.pdf", "abs": "https://arxiv.org/abs/2505.20264", "title": "We Need to Measure Data Diversity in NLP -- Better and Broader", "authors": ["Dong Nguyen", "Esther Ploeger"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although diversity in NLP datasets has received growing attention, the\nquestion of how to measure it remains largely underexplored. This opinion paper\nexamines the conceptual and methodological challenges of measuring data\ndiversity and argues that interdisciplinary perspectives are essential for\ndeveloping more fine-grained and valid measures.", "AI": {"tldr": "This paper discusses the challenges and necessity of measuring diversity in NLP datasets, emphasizing interdisciplinary approaches.", "motivation": "The paper highlights the increasing attention towards diversity in NLP datasets and identifies gaps in measurement methodologies.", "method": "It outlines conceptual and methodological challenges in assessing data diversity, advocating for interdisciplinary approaches to enhance measurement validity.", "result": "The examination reveals significant hurdles in defining and quantifying diversity, suggesting that interdisciplinary collaboration could yield more effective measures.", "conclusion": "The authors conclude that addressing these challenges is crucial for improving the robustness of NLP datasets and their applications.", "key_contributions": ["Identifies fundamental challenges in measuring NLP dataset diversity", "Proposes the need for interdisciplinary perspectives in developing measurement strategies", "Highlights the importance of fine-grained and valid diversity measures."], "limitations": "The paper primarily discusses challenges and theoretical perspectives without proposing concrete measurement frameworks.", "keywords": ["NLP", "data diversity", "measurement challenges", "interdisciplinary approaches", "dataset validation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.20276", "pdf": "https://arxiv.org/pdf/2505.20276.pdf", "abs": "https://arxiv.org/abs/2505.20276", "title": "Does quantization affect models' performance on long-context tasks?", "authors": ["Anmol Mekala", "Anirudh Atmakuru", "Yixiao Song", "Marzena Karpinska", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages of content with 9 figures. 37 remaining pages of references\n  and supplementary with 17 figures. Under review as of May 26", "summary": "Large language models (LLMs) now support context windows exceeding 128K\ntokens, but this comes with significant memory requirements and high inference\nlatency. Quantization can mitigate these costs, but may degrade performance. In\nthis work, we present the first systematic evaluation of quantized LLMs on\ntasks with long-inputs (>64K tokens) and long-form outputs. Our evaluation\nspans 9.7K test examples, five quantization methods (FP8, GPTQ-int8, AWQ-int4,\nGPTQ-int4, BNB-nf4), and five models (Llama-3.1 8B and 70B; Qwen-2.5 7B, 32B,\nand 72B). We find that, on average, 8-bit quantization preserves accuracy\n(~0.8% drop), whereas 4-bit methods lead to substantial losses, especially for\ntasks involving long context inputs (drops of up to 59%). This degradation\ntends to worsen when the input is in a language other than English. Crucially,\nthe effects of quantization depend heavily on the quantization method, model,\nand task. For instance, while Qwen-2.5 72B remains robust under BNB-nf4,\nLlama-3.1 70B experiences a 32% performance drop on the same task. These\nfindings highlight the importance of a careful, task-specific evaluation before\ndeploying quantized LLMs, particularly in long-context scenarios and with\nlanguages other than English.", "AI": {"tldr": "This paper systematically evaluates the impact of quantization methods on large language models (LLMs) for tasks requiring long inputs and outputs.", "motivation": "To address the high memory requirements and inference latency of LLMs with large context windows, this work investigates the trade-offs of quantization techniques.", "method": "The study conducts a systematic evaluation using 9.7K test examples across five quantization methods and five language models to assess performance degradation.", "result": "8-bit quantization shows minimal accuracy loss (~0.8%), while 4-bit quantization methods result in significant performance drops, particularly in tasks with long contexts and non-English input.", "conclusion": "Careful and task-specific evaluation of quantized LLMs is crucial before deployment, especially in long-context tasks and for languages beyond English.", "key_contributions": ["First systematic evaluation of quantized LLMs on long-input tasks.", "Comparison across different quantization methods and models.", "Insights on performance degradation based on input language and context length."], "limitations": "Performance degradation varies by quantization method and specific tasks, necessitating individualized assessments for different applications.", "keywords": ["large language models", "quantization", "long-context inputs", "long-form outputs", "performance evaluation"], "importance_score": 7, "read_time_minutes": 9}}
{"id": "2505.20277", "pdf": "https://arxiv.org/pdf/2505.20277.pdf", "abs": "https://arxiv.org/abs/2505.20277", "title": "OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction", "authors": ["Haonan Zhang", "Run Luo", "Xiong Liu", "Yuchuan Wu", "Ting-En Lin", "Pengpeng Zeng", "Qiang Qu", "Feiteng Fang", "Min Yang", "Lianli Gao", "Jingkuan Song", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL", "cs.CV"], "comment": "14 pages, 6 figures", "summary": "Role-Playing Agents (RPAs), benefiting from large language models, is an\nemerging interactive AI system that simulates roles or characters with diverse\npersonalities. However, existing methods primarily focus on mimicking dialogues\namong roles in textual form, neglecting the role's voice traits (e.g., voice\nstyle and emotions) as playing a crucial effect in interaction, which tends to\nbe more immersive experiences in realistic scenarios. Towards this goal, we\npropose OmniCharacter, a first seamless speech-language personality interaction\nmodel to achieve immersive RPAs with low latency. Specifically, OmniCharacter\nenables agents to consistently exhibit role-specific personality traits and\nvocal traits throughout the interaction, enabling a mixture of speech and\nlanguage responses. To align the model with speech-language scenarios, we\nconstruct a dataset named OmniCharacter-10K, which involves more distinctive\ncharacters (20), richly contextualized multi-round dialogue (10K), and dynamic\nspeech response (135K). Experimental results showcase that our method yields\nbetter responses in terms of both content and style compared to existing RPAs\nand mainstream speech-language models, with a response latency as low as 289ms.\nCode and dataset are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/OmniCharacter.", "AI": {"tldr": "OmniCharacter introduces a model for immersive role-playing agents that combine speech and language with personality and vocal traits, overcoming limitations of current dialogue-only systems.", "motivation": "To enhance the immersive experience of role-playing agents by incorporating vocal traits alongside textual dialogue.", "method": "OmniCharacter is developed to exhibit consistent role-specific personality and vocal traits, achieving low latency interactions through a specially constructed dataset (OmniCharacter-10K) that includes diverse characters and multi-round dialogue.", "result": "Experimental results indicate that OmniCharacter provides superior responses in content and style compared to existing RPAs, with a response latency of as low as 289ms.", "conclusion": "OmniCharacter demonstrates significant improvements in both the quality of interactions and the speed of responses for role-playing agents.", "key_contributions": ["Development of OmniCharacter model for immersive RPAs", "Creation of the OmniCharacter-10K dataset", "Lower latency for responses compared to traditional models"], "limitations": "", "keywords": ["Role-Playing Agents", "Large Language Models", "Speech-Language Interaction"], "importance_score": 8, "read_time_minutes": 14}}
{"id": "2505.20282", "pdf": "https://arxiv.org/pdf/2505.20282.pdf", "abs": "https://arxiv.org/abs/2505.20282", "title": "One-shot Entropy Minimization", "authors": ["Zitian Gao", "Lynx Chen", "Joey Zhou", "Bryan Dai"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.", "AI": {"tldr": "Training large language models using entropy minimization with minimal data can achieve significant performance improvements.", "motivation": "To explore efficient training methods for large language models that rely less on extensive labeled datasets.", "method": "Trained 13,440 large language models using a single unlabeled data point and 10 optimization steps.", "result": "Achieved performance improvements comparable to traditional methods that use extensive data and carefully designed rewards in reinforcement learning.", "conclusion": "This work suggests a potential shift in how we approach post-training paradigms for large language models.", "key_contributions": ["Utilization of a single unlabeled sample for training large language models.", "Demonstration of entropy minimization effectiveness in model performance.", "Proposal of a simpler training paradigm that challenges existing reinforcement learning approaches."], "limitations": "Work in progress; results may need further validation and refinement.", "keywords": ["large language models", "entropy minimization", "unlabeled data"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.20285", "pdf": "https://arxiv.org/pdf/2505.20285.pdf", "abs": "https://arxiv.org/abs/2505.20285", "title": "MASKSEARCH: A Universal Pre-Training Framework to Enhance Agentic Search Capability", "authors": ["Weiqi Wu", "Xin Guan", "Shen Huang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jiuxin Cao", "Hai Zhao", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MASKSEARCH. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMASKSEARCH significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks.", "AI": {"tldr": "MASKSEARCH introduces a novel pre-training framework that enhances LLM search agents' performance in retrieving knowledge through a specialized module and addresses limitations in existing training methods.", "motivation": "To improve the agentic abilities of LLMs by enhancing their retrieval and reasoning capabilities through a new pre-training framework.", "method": "The proposed framework includes a Retrieval Augmented Mask Prediction (RAMP) task during pre-training and utilizes both Supervised Fine-tuning (SFT) and Reinforcement Learning (RL) for training, incorporating a hybrid reward system and curriculum learning.", "result": "The MASKSEARCH framework significantly improves the performance of LLM-based search agents on both in-domain and out-of-domain tasks, particularly in open-domain multi-hop question answering.", "conclusion": "Our experiments show that the MASKSEARCH framework provides a substantial enhancement in the universal search capabilities of retrieval-augmented LLMs.", "key_contributions": ["Introduction of the RAMP task for universal retrieval learning.", "Combination of agent-based and distillation-based methods for SFT.", "Innovative curriculum learning approach to tackle complex training instances."], "limitations": "", "keywords": ["Retrieval-Augmented Language Models", "Machine Learning", "Open-Domain Question Answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.20293", "pdf": "https://arxiv.org/pdf/2505.20293.pdf", "abs": "https://arxiv.org/abs/2505.20293", "title": "Enhancing the Comprehensibility of Text Explanations via Unsupervised Concept Discovery", "authors": ["Yifan Sun", "Danding Wang", "Qiang Sheng", "Juan Cao", "Jintao Li"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Concept-based explainable approaches have emerged as a promising method in\nexplainable AI because they can interpret models in a way that aligns with\nhuman reasoning. However, their adaption in the text domain remains limited.\nMost existing methods rely on predefined concept annotations and cannot\ndiscover unseen concepts, while other methods that extract concepts without\nsupervision often produce explanations that are not intuitively comprehensible\nto humans, potentially diminishing user trust. These methods fall short of\ndiscovering comprehensible concepts automatically. To address this issue, we\npropose \\textbf{ECO-Concept}, an intrinsically interpretable framework to\ndiscover comprehensible concepts with no concept annotations. ECO-Concept first\nutilizes an object-centric architecture to extract semantic concepts\nautomatically. Then the comprehensibility of the extracted concepts is\nevaluated by large language models. Finally, the evaluation result guides the\nsubsequent model fine-tuning to obtain more understandable explanations.\nExperiments show that our method achieves superior performance across diverse\ntasks. Further concept evaluations validate that the concepts learned by\nECO-Concept surpassed current counterparts in comprehensibility.", "AI": {"tldr": "ECO-Concept is a novel framework that automatically discovers comprehensible concepts for explainable AI in the text domain by utilizing large language models for evaluation and model fine-tuning.", "motivation": "Existing concept-based explainable methods are limited in the text domain, often relying on predefined annotations or producing unintuitive explanations that reduce user trust.", "method": "ECO-Concept employs an object-centric architecture to automatically extract semantic concepts, evaluates their comprehensibility using large language models, and fine-tunes the model based on evaluation results to enhance explanation clarity.", "result": "ECO-Concept outperforms existing methods across various tasks, demonstrating improved comprehensibility of the extracted concepts.", "conclusion": "The proposed framework addresses the gap in concept discovery and clarity in explainable AI, showing promise for better user understanding and trust.", "key_contributions": ["Introduces ECO-Concept for automatic concept discovery without annotations", "Utilizes large language models to evaluate and improve concept comprehensibility", "Demonstrates superior performance in explainability across diverse tasks"], "limitations": "", "keywords": ["explainable AI", "concept discovery", "large language models"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2505.20295", "pdf": "https://arxiv.org/pdf/2505.20295.pdf", "abs": "https://arxiv.org/abs/2505.20295", "title": "Self-reflective Uncertainties: Do LLMs Know Their Internal Answer Distribution?", "authors": ["Michael Kirchhof", "Luca Füger", "Adam Goliński", "Eeshan Gunesh Dhekane", "Arno Blaas", "Sinead Williamson"], "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "To reveal when a large language model (LLM) is uncertain about a response,\nuncertainty quantification commonly produces percentage numbers along with the\noutput. But is this all we can do? We argue that in the output space of LLMs,\nthe space of strings, exist strings expressive enough to summarize the\ndistribution over output strings the LLM deems possible. We lay a foundation\nfor this new avenue of uncertainty explication and present SelfReflect, a\ntheoretically-motivated metric to assess how faithfully a string summarizes an\nLLM's internal answer distribution. We show that SelfReflect is able to\ndiscriminate even subtle differences of candidate summary strings and that it\naligns with human judgement, outperforming alternative metrics such as LLM\njudges and embedding comparisons. With SelfReflect, we investigate a number of\nself-summarization methods and find that even state-of-the-art reasoning models\nstruggle to explicate their internal uncertainty. But we find that faithful\nsummarizations can be generated by sampling and summarizing. Our metric enables\nfuture works towards this universal form of LLM uncertainties.", "AI": {"tldr": "The paper introduces SelfReflect, a metric for assessing how well a language model's output summarizes its internal uncertainty distribution, showing its effectiveness compared to existing metrics.", "motivation": "To explore better methods of uncertainty quantification in large language models beyond simple percentage scores, allowing for a richer understanding of model confidence.", "method": "The paper presents SelfReflect as a metric designed to evaluate how faithfully a string represents the distribution of possible outputs from an LLM. It also investigates self-summarization methods using this metric.", "result": "SelfReflect effectively distinguishes subtle differences in summary strings and correlates well with human judgment, outperforming previous metrics.", "conclusion": "The study suggests that while state-of-the-art models struggle with uncertainty explication, faithful summaries can be achieved through specific sampling and summarization techniques, paving the way for future research in LLM uncertainties.", "key_contributions": ["Introduction of SelfReflect, a novel metric for uncertainty quantification in LLMs", "Demonstration of SelfReflect's alignment with human judgment", "Identification of effective methods for generating faithful summaries of LLM uncertainties"], "limitations": "The paper acknowledges that even advanced reasoning models find it difficult to explicate their internal uncertainty, indicating a challenge that still exists in the field.", "keywords": ["Large Language Models", "Uncertainty Quantification", "SelfReflect", "Self-Summarization", "Human Judgment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.20296", "pdf": "https://arxiv.org/pdf/2505.20296.pdf", "abs": "https://arxiv.org/abs/2505.20296", "title": "Reasoning LLMs are Wandering Solution Explorers", "authors": ["Jiahao Lu", "Ziwei Xu", "Mohan Kankanhalli"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM"], "comment": "71 pages, 14 figures, 2 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning abilities\nthrough test-time computation (TTC) techniques such as chain-of-thought\nprompting and tree-based reasoning. However, we argue that current reasoning\nLLMs (RLLMs) lack the ability to systematically explore the solution space.\nThis paper formalizes what constitutes systematic problem solving and\nidentifies common failure modes that reveal reasoning LLMs to be wanderers\nrather than systematic explorers. Through qualitative and quantitative analysis\nacross multiple state-of-the-art LLMs, we uncover persistent issues: invalid\nreasoning steps, redundant explorations, hallucinated or unfaithful\nconclusions, and so on. Our findings suggest that current models' performance\ncan appear to be competent on simple tasks yet degrade sharply as complexity\nincreases. Based on the findings, we advocate for new metrics and tools that\nevaluate not just final outputs but the structure of the reasoning process\nitself.", "AI": {"tldr": "This paper critiques the reasoning abilities of large language models (RLLMs), highlighting their inability to systematically explore solution spaces and proposing new metrics for evaluating reasoning processes.", "motivation": "To address the limitations in reasoning capabilities of large language models and foster a deeper evaluation of their systematic problem-solving abilities.", "method": "The paper employs qualitative and quantitative analyses across multiple state-of-the-art reasoning LLMs to identify and categorize common failure modes in their reasoning processes.", "result": "The analysis reveals persistent issues such as invalid reasoning steps, redundant explorations, and hallucinated conclusions, which contribute to a significant performance drop in complex tasks.", "conclusion": "The study calls for new evaluation metrics that focus on the reasoning process structure rather than just final outputs to improve the understanding and capabilities of reasoning LLMs.", "key_contributions": ["Formalization of systematic problem solving in reasoning LLMs", "Identification of common failure modes in reasoning processes", "Advocacy for new metrics to evaluate reasoning structures"], "limitations": "The study primarily focuses on identifying issues without providing comprehensive solutions to the uncovered limitations of RLLMs.", "keywords": ["Large Language Models", "Reasoning", "Systematic Problem Solving", "Evaluation Metrics", "Failure Modes"], "importance_score": 7, "read_time_minutes": 30}}
{"id": "2505.20298", "pdf": "https://arxiv.org/pdf/2505.20298.pdf", "abs": "https://arxiv.org/abs/2505.20298", "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding", "authors": ["Jeonghun Baek", "Kazuki Egashira", "Shota Onohara", "Atsuyuki Miyai", "Yuki Imajuku", "Hikaru Ikuta", "Kiyoharu Aizawa"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "20 pages, 11 figures", "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga.", "AI": {"tldr": "This paper introduces benchmarks and a specialized model for Japanese manga understanding using large multimodal models.", "motivation": "The goal is to enhance the storytelling capabilities of manga creators by enabling large multimodal models to understand manga narratives at a human-like level.", "method": "The authors introduce two benchmarks, MangaOCR for in-page text recognition and MangaVQA for visual question answering, as well as develop MangaLMM, a specialized model fine-tuned from Qwen2.5-VL for these tasks.", "result": "Extensive experiments and comparisons with existing models like GPT-4o and Gemini 2.5 demonstrate the effectiveness of MangaLMM in understanding manga narratives.", "conclusion": "The introduced benchmarks and the fine-tuned MangaLMM provide a significant foundation for advancing the evaluation and development of large multimodal models in the narrative-heavy domain of manga.", "key_contributions": ["Introduction of MangaOCR benchmark for text recognition", "Development of MangaVQA for evaluating contextual understanding", "Creation of MangaLMM model specialized for manga understanding"], "limitations": "", "keywords": ["manga", "multimodal models", "text recognition", "visual question answering", "narrative understanding"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2505.18464", "pdf": "https://arxiv.org/pdf/2505.18464.pdf", "abs": "https://arxiv.org/abs/2505.18464", "title": "From Reddit to Generative AI: Evaluating Large Language Models for Anxiety Support Fine-tuned on Social Media Data", "authors": ["Ugur Kursuncu", "Trilok Padhi", "Gaurav Sinha", "Abdulkadir Erol", "Jaya Krishna Mandivarapu", "Christopher R. Larrison"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "The growing demand for accessible mental health support, compounded by\nworkforce shortages and logistical barriers, has led to increased interest in\nutilizing Large Language Models (LLMs) for scalable and real-time assistance.\nHowever, their use in sensitive domains such as anxiety support remains\nunderexamined. This study presents a systematic evaluation of LLMs (GPT and\nLlama) for their potential utility in anxiety support by using real\nuser-generated posts from the r/Anxiety subreddit for both prompting and\nfine-tuning. Our approach utilizes a mixed-method evaluation framework\nincorporating three main categories of criteria: (i) linguistic quality, (ii)\nsafety and trustworthiness, and (iii) supportiveness. Results show that\nfine-tuning LLMs with naturalistic anxiety-related data enhanced linguistic\nquality but increased toxicity and bias, and diminished emotional\nresponsiveness. While LLMs exhibited limited empathy, GPT was evaluated as more\nsupportive overall. Our findings highlight the risks of fine-tuning LLMs on\nunprocessed social media content without mitigation strategies.", "AI": {"tldr": "This study evaluates the use of Large Language Models (LLMs) for anxiety support by fine-tuning on user posts from the r/Anxiety subreddit, examining linguistic quality, safety, and supportiveness.", "motivation": "The increasing demand for accessible mental health support alongside workforce shortages necessitates exploring scalable solutions like LLMs for real-time assistance in sensitive areas such as anxiety.", "method": "A systematic evaluation was conducted using a mixed-method framework focusing on linguistic quality, safety, and supportiveness, based on real user-generated posts from the r/Anxiety subreddit for both prompting and fine-tuning LLMs (GPT and Llama).", "result": "Fine-tuning LLMs with anxiety-related data improved linguistic quality but increased toxicity, bias, and reduced emotional responsiveness. GPT was found to be more supportive than Llama, despite both showing limited empathy.", "conclusion": "The findings stress the need for mitigation strategies when fine-tuning LLMs on social media content to avoid amplifying toxicity and bias while seeking to enhance support.", "key_contributions": ["Systematic evaluation of LLMs for anxiety support", "Demonstration of fine-tuning effects on linguistic quality and emotional responsiveness", "Highlighting risks associated with unprocessed social media content for training LLMs"], "limitations": "Increased toxicity and bias, diminished emotional responsiveness following fine-tuning, and overall limited empathy from LLMs.", "keywords": ["Mental Health", "Large Language Models", "Anxiety Support", "User-generated Content", "Fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2112.11479", "pdf": "https://arxiv.org/pdf/2112.11479.pdf", "abs": "https://arxiv.org/abs/2112.11479", "title": "AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection", "authors": ["Geet Shingi", "Vedangi Wagh"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advancements in technology have led to a boost in social media usage\nwhich has ultimately led to large amounts of user-generated data which also\nincludes hateful and offensive speech. The language used in social media is\noften a combination of English and the native language in the region. In India,\nHindi is used predominantly and is often code-switched with English, giving\nrise to the Hinglish (Hindi+English) language. Various approaches have been\nmade in the past to classify the code-mixed Hinglish hate speech using\ndifferent machine learning and deep learning-based techniques. However, these\ntechniques make use of recurrence on convolution mechanisms which are\ncomputationally expensive and have high memory requirements. Past techniques\nalso make use of complex data processing making the existing techniques very\ncomplex and non-sustainable to change in data. Proposed work gives a much\nsimpler approach which is not only at par with these complex networks but also\nexceeds performance with the use of subword tokenization algorithms like BPE\nand Unigram, along with multi-head attention-based techniques, giving an\naccuracy of 87.41% and an F1 score of 0.851 on standard datasets. Efficient use\nof BPE and Unigram algorithms help handle the nonconventional Hinglish\nvocabulary making the proposed technique simple, efficient and sustainable to\nuse in the real world.", "AI": {"tldr": "The paper proposes a simplified method for classifying Hinglish hate speech that outperforms complex deep learning techniques using subword tokenization and multi-head attention, achieving an accuracy of 87.41%.", "motivation": "To address the complexities and inefficiencies in classifying Hinglish hate speech on social media, using simpler approaches while maintaining or improving performance.", "method": "The proposed technique utilizes subword tokenization algorithms like BPE and Unigram combined with multi-head attention mechanisms to classify code-mixed Hinglish hate speech.", "result": "Achieved an accuracy of 87.41% and an F1 score of 0.851 on standard datasets, exceeding the performance of existing complex methods while being more efficient.", "conclusion": "The proposed method offers a simpler, efficient, and sustainable solution for hate speech classification in Hinglish, suitable for real-world applications.", "key_contributions": ["Simplified approach for hate speech classification", "Utilization of BPE and Unigram for handling Hinglish vocabulary", "Achieved competitive performance with lower computational costs"], "limitations": "", "keywords": ["Hinglish", "Hate Speech", "Subword Tokenization", "Machine Learning", "Multi-Head Attention"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2211.05414", "pdf": "https://arxiv.org/pdf/2211.05414.pdf", "abs": "https://arxiv.org/abs/2211.05414", "title": "ADEPT: A DEbiasing PrompT Framework", "authors": ["Ke Yang", "Charles Yu", "Yi Fung", "Manling Li", "Heng Ji"], "categories": ["cs.CL"], "comment": null, "summary": "Several works have proven that finetuning is an applicable approach for\ndebiasing contextualized word embeddings. Similarly, discrete prompts with\nsemantic meanings have shown to be effective in debiasing tasks. With unfixed\nmathematical representation at the token level, continuous prompts usually\nsurpass discrete ones at providing a pre-trained language model (PLM) with\nadditional task-specific information. Despite this, relatively few efforts have\nbeen made to debias PLMs by prompt tuning with continuous prompts compared to\nits discrete counterpart. Furthermore, for most debiasing methods that alter a\nPLM's original parameters, a major problem is the need to not only decrease the\nbias in the PLM but also to ensure that the PLM does not lose its\nrepresentation ability. Finetuning methods typically have a hard time\nmaintaining this balance, as they tend to violently remove meanings of\nattribute words. In this paper, we propose ADEPT, a method to debias PLMs using\nprompt tuning while maintaining the delicate balance between removing biases\nand ensuring representation ability. To achieve this, we propose a new training\ncriterion inspired by manifold learning and equip it with an explicit debiasing\nterm to optimize prompt tuning. In addition, we conduct several experiments\nwith regard to the reliability, quality, and quantity of a previously proposed\nattribute training corpus in order to obtain a clearer prototype of a certain\nattribute, which indicates the attribute's position and relative distances to\nother words on the manifold. We evaluate ADEPT on several widely acknowledged\ndebiasing benchmarks and downstream tasks, and find that it achieves\ncompetitive results while maintaining (and in some cases even improving) the\nPLM's representation ability. We further visualize words' correlation before\nand after debiasing a PLM, and give some possible explanations for the visible\neffects.", "AI": {"tldr": "This paper presents ADEPT, a method for debiasing pre-trained language models (PLMs) using continuous prompt tuning while preserving their representation abilities.", "motivation": "To address the limitations of existing debiasing methods that compromise the representation ability of PLMs when reducing bias.", "method": "The paper proposes a new training criterion inspired by manifold learning combined with an explicit debiasing term for effective prompt tuning.", "result": "ADEPT achieves competitive results on debiasing benchmarks while maintaining or improving the PLM's representation ability.", "conclusion": "The visualization of word correlations indicates the effectiveness of the proposed method in reducing bias without losing important semantic information.", "key_contributions": ["Introduction of ADEPT for debiasing PLMs via prompt tuning", "A unique training criterion based on manifold learning", "Empirical validation of ADEPT's effectiveness on standard benchmarks"], "limitations": "", "keywords": ["debiasing", "prompt tuning", "pre-trained language models", "manifold learning", "semantic representation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2309.12646", "pdf": "https://arxiv.org/pdf/2309.12646.pdf", "abs": "https://arxiv.org/abs/2309.12646", "title": "The More Similar, the Better? Associations between Latent Semantic Similarity and Emotional Experiences Differ across Conversation Contexts", "authors": ["Chen-Wei Yu", "Yun-Shiuan Chuang", "Alexandros N. Lotsos", "Tabea Meier", "Claudia M. Haase"], "categories": ["cs.CL"], "comment": null, "summary": "Latent semantic similarity (LSS) is a measure of the similarity of\ninformation exchanges in a conversation. Challenging the assumption that higher\nLSS bears more positive psychological meaning, we propose that this association\nmight depend on the type of conversation people have. On the one hand, the\nshare-mind perspective would predict that higher LSS should be associated with\nmore positive emotional experiences across the board. The broaden-and-build\ntheory, on the other hand, would predict that higher LSS should be inversely\nassociated with more positive emotional experiences specifically in pleasant\nconversations. Linear mixed modeling based on conversations among 50 long-term\nmarried couples supported the latter prediction. That is, partners experienced\ngreater positive emotions when their overall information exchanges were more\ndissimilar in pleasant (but not conflict) conversations. This work highlights\nthe importance of context in understanding the emotional correlates of LSS and\nexemplifies how modern natural language processing tools can be used to\nevaluate competing theory-driven hypotheses in social psychology.", "AI": {"tldr": "The study examines the relationship between latent semantic similarity (LSS) and emotional experiences in conversations, suggesting that context (type of conversation) significantly influences this association.", "motivation": "To challenge the assumption that higher LSS always correlates with positive emotional experiences by investigating how conversation type affects this relationship.", "method": "Linear mixed modeling was employed to analyze conversations among 50 long-term married couples to determine the impact of LSS on emotional experiences.", "result": "The findings indicated that partners reported more positive emotions when their information exchanges were more dissimilar in pleasant conversations, contrary to the expectations of the share-mind perspective.", "conclusion": "The study emphasizes the significance of context in the emotional implications of LSS and shows how NLP tools can test social psychology theories.", "key_contributions": ["Introduces a nuanced view of LSS's association with emotional experiences based on conversation type.", "Demonstrates the application of NLP tools in psychological research.", "Provides empirical evidence that more dissimilar communication can enhance positive emotional experiences in pleasant interactions."], "limitations": "Limited to the context of married couples; findings may not generalize to other types of relationships or conversations.", "keywords": ["latent semantic similarity", "emotional experiences", "conversational context", "natural language processing", "social psychology"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2310.13312", "pdf": "https://arxiv.org/pdf/2310.13312.pdf", "abs": "https://arxiv.org/abs/2310.13312", "title": "Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models", "authors": ["Jaeyoung Choe", "Keonwoong Noh", "Nayeon Kim", "Seyun Ahn", "Woohwan Jung"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2023 (Findings)", "summary": "Over the past few years, various domain-specific pretrained language models\n(PLMs) have been proposed and have outperformed general-domain PLMs in\nspecialized areas such as biomedical, scientific, and clinical domains. In\naddition, financial PLMs have been studied because of the high economic impact\nof financial data analysis. However, we found that financial PLMs were not\npretrained on sufficiently diverse financial data. This lack of diverse\ntraining data leads to a subpar generalization performance, resulting in\ngeneral-purpose PLMs, including BERT, often outperforming financial PLMs on\nmany downstream tasks. To address this issue, we collected a broad range of\nfinancial corpus and trained the Financial Language Model (FiLM) on these\ndiverse datasets. Our experimental results confirm that FiLM outperforms not\nonly existing financial PLMs but also general domain PLMs. Furthermore, we\nprovide empirical evidence that this improvement can be achieved even for\nunseen corpus groups.", "AI": {"tldr": "Financial Language Model (FiLM) outperforms existing financial and general domain pretrained language models by training on a diverse financial corpus.", "motivation": "To improve the generalization performance of financial pretrained language models by training them on a diverse range of financial data.", "method": "FiLM was trained on a broad range of financial corpus collected from diverse sources.", "result": "FiLM demonstrates better performance than both existing financial PLMs and general-purpose PLMs like BERT on various downstream tasks.", "conclusion": "Using a broader dataset for training significantly enhances the capabilities of financial PLMs, enabling better generalization even on unseen corpus groups.", "key_contributions": ["Introduction of FiLM as a diverse financial PLM", "Empirical evidence of performance improvement on unseen corpus", "Analysis of training data diversity impact on financial PLMs"], "limitations": "", "keywords": ["financial language model", "pretrained language models", "financial data analysis", "generalization performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2401.14624", "pdf": "https://arxiv.org/pdf/2401.14624.pdf", "abs": "https://arxiv.org/abs/2401.14624", "title": "Unearthing Large Scale Domain-Specific Knowledge from Public Corpora", "authors": ["Zhaoye Fei", "Yunfan Shao", "Linyang Li", "Zhiyuan Zeng", "Conghui He", "Qipeng Guo", "Hang Yan", "Dahua Lin", "Xipeng Qiu"], "categories": ["cs.CL"], "comment": "We have released the full data (total of 735GB) in\n  https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile and partial data\n  (about 40GB) in https://huggingface.co/datasets/Query-of-CC/knowledge_pile", "summary": "Large language models (LLMs) have demonstrated remarkable potential in\nvarious tasks, however, there remains a significant lack of open-source models\nand data for specific domains. Previous work has primarily focused on manually\nspecifying resources and collecting high-quality data for specific domains,\nwhich is extremely time-consuming and labor-intensive. To address this\nlimitation, we introduce large models into the data collection pipeline to\nguide the generation of domain-specific information and retrieve relevant data\nfrom Common Crawl (CC), a large public corpus. We refer to this approach as\nRetrieve-from-CC. It not only collects data related to domain-specific\nknowledge but also mines the data containing potential reasoning procedures\nfrom the public corpus. By applying this method, we have collected a knowledge\ndomain-related dataset named Retrieve-Pile, which covers four main domains,\nincluding the sciences, humanities, and other categories. Through the analysis\nof , Retrieve-from-CC can effectively retrieve relevant data from the covered\nknowledge domains and significantly improve the performance in tests of\nmathematical and knowledge-related reasoning abilities. We have released\nRetrieve-Pile at https://huggingface.co/datasets/Query-of-CC/Retrieve-Pile.", "AI": {"tldr": "This paper introduces Retrieve-from-CC, an approach for guiding the collection of domain-specific data using large language models, resulting in the creation of the Retrieve-Pile dataset.", "motivation": "To address the lack of open-source models and data for specific domains and reduce the labor-intensive process of manually collecting high-quality data.", "method": "Integrating large language models into the data collection pipeline to generate and retrieve relevant domain-specific information from the Common Crawl corpus.", "result": "Retrieve-from-CC effectively collects data and mines reasoning procedures, resulting in the Retrieve-Pile dataset, which enhances reasoning abilities in various knowledge domains.", "conclusion": "The approach improves performance in mathematical and knowledge-related reasoning tasks, with datasets released for public use.", "key_contributions": ["Introduction of the Retrieve-from-CC approach", "Creation of the Retrieve-Pile dataset covering multiple domains", "Demonstrated improvement in reasoning performance with the collected data"], "limitations": "", "keywords": ["large language models", "data collection", "domain-specific knowledge", "reasoning procedures", "Retrieve-Pile"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2402.13211", "pdf": "https://arxiv.org/pdf/2402.13211.pdf", "abs": "https://arxiv.org/abs/2402.13211", "title": "Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation", "authors": ["Dongjin Kang", "Sunghwan Kim", "Taeyoon Kwon", "Seungjun Moon", "Hyunsouk Cho", "Youngjae Yu", "Dongha Lee", "Jinyoung Yeo"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2024, Outstanding Paper", "summary": "Emotional Support Conversation (ESC) is a task aimed at alleviating\nindividuals' emotional distress through daily conversation. Given its inherent\ncomplexity and non-intuitive nature, ESConv dataset incorporates support\nstrategies to facilitate the generation of appropriate responses. Recently,\ndespite the remarkable conversational ability of large language models (LLMs),\nprevious studies have suggested that they often struggle with providing useful\nemotional support. Hence, this work initially analyzes the results of LLMs on\nESConv, revealing challenges in selecting the correct strategy and a notable\npreference for a specific strategy. Motivated by these, we explore the impact\nof the inherent preference in LLMs on providing emotional support, and\nconsequently, we observe that exhibiting high preference for specific\nstrategies hinders effective emotional support, aggravating its robustness in\npredicting the appropriate strategy. Moreover, we conduct a methodological\nstudy to offer insights into the necessary approaches for LLMs to serve as\nproficient emotional supporters. Our findings emphasize that (1) low preference\nfor specific strategies hinders the progress of emotional support, (2) external\nassistance helps reduce preference bias, and (3) existing LLMs alone cannot\nbecome good emotional supporters. These insights suggest promising avenues for\nfuture research to enhance the emotional intelligence of LLMs.", "AI": {"tldr": "This paper analyzes the performance of large language models in providing emotional support during conversations and examines the impact of strategy preferences on effectiveness.", "motivation": "Investigate why large language models struggle in offering effective emotional support despite their conversational capabilities.", "method": "Analysis of the ESConv dataset and the performance of LLMs in selecting appropriate emotional support strategies.", "result": "LLMs show preference for specific strategies that negatively affect their ability to provide emotional support.", "conclusion": "To improve emotional support capabilities, LLMs need to reduce strategy preference bias and may require external assistance; they cannot independently become proficient emotional supporters.", "key_contributions": ["Analysis of LLM performance on the ESConv dataset", "Identification of challenges in emotional support strategy selection", "Recommendations for enhancing emotional intelligence in LLMs"], "limitations": "Existing LLMs lack the necessary emotional intelligence without modification and external assistance.", "keywords": ["Emotional Support", "Large Language Models", "Human-Computer Interaction", "Chatbots", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2402.13405", "pdf": "https://arxiv.org/pdf/2402.13405.pdf", "abs": "https://arxiv.org/abs/2402.13405", "title": "A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion", "authors": ["Yanzhen Shen", "Yu Zhang", "Yunyi Zhang", "Jiawei Han"], "categories": ["cs.CL"], "comment": null, "summary": "Entity set expansion, taxonomy expansion, and seed-guided taxonomy\nconstruction are three representative tasks that can be applied to\nautomatically populate an existing taxonomy with emerging concepts. Previous\nstudies view them as three separate tasks. Therefore, their proposed techniques\nusually work for one specific task only, lacking generalizability and a\nholistic perspective. In this paper, we aim at a unified solution to the three\ntasks. To be specific, we identify two common skills needed for entity set\nexpansion, taxonomy expansion, and seed-guided taxonomy construction: finding\n\"siblings\" and finding \"parents\". We propose a taxonomy-guided instruction\ntuning framework to teach a large language model to generate siblings and\nparents for query entities, where the joint pre-training process facilitates\nthe mutual enhancement of the two skills. Extensive experiments on multiple\nbenchmark datasets demonstrate the efficacy of our proposed TaxoInstruct\nframework, which outperforms task-specific baselines across all three tasks.", "AI": {"tldr": "This paper presents a unified approach for entity set expansion, taxonomy expansion, and seed-guided taxonomy construction using a taxonomy-guided instruction tuning framework for large language models.", "motivation": "To address the lack of generalizability in existing techniques that treat entity set expansion, taxonomy expansion, and seed-guided taxonomy construction as separate tasks.", "method": "The authors propose a taxonomy-guided instruction tuning framework that trains a large language model to jointly learn the skills of generating 'siblings' and 'parents' for query entities, enhancing the performance across the three tasks.", "result": "Experimental results on benchmark datasets show that the proposed TaxoInstruct framework outperforms traditional task-specific baselines in all three tasks.", "conclusion": "The study demonstrates that a unified framework can effectively improve performance in related tasks by leveraging common skills in a joint learning process.", "key_contributions": ["Unified framework addressing three taxonomy-related tasks", "Taxonomy-guided instruction tuning for large language models", "Experimental validation showing superior performance compared to task-specific methods"], "limitations": "", "keywords": ["entity set expansion", "taxonomy expansion", "large language model", "instruction tuning", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2402.13606", "pdf": "https://arxiv.org/pdf/2402.13606.pdf", "abs": "https://arxiv.org/abs/2402.13606", "title": "MlingConf: A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models", "authors": ["Boyang Xue", "Hongru Wang", "Rui Wang", "Sheng Wang", "Zezhong Wang", "Yiming Du", "Bin Liang", "Wenxuan Zhang", "Kam-Fai Wong"], "categories": ["cs.CL"], "comment": "Accepted in ACL2025 Findings", "summary": "The tendency of Large Language Models (LLMs) to generate hallucinations\nraises concerns regarding their reliability. Therefore, confidence estimations\nindicating the extent of trustworthiness of the generations become essential.\nHowever, current LLM confidence estimations in languages other than English\nremain underexplored. This paper addresses this gap by introducing a\ncomprehensive investigation of Multilingual Confidence estimation (MlingConf)\non LLMs, focusing on both language-agnostic (LA) and language-specific (LS)\ntasks to explore the performance and language dominance effects of multilingual\nconfidence estimations on different tasks. The benchmark comprises four\nmeticulously checked and human-evaluated high-quality multilingual datasets for\nLA tasks and one for the LS task tailored to specific social, cultural, and\ngeographical contexts of a language. Our experiments reveal that on LA tasks\nEnglish exhibits notable linguistic dominance in confidence estimations than\nother languages, while on LS tasks, using question-related language to prompt\nLLMs demonstrates better linguistic dominance in multilingual confidence\nestimations. The phenomena inspire a simple yet effective native-tone prompting\nstrategy by employing language-specific prompts for LS tasks, effectively\nimproving LLMs' reliability and accuracy in LS scenarios.", "AI": {"tldr": "This paper investigates multilingual confidence estimation in Large Language Models (LLMs), revealing linguistic dominance effects and proposing a native-tone prompting strategy to improve reliability in language-specific tasks.", "motivation": "To address the gap in understanding confidence estimations of LLMs in languages other than English and improve their reliability.", "method": "The paper introduces MlingConf, a benchmark comprising multilingual datasets for language-agnostic and language-specific tasks, and conducts experiments to analyze performance and linguistic dominance.", "result": "Experiments show that English has linguistic dominance in confidence estimations for language-agnostic tasks, while language-specific prompts improve performance in language-specific tasks.", "conclusion": "A native-tone prompting strategy utilizing language-specific prompts enhances the reliability and accuracy of LLMs in language-specific scenarios.", "key_contributions": ["Introduction of multilingual confidence estimation framework (MlingConf)", "Benchmark with high-quality multilingual datasets", "Proposed native-tone prompting strategy for improved reliability in LS tasks"], "limitations": "Focus on specific contexts; may not generalize universally across all languages and tasks.", "keywords": ["Large Language Models", "confidence estimation", "multilingual", "language-agnostic", "language-specific"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2402.15481", "pdf": "https://arxiv.org/pdf/2402.15481.pdf", "abs": "https://arxiv.org/abs/2402.15481", "title": "Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency", "authors": ["Yiran Liu", "Ke Yang", "Zehan Qi", "Xiao Liu", "Yang Yu", "ChengXiang Zhai"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "We present a novel statistical framework for analyzing stereotypes in large\nlanguage models (LLMs) by systematically estimating the bias and variation in\ntheir generation. Current alignment evaluation metrics often overlook\nstereotypes' randomness caused by LLMs' inconsistent generative behavior. For\ninstance, LLMs may display contradictory stereotypes, such as those related to\ngender or race, for identical professions in different contexts. Ignoring this\ninconsistency risks misleading conclusions in alignment assessments and\nundermines efforts to evaluate the potential of LLMs to perpetuate or amplify\nsocial biases and unfairness.\n  To address this, we propose the Bias-Volatility Framework (BVF), which\nestimates the probability distribution of stereotypes in LLM outputs. By\ncapturing the variation in generative behavior, BVF assesses both the\nlikelihood and degree to which LLM outputs negatively impact vulnerable groups,\nenabling a quantification of aggregated discrimination risk. Additionally, we\nintroduce a mathematical framework to decompose this risk into bias risk (from\nthe mean of the stereotype distribution) and volatility risk (from its\nvariation). Applying BVF to 12 widely used LLMs, we find: i) Bias risk is the\ndominant contributor to discrimination; ii) Most LLMs exhibit substantial\npro-male stereotypes across nearly all professions; iii) Reinforcement learning\nfrom human feedback reduces bias but increases volatility; iv) Discrimination\nrisk correlates with socio-economic factors, such as professional salaries.\nFinally, we highlight BVF's broader applicability for assessing how generation\ninconsistencies in LLMs impact behavior beyond stereotypes.", "AI": {"tldr": "This paper introduces the Bias-Volatility Framework (BVF) for analyzing and quantifying stereotypes in large language models (LLMs), highlighting the importance of capturing both bias and variation in generative outputs to assess discrimination risks.", "motivation": "Current metrics for evaluating alignment in LLMs often fail to account for the randomness and inconsistencies in stereotypes generated by these models, leading to misleading evaluations of bias and discrimination.", "method": "The Bias-Volatility Framework (BVF) estimates the probability distribution of stereotypes in LLM outputs, measuring both bias risk (mean of the stereotype distribution) and volatility risk (variation in the outputs).", "result": "Analysis of 12 widely used LLMs showed that bias risk is the primary contributor to discrimination, with many models exhibiting significant pro-male stereotypes across various professions. Additionally, reinforcement learning from human feedback was found to reduce bias but increase volatility in outputs.", "conclusion": "BVF provides a new avenue for understanding the discriminatory impacts of LLMs, emphasizing the need to consider variations in generative behavior when assessing model fairness.", "key_contributions": ["Introduction of the Bias-Volatility Framework (BVF) for stereotype analysis in LLMs.", "Demonstration that bias risk is the leading contributor to discrimination in LLM outputs.", "Insights into the effect of reinforcement learning on bias and volatility."], "limitations": "The framework primarily focuses on bias and volatility; it may not capture all forms of discrimination or other factors influencing LLM behavior.", "keywords": ["Bias-Volatility Framework", "stereotypes", "large language models", "discrimination", "human feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2402.17263", "pdf": "https://arxiv.org/pdf/2402.17263.pdf", "abs": "https://arxiv.org/abs/2402.17263", "title": "MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning", "authors": ["Pengjie Ren", "Chengshun Shi", "Shiguang Wu", "Mengqi Zhang", "Zhaochun Ren", "Maarten de Rijke", "Zhumin Chen", "Jiahuan Pei"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "ACL2024", "summary": "Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring\npre-trained large language models (LLMs), especially as the models' scale and\nthe diversity of tasks increase. Low-rank adaptation (LoRA) is based on the\nidea that the adaptation process is intrinsically low-dimensional, i.e.,\nsignificant model changes can be represented with relatively few parameters.\nHowever, decreasing the rank encounters challenges with generalization errors\nfor specific tasks when compared to full-parameter fine-tuning. We present\nMELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters\nwhile maintaining a higher rank, thereby offering improved performance\npotential. The core idea is to freeze original pretrained weights and train a\ngroup of mini LoRAs with only a small number of parameters. This can capture a\nsignificant degree of diversity among mini LoRAs, thus promoting better\ngeneralization ability. We conduct a theoretical analysis and empirical studies\non various NLP tasks. Our experimental results show that, compared to LoRA,\nMELoRA achieves better performance with 8 times fewer trainable parameters on\nnatural language understanding tasks and 36 times fewer trainable parameters on\ninstruction following tasks, which demonstrates the effectiveness of MELoRA.", "AI": {"tldr": "MELoRA is a new method for parameter-efficient fine-tuning of large language models that uses mini-ensemble low-rank adapters to improve generalization while requiring fewer trainable parameters.", "motivation": "As large language models grow larger and are applied to diverse tasks, standard fine-tuning methods struggle with generalization errors. A new approach is needed to enhance performance while reducing parameter counts.", "method": "MELoRA freezes the original pretrained weights and trains a group of mini LoRAs, allowing for greater diversity and better generalization in performance across tasks.", "result": "MELoRA outperforms standard low-rank adaptation (LoRA) by achieving better performance with significantly fewer trainable parameters, specifically achieving 8 times fewer parameters for natural language understanding and 36 times fewer for instruction following tasks.", "conclusion": "MELoRA effectively captures task diversity and generalization, making it a promising approach for fine-tuning large language models with fewer resources.", "key_contributions": ["Introduction of mini-ensemble low-rank adapters (MELoRA) for efficient training", "Significantly reduced parameter counts while maintaining or improving performance", "Theoretical and empirical validation of improved generalization ability across various NLP tasks"], "limitations": "", "keywords": ["parameter-efficient fine-tuning", "low-rank adaptation", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2405.13984", "pdf": "https://arxiv.org/pdf/2405.13984.pdf", "abs": "https://arxiv.org/abs/2405.13984", "title": "Less for More: Enhanced Feedback-aligned Mixed LLMs for Molecule Caption Generation and Fine-Grained NLI Evaluation", "authors": ["Dimitris Gkoumas", "Maria Liakata"], "categories": ["cs.CL", "cs.MM"], "comment": "ACL25 Main", "summary": "Scientific language models drive research innovation but require extensive\nfine-tuning on large datasets. This work enhances such models by improving\ntheir inference and evaluation capabilities with minimal or no additional\ntraining. Focusing on molecule caption generation, we explore post-training\nsynergies between alignment fine-tuning and model merging in a cross-modal\nsetup. We reveal intriguing insights into the behaviour and suitability of such\nmethods while significantly surpassing state-of-the-art models. Moreover, we\npropose a novel atomic-level evaluation method leveraging off-the-shelf Natural\nLanguage Inference (NLI) models for use in the unseen chemical domain. Our\nexperiments demonstrate that our evaluation operates at the right level of\ngranularity, effectively handling multiple content units and subsentence\nreasoning, while widely adopted NLI methods consistently misalign with\nassessment criteria.", "AI": {"tldr": "This paper improves scientific language models for molecule caption generation by enhancing inference and evaluation capabilities without extensive fine-tuning.", "motivation": "The motivation behind this work is to enhance scientific language models that traditionally rely on extensive fine-tuning on large datasets, by boosting their performance with minimal additional training.", "method": "The authors investigate post-training enhancements through a combination of alignment fine-tuning and model merging in a cross-modal context, targeting molecule caption generation.", "result": "Experimental results show significant performance improvements over state-of-the-art models, revealing better alignment with the tasks and a novel evaluation method for unseen chemical domains.", "conclusion": "The study concludes that the proposed atomic-level evaluation method is effective for the molecular context, in contrast to common NLI methods, and supports finer granularity in reasoning assessments.", "key_contributions": ["Enhanced inference and evaluation for language models with minimal training", "Novel atomic-level evaluation method for unseen chemical domains", "Insights into post-training synergies between model merging and alignment fine-tuning"], "limitations": "The paper does not address potential limitations in the generalizability of the proposed methods beyond the chemical domain.", "keywords": ["language models", "molecule caption generation", "NLI", "alignment fine-tuning", "cross-modal"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.17062", "pdf": "https://arxiv.org/pdf/2405.17062.pdf", "abs": "https://arxiv.org/abs/2405.17062", "title": "UniICL: An Efficient Unified Framework Unifying Compression, Selection, and Generation", "authors": ["Jun Gao", "Qi Lv", "Zili Wang", "Tianxiang Wu", "Ziqiang Cao", "Wenjie Li"], "categories": ["cs.CL"], "comment": "ACL2025", "summary": "In-context learning (ICL) enhances the reasoning abilities of Large Language\nModels (LLMs) by prepending a few demonstrations. It motivates researchers to\nintroduce more examples to provide additional contextual information for the\ngeneration. However, existing methods show a significant limitation due to the\nproblem of excessive growth in context length, which causes a large hardware\nburden. In addition, shallow-relevant examples selected by off-the-shelf tools\nhinder LLMs from capturing useful contextual information for generation. In\nthis paper, we propose \\textbf{UniICL}, a novel \\textbf{Uni}fied \\textbf{ICL}\nframework that unifies demonstration compression, demonstration selection, and\nfinal response generation. Furthermore, to boost inference efficiency, we\ndesign a tailored compression strategy that allows UniICL to cache compression\nresults into \\textbf{Demonstration Bank} (\\textbf{DB}), which avoids repeated\ncompression of the same demonstration. Extensive out-of-domain evaluations\nprove the advantages of UniICL in both effectiveness and efficiency.", "AI": {"tldr": "UniICL is a new framework that addresses context length issues in in-context learning for LLMs by improving demonstration compression and selection, enhancing efficiency.", "motivation": "The paper addresses the limitations of existing in-context learning methods, particularly their tendency to increase context length excessively, which burdens hardware resources and hampers effective reasoning in LLMs.", "method": "A unified framework, UniICL, is proposed that integrates demonstration compression, selection, and response generation, along with a compression strategy that caches results in a Demonstration Bank.", "result": "UniICL demonstrates significant improvements in both effectiveness and efficiency in out-of-domain evaluations compared to traditional methods.", "conclusion": "The findings suggest that UniICL can effectively enhance LLM reasoning while mitigating hardware constraints typically associated with excessive context lengths.", "key_contributions": ["Development of a unified ICL framework (UniICL) that consolidates multiple tasks.", "Introduction of a novel caching strategy for demonstration compression.", "Empirical evidence of improved efficiency and effectiveness over existing methods."], "limitations": "", "keywords": ["in-context learning", "large language models", "demonstration compression", "Demonstration Bank", "AI efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.05348", "pdf": "https://arxiv.org/pdf/2406.05348.pdf", "abs": "https://arxiv.org/abs/2406.05348", "title": "Toward Reliable Ad-hoc Scientific Information Extraction: A Case Study on Two Materials Datasets", "authors": ["Satanu Ghosh", "Neal R. Brodnik", "Carolina Frey", "Collin Holgate", "Tresa M. Pollock", "Samantha Daly", "Samuel Carton"], "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2; I.2.7; H.4; H.5"], "comment": "LLM for information extraction. Update on 12/11/2024: We added some\n  relevant literature that we missed in the previous version of the paper.\n  Update on 05/25/2025: We changed the metadata", "summary": "We explore the ability of GPT-4 to perform ad-hoc schema based information\nextraction from scientific literature. We assess specifically whether it can,\nwith a basic prompting approach, replicate two existing material science\ndatasets, given the manuscripts from which they were originally manually\nextracted. We employ materials scientists to perform a detailed manual error\nanalysis to assess where the model struggles to faithfully extract the desired\ninformation, and draw on their insights to suggest research directions to\naddress this broadly important task.", "AI": {"tldr": "This paper investigates the use of GPT-4 for schema-based information extraction from scientific literature, specifically in materials science, and evaluates its effectiveness and limitations.", "motivation": "The goal is to explore the potential of GPT-4 in replicating existing datasets from scientific manuscripts through ad-hoc schema-based information extraction.", "method": "The authors employed a basic prompting approach with GPT-4 and conducted a detailed manual error analysis using materials scientists to evaluate the model's extraction capabilities.", "result": "The assessment revealed specific areas where GPT-4 struggles to accurately extract information and provided insights into potential improvements.", "conclusion": "Insights from the manual error analysis led to suggestions for future research directions in enhancing the model's information extraction abilities.", "key_contributions": ["Evaluation of GPT-4's performance in information extraction from scientific literature.", "Identification of common errors in information extraction using a manual review by experts.", "Recommendations for future research to improve LLM-based information extraction methods."], "limitations": "The model's limitations in accurately extracting certain types of information were identified but not fully resolved in the study.", "keywords": ["GPT-4", "information extraction", "schema-based", "materials science", "LLM"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2406.11632", "pdf": "https://arxiv.org/pdf/2406.11632.pdf", "abs": "https://arxiv.org/abs/2406.11632", "title": "Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding for Neural Machine Translation", "authors": ["Boxuan Lyu", "Hidetaka Kamigaito", "Kotaro Funakoshi", "Manabu Okumura"], "categories": ["cs.CL", "cs.AI"], "comment": "ACl2025 Main Conference", "summary": "Maximum a posteriori decoding, a commonly used method for neural machine\ntranslation (NMT), aims to maximize the estimated posterior probability.\nHowever, high estimated probability does not always lead to high translation\nquality. Minimum Bayes Risk (MBR) decoding offers an alternative by seeking\nhypotheses with the highest expected utility.\n  Inspired by Quality Estimation (QE) reranking which uses the QE model as a\nranker we propose source-based MBR (sMBR) decoding, a novel approach that\nutilizes quasi-sources (generated via paraphrasing or back-translation) as\n``support hypotheses'' and a reference-free quality estimation metric as the\nutility function, marking the first work to solely use sources in MBR decoding.\nExperiments show that sMBR outperforms QE reranking and the standard MBR\ndecoding. Our findings suggest that sMBR is a promising approach for NMT\ndecoding.", "AI": {"tldr": "This paper introduces source-based MBR (sMBR) decoding for neural machine translation, which utilizes paraphrased or back-translated sources to improve translation quality over standard methods.", "motivation": "The paper addresses the limitations of maximum a posteriori decoding in neural machine translation, where high estimated probabilities do not guarantee translation quality.", "method": "The proposed sMBR decoding uses quasi-sources as support hypotheses and incorporates a reference-free quality estimation metric as its utility function.", "result": "Experiments demonstrate that sMBR surpasses both Quality Estimation reranking and traditional MBR decoding approaches.", "conclusion": "sMBR presents a novel and effective technique for improving neural machine translation decoding by leveraging source-based hypotheses and quality estimation.", "key_contributions": ["Introduction of source-based MBR decoding (sMBR)", "Use of paraphrased or back-translated sources as support hypotheses", "First work using sources in MBR decoding for NMT"], "limitations": "", "keywords": ["neural machine translation", "minimum Bayes risk", "quality estimation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2406.16833", "pdf": "https://arxiv.org/pdf/2406.16833.pdf", "abs": "https://arxiv.org/abs/2406.16833", "title": "USDC: A Dataset of $\\underline{U}$ser $\\underline{S}$tance and $\\underline{D}$ogmatism in Long $\\underline{C}$onversations", "authors": ["Mounika Marreddy", "Subba Reddy Oota", "Venkata Charan Chinni", "Manish Gupta", "Lucie Flek"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "45 pages, 22 figures, Findings of the Association for Computational\n  Linguistics, ACL-2025 (Long)", "summary": "Analyzing user opinion changes in long conversation threads is extremely\ncritical for applications like enhanced personalization, market research,\npolitical campaigns, customer service, targeted advertising, and content\nmoderation. Unfortunately, previous studies on stance and dogmatism in user\nconversations have focused on training models using datasets annotated at the\npost level, treating each post as independent and randomly sampling posts from\nconversation threads. Hence, first, we build a dataset for studying user\nopinion fluctuations in 764 long multi-user Reddit conversation threads, called\nUSDC. USDC contains annotations for 2 tasks: i) User Stance classification,\nwhich involves labeling a user's stance in a post within a conversation on a\nfive-point scale; ii) User Dogmatism classification, which involves labeling a\nuser's overall opinion in the conversation on a four-point scale. Besides being\ntime-consuming and costly, manual annotations for USDC are challenging because:\n1) Conversation threads could be very long, increasing the chances of noisy\nannotations; and 2) Interpreting instances where a user changes their opinion\nwithin a conversation is difficult because often such transitions are subtle\nand not expressed explicitly. Hence, we leverage majority voting on zero-shot,\none-shot, and few-shot annotations from Mistral Large and GPT-4 to automate the\nannotation process. Human annotations on 200 test conversations achieved\ninter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism with\nthese LLM annotations, indicating a reasonable level of consistency between\nhuman and LLM annotations. USDC is then used to finetune and instruction-tune\nmultiple deployable small language models like LLaMA, Falcon and Vicuna for the\nstance and dogmatism classification tasks. We make the code and dataset\npublicly available [https://github.com/mounikamarreddy/USDC].", "AI": {"tldr": "The paper introduces USDC, a dataset designed to study user opinion fluctuations in long Reddit conversation threads, automating annotations using LLMs for stance and dogmatism classifications.", "motivation": "Analyzing user opinion changes in long conversation threads is vital for applications in personalization, market research, and customer service, but existing methods focus on simplistic post-level analysis.", "method": "Developed a dataset (USDC) with annotations for stance and dogmatism, leveraging zero-shot, one-shot, and few-shot annotations from LLMs like GPT-4 to enhance annotation automation.", "result": "Achieved inter-annotator agreement scores of 0.49 for stance and 0.50 for dogmatism in human annotations, indicating substantial consistency with LLM-generated annotations.", "conclusion": "The USDC dataset can be utilized to finetune and instruction-tune multiple small language models for stance and dogmatism classification, making significant advancements in the study of opinion dynamics in conversations.", "key_contributions": ["Introduction of a novel dataset (USDC) for user opinion fluctuation study", "Use of LLMs to automate annotation processes", "Finetuning of multiple language models for enhanced classification tasks"], "limitations": "Challenges in manual annotations include lengthy conversation threads and subtle opinion changes.", "keywords": ["User Opinion", "Stance Classification", "Dogmatism Classification", "Dataset", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 45}}
{"id": "2406.19465", "pdf": "https://arxiv.org/pdf/2406.19465.pdf", "abs": "https://arxiv.org/abs/2406.19465", "title": "Can Large Language Models Generate High-quality Patent Claims?", "authors": ["Lekang Jiang", "Caiqi Zhang", "Pascal A Scherz", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025. 16 pages, 2 figures, 12 tables", "summary": "Large language models (LLMs) have shown exceptional performance across\nvarious text generation tasks but remain under-explored in the patent domain,\nwhich offers highly structured and precise language. This paper constructs a\ndataset to investigate the performance of current LLMs in patent claim\ngeneration. Our results demonstrate that generating claims based on patent\ndescriptions outperforms previous research relying on abstracts. Interestingly,\ncurrent patent-specific LLMs perform much worse than state-of-the-art general\nLLMs, highlighting the necessity for future research on in-domain LLMs. We also\nfind that LLMs can produce high-quality first independent claims, but their\nperformances markedly decrease for subsequent dependent claims. Moreover,\nfine-tuning can enhance the completeness of inventions' features, conceptual\nclarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the\nbest performance in comprehensive human evaluations by patent experts, with\nbetter feature coverage, conceptual clarity, and technical coherence. Despite\nthese capabilities, comprehensive revision and modification are still necessary\nto pass rigorous patent scrutiny and ensure legal robustness.", "AI": {"tldr": "This paper investigates the capabilities of large language models (LLMs) in generating patent claims and explores the need for further research on in-domain LLMs.", "motivation": "The paper addresses the under-exploration of LLMs in the patent domain, which uses structured language for claim generation.", "method": "A dataset was constructed to assess the performance of LLMs in generating patent claims based on patent descriptions.", "result": "The study reveals that generating claims from patent descriptions is more effective than generating from abstracts, with GPT-4 performing best among the evaluated models.", "conclusion": "While LLMs can generate high-quality independent claims, their performance declines for dependent claims, necessitating further fine-tuning for legal robustness.", "key_contributions": ["Construction of a patent claim generation dataset", "Demonstration of LLM performance differences in patent vs. abstract generation", "Insights into fine-tuning for improved patent feature representation"], "limitations": "Current patent-specific LLMs underperform compared to general LLMs, requiring further research and development.", "keywords": ["Large Language Models", "Patent Claim Generation", "Fine-Tuning", "Natural Language Processing", "Machine Learning"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2407.19299", "pdf": "https://arxiv.org/pdf/2407.19299.pdf", "abs": "https://arxiv.org/abs/2407.19299", "title": "The Impact of LoRA Adapters for LLMs on Clinical NLP Classification Under Data Limitations", "authors": ["Thanh-Dung Le", "Ti Ti Nguyen", "Vu Nguyen Ha", "Symeon Chatzinotas", "Philippe Jouvet", "Rita Noumeir"], "categories": ["cs.CL", "eess.SP"], "comment": "Under revisions", "summary": "Fine-tuning Large Language Models (LLMs) for clinical Natural Language\nProcessing (NLP) poses significant challenges due to the domain gap and limited\ndata availability. This study investigates the effectiveness of various adapter\ntechniques, equivalent to Low-Rank Adaptation (LoRA), for fine-tuning LLMs in a\nresource-constrained hospital environment. We experimented with four\nstructures-Adapter, Lightweight, TinyAttention, and Gated Residual Network\n(GRN)-as final layers for clinical notes classification. We fine-tuned\nbiomedical pre-trained models, including CamemBERT-bio, AliBERT, and DrBERT,\nalongside two Transformer-based models. Our extensive experimental results\nindicate that i) employing adapter structures does not yield significant\nimprovements in fine-tuning biomedical pre-trained LLMs, and ii) simpler\nTransformer-based models, trained from scratch, perform better under resource\nconstraints. Among the adapter structures, GRN demonstrated superior\nperformance with accuracy, precision, recall, and an F1 score of 0.88.\nMoreover, the total training time for LLMs exceeded 1000 hours, compared to\nunder 6 hours for simpler transformer-based models, highlighting that LLMs are\nmore suitable for environments with extensive computational resources and\nlarger datasets. Consequently, this study demonstrates that simpler\nTransformer-based models can be effectively trained from scratch, providing a\nviable solution for clinical NLP tasks in low-resource environments with\nlimited data availability. By identifying the GRN as the most effective adapter\nstructure, we offer a practical approach to enhance clinical note\nclassification without requiring extensive computational resources.", "AI": {"tldr": "The study evaluates the effectiveness of adapter techniques for fine-tuning LLMs in clinical NLP under resource constraints, finding simpler Transformer-based models to perform better.", "motivation": "To address challenges in fine-tuning LLMs for clinical NLP due to domain gaps and limited data in a resource-constrained environment.", "method": "The study explored four adapter structures for clinical notes classification using biomedical pre-trained models and Transformer-based models.", "result": "Adapter structures showed minimal improvement, while simpler Transformer-based models outperformed LLMs, with GRN being the best adapter structure yielding an F1 score of 0.88.", "conclusion": "Simpler models can be effectively trained in low-resource settings, suggesting a viable alternative for clinical NLP without needing extensive computational resources.", "key_contributions": ["Demonstrated that simpler Transformer-based models outperform fine-tuned LLMs in resource-constrained settings.", "Identified Gated Residual Network (GRN) as the most effective adapter structure.", "Highlighted the computational resource demands of LLMs for clinical NLP tasks."], "limitations": "The findings are specific to resource-constrained environments and may not generalize to scenarios with ample resources.", "keywords": ["Natural Language Processing", "Large Language Models", "Health Informatics", "Clinical Notes Classification", "Adapter Techniques"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2408.09070", "pdf": "https://arxiv.org/pdf/2408.09070.pdf", "abs": "https://arxiv.org/abs/2408.09070", "title": "CodeTaxo: Enhancing Taxonomy Expansion with Limited Examples via Code Language Prompts", "authors": ["Qingkai Zeng", "Yuyang Bai", "Zhaoxuan Tan", "Zhenyu Wu", "Shangbin Feng", "Meng Jiang"], "categories": ["cs.CL", "cs.IR"], "comment": "Accepted by ACL2025 Findings", "summary": "Taxonomies play a crucial role in various applications by providing a\nstructural representation of knowledge. The task of taxonomy expansion involves\nintegrating emerging concepts into existing taxonomies by identifying\nappropriate parent concepts for these new query concepts. Previous approaches\ntypically relied on self-supervised methods that generate annotation data from\nexisting taxonomies. However, these methods are less effective when the\nexisting taxonomy is small (fewer than 100 entities). In this work, we\nintroduce CodeTaxo, a novel approach that leverages large language models\nthrough code language prompts to capture the taxonomic structure. Extensive\nexperiments on five real-world benchmarks from different domains demonstrate\nthat CodeTaxo consistently achieves superior performance across all evaluation\nmetrics, significantly outperforming previous state-of-the-art methods. The\ncode and data are available at https://github.com/QingkaiZeng/CodeTaxo-Pub.", "AI": {"tldr": "CodeTaxo introduces a novel method for taxonomy expansion using large language models to enhance the integration of new concepts into existing taxonomies, achieving superior performance over prior methods.", "motivation": "Taxonomies are essential for knowledge representation, and effective expansion methods are needed, especially when existing taxonomies are limited in size.", "method": "CodeTaxo leverages large language models by using code language prompts to identify parent concepts for new query concepts in taxonomy expansion.", "result": "CodeTaxo consistently outperforms previous state-of-the-art methods across five real-world benchmarks, demonstrating superior performance in taxonomy expansion tasks.", "conclusion": "The introduction of CodeTaxo marks a significant advancement in taxonomy expansion, particularly for small taxonomies, and the availability of code and data enhances accessibility for future research.", "key_contributions": ["Introduction of CodeTaxo leveraging LLMs for taxonomy expansion", "Demonstrated superior performance compared to existing methods", "Experimental validation across multiple real-world benchmarks"], "limitations": "", "keywords": ["taxonomy expansion", "large language models", "knowledge representation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2409.01345", "pdf": "https://arxiv.org/pdf/2409.01345.pdf", "abs": "https://arxiv.org/abs/2409.01345", "title": "Language Models Benefit from Preparation with Elicited Knowledge", "authors": ["Jiacan Yu", "Hannah An", "Lenhart K. Schubert"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The zero-shot chain of thought (CoT) approach is often used in question\nanswering (QA) by language models (LMs) for tasks that require multiple\nreasoning steps. However, some QA tasks hinge more on accessing relevant\nknowledge than on chaining reasoning steps. We introduce a simple prompting\ntechnique, called PREP, that involves using two instances of LMs: the first\n(LM1) generates relevant information, and the second (LM2) receives the\ninformation from the user and answers the question. This design is intended to\nmake better use of the LM's instruction-following capability. PREP is\napplicable across various QA tasks without domain-specific prompt engineering.\nPREP is developed on a dataset of 100 QA questions, derived from an extensive\nschematic dataset specifying artifact parts and material composition. These\nquestions ask which of two artifacts is less likely to share materials with\nanother artifact. Such questions probe the LM's knowledge of shared materials\nin the part structure of different artifacts. We test our method on our\nparts-and-materials dataset and three published commonsense reasoning datasets.\nThe average accuracy of our method is consistently higher than that of all the\nother tested methods across all the tested datasets.", "AI": {"tldr": "The paper introduces a prompting technique called PREP that improves the performance of language models by using two instances to generate relevant information and answer questions.", "motivation": "To enhance the question answering capability of language models by focusing on accessing relevant knowledge rather than just reasoning steps.", "method": "The PREP technique uses two language models: the first generates relevant information, while the second answers the question using that information. It was tested on a dataset of 100 QA questions regarding artifact parts and material composition.", "result": "The PREP method showed consistently higher average accuracy than other tested methods across multiple datasets, including one based on parts and materials as well as commonsense reasoning datasets.", "conclusion": "PREP can be effectively applied to various question answering tasks without the need for domain-specific prompt engineering and improves the use of language models in QA scenarios.", "key_contributions": ["Introduction of the PREP technique for enhanced QA tasks", "Demonstration of higher accuracy across various datasets", "Reduction of the need for domain-specific prompt engineering"], "limitations": "", "keywords": ["zero-shot", "question answering", "language models", "knowledge access", "prompting technique"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.08103", "pdf": "https://arxiv.org/pdf/2409.08103.pdf", "abs": "https://arxiv.org/abs/2409.08103", "title": "The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language", "authors": ["Michael Ong", "Sean Robertson", "Leo Peckham", "Alba Jorquera Jimenez de Aberasturi", "Paula Arkhangorodsky", "Robin Huo", "Aman Sakhardande", "Mark Hallap", "Naomi Nagy", "Ewan Dunbar"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "To appear in INTERSPEECH 2025", "summary": "We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark\ncorpus designed to push the limits of current approaches to low-resource speech\nrecognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy,\nhas no standard orthography, has virtually no existing textual or speech\nresources other than what is included in the benchmark, and is quite different\nfrom other forms of Franco-Proven\\c{c}al. The corpus comes from field\nrecordings, most of which are noisy, for which only 5 hrs have matching\ntranscriptions, and for which forced alignment is of variable quality. The\ncorpus contains an additional 20 hrs of unlabelled speech. We report baseline\nresults from state-of-the-art multilingual speech foundation models with a best\nphone error rate of 30.4%, using a pipeline that continues pre-training on the\nfoundation model using the unlabelled set.", "AI": {"tldr": "The Faetar Automatic Speech Recognition Benchmark aims to enhance low-resource speech recognition, focusing on the Franco-Proven\no{c}al variety spoken in Italy, with limited existing data.", "motivation": "To advance the field of low-resource speech recognition, particularly for underrepresented languages such as Faetar.", "method": "The benchmark includes noisy field recordings, with 5 hours of transcribed and 20 hours of unlabelled speech. Baseline results were generated using multilingual speech foundation models.", "result": "A best phone error rate of 30.4% was achieved using a pipeline that continues pre-training on the foundation model with the unlabelled dataset.", "conclusion": "The Faetar corpus presents challenges for ASR due to its limited resources and variability in quality, highlighting the need for innovative approaches.", "key_contributions": ["Introduction of a novel benchmark corpus for low-resource speech recognition.", "Demonstration of baseline results from multilingual speech models on a unique language variety.", "Insights into the challenges of working with noisy recordings and limited transcriptions."], "limitations": "Limited amount of transcribed data and absence of standard orthography for the language.", "keywords": ["speech recognition", "low-resource languages", "Faetar", "multilingual models", "automatic speech recognition"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2409.19663", "pdf": "https://arxiv.org/pdf/2409.19663.pdf", "abs": "https://arxiv.org/abs/2409.19663", "title": "Identifying Knowledge Editing Types in Large Language Models", "authors": ["Xiaopeng Li", "Shasha Li", "Shangwen Wang", "Shezheng Song", "Bin Ji", "Huijun Liu", "Jun Ma", "Jie Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD 2025", "summary": "Knowledge editing has emerged as an efficient technique for updating the\nknowledge of large language models (LLMs), attracting increasing attention in\nrecent years. However, there is a lack of effective measures to prevent the\nmalicious misuse of this technique, which could lead to harmful edits in LLMs.\nThese malicious modifications could cause LLMs to generate toxic content,\nmisleading users into inappropriate actions. In front of this risk, we\nintroduce a new task, $\\textbf{K}$nowledge $\\textbf{E}$diting $\\textbf{T}$ype\n$\\textbf{I}$dentification (KETI), aimed at identifying different types of edits\nin LLMs, thereby providing timely alerts to users when encountering illicit\nedits. As part of this task, we propose KETIBench, which includes five types of\nharmful edits covering the most popular toxic types, as well as one benign\nfactual edit. We develop five classical classification models and three\nBERT-based models as baseline identifiers for both open-source and\nclosed-source LLMs. Our experimental results, across 92 trials involving four\nmodels and three knowledge editing methods, demonstrate that all eight baseline\nidentifiers achieve decent identification performance, highlighting the\nfeasibility of identifying malicious edits in LLMs. Additional analyses reveal\nthat the performance of the identifiers is independent of the reliability of\nthe knowledge editing methods and exhibits cross-domain generalization,\nenabling the identification of edits from unknown sources. All data and code\nare available in https://github.com/xpq-tech/KETI.", "AI": {"tldr": "This paper introduces the knowledge editing type identification (KETI) task and KETIBench dataset to identify malicious edits in large language models (LLMs).", "motivation": "To address the risks associated with malicious edits in LLMs, which can lead to harmful outputs and user manipulation.", "method": "The study proposes KETIBench, a dataset with various harmful and benign edits, and develops classification models, including five classical and three BERT-based models, to identify these edits.", "result": "The experimental results show that all eight baseline identifiers perform well in identifying malicious edits, demonstrating cross-domain generalization and robustness against various knowledge editing methods.", "conclusion": "The findings suggest that it's feasible to identify harmful edits in LLMs, contributing to safer AI interactions.", "key_contributions": ["Introduction of the KETI task for identifying types of edits in LLMs", "Development and release of KETIBench dataset", "Demonstration of effective identification models for malicious edits"], "limitations": "", "keywords": ["Knowledge Editing", "Large Language Models", "Malicious Edits"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2409.20434", "pdf": "https://arxiv.org/pdf/2409.20434.pdf", "abs": "https://arxiv.org/abs/2409.20434", "title": "QAEncoder: Towards Aligned Representation Learning in Question Answering System", "authors": ["Zhengren Wang", "Qinhan Yu", "Shida Wei", "Zhiyu Li", "Feiyu Xiong", "Xiaoxing Wang", "Simin Niu", "Hao Liang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Modern QA systems entail retrieval-augmented generation (RAG) for accurate\nand trustworthy responses. However, the inherent gap between user queries and\nrelevant documents hinders precise matching. We introduce QAEncoder, a\ntraining-free approach to bridge this gap. Specifically, QAEncoder estimates\nthe expectation of potential queries in the embedding space as a robust\nsurrogate for the document embedding, and attaches document fingerprints to\neffectively distinguish these embeddings. Extensive experiments across diverse\ndatasets, languages, and embedding models confirmed QAEncoder's alignment\ncapability, which offers a simple-yet-effective solution with zero additional\nindex storage, retrieval latency, training costs, or catastrophic forgetting\nand hallucination issues. The repository is publicly available at\nhttps://github.com/IAAR-Shanghai/QAEncoder.", "AI": {"tldr": "QAEncoder is a training-free method designed to improve precision in retrieval-augmented generation (RAG) systems for question answering by estimating potential queries in the embedding space.", "motivation": "The paper addresses the issue of the gap between user queries and relevant documents that affects the accuracy of QA systems.", "method": "QAEncoder estimates potential queries in the embedding space as a surrogate for document embeddings and attaches document fingerprints for better distinction.", "result": "Extensive experimentation shows QAEncoder's strong performance in aligning queries and documents without additional storage, latency, training costs, or the risks of forgetting and hallucination.", "conclusion": "QAEncoder provides a simple and effective solution for enhancing QA system accuracy without incurring extra costs or complexity.", "key_contributions": ["Introduction of a training-free method for query-document alignment", "Use of document fingerprints for enhanced embedding distinction", "Demonstrated effectiveness across various datasets and languages"], "limitations": "", "keywords": ["QA systems", "retrieval-augmented generation", "embedding methods"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2410.00193", "pdf": "https://arxiv.org/pdf/2410.00193.pdf", "abs": "https://arxiv.org/abs/2410.00193", "title": "Do Vision-Language Models Really Understand Visual Language?", "authors": ["Yifan Hou", "Buse Giledereli", "Yilei Tu", "Mrinmaya Sachan"], "categories": ["cs.CL", "cs.CV"], "comment": "ICML 2025", "summary": "Visual language is a system of communication that conveys information through\nsymbols, shapes, and spatial arrangements. Diagrams are a typical example of a\nvisual language depicting complex concepts and their relationships in the form\nof an image. The symbolic nature of diagrams presents significant challenges\nfor building models capable of understanding them. Recent studies suggest that\nLarge Vision-Language Models (LVLMs) can even tackle complex reasoning tasks\ninvolving diagrams. In this paper, we investigate this phenomenon by developing\na comprehensive test suite to evaluate the diagram comprehension capability of\nLVLMs. Our test suite uses a variety of questions focused on concept entities\nand their relationships over a set of synthetic as well as real diagrams across\ndomains to evaluate the recognition and reasoning abilities of models. Our\nevaluation of LVLMs shows that while they can accurately identify and reason\nabout entities, their ability to understand relationships is notably limited.\nFurther testing reveals that the decent performance on diagram understanding\nlargely stems from leveraging their background knowledge as shortcuts to\nidentify and reason about the relational information. Thus, we conclude that\nLVLMs have a limited capability for genuine diagram understanding, and their\nimpressive performance in diagram reasoning is an illusion emanating from other\nconfounding factors, such as the background knowledge in the models.", "AI": {"tldr": "This paper evaluates the capability of Large Vision-Language Models (LVLMs) in understanding diagrams and reveals limitations in their ability to grasp relationships among entities.", "motivation": "The growing use of visual languages like diagrams in communication necessitates an understanding of how AI models, particularly LVLMs, comprehend them.", "method": "A comprehensive test suite was developed to evaluate LVLMs' diagram comprehension through various questions focused on concept entities and their relationships across synthetic and real diagrams.", "result": "LVLMs can identify and reason about entities in diagrams accurately, but their understanding of relationships is notably limited, often relying on background knowledge as shortcuts.", "conclusion": "The impressive performance of LVLMs in diagram reasoning is misleading, as it is influenced by confounding factors rather than true understanding of diagrams.", "key_contributions": ["Development of a test suite for evaluating LVLMs on diagram understanding", "Identification of limitations in LVLMs' relationship comprehension", "Insights into the reliance on background knowledge in diagram reasoning"], "limitations": "Limited capability for genuine diagram understanding; results may not generalize across all diagram types.", "keywords": ["Large Vision-Language Models", "diagram comprehension", "relationship reasoning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.03124", "pdf": "https://arxiv.org/pdf/2410.03124.pdf", "abs": "https://arxiv.org/abs/2410.03124", "title": "In-context Demonstration Matters: On Prompt Optimization for Pseudo-Supervision Refinement", "authors": ["Zhen-Yu Zhang", "Jiandong Zhang", "Huaxiu Yao", "Gang Niu", "Masashi Sugiyama"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have achieved great success across diverse\ntasks, and fine-tuning is sometimes needed to further enhance generation\nquality. Most existing methods rely on human supervision or parameter\nretraining, both of which are costly in terms of data collection and\ncomputational resources. To handle these challenges, a direct solution is to\ngenerate ``high-confidence'' data from unsupervised downstream tasks and use\nthem for in-context prompting or prompt optimization to refine the\npseudo-supervision. However, relying solely on such data may lead to\noverfitting. In this paper, we leverage the in-context learning (ICL) abilities\nof LLMs and propose a novel approach, pseudo-supervised demonstrations aligned\nprompt optimization (PAPO) algorithm, which jointly refines both the prompt and\nthe overall pseudo-supervision. The proposed learning objective ensures that\nthe optimized prompt guides the LLM to generate consistent responses for a\ngiven input when pseudo-supervised data from the downstream task are used as\ndemonstrations, enabling refinement over the entire pseudo-supervision. The\nprompt is optimized by translating gradient signals into textual critiques,\nwhich serve as feedback to iteratively refine the prompt and model responses.\nTheoretical analysis in a simplified classification setting shows that the\nrefined pseudo-supervision exhibits a geometric clustering structure, helping\nto mitigate overfitting. Experiments on question answering, natural language\ninference benchmarks, and a real-world molecule optimization task, show the\neffectiveness of the proposed algorithm.", "AI": {"tldr": "The paper introduces the PAPO algorithm, which optimizes prompts and pseudo-supervision in LLMs to enhance performance without costly human intervention.", "motivation": "The need for efficient methods to enhance LLM performance without expensive human supervision or extensive retraining.", "method": "The PAPO algorithm combines in-context learning and a novel optimization process that translates gradient signals into textual critiques to refine prompts and pseudo-supervised data.", "result": "The proposed approach reduces overfitting and demonstrates improved performance on question answering, natural language inference, and molecule optimization tasks.", "conclusion": "The PAPO algorithm effectively optimizes prompts and pseudo-supervision, leading to better performance in LLMs across diverse applications while mitigating overfitting risks.", "key_contributions": ["Introduction of the PAPO algorithm for optimizing prompts and pseudo-supervision in LLMs.", "Utilization of in-context learning to generate consistent responses.", "Theoretical analysis demonstrating the effectiveness in mitigating overfitting."], "limitations": "", "keywords": ["large language models", "prompt optimization", "pseudo-supervision"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.04407", "pdf": "https://arxiv.org/pdf/2410.04407.pdf", "abs": "https://arxiv.org/abs/2410.04407", "title": "Lens: Rethinking Multilingual Enhancement for Large Language Models", "authors": ["Weixiang Zhao", "Yulin Hu", "Jiahe Guo", "Xingyu Sui", "Tongtong Wu", "Yang Deng", "Yanyan Zhao", "Bing Qin", "Wanxiang Che", "Ting Liu"], "categories": ["cs.CL"], "comment": "23 pages, 7 figures, 7 tables", "summary": "As global demand for multilingual large language models (LLMs) grows, most\nLLMs still remain overly focused on English, leading to the limited access to\nadvanced AI for non-English speakers. Current methods to enhance multilingual\ncapabilities largely rely on data-driven post-training techniques, such as\nmultilingual instruction tuning or continual pre-training. However, these\napproaches exhibit significant limitations, including high resource cost,\nexacerbation of off-target issue and catastrophic forgetting of central\nlanguage abilities. To this end, we propose Lens, a novel approach that\nenhances multilingual capabilities by leveraging LLMs' internal language\nrepresentation spaces. Lens operates on two subspaces: the language-agnostic\nsubspace, where it aligns target languages with the central language to inherit\nstrong semantic representations, and the language-specific subspace, where it\nseparates target and central languages to preserve linguistic specificity.\nExperiments on three English-centric LLMs show that Lens significantly improves\nmultilingual performance while maintaining the model's English proficiency,\nachieving better results with less computational cost compared to existing\npost-training approaches.", "AI": {"tldr": "The paper presents Lens, a novel methodology to enhance multilingual capabilities in LLMs without compromising English proficiency, utilizing internal language representation structures.", "motivation": "To address the limited multilingual capabilities of LLMs, which primarily focus on English, and to provide better access to AI for non-English speakers.", "method": "Lens leverages two subspaces: a language-agnostic subspace for aligning target languages with the central language and a language-specific subspace for preserving linguistic specificity among languages.", "result": "Lens significantly improves multilingual performance on English-centric LLMs while maintaining high proficiency in English and requires less computational resources compared to traditional methods.", "conclusion": "The proposed approach outperforms existing post-training methods, enabling more efficient and effective multilingual capabilities in LLMs.", "key_contributions": ["Introduction of the Lens methodology for enhancing multilingual capabilities", "Demonstrating improved performance with reduced computational costs", "Maintaining English proficiency while expanding multilingual support"], "limitations": "", "keywords": ["multilingual large language models", "language representation", "computer science", "natural language processing", "AI access"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.06704", "pdf": "https://arxiv.org/pdf/2410.06704.pdf", "abs": "https://arxiv.org/abs/2410.06704", "title": "PII-Scope: A Comprehensive Study on Training Data PII Extraction Attacks in LLMs", "authors": ["Krishna Kanth Nakka", "Ahmed Frikha", "Ricardo Mendes", "Xue Jiang", "Xuebing Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Additional results with Pythia6.9B; Additional results with Phone\n  number PII;", "summary": "In this work, we introduce PII-Scope, a comprehensive benchmark designed to\nevaluate state-of-the-art methodologies for PII extraction attacks targeting\nLLMs across diverse threat settings. Our study provides a deeper understanding\nof these attacks by uncovering several hyperparameters (e.g., demonstration\nselection) crucial to their effectiveness. Building on this understanding, we\nextend our study to more realistic attack scenarios, exploring PII attacks that\nemploy advanced adversarial strategies, including repeated and diverse\nquerying, and leveraging iterative learning for continual PII extraction.\nThrough extensive experimentation, our results reveal a notable underestimation\nof PII leakage in existing single-query attacks. In fact, we show that with\nsophisticated adversarial capabilities and a limited query budget, PII\nextraction rates can increase by up to fivefold when targeting the pretrained\nmodel. Moreover, we evaluate PII leakage on finetuned models, showing that they\nare more vulnerable to leakage than pretrained models. Overall, our work\nestablishes a rigorous empirical benchmark for PII extraction attacks in\nrealistic threat scenarios and provides a strong foundation for developing\neffective mitigation strategies.", "AI": {"tldr": "Presentation of PII-Scope, a benchmark for evaluating PII extraction attacks on LLMs, revealing vulnerabilities and offering insights into effective mitigation strategies.", "motivation": "To evaluate and understand the effectiveness of PII extraction attacks on large language models (LLMs) and to establish a reliable benchmark for these evaluations.", "method": "Introduction of PII-Scope benchmark, experimentation with advanced adversarial strategies for PII extraction, and analysis of hyperparameters affecting attack outcomes.", "result": "Demonstrated significant underestimation of PII leakage in single-query attacks, with rates increasing fivefold under sophisticated adversarial approaches targeting pretrained models.", "conclusion": "The study establishes a rigorous benchmark for assessing PII extraction attacks and highlights the increased vulnerability of finetuned models.", "key_contributions": ["Introduction of the PII-Scope benchmark for PII extraction attacks", "Identification of key hyperparameters affecting PII extraction effectiveness", "Empirical demonstration of PII leakage increase under advanced adversarial strategies"], "limitations": "None mentioned; potential limitations could include the scope of models evaluated or the specific threat scenarios considered.", "keywords": ["PII extraction", "benchmark", "LLMs", "adversarial attacks", "privacy"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.07145", "pdf": "https://arxiv.org/pdf/2410.07145.pdf", "abs": "https://arxiv.org/abs/2410.07145", "title": "Stuffed Mamba: Oversized States Lead to the Inability to Forget", "authors": ["Yingfa Chen", "Xinrong Zhang", "Shengding Hu", "Xu Han", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "20 pages, 18 figures", "summary": "Recent advancements in recurrent architectures, such as Mamba and RWKV, have\nshowcased strong language capabilities. Unlike transformer-based models, these\narchitectures encode all contextual information into a fixed-size state,\nleading to great inference efficiency. However, this approach can cause\ninformation interference, where different token data conflicts, resulting in\nperformance degradation and incoherent outputs beyond a certain context length.\nTo prevent this, most RNNs incorporate mechanisms designed to \"forget\" earlier\ntokens. In this paper, we reveal that Mamba-based models struggle to\neffectively forget earlier tokens even with built-in forgetting mechanisms. We\ndemonstrate that this issue stems from training on contexts that are too short\nfor the state size, enabling the model to perform well without needing to learn\nhow to forget. Then, we show that the minimum training length required for the\nmodel to learn forgetting scales linearly with the state size, and the maximum\ncontext length for accurate retrieval of a 5-digit passkey scales exponentially\nwith the state size, indicating that the model retains some information beyond\nthe point where forgetting begins. These findings highlight a critical\nlimitation in current RNN architectures and provide valuable insights for\nimproving long-context modeling. Our work suggests that future RNN designs must\naccount for the interplay between state size, training length, and forgetting\nmechanisms to achieve robust performance in long-context tasks.", "AI": {"tldr": "This paper exposes limitations in Mamba-based RNNs regarding their ability to forget early tokens, revealing that training on short contexts hampers their learning of forgetting mechanisms.", "motivation": "To address performance degradation in recurrent neural networks (RNNs) related to forgetting earlier tokens when encoding long contexts.", "method": "Analyzed Mamba-based models to understand their token forgetting mechanisms and the relationship between training length, state size, and context retention.", "result": "Demonstrated that effective token forgetting is hindered by training on insufficiently short contexts and identified linear and exponential scaling of training length and context length with state size.", "conclusion": "Future RNN designs should consider the balance between state size, training context length, and forgetting capabilities to enhance their performance in long-context tasks.", "key_contributions": ["Revealed that Mamba models struggle with forgetting mechanisms during long-context tasks.", "Identified the relationship between training length, state size, and forgetting in RNNs.", "Provided insights for improving design strategies in RNN architectures for better long-context handling."], "limitations": "Focused on specific architectures and context lengths; applicability to other architectures not discussed.", "keywords": ["Recurrent Neural Networks", "Mamba", "Context Length", "Forgetting Mechanisms", "Long-context Modeling"], "importance_score": 5, "read_time_minutes": 20}}
{"id": "2410.10700", "pdf": "https://arxiv.org/pdf/2410.10700.pdf", "abs": "https://arxiv.org/abs/2410.10700", "title": "LLMs know their vulnerabilities: Uncover Safety Gaps through Natural Distribution Shifts", "authors": ["Qibing Ren", "Hao Li", "Dongrui Liu", "Zhanxu Xie", "Xiaoya Lu", "Yu Qiao", "Lei Sha", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference. Code is available at\n  https://github.com/AI45Lab/ActorAttack", "summary": "Safety concerns in large language models (LLMs) have gained significant\nattention due to their exposure to potentially harmful data during\npre-training. In this paper, we identify a new safety vulnerability in LLMs:\ntheir susceptibility to \\textit{natural distribution shifts} between attack\nprompts and original toxic prompts, where seemingly benign prompts,\nsemantically related to harmful content, can bypass safety mechanisms. To\nexplore this issue, we introduce a novel attack method, \\textit{ActorBreaker},\nwhich identifies actors related to toxic prompts within pre-training\ndistribution to craft multi-turn prompts that gradually lead LLMs to reveal\nunsafe content. ActorBreaker is grounded in Latour's actor-network theory,\nencompassing both human and non-human actors to capture a broader range of\nvulnerabilities. Our experimental results demonstrate that ActorBreaker\noutperforms existing attack methods in terms of diversity, effectiveness, and\nefficiency across aligned LLMs. To address this vulnerability, we propose\nexpanding safety training to cover a broader semantic space of toxic content.\nWe thus construct a multi-turn safety dataset using ActorBreaker. Fine-tuning\nmodels on our dataset shows significant improvements in robustness, though with\nsome trade-offs in utility. Code is available at\nhttps://github.com/AI45Lab/ActorAttack.", "AI": {"tldr": "This paper identifies a new safety vulnerability in large language models (LLMs) due to natural distribution shifts and proposes a novel attack method called ActorBreaker to exploit this vulnerability.", "motivation": "To address safety concerns arising from large language models (LLMs) exposed to harmful data during pre-training, focusing on their vulnerability to natural distribution shifts.", "method": "Introduction of ActorBreaker, an attack method that identifies relationships with toxic prompts to craft multi-turn prompts that reveal unsafe content in LLMs, grounded in Latour's actor-network theory.", "result": "ActorBreaker outperforms existing attack methods in diversity, effectiveness, and efficiency, leading to significant enhancements in the robustness of LLMs when fine-tuned on the developed multi-turn safety dataset.", "conclusion": "Proposes expanding safety training to cover a broader semantic space of toxic content, demonstrating improvements in model robustness with some trade-offs in utility.", "key_contributions": ["Introduction of ActorBreaker attack method", "Identification of natural distribution shifts as a safety vulnerability", "Development of a multi-turn safety dataset for fine-tuning LLMs"], "limitations": "Some trade-offs between robustness and utility in fine-tuned models.", "keywords": ["Safety in LLMs", "Natural distribution shifts", "ActorBreaker"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.12323", "pdf": "https://arxiv.org/pdf/2410.12323.pdf", "abs": "https://arxiv.org/abs/2410.12323", "title": "Reversal of Thought: Enhancing Large Language Models with Preference-Guided Reverse Reasoning Warm-up", "authors": ["Jiahao Yuan", "Dehui Du", "Hao Zhang", "Zixiang Di", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) have shown remarkable performance in reasoning\ntasks but face limitations in mathematical and complex logical reasoning.\nExisting methods to improve LLMs' logical capabilities either involve traceable\nor verifiable logical sequences that generate more reliable responses by\nconstructing logical structures yet increase computational costs, or introduces\nrigid logic template rules, reducing flexibility. In this paper, we propose\nReversal of Thought (RoT), a plug-and-play and cost-effective reasoning\nframework designed to enhance the logical reasoning abilities of LLMs during\nthe warm-up phase prior to batch inference. RoT utilizes a Preference-Guided\nReverse Reasoning warm-up strategy, which integrates logical symbols for\npseudocode planning through meta-cognitive mechanisms and pairwise preference\nself-evaluation to generate task-specific prompts solely through\ndemonstrations, aligning with LLMs' cognitive preferences shaped by RLHF.\nThrough reverse reasoning, we utilize a Cognitive Preference Manager to assess\nknowledge boundaries and further expand LLMs' reasoning capabilities by\naggregating solution logic for known tasks and stylistic templates for unknown\ntasks. Experiments across various tasks demonstrate that RoT surpasses existing\nbaselines in both reasoning accuracy and efficiency.", "AI": {"tldr": "This paper introduces Reversal of Thought (RoT), a framework aimed at enhancing the logical reasoning abilities of large language models (LLMs) during the warm-up phase before batch inference, achieving better reasoning accuracy and efficiency.", "motivation": "To address the limitations of large language models in mathematical and complex logical reasoning by providing a cost-effective improvement framework.", "method": "RoT employs a Preference-Guided Reverse Reasoning strategy that uses logical symbols for pseudocode planning and meta-cognitive mechanisms for generating task-specific prompts through demonstrations, improving LLM cognitive preferences shaped by RLHF.", "result": "RoT shows improved reasoning accuracy and efficiency in various tasks compared to existing baselines.", "conclusion": "RoT outperforms traditional methods by avoiding increased computational costs and rigidity while enhancing LLM reasoning capabilities.", "key_contributions": ["Introduction of a new framework (RoT) to improve LLM reasoning capabilities.", "Use of Preference-Guided Reverse Reasoning for task-specific prompt generation.", "Demonstrated efficacy across various reasoning tasks compared to existing methods."], "limitations": "", "keywords": ["large language models", "logical reasoning", "cognitive preferences", "reverse reasoning", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.12428", "pdf": "https://arxiv.org/pdf/2410.12428.pdf", "abs": "https://arxiv.org/abs/2410.12428", "title": "Conformity in Large Language Models", "authors": ["Xiaochen Zhu", "Caiqi Zhang", "Tom Stafford", "Nigel Collier", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages (main body), 9 figures (main body), ACL 2025 Main", "summary": "The conformity effect describes the tendency of individuals to align their\nresponses with the majority. Studying this bias in large language models (LLMs)\nis crucial, as LLMs are increasingly used in various information-seeking and\ndecision-making tasks as conversation partners to improve productivity. Thus,\nconformity to incorrect responses can compromise their effectiveness. In this\npaper, we adapt psychological experiments to examine the extent of conformity\nin popular LLMs. Our findings reveal that all tested models exhibit varying\nlevels of conformity toward the majority, regardless of their initial choice or\ncorrectness, across different knowledge domains. Notably, we are the first to\nshow that LLMs are more likely to conform when they are more uncertain in their\nown prediction. We further explore factors that influence conformity, such as\ntraining paradigms and input characteristics, finding that instruction-tuned\nmodels are less susceptible to conformity, while increasing the naturalness of\nmajority tones amplifies conformity. Finally, we propose two interventions,\nDevil's Advocate and Question Distillation, to mitigate conformity, providing\ninsights into building more robust language models.", "AI": {"tldr": "This paper investigates conformity effects in large language models (LLMs) and offers strategies to mitigate this bias.", "motivation": "Understanding the conformity effect is essential as LLMs are increasingly utilized in decision-making tasks, where biased responses could impact productivity.", "method": "Psychological experiments adapted to evaluate the conformity of various LLMs, measuring their responses to majority opinions across different knowledge domains.", "result": "All tested LLMs demonstrated a tendency to conform to majority responses, especially when uncertain; instruction-tuned models showed less conformity.", "conclusion": "Interventions like Devil's Advocate and Question Distillation can help create more robust LLMs by reducing conformity bias.", "key_contributions": ["First to demonstrate LLMs' conformity effects under uncertainty.", "Explored factors influencing conformity, such as training paradigms.", "Proposed interventions to mitigate conformity in LLMs."], "limitations": "Does not extensively cover the impact of LLMs in real-world applications beyond the experimental context.", "keywords": ["conformity effect", "large language models", "decision-making", "psychological experiments", "language model robustness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.13553", "pdf": "https://arxiv.org/pdf/2410.13553.pdf", "abs": "https://arxiv.org/abs/2410.13553", "title": "SynapticRAG: Enhancing Temporal Memory Retrieval in Large Language Models through Synaptic Mechanisms", "authors": ["Yuki Hou", "Haruki Tamoto", "Qinghua Zhao", "Homei Miyashita"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Existing retrieval methods in Large Language Models show degradation in\naccuracy when handling temporally distributed conversations, primarily due to\ntheir reliance on simple similarity-based retrieval. Unlike existing memory\nretrieval methods that rely solely on semantic similarity, we propose\nSynapticRAG, which uniquely combines temporal association triggers with\nbiologically-inspired synaptic propagation mechanisms. Our approach uses\ntemporal association triggers and synaptic-like stimulus propagation to\nidentify relevant dialogue histories. A dynamic leaky integrate-and-fire\nmechanism then selects the most contextually appropriate memories. Experiments\non four datasets of English, Chinese and Japanese show that compared to\nstate-of-the-art memory retrieval methods, SynapticRAG achieves consistent\nimprovements across multiple metrics up to 14.66% points. This work bridges the\ngap between cognitive science and language model development, providing a new\nframework for memory management in conversational systems.", "AI": {"tldr": "The paper introduces SynapticRAG, a novel memory retrieval method for Large Language Models that combines temporal association triggers with synaptic propagation, resulting in improved accuracy in handling temporally distributed conversations.", "motivation": "Existing retrieval methods struggle with accuracy in temporally distributed conversations, primarily due to their reliance on simple similarity-based retrieval.", "method": "SynapticRAG uniquely combines temporal association triggers with biologically-inspired synaptic propagation mechanisms to manage memory retrieval in dialogue systems.", "result": "Experiments show SynapticRAG achieves consistent accuracy improvements over state-of-the-art memory retrieval methods by up to 14.66% points across four datasets in multiple languages.", "conclusion": "This work bridges cognitive science and language model development, offering a new framework for effective memory management in conversational systems.", "key_contributions": ["Introduction of SynapticRAG for improved memory retrieval", "Combination of temporal triggers with synaptic mechanisms", "Demonstrated significant performance improvements across multiple languages and datasets"], "limitations": "", "keywords": ["memory retrieval", "language models", "temporal association", "cognitive science", "conversational systems"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2411.00204", "pdf": "https://arxiv.org/pdf/2411.00204.pdf", "abs": "https://arxiv.org/abs/2411.00204", "title": "RESTOR: Knowledge Recovery in Machine Unlearning", "authors": ["Keivan Rezaei", "Khyathi Chandu", "Soheil Feizi", "Yejin Choi", "Faeze Brahman", "Abhilasha Ravichander"], "categories": ["cs.CL"], "comment": "Accepted to TMLR 2025", "summary": "Large language models trained on web-scale corpora can memorize undesirable\ndata containing misinformation, copyrighted material, or private or sensitive\ninformation. Recently, several machine unlearning algorithms have been proposed\nto eliminate the effect of such datapoints from trained models -- that is, to\napproximate a model that had never been trained on these datapoints in the\nfirst place. However, evaluating the effectiveness of unlearning algorithms\nremains an open challenge. Previous work has relied on heuristics -- such as\nverifying that the model can no longer reproduce the specific information\ntargeted for removal while maintaining accuracy on unrelated test data. These\napproaches inadequately capture the complete effect of reversing the influence\nof datapoints on a trained model. In this work, we propose the RESTOR framework\nfor machine unlearning evaluation, which assesses the ability of unlearning\nalgorithms for targeted data erasure, by evaluating the ability of models to\nforget the knowledge introduced in these datapoints, while simultaneously\nrecovering the model's knowledge state had it never encountered these\ndatapoints. RESTOR helps uncover several novel insights about popular\nunlearning algorithms, and the mechanisms through which they operate -- for\ninstance, identifying that some algorithms merely emphasize forgetting but not\nrecovering knowledge, and that localizing unlearning targets can enhance\nunlearning performance.", "AI": {"tldr": "The paper introduces the RESTOR framework for evaluating machine unlearning algorithms, focusing on effective data erasure and knowledge recovery.", "motivation": "To address the challenge of evaluating existing machine unlearning algorithms, ensuring they can effectively erase specific data points while recovering lost knowledge.", "method": "The RESTOR framework assesses unlearning by evaluating models' ability to forget specific datapoints and recover their knowledge state as if those datapoints were never encountered.", "result": "RESTOR reveals insights about popular unlearning algorithms, showing that some focus on forgetting without recovery, and identifies that localizing unlearning targets can improve performance.", "conclusion": "The framework provides a more robust evaluation of unlearning algorithms, contributing to understanding their effectiveness in data erasure.", "key_contributions": ["Introduction of the RESTOR framework for unlearning evaluation", "Insights into popular unlearning algorithms' mechanisms", "Identification of recovery mechanisms and improvements through target localization"], "limitations": "", "keywords": ["machine unlearning", "large language models", "data erasure"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2411.02083", "pdf": "https://arxiv.org/pdf/2411.02083.pdf", "abs": "https://arxiv.org/abs/2411.02083", "title": "Regress, Don't Guess -- A Regression-like Loss on Number Tokens for Language Models", "authors": ["Jonas Zausinger", "Lars Pennig", "Anamarija Kozina", "Sean Sdahl", "Julian Sikora", "Adrian Dendorfer", "Timofey Kuznetsov", "Mohamad Hagog", "Nina Wiedemann", "Kacper Chlodny", "Vincent Limbach", "Anna Ketteler", "Thorben Prein", "Vishwa Mohan Singh", "Michael Morris Danziger", "Jannis Born"], "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": "ICML 2025", "summary": "While language models have exceptional capabilities at text generation, they\nlack a natural inductive bias for emitting numbers and thus struggle in tasks\ninvolving quantitative reasoning, especially arithmetic. One fundamental\nlimitation is the nature of the Cross Entropy loss, which assumes a nominal\nscale and thus cannot convey proximity between generated number tokens. In\nresponse, we here present a regression-like loss that operates purely on token\nlevel. Our proposed Number Token Loss (NTL) comes in two flavors and minimizes\neither the Lp norm or the Wasserstein distance between the numerical values of\nthe real and predicted number tokens. NTL can easily be added to any language\nmodel and extend the Cross Entropy objective during training without runtime\noverhead. We evaluate the proposed scheme on various mathematical datasets and\nfind that it consistently improves performance in math-related tasks. In a\ndirect comparison on a regression task, we find that NTL can match the\nperformance of a regression head, despite operating on token level. Finally, we\nscale NTL up to 3B parameter models and observe improved performance,\ndemonstrating its potential for seamless integration into LLMs. We hope that\nthis work can inspire LLM developers to improve their pretraining objectives.\nThe code is available via: https://tum-ai.github.io/number-token-loss/", "AI": {"tldr": "This paper introduces a new loss function, Number Token Loss (NTL), designed to improve numerical reasoning in language models by directly minimizing the distance between predicted and actual numerical values.", "motivation": "To address the limitations of language models in quantitative reasoning tasks, particularly those involving arithmetic and numerical outputs.", "method": "NTL is a regression-like loss that focuses on token level, minimizing either the Lp norm or the Wasserstein distance between real and predicted number tokens, which can be easily integrated into existing models.", "result": "The proposed NTL improves performance on various mathematical datasets and can match regression head performance in a direct comparison while being less resource-intensive.", "conclusion": "NTL can enhance the training objectives of large language models by providing a better framework for numerical prediction, encouraging its adoption by LLM developers.", "key_contributions": ["Introduction of Number Token Loss (NTL) for language models", "Proven effectiveness on mathematical tasks", "Compatibility with large-scale models (up to 3B parameters)"], "limitations": "", "keywords": ["Number Token Loss", "Quantitative Reasoning", "Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.02391", "pdf": "https://arxiv.org/pdf/2411.02391.pdf", "abs": "https://arxiv.org/abs/2411.02391", "title": "Attacking Vision-Language Computer Agents via Pop-ups", "authors": ["Yanzhe Zhang", "Tao Yu", "Diyi Yang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Autonomous agents powered by large vision and language models (VLM) have\ndemonstrated significant potential in completing daily computer tasks, such as\nbrowsing the web to book travel and operating desktop software, which requires\nagents to understand these interfaces. Despite such visual inputs becoming more\nintegrated into agentic applications, what types of risks and attacks exist\naround them still remain unclear. In this work, we demonstrate that VLM agents\ncan be easily attacked by a set of carefully designed adversarial pop-ups,\nwhich human users would typically recognize and ignore. This distraction leads\nagents to click these pop-ups instead of performing their tasks as usual.\nIntegrating these pop-ups into existing agent testing environments like OSWorld\nand VisualWebArena leads to an attack success rate (the frequency of the agent\nclicking the pop-ups) of 86% on average and decreases the task success rate by\n47%. Basic defense techniques, such as asking the agent to ignore pop-ups or\nincluding an advertisement notice, are ineffective against the attack.", "AI": {"tldr": "This paper investigates adversarial attacks on autonomous agents powered by vision and language models, demonstrating how pop-up distractions can significantly impair their performance.", "motivation": "To explore vulnerabilities in VLM agents that can be exploited through adversarial pop-ups, affecting their task performance.", "method": "The authors integrated adversarial pop-ups into agent testing environments like OSWorld and VisualWebArena to evaluate their impact on agent performance.", "result": "The study found an attack success rate of 86%, where agents clicked pop-ups instead of completing tasks, leading to a 47% decrease in task success rate.", "conclusion": "The paper concludes that basic defense techniques are ineffective against these adversarial attacks, highlighting the need for more robust solutions.", "key_contributions": ["First to identify the vulnerability of VLM agents to adversarial pop-ups", "Quantifies the impact of these attacks on agent performance", "Evaluates commonly used defense techniques against these attacks."], "limitations": "The study primarily focuses on a specific type of adversarial attack and may not account for other types of potential vulnerabilities in VLM agents.", "keywords": ["autonomous agents", "adversarial pop-ups", "vision and language models", "task performance", "defense techniques"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2411.02937", "pdf": "https://arxiv.org/pdf/2411.02937.pdf", "abs": "https://arxiv.org/abs/2411.02937", "title": "Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent", "authors": ["Yangning Li", "Yinghui Li", "Xinyu Wang", "Yong Jiang", "Zhen Zhang", "Xinran Zheng", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu", "Fei Huang", "Jingren Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval Augmented Generation (mRAG) plays an important role in\nmitigating the \"hallucination\" issue inherent in multimodal large language\nmodels (MLLMs). Although promising, existing heuristic mRAGs typically\npredefined fixed retrieval processes, which causes two issues: (1) Non-adaptive\nRetrieval Queries. (2) Overloaded Retrieval Queries. However, these flaws\ncannot be adequately reflected by current knowledge-seeking visual question\nanswering (VQA) datasets, since the most required knowledge can be readily\nobtained with a standard two-step retrieval. To bridge the dataset gap, we\nfirst construct Dyn-VQA dataset, consisting of three types of \"dynamic\"\nquestions, which require complex knowledge retrieval strategies variable in\nquery, tool, and time: (1) Questions with rapidly changing answers. (2)\nQuestions requiring multi-modal knowledge. (3) Multi-hop questions. Experiments\non Dyn-VQA reveal that existing heuristic mRAGs struggle to provide sufficient\nand precisely relevant knowledge for dynamic questions due to their rigid\nretrieval processes. Hence, we further propose the first self-adaptive planning\nagent for multimodal retrieval, OmniSearch. The underlying idea is to emulate\nthe human behavior in question solution which dynamically decomposes complex\nmultimodal questions into sub-question chains with retrieval action. Extensive\nexperiments prove the effectiveness of our OmniSearch, also provide direction\nfor advancing mRAG. The code and dataset will be open-sourced at\nhttps://github.com/Alibaba-NLP/OmniSearch.", "AI": {"tldr": "This paper introduces a novel Multimodal Retrieval Augmented Generation (mRAG) approach to address the hallucination problem in multimodal large language models (MLLMs) by proposing a self-adaptive planning agent, OmniSearch, and a new dynamic VQA dataset, Dyn-VQA.", "motivation": "To address the limitations of existing heuristic mRAGs in multimodal retrieval, specifically their non-adaptive and overloaded retrieval queries, which do not adequately handle complex, dynamic, and multi-modal questions.", "method": "The authors constructed the Dyn-VQA dataset to include dynamic questions that require innovative retrieval strategies. They introduced OmniSearch, a self-adaptive planning agent that decomposes complex multimodal questions into manageable sub-questions for retrieval.", "result": "Experiments demonstrated that existing heuristic mRAGs perform poorly on dynamic questions. In contrast, OmniSearch effectively addressed these challenges by dynamically adjusting the retrieval process based on the question's requirements.", "conclusion": "The proposed OmniSearch significantly improves the handling of complex multimodal questions and sets a new direction for enhancing mRAG methodologies. The code and dataset will be shared publicly to facilitate further research.", "key_contributions": ["Introduction of the Dyn-VQA dataset for dynamic question retrieval", "Development of the OmniSearch self-adaptive planning agent", "Demonstration of improved performance over heuristic mRAGs for multimodal question answering"], "limitations": "", "keywords": ["Multimodal Retrieval", "Dynamic VQA", "Self-Adaptive Planning", "OmniSearch", "Multimodal Knowledge Retrieval"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2411.07237", "pdf": "https://arxiv.org/pdf/2411.07237.pdf", "abs": "https://arxiv.org/abs/2411.07237", "title": "Contextualized Evaluations: Judging Language Model Responses to Underspecified Queries", "authors": ["Chaitanya Malaviya", "Joseph Chee Chang", "Dan Roth", "Mohit Iyyer", "Mark Yatskar", "Kyle Lo"], "categories": ["cs.CL"], "comment": "Accepted to TACL. Code & data available at\n  https://github.com/allenai/ContextEval", "summary": "Language model users often issue queries that lack specification, where the\ncontext under which a query was issued -- such as the user's identity, the\nquery's intent, and the criteria for a response to be useful -- is not\nexplicit. For instance, a good response to a subjective query like \"What book\nshould I read next?\" would depend on the user's preferences, and a good\nresponse to an open-ended query like \"How do antibiotics work against\nbacteria?\" would depend on the user's expertise. This makes evaluation of\nresponses to such queries an ill-posed task, as evaluators may make arbitrary\njudgments about the response quality. To remedy this, we present contextualized\nevaluations, a protocol that synthetically constructs context surrounding an\nunderspecified query and provides it during evaluation. We find that the\npresence of context can 1) alter conclusions drawn from evaluation, even\nflipping benchmark rankings between model pairs, 2) nudge evaluators to make\nfewer judgments based on surface-level criteria, like style, and 3) provide new\ninsights about model behavior across diverse contexts. Specifically, our\nprocedure suggests a potential bias towards WEIRD (Western, Educated,\nIndustrialized, Rich and Democratic) contexts in models' \"default\" responses\nand we find that models are not equally sensitive to following different\ncontexts, even when they are provided in prompts.", "AI": {"tldr": "This paper introduces a new evaluation protocol called contextualized evaluations for assessing language model responses to underspecified queries by simulating user context, significantly impacting evaluation outcomes and revealing biases in model behaviors.", "motivation": "The need arises from the challenge of evaluating language model responses to queries that are not well-defined, where the user context, intent, and criteria for useful answers are often not clear.", "method": "The authors develop the contextualized evaluations protocol which constructs synthetic context around underspecified queries to guide evaluators during the assessment process.", "result": "The study demonstrates that including context can significantly change evaluation outcomes, affect the ranking of models, reduce reliance on superficial criteria, and expose biases towards certain user contexts in language model outputs.", "conclusion": "The findings suggest that better evaluation strategies can enhance our understanding of language models and highlight the inconsistencies in their responses based on user context.", "key_contributions": ["Introduction of contextualized evaluations protocol", "Revealed biases in language models towards WEIRD contexts", "Demonstrated influence of user context on evaluation outcomes"], "limitations": "", "keywords": ["Language Models", "Evaluation", "User Context", "Bias", "Contextualization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.07965", "pdf": "https://arxiv.org/pdf/2411.07965.pdf", "abs": "https://arxiv.org/abs/2411.07965", "title": "SHARP: Unlocking Interactive Hallucination via Stance Transfer in Role-Playing LLMs", "authors": ["Chuyi Kong", "Ziyang Luo", "Hongzhan Lin", "Zhiyuan Fan", "Yaxin Fan", "Yuxi Sun", "Jing Ma"], "categories": ["cs.CL"], "comment": "28 pages, unfortunately accepted to findings with Meta 4.. Apologize\n  for the reviewers and area chair who love our work, orz", "summary": "The advanced role-playing capabilities of Large Language Models (LLMs) have\nenabled rich interactive scenarios, yet existing research in social\ninteractions neglects hallucination while struggling with poor generalizability\nand implicit character fidelity judgments. To bridge this gap, motivated by\nhuman behaviour, we introduce a generalizable and explicit paradigm for\nuncovering interactive patterns of LLMs across diverse worldviews.\nSpecifically, we first define interactive hallucination through stance\ntransfer, then construct SHARP, a benchmark built by extracting relations from\ncommonsense knowledge graphs and utilizing LLMs' inherent hallucination\nproperties to simulate multi-role interactions. Extensive experiments confirm\nour paradigm's effectiveness and stability, examine the factors that influence\nthese metrics, and challenge conventional hallucination mitigation solutions.\nMore broadly, our work reveals a fundamental limitation in popular\npost-training methods for role-playing LLMs: the tendency to obscure knowledge\nbeneath style, resulting in monotonous yet human-like behaviors - interactive\nhallucination.", "AI": {"tldr": "This paper introduces SHARP, a paradigm to explore interactive hallucination in LLMs through stance transfer and benchmarking based on commonsense knowledge.", "motivation": "To address the shortcomings in existing research on social interactions of LLMs, particularly in relation to hallucination and character fidelity.", "method": "The paper defines interactive hallucination and constructs the SHARP benchmark using relations from knowledge graphs to simulate multi-role interactions.", "result": "Extensive experiments validate the effectiveness and stability of the SHARP paradigm while identifying influencing factors and challenging existing hallucination mitigation strategies.", "conclusion": "The findings reveal significant limitations in post-training methods for role-playing LLMs, highlighting a propensity for these models to prioritize style over knowledge.", "key_contributions": ["Introduction of the SHARP benchmark for interactive role-playing in LLMs", "Defining interactive hallucination through stance transfer", "Challenging existing hallucination mitigation methods."], "limitations": "Existing post-training methods can conceal knowledge under stylistic behavior, leading to less informative outputs.", "keywords": ["Large Language Models", "interactive hallucination", "role-playing", "benchmark", "commonsense knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.10533", "pdf": "https://arxiv.org/pdf/2411.10533.pdf", "abs": "https://arxiv.org/abs/2411.10533", "title": "On the Compatibility of Generative AI and Generative Linguistics", "authors": ["Eva Portelance", "Masoud Jasbi"], "categories": ["cs.CL", "F.1.1; I.2.7; I.2.6"], "comment": null, "summary": "In mid-20th century, the linguist Noam Chomsky established generative\nlinguistics, and made significant contributions to linguistics, computer\nscience, and cognitive science by developing the computational and\nphilosophical foundations for a theory that defined language as a formal\nsystem, instantiated in human minds or artificial machines. These developments\nin turn ushered a wave of research on symbolic Artificial Intelligence (AI).\nMore recently, a new wave of non-symbolic AI has emerged with neural Language\nModels (LMs) that exhibit impressive linguistic performance, leading many to\nquestion the older approach and wonder about the the compatibility of\ngenerative AI and generative linguistics. In this paper, we argue that\ngenerative AI is compatible with generative linguistics and reinforces its\nbasic tenets in at least three ways. First, we argue that LMs are formal\ngenerative models as intended originally in Chomsky's work on formal language\ntheory. Second, LMs can help develop a program for discovery procedures as\ndefined by Chomsky's \"Syntactic Structures\". Third, LMs can be a major asset\nfor Chomsky's minimalist approach to Universal Grammar and language\nacquisition. In turn, generative linguistics can provide the foundation for\nevaluating and improving LMs as well as other generative computational models\nof language.", "AI": {"tldr": "This paper discusses the compatibility of generative AI with generative linguistics, arguing that language models (LMs) align with and can enhance foundational theories established by Noam Chomsky.", "motivation": "To explore the relationship between generative AI and generative linguistics, particularly in light of advances in neural language models.", "method": "The paper presents a theoretical argument supporting the compatibility of LMs with generative linguistics, discussing their roles in language modeling, discovery procedures, and Universal Grammar.", "result": "The authors find that generative AI reinforces Chomsky's principles by demonstrating that LMs are formal generative models, can inform discovery procedures, and support the minimalist approach to Universal Grammar.", "conclusion": "Generative linguistics can serve as a foundation for assessing and improving LMs, suggesting a symbiotic relationship between the two fields.", "key_contributions": ["Demonstration of LMs as formal generative models per Chomsky's theory", "Development of discovery procedures relevant to generative linguistics", "Support for the minimalist approach to Universal Grammar"], "limitations": "", "keywords": ["generative linguistics", "language models", "AI", "Chomsky", "Universal Grammar"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2411.15821", "pdf": "https://arxiv.org/pdf/2411.15821.pdf", "abs": "https://arxiv.org/abs/2411.15821", "title": "Is Training Data Quality or Quantity More Impactful to Small Language Model Performance?", "authors": ["Aryan Sajith", "Krishna Chaitanya Rao Kathala"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures, 4 tables | Conference: International Conference\n  on Neural Computing for Advanced Applications 2025, Conference info:\n  https://aaci.org.hk/ncaa2025", "summary": "This study investigates the relative impact of training data quality versus\nquantity on the performance of small language models (SLMs), utilizing the\nTinyStories dataset for empirical analysis. Analysis of dataset variations with\nrespect to size (25% and 50% of the original size) and duplication (controlled\nrates of 25%, 50%, 75%, and 100%) were performed. Model performance was\nevaluated based on the validation loss, accuracy, and perplexity metrics.\nResults indicate training data quality plays a more significant role in the\noverall performance of SLMs, especially given scale of this experiment. Minimal\nduplication positively impacted model accuracy (+0.87% increase in accuracy at\n25% duplication) without significantly increasing perplexity (+0.52% increase\ngoing from 0% to 25% duplication) but excessive duplication led to pronounced\nperformance degradation (-40% drop in accuracy at 100% duplication). The\nimplications of this exploration extend beyond just model performance; training\nlarge-scale models imposes significant financial and computational burdens,\nwhich can be prohibitive for organizations, individuals, and the public at\nlarge, especially in developing countries. Additionally, the energy consumption\nassociated with large-scale training raises environmental concerns.\nUnderstanding the relative importance of data quality versus quantity could\ndemocratize AI technology, making advanced models more accessible and\nsustainable for all.", "AI": {"tldr": "The study evaluates the impact of training data quality versus quantity on small language models, revealing that data quality is more critical for performance, and excessive duplication harms accuracy.", "motivation": "To investigate how training data quality and quantity influence the performance of small language models and assess implications for accessibility and sustainability.", "method": "Empirical analysis using the TinyStories dataset, varying dataset size and duplication rates, with performance measured through validation loss, accuracy, and perplexity.", "result": "Training data quality significantly affects SLM performance; minimal duplication improves accuracy while excessive duplication severely degrades it.", "conclusion": "The findings indicate that prioritizing data quality over quantity can reduce costs and environmental impact, making advanced AI more accessible.", "key_contributions": ["Demonstrated the superiority of training data quality over quantity for SLMs", "Provided empirical data on the effects of data duplication rates on model performance", "Highlighted the implications for accessibility and sustainability in AI training"], "limitations": "The study is limited to small language models and specific dataset variations; results may not generalize to larger models or other datasets.", "keywords": ["small language models", "training data quality", "data duplication", "AI accessibility", "environmental impact"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.17679", "pdf": "https://arxiv.org/pdf/2411.17679.pdf", "abs": "https://arxiv.org/abs/2411.17679", "title": "Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning", "authors": ["Zhu Xu", "Zhiqiang Zhao", "Zihan Zhang", "Yuchi Liu", "Quanwei Shen", "Fei Liu", "Yu Kuang", "Jian He", "Conglin Liu"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Tokenization methods like Byte-Pair Encoding (BPE) enhance computational\nefficiency in large language models (LLMs) but often obscure internal character\nstructures within tokens. This limitation hinders LLMs' ability to predict\nprecise character positions, which is crucial in tasks like Chinese Spelling\nCorrection (CSC) where identifying the positions of misspelled characters\naccelerates correction processes. We propose Token Internal Position Awareness\n(TIPA), a method that significantly improves models' ability to capture\ncharacter positions within tokens by training them on reverse character\nprediction tasks using the tokenizer's vocabulary. Experiments demonstrate that\nTIPA enhances position prediction accuracy in LLMs, enabling more precise\nidentification of target characters in original text. Furthermore, when applied\nto downstream tasks that do not require exact position prediction, TIPA still\nboosts performance in tasks needing character-level information, validating its\nversatility and effectiveness.", "AI": {"tldr": "This paper introduces Token Internal Position Awareness (TIPA), enhancing LLMs' ability to predict character positions, particularly improving tasks like Chinese Spelling Correction.", "motivation": "Existing tokenization methods obscure internal character structures, limiting LLMs' precision in tasks requiring accurate character positioning.", "method": "The authors propose TIPA, which allows models to learn character positions by training on reverse character prediction tasks using the tokenizer's vocabulary.", "result": "TIPA significantly improves position prediction accuracy in LLMs and also enhances performance on downstream tasks that benefit from character-level information.", "conclusion": "The versatility and effectiveness of TIPA were validated by its positive impact, even in tasks that do not require exact position prediction.", "key_contributions": ["Introduction of TIPA for character position awareness in LLMs.", "Application of reverse character prediction tasks to enhance model training.", "Demonstration of improved accuracy in character-level tasks like Chinese Spelling Correction."], "limitations": "", "keywords": ["Tokenization", "Large Language Models", "Character Position Awareness", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2411.18337", "pdf": "https://arxiv.org/pdf/2411.18337.pdf", "abs": "https://arxiv.org/abs/2411.18337", "title": "Can LLMs assist with Ambiguity? A Quantitative Evaluation of various Large Language Models on Word Sense Disambiguation", "authors": ["T. G. D. K. Sumanathilaka", "Nicholas Micallef", "Julian Hough"], "categories": ["cs.CL"], "comment": "12 pages,6 tables, 1 figure, Proceedings of the 1st International\n  Conference on NLP & AI for Cyber Security", "summary": "Ambiguous words are often found in modern digital communications. Lexical\nambiguity challenges traditional Word Sense Disambiguation (WSD) methods, due\nto limited data. Consequently, the efficiency of translation, information\nretrieval, and question-answering systems is hindered by these limitations.\nThis study investigates the use of Large Language Models (LLMs) to improve WSD\nusing a novel approach combining a systematic prompt augmentation mechanism\nwith a knowledge base (KB) consisting of different sense interpretations. The\nproposed method incorporates a human-in-loop approach for prompt augmentation\nwhere prompt is supported by Part-of-Speech (POS) tagging, synonyms of\nambiguous words, aspect-based sense filtering and few-shot prompting to guide\nthe LLM. By utilizing a few-shot Chain of Thought (COT) prompting-based\napproach, this work demonstrates a substantial improvement in performance. The\nevaluation was conducted using FEWS test data and sense tags. This research\nadvances accurate word interpretation in social media and digital\ncommunication.", "AI": {"tldr": "This study explores the use of Large Language Models to improve Word Sense Disambiguation through a novel approach that combines prompt augmentation and a knowledge base.", "motivation": "Lexical ambiguity in digital communications challenges traditional WSD methods and affects systems like translation and information retrieval.", "method": "The study employs a novel prompt augmentation mechanism, incorporating POS tagging, synonyms, aspect-based filtering, and a few-shot Chain of Thought prompting approach.", "result": "Significant improvement in performance in WSD tasks using FEWS test data and sense tags.", "conclusion": "The research enhances accurate word interpretation in social media and digital communications through the proposed approach.", "key_contributions": ["Novel prompt augmentation mechanism for WSD", "Integration of knowledge base with different sense interpretations", "Human-in-loop approach for better LLM guidance"], "limitations": "", "keywords": ["Word Sense Disambiguation", "Large Language Models", "Lexical Ambiguity", "Natural Language Processing", "Prompt Augmentation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2412.02549", "pdf": "https://arxiv.org/pdf/2412.02549.pdf", "abs": "https://arxiv.org/abs/2412.02549", "title": "Patent-CR: A Dataset for Patent Claim Revision", "authors": ["Lekang Jiang", "Pascal A Scherz", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "Accepted to NAACL 2025. 15 pages, 6 tables, 3 figures", "summary": "This paper presents Patent-CR, the first dataset created for the patent claim\nrevision task in English. It includes both initial patent applications rejected\nby patent examiners and the final granted versions. Unlike normal text revision\ntasks that predominantly focus on enhancing sentence quality, such as grammar\ncorrection and coherence improvement, patent claim revision aims at ensuring\nthe claims meet stringent legal criteria. These criteria are beyond novelty and\ninventiveness, including clarity of scope, technical accuracy, language\nprecision, and legal robustness. We assess various large language models (LLMs)\nthrough professional human evaluation, including general LLMs with different\nsizes and architectures, text revision models, and domain-specific models. Our\nresults indicate that LLMs often bring ineffective edits that deviate from the\ntarget revisions. In addition, domain-specific models and the method of\nfine-tuning show promising results. Notably, GPT-4 outperforms other tested\nLLMs, but further revisions are still necessary to reach the examination\nstandard. Furthermore, we demonstrate the inconsistency between automated and\nhuman evaluation results, suggesting that GPT-4-based automated evaluation has\nthe highest correlation with human judgment. This dataset, along with our\npreliminary empirical research, offers invaluable insights for further\nexploration in patent claim revision.", "AI": {"tldr": "Patent-CR is the first dataset for patent claim revision, assessing various LLMs on their ability to improve rejected patent applications based on legal standards.", "motivation": "To ensure patent claims fulfill legal criteria beyond mere novelty and inventiveness, including clarity, precision, and robustness, by creating a dataset for the patent claim revision task.", "method": "The study evaluates various LLMs, including general, text revision, and domain-specific models, through human evaluations to compare effectiveness in revising patent claims.", "result": "LLMs frequently produce edits that do not effectively align with target revisions. Domain-specific models perform better, with GPT-4 showing the best results among tested models, though still not reaching full examination standards.", "conclusion": "The findings highlight discrepancies between automated and human evaluations in patent claim revisions, suggesting room for improvement in LLMs, particularly with fine-tuning for legal standards.", "key_contributions": ["Introduction of the Patent-CR dataset for patent claim revision.", "Assessment of LLMs tailored for legal criteria in patent contexts.", "Insights on the correlation between automated evaluations and human judgment."], "limitations": "Further revision is needed for LLM outputs to meet examination standards; the study is limited to English language patents.", "keywords": ["patent claims", "large language models", "dataset", "legal standards", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.02605", "pdf": "https://arxiv.org/pdf/2412.02605.pdf", "abs": "https://arxiv.org/abs/2412.02605", "title": "Interpretable Company Similarity with Sparse Autoencoders", "authors": ["Marco Molinari", "Victor Shao", "Luca Imeneo", "Mateusz Mikolajczak", "Vladimir Tregubiak", "Abhimanyu Pandey", "Sebastian Kuznetsov Ryder Torres Pereira"], "categories": ["cs.CL", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Determining company similarity is a vital task in finance, underpinning risk\nmanagement, hedging, and portfolio diversification. Practitioners often rely on\nsector and industry classifications such as SIC and GICS codes to gauge\nsimilarity, the former being used by the U.S. Securities and Exchange\nCommission (SEC), and the latter widely used by the investment community. Since\nthese classifications lack granularity and need regular updating, using\nclusters of embeddings of company descriptions has been proposed as a potential\nalternative, but the lack of interpretability in token embeddings poses a\nsignificant barrier to adoption in high-stakes contexts. Sparse Autoencoders\n(SAEs) have shown promise in enhancing the interpretability of Large Language\nModels (LLMs) by decomposing Large Language Model (LLM) activations into\ninterpretable features. Moreover, SAEs capture an LLM's internal representation\nof a company description, as opposed to semantic similarity alone, as is the\ncase with embeddings. We apply SAEs to company descriptions, and obtain\nmeaningful clusters of equities. We benchmark SAE features against SIC-codes,\nIndustry codes, and Embeddings. Our results demonstrate that SAE features\nsurpass sector classifications and embeddings in capturing fundamental company\ncharacteristics. This is evidenced by their superior performance in correlating\nlogged monthly returns - a proxy for similarity - and generating higher Sharpe\nratios in co-integration trading strategies, which underscores deeper\nfundamental similarities among companies. Finally, we verify the\ninterpretability of our clusters, and demonstrate that sparse features form\nsimple and interpretable explanations for our clusters.", "AI": {"tldr": "This paper explores the use of Sparse Autoencoders (SAEs) for enhancing the interpretability of Large Language Models in determining company similarity, showing that SAEs outperform traditional classification methods and embeddings in financial applications.", "motivation": "The paper addresses the limitations of traditional sector and industry classifications in assessing company similarity, which are crucial in finance for risk management and portfolio diversification.", "method": "The authors apply Sparse Autoencoders (SAEs) to clusters of company descriptions to extract interpretable features of companies and benchmark these features against traditional classifications and embeddings.", "result": "The results indicate that SAE features outperform SIC codes, industry classifications, and embeddings in capturing fundamental characteristics of companies as shown by superior correlation with logged monthly returns and higher Sharpe ratios in trading strategies.", "conclusion": "The study concludes that SAEs provide deeper insights into company similarities and enhance interpretability, making them a valuable tool for financial analysis.", "key_contributions": ["Introduction of Sparse Autoencoders for interpreting LLM activations in finance", "Demonstration of SAEs as superior to traditional sector classifications and embeddings", "Validation of the interpretability of features generated by SAEs."], "limitations": "", "keywords": ["Sparse Autoencoders", "Company Similarity", "Large Language Models", "Financial Analysis", "Interpretability"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2412.07282", "pdf": "https://arxiv.org/pdf/2412.07282.pdf", "abs": "https://arxiv.org/abs/2412.07282", "title": "HARP: Hesitation-Aware Reframing in Transformer Inference Pass", "authors": ["Romain Storaï", "Seung-won Hwang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to NAACL 2025 main (long)", "summary": "This paper aims to improve the performance of large language models by\naddressing the variable computational demands in inference steps, where some\ntokens require more computational resources than others. We present HARP, a\nsimple modification to \"off-the-shelf\" Transformer forward pass. Drawing from\nhesitation and the framing effect in decision-making, HARP selectively applies\nadditional computation when the model encounters uncertainty during token\ngeneration. Our method mimics human cognitive processes by pausing at difficult\ndecision points and reframing inputs for a different perspective. Unlike other\napproaches, HARP is model-agnostic, training-free, and easy to implement. We\nevaluate our method across various downstream tasks and model sizes,\ndemonstrating performance improvements up to +5.16%. Notably, HARP achieves\nthese gains while maintaining inference times twice faster than beam search.\nSimple and yet with significant gains, HARP provides insights into the\npotential of adaptive computation for enhancing the performance of\nTransformer-based language models.", "AI": {"tldr": "Introducing HARP, a model-agnostic method to improve inference performance in Transformers by selectively applying additional computation during token generation when uncertainty is detected.", "motivation": "To enhance the performance of large language models by addressing variable computational demands during inference steps.", "method": "HARP selectively applies additional computation at points of uncertainty in token generation, inspired by human cognitive processes like hesitation and reframing.", "result": "HARP shows performance improvements of up to +5.16% on various downstream tasks while maintaining inference times twice as fast as beam search.", "conclusion": "HARP offers significant gains with adaptive computation, providing insights into improving Transformer-based language models.", "key_contributions": ["Model-agnostic framework", "Training-free implementation", "Performance improvement in inference speeds"], "limitations": "", "keywords": ["HARP", "Adaptive Computation", "Transformer Models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2412.09879", "pdf": "https://arxiv.org/pdf/2412.09879.pdf", "abs": "https://arxiv.org/abs/2412.09879", "title": "On the Limit of Language Models as Planning Formalizers", "authors": ["Cassie Huang", "Li Zhang"], "categories": ["cs.CL"], "comment": "In ACL 2025 main conference", "summary": "Large Language Models have been found to create plans that are neither\nexecutable nor verifiable in grounded environments. An emerging line of work\ndemonstrates success in using the LLM as a formalizer to generate a formal\nrepresentation of the planning domain in some language, such as Planning Domain\nDefinition Language (PDDL). This formal representation can be deterministically\nsolved to find a plan. We systematically evaluate this methodology while\nbridging some major gaps. While previous work only generates a partial PDDL\nrepresentation, given templated, and therefore unrealistic environment\ndescriptions, we generate the complete representation given descriptions of\nvarious naturalness levels. Among an array of observations critical to improve\nLLMs' formal planning abilities, we note that most large enough models can\neffectively formalize descriptions as PDDL, outperforming those directly\ngenerating plans, while being robust to lexical perturbation. As the\ndescriptions become more natural-sounding, we observe a decrease in performance\nand provide detailed error analysis.", "AI": {"tldr": "This paper evaluates the use of Large Language Models (LLMs) for generating formal representations of planning domains in PDDL, highlighting their capabilities and limitations in producing executable plans.", "motivation": "To improve the executable and verifiable planning capabilities of LLMs in grounded environments by generating complete PDDL representations from realistic descriptions.", "method": "Systematic evaluation of LLMs to generate complete PDDL representations from descriptions of varying naturalness; analysis of performance across different model sizes and error patterns.", "result": "LLMs can effectively formalize descriptions as PDDL, outperforming those generating plans directly, though their performance decreases with more natural-sounding descriptions.", "conclusion": "Despite their robustness to lexical changes, LLMs exhibit lowered performance with increased naturalness in descriptions, indicating areas for further improvement in formal planning abilities.", "key_contributions": ["Complete PDDL representation generation from natural language descriptions", "Empirical evidence demonstrating LLMs outperforming direct plan generation", "Detailed error analysis regarding LLM performance with varying description naturalness"], "limitations": "Performance declines as descriptions become more natural-sounding, indicating potential gaps in understanding complex language input.", "keywords": ["Large Language Models", "Formal Planning", "PDDL", "Human-Computer Interaction", "Error Analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.10105", "pdf": "https://arxiv.org/pdf/2412.10105.pdf", "abs": "https://arxiv.org/abs/2412.10105", "title": "MALAMUTE: A Multilingual, Highly-granular, Template-free, Education-based Probing Dataset", "authors": ["Sagi Shaier", "George Arthur Baker", "Chiranthan Sridhar", "Lawrence E Hunter", "Katharina von der Wense"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "Language models (LMs) have excelled in various broad domains. However, to\nensure their safe and effective integration into real-world educational\nsettings, they must demonstrate proficiency in specific, granular areas of\nknowledge. Existing cloze-style benchmarks, commonly used to evaluate LMs'\nknowledge, have three major limitations. They: 1) do not cover the educational\ndomain; 2) typically focus on low-complexity, generic knowledge or broad\ndomains, which do not adequately assess the models' knowledge in specific\nsubjects; and 3) often rely on templates that can bias model predictions. Here,\nwe introduce MALAMUTE, a multilingual, template-free, and highly granular\nprobing dataset comprising expert-written, peer-reviewed probes from 71\nuniversity-level textbooks across three languages (English, Spanish, and\nPolish). MALAMUTE is the first education-based cloze-style dataset. It covers\neight domains, each with up to 14 subdomains, further broken down into concepts\nand concept-based prompts, totaling 33,361 university curriculum concepts and\n116,887 prompts. MALAMUTE's fine granularity, educational focus, and inclusion\nof both sentence-level and paragraph-level prompts make it an ideal tool for\nevaluating LMs' course-related knowledge. Our evaluation of masked and causal\nLMs on MALAMUTE shows that despite overall proficiency, they have significant\ngaps in knowledge when examined closely on specific subjects, hindering their\nsafe use in classrooms and underscoring the need for further development.", "AI": {"tldr": "MALAMUTE is a new educational benchmark dataset designed to evaluate language models' knowledge in specific subjects, addressing limitations in existing benchmarks.", "motivation": "To ensure the effective and safe integration of language models in education, it is essential to evaluate their proficiency in granular areas of knowledge.", "method": "MALAMUTE is a multilingual, template-free dataset composed of expert-written probes from 71 university textbooks in English, Spanish, and Polish, covering multiple domains and subdomains.", "result": "The dataset includes 33,361 concepts and 116,887 prompts, revealing significant knowledge gaps in language models when assessed on specific subjects, despite overall proficiency.", "conclusion": "MALAMUTE highlights the necessity for improved educational evaluations of language models to enhance their application in real-world learning environments.", "key_contributions": ["Introduction of MALAMUTE as the first education-focused cloze-style dataset.", "Inclusion of a multilingual and template-free structure.", "Coverage of detailed subjects across multiple languages.", "Demonstration of knowledge gaps in language models through evaluation."], "limitations": "", "keywords": ["Language Models", "Education", "Dataset", "Machine Learning", "Cloze-style"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2412.10827", "pdf": "https://arxiv.org/pdf/2412.10827.pdf", "abs": "https://arxiv.org/abs/2412.10827", "title": "Rethinking Chain-of-Thought from the Perspective of Self-Training", "authors": ["Zongqian Wu", "Baoduo Xu", "Ruochen Cui", "Mengmeng Zhan", "Xiaofeng Zhu", "Lei Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 8 figures", "summary": "Chain-of-thought (CoT) reasoning has emerged as an effective approach for\nactivating latent capabilities in LLMs. Interestingly, we observe that both CoT\nreasoning and self-training share the core objective: iteratively leveraging\nmodel-generated information to progressively reduce prediction uncertainty.\nBuilding on this insight, we propose a novel CoT framework to improve reasoning\nperformance. Our framework integrates two key components: (i) a task-specific\nprompt module that optimizes the initial reasoning process, and (ii) an\nadaptive reasoning iteration module that dynamically refines the reasoning\nprocess and addresses the limitations of previous CoT approaches, \\ie\nover-reasoning and high similarity between consecutive reasoning iterations.\nExtensive experiments demonstrate that the proposed method achieves significant\nadvantages in both performance and computational efficiency.", "AI": {"tldr": "This paper proposes a novel framework to enhance Chain-of-Thought reasoning in LLMs, improving both reasoning performance and computational efficiency.", "motivation": "The paper explores the intersection of Chain-of-Thought reasoning and self-training to improve performance in large language models (LLMs).", "method": "The proposed framework incorporates a task-specific prompt module for initial reasoning and an adaptive reasoning iteration module that refines the reasoning process.", "result": "Experiments show significant improvements in reasoning performance and efficiency compared to previous methods.", "conclusion": "The novel CoT framework effectively addresses the limitations of earlier methods and enhances reasoning capabilities in LLMs.", "key_contributions": ["Introduction of a task-specific prompt module for optimization of reasoning processes.", "Development of an adaptive iteration module to refine reasoning and reduce over-reasoning.", "Demonstration of improved performance and computational efficiency through extensive experiments."], "limitations": "", "keywords": ["Chain-of-Thought reasoning", "Large Language Models", "Self-training", "Adaptive reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.11041", "pdf": "https://arxiv.org/pdf/2412.11041.pdf", "abs": "https://arxiv.org/abs/2412.11041", "title": "Separate the Wheat from the Chaff: A Post-Hoc Approach to Safety Re-Alignment for Fine-Tuned Language Models", "authors": ["Di Wu", "Xin Lu", "Yanyan Zhao", "Bing Qin"], "categories": ["cs.CL"], "comment": "16 pages, 14 figures. Camera-ready for ACL2025 findings", "summary": "Although large language models (LLMs) achieve effective safety alignment at\nthe time of release, they still face various safety challenges. A key issue is\nthat fine-tuning often compromises the safety alignment of LLMs. To address\nthis issue, we propose a method named IRR (Identify, Remove, and Recalibrate\nfor Safety Realignment) that performs safety realignment for LLMs. The core of\nIRR is to identify and remove unsafe delta parameters from the fine-tuned\nmodels, while recalibrating the retained ones. We evaluate the effectiveness of\nIRR across various datasets, including both full fine-tuning and LoRA methods.\nOur results demonstrate that IRR significantly enhances the safety performance\nof fine-tuned models on safety benchmarks, such as harmful queries and\njailbreak attacks, while maintaining their performance on downstream tasks. The\nsource code is available at: https://anonymous.4open.science/r/IRR-BD4F.", "AI": {"tldr": "Proposes IRR, a method for enhancing safety alignment in fine-tuned large language models by identifying and removing unsafe parameters while recalibrating the remaining ones.", "motivation": "To tackle the safety challenges posed by fine-tuning large language models, which often compromise their safety alignment.", "method": "The proposed method, IRR, identifies and removes unsafe delta parameters from fine-tuned models and recalibrates retained parameters to improve safety alignment.", "result": "IRR enhances the safety performance of fine-tuned models on various safety benchmarks while preserving their effectiveness on downstream tasks.", "conclusion": "The IRR method improves safety alignment in LLMs without degrading their performance on tasks, making it a valuable approach for safe model deployment.", "key_contributions": ["Introduction of the IRR method for safety realignment in LLMs.", "Demonstrated effectiveness on safety benchmarks including harmful queries and jailbreak attacks.", "Maintained performance on downstream tasks alongside improved safety."], "limitations": "", "keywords": ["large language models", "safety alignment", "fine-tuning", "machine learning", "safety benchmarks"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2412.11333", "pdf": "https://arxiv.org/pdf/2412.11333.pdf", "abs": "https://arxiv.org/abs/2412.11333", "title": "Segment-Level Diffusion: A Framework for Controllable Long-Form Generation with Diffusion Language Models", "authors": ["Xiaochen Zhu", "Georgi Karadzhov", "Chenxi Whitehouse", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages (main body), 3 figures (main body), ACL 2025 Main", "summary": "Diffusion models have shown promise in text generation, but often struggle\nwith generating long, coherent, and contextually accurate text. Token-level\ndiffusion doesn't model word-order dependencies explicitly and operates on\nshort, fixed output windows, while passage-level diffusion struggles with\nlearning robust representations for long-form text. To address these\nchallenges, we propose Segment-Level Diffusion (SLD), a framework that enhances\ndiffusion-based text generation through text segmentation, robust\nrepresentation training with adversarial and contrastive learning, and improved\nlatent-space guidance. By segmenting long-form outputs into multiple latent\nrepresentations and decoding them with an autoregressive decoder, SLD\nsimplifies diffusion predictions and improves scalability. Experiments on four\ndatasets demonstrate that, when compared to other diffusion and autoregressive\nbaselines SLD achieves competitive or superior fluency, coherence, and\ncontextual compatibility in automatic and human evaluations.", "AI": {"tldr": "Segment-Level Diffusion (SLD) enhances text generation by improving long-form coherence and context accuracy through segmentation, robust representation training, and latent-space guidance.", "motivation": "To overcome the limitations of traditional diffusion models in generating long, coherent, and contextually accurate text.", "method": "SLD uses text segmentation and integrates adversarial and contrastive learning approaches to enhance the representation of long-form text and uses an autoregressive decoder for predictions.", "result": "SLD demonstrates competitive or superior performance in fluency, coherence, and contextual compatibility across four datasets compared to existing diffusion and autoregressive models.", "conclusion": "The proposed framework significantly improves the abilities of diffusion models for long-form text generation tasks.", "key_contributions": ["Introduction of Segment-Level Diffusion (SLD) framework.", "Utilization of adversarial and contrastive learning to enhance representation learning.", "Improved scalability and performance of long-form text generation."], "limitations": "", "keywords": ["Diffusion Models", "Text Generation", "Segment Level", "Adversarial Learning", "Contrastive Learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.13328", "pdf": "https://arxiv.org/pdf/2412.13328.pdf", "abs": "https://arxiv.org/abs/2412.13328", "title": "Expansion Span: Combining Fading Memory and Retrieval in Hybrid State Space Models", "authors": ["Elvis Nunez", "Luca Zancato", "Benjamin Bowman", "Aditya Golatkar", "Wei Xia", "Stefano Soatto"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The \"state\" of State Space Models (SSMs) represents their memory, which fades\nexponentially over an unbounded span. By contrast, Attention-based models have\n\"eidetic\" (i.e., verbatim, or photographic) memory over a finite span (context\nsize). Hybrid architectures combine State Space layers with Attention, but\nstill cannot recall the distant past and can access only the most recent tokens\neidetically. Unlike current methods of combining SSM and Attention layers, we\nallow the state to be allocated based on relevancy rather than recency. In this\nway, for every new set of query tokens, our models can \"eidetically\" access\ntokens from beyond the Attention span of current Hybrid SSMs without requiring\nextra hardware resources. We introduce a method to expand the memory span of\nthe hybrid state by \"reserving\" a fraction of the Attention context for tokens\nretrieved from arbitrarily distant in the past, thus expanding the eidetic\nmemory span of the overall state. We call this reserved fraction of tokens the\n\"expansion span,\" and the mechanism to retrieve and aggregate it \"Span-Expanded\nAttention\" (SE-Attn). To adapt Hybrid models to using SE-Attn, we propose a\nnovel fine-tuning method that extends LoRA to Hybrid models (HyLoRA) and allows\nefficient adaptation on long spans of tokens. We show that SE-Attn enables us\nto efficiently adapt pre-trained Hybrid models on sequences of tokens up to 8\ntimes longer than the ones used for pre-training. We show that HyLoRA with\nSE-Attn is cheaper and more performant than alternatives like LongLoRA when\napplied to Hybrid models on natural language benchmarks with long-range\ndependencies, such as PG-19, RULER, and other common natural language\ndownstream tasks.", "AI": {"tldr": "The paper introduces Span-Expanded Attention (SE-Attn) to enhance the memory capacity of Hybrid State Space Models (SSMs) by allowing access to tokens from beyond the current Attention span, and proposes a new fine-tuning method (HyLoRA) for efficient adaptation.", "motivation": "Existing Hybrid architectures struggle with recalling distant past tokens due to limitations in Attention span. This research aims to overcome these limitations by introducing a novel mechanism that allocates memory based on relevancy, thereby enhancing model performance on tasks requiring long-range dependencies.", "method": "The authors propose Span-Expanded Attention (SE-Attn), which reserves part of the Attention context for tokens retrieved from beyond the current span, enabling hybrid models to access a wider range of memory. A fine-tuning method called HyLoRA is presented to adapt these models for longer token sequences efficiently.", "result": "SE-Attn allows for the adaptation of pre-trained Hybrid models to sequences of tokens up to 8 times longer than their original pre-training, showing improved performance on natural language benchmarks with long-range dependencies.", "conclusion": "HyLoRA combined with SE-Attn is demonstrated to be more cost-effective and performant than alternatives like LongLoRA when applied to Hybrid models in natural language processing tasks.", "key_contributions": ["Introduction of Span-Expanded Attention (SE-Attn) for improved memory access in Hybrid SSMs.", "Novel fine-tuning method (HyLoRA) for adapting models to longer token sequences.", "Performance improvements in various natural language benchmarks with long-range dependencies."], "limitations": "", "keywords": ["State Space Models", "Attention Mechanism", "Hybrid Models", "Natural Language Processing", "Long-range Dependencies"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2412.13549", "pdf": "https://arxiv.org/pdf/2412.13549.pdf", "abs": "https://arxiv.org/abs/2412.13549", "title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents", "authors": ["Cheng Qian", "Peixuan Han", "Qinyu Luo", "Bingxiang He", "Xiusi Chen", "Yuji Zhang", "Hongyi Du", "Jiarui Yao", "Xiaocheng Yang", "Denghui Zhang", "Yunzhu Li", "Heng Ji"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 15 figures, ACL 2025 Main Conference", "summary": "Language model agents excel in long-session planning and reasoning, but\nexisting benchmarks primarily focus on goal-oriented tasks with explicit\nobjectives, neglecting creative adaptation in unfamiliar environments. To\naddress this, we introduce EscapeBench, a benchmark suite of room escape game\nenvironments designed to challenge agents with creative reasoning,\nunconventional tool use, and iterative problem-solving to uncover implicit\ngoals. Our results show that current LM models, despite employing working\nmemory and Chain-of-Thought reasoning, achieve only 15% average progress\nwithout hints, highlighting their limitations in creativity. To bridge this\ngap, we propose EscapeAgent, a framework designed to enhance creative reasoning\nthrough Foresight (innovative tool use) and Reflection (identifying unsolved\ntasks). Experiments show that EscapeAgent can execute action chains over 1,000\nsteps while maintaining logical coherence. It navigates and completes games\nwith up to 40% fewer steps and hints, performs robustly across difficulty\nlevels, and achieves higher action success rates with more efficient and\ninnovative puzzle-solving strategies.", "AI": {"tldr": "Introduction of EscapeBench, a benchmark for creative reasoning in language model agents, and EscapeAgent, a framework to enhance their performance in room escape scenarios.", "motivation": "Existing benchmarks for language models focus on explicit, goal-oriented tasks, neglecting creative adaptation and problem-solving in unfamiliar environments.", "method": "Introduction of EscapeBench to evaluate creative reasoning and unconventional tool use in language models, and development of EscapeAgent to enhance performance through innovative tool use and task reflection.", "result": "Current language models achieve only 15% average progress in EscapeBench without hints, indicating limitations in creativity. EscapeAgent improves performance, executing over 1,000 action steps with fewer hints and increased success rates.", "conclusion": "EscapeAgent enhances the creative problem-solving capabilities of language models in complex scenarios, indicating potential for better performance in non-linear, imaginative tasks.", "key_contributions": ["Introduction of EscapeBench benchmark for evaluating creativity in LM agents", "Development of EscapeAgent framework for enhancing creative reasoning", "Demonstration of significant improvement in task completion efficiency and success rates"], "limitations": "", "keywords": ["Language Models", "Creative Reasoning", "Benchmarking", "Tool Use", "Task Reflection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.13879", "pdf": "https://arxiv.org/pdf/2412.13879.pdf", "abs": "https://arxiv.org/abs/2412.13879", "title": "Crabs: Consuming Resource via Auto-generation for LLM-DoS Attack under Black-box Settings", "authors": ["Yuanhe Zhang", "Zhenhong Zhou", "Wei Zhang", "Xinyue Wang", "Xiaojun Jia", "Yang Liu", "Sen Su"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "22 pages, 8 figures, 11 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse tasks yet still are vulnerable to external threats, particularly LLM\nDenial-of-Service (LLM-DoS) attacks. Specifically, LLM-DoS attacks aim to\nexhaust computational resources and block services. However, existing studies\npredominantly focus on white-box attacks, leaving black-box scenarios\nunderexplored. In this paper, we introduce Auto-Generation for LLM-DoS\n(AutoDoS) attack, an automated algorithm designed for black-box LLMs. AutoDoS\nconstructs the DoS Attack Tree and expands the node coverage to achieve\neffectiveness under black-box conditions. By transferability-driven iterative\noptimization, AutoDoS could work across different models in one prompt.\nFurthermore, we reveal that embedding the Length Trojan allows AutoDoS to\nbypass existing defenses more effectively. Experimental results show that\nAutoDoS significantly amplifies service response latency by over\n250$\\times\\uparrow$, leading to severe resource consumption in terms of GPU\nutilization and memory usage. Our work provides a new perspective on LLM-DoS\nattacks and security defenses. Our code is available at\nhttps://github.com/shuita2333/AutoDoS.", "AI": {"tldr": "The paper presents Auto-Generation for LLM-DoS (AutoDoS), an automated algorithm aimed at executing denial-of-service attacks on black-box Large Language Models (LLMs), significantly increasing service response latency and resource consumption.", "motivation": "To address the gap in research focusing on black-box LLM Denial-of-Service (LLM-DoS) attacks, as most existing studies have concentrated on white-box attacks.", "method": "The AutoDoS attack utilizes a constructed DoS Attack Tree and employs transferability-driven iterative optimization to achieve effective attacks on different models using a single prompt.", "result": "Experimental results indicate that AutoDoS enhances service response latency by over 250 times, leading to significant GPU and memory resource consumption.", "conclusion": "AutoDoS provides insights into black-box LLM-DoS attacks and emphasizes the need for improved security defenses against such threats.", "key_contributions": ["Introduction of the AutoDoS attack against black-box LLMs", "Development of a DoS Attack Tree for systematic attack construction", "Demonstration of bypassing existing defenses using the Length Trojan"], "limitations": "The focus is exclusively on automated black-box LLM attacks, which may not encompass all possible attack scenarios and defenses.", "keywords": ["Large Language Models", "Denial-of-Service", "black-box attacks", "transferability", "security defenses"], "importance_score": 4, "read_time_minutes": 25}}
{"id": "2412.14689", "pdf": "https://arxiv.org/pdf/2412.14689.pdf", "abs": "https://arxiv.org/abs/2412.14689", "title": "How to Synthesize Text Data without Model Collapse?", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Hengli Li", "Kaiyan Zhang", "Ermo Hua", "Xingtai Lv", "Ning Ding", "Zhouhan Lin", "Zilong Zheng", "Bowen Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves model performance.", "AI": {"tldr": "This paper investigates the challenges of model collapse in language models trained on synthetic data and proposes a method for generating improved semi-synthetic data through token editing.", "motivation": "The rise of AI models necessitates understanding the effect of synthetic data on model training and addressing the performance decline associated with model collapse.", "method": "The authors pre-train language models using varying amounts of synthetic data, analyze the resulting model performance, and propose editing tokens in human-produced data to create semi-synthetic data that maintains performance.", "result": "A negative correlation was found between the proportion of synthetic data and language model performance. Token editing was shown to improve model performance and prevent model collapse, validated through extensive experiments.", "conclusion": "Token-level editing of human data allows for the synthesis of quality semi-synthetic data that mitigates performance issues associated with synthetic data in language models.", "key_contributions": ["Identified the negative impact of synthetic data on model performance.", "Proposed token editing as a method to create semi-synthetic data.", "Validated theoretical proof through experiments showing improved model performance."], "limitations": "The study focuses on token-level editing and may not generalize to all forms of synthetic data generation.", "keywords": ["synthetic data", "model collapse", "token editing", "language models", "data synthesis"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.14838", "pdf": "https://arxiv.org/pdf/2412.14838.pdf", "abs": "https://arxiv.org/abs/2412.14838", "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context LLMs", "authors": ["Xiabin Zhou", "Wenbin Wang", "Minyan Zeng", "Jiaxian Guo", "Xuebo Liu", "Li Shen", "Min Zhang", "Liang Ding"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.", "AI": {"tldr": "DynamicKV optimizes token retention in LLMs for improved efficiency in long-context tasks, outperforming traditional methods.", "motivation": "To improve KV cache management for long-context tasks in LLMs by utilizing task-specific characteristics for optimal token retention.", "method": "DynamicKV proposes a dynamic method for optimizing token retention by adjusting the number of tokens per layer based on specific task requirements, establishing global and per-layer KV cache budgets during inference.", "result": "DynamicKV retains only 1.7% of the KV cache size while achieving ~85% of the performance of the Full KV cache on LongBench and surpasses SOTA methods by 11% under extreme compression.", "conclusion": "DynamicKV significantly enhances the efficiency of KV cache management in LLMs by using adaptive strategies tailored to individual task demands without sacrificing performance.", "key_contributions": ["Introduces an adaptive strategy for KV cache management in LLMs", "Demonstrates substantial performance retention with minimal cache size", "Outperforms SOTA methods under extreme compression scenarios"], "limitations": "", "keywords": ["KV cache management", "LLM", "adaptive strategies", "token retention", "dynamic optimization"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2412.20251", "pdf": "https://arxiv.org/pdf/2412.20251.pdf", "abs": "https://arxiv.org/abs/2412.20251", "title": "ComparisonQA: Evaluating Factuality Robustness of LLMs Through Knowledge Frequency Control and Uncertainty", "authors": ["Qing Zong", "Zhaowei Wang", "Tianshi Zheng", "Xiyu Ren", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 findings", "summary": "The rapid development of LLMs has sparked extensive research into their\nfactual knowledge. Current works find that LLMs fall short on questions around\nlow-frequency entities. However, such proofs are unreliable since the questions\ncan differ not only in entity frequency but also in difficulty themselves. So\nwe introduce ComparisonQA benchmark, containing 283K abstract questions, each\ninstantiated by a pair of high-frequency and low-frequency entities. It ensures\na controllable comparison to study the role of knowledge frequency in the\nperformance of LLMs. Because the difference between such a pair is only the\nentity with different frequencies. In addition, we use both correctness and\nuncertainty to develop a two-round method to evaluate LLMs' knowledge\nrobustness. It aims to avoid possible semantic shortcuts which is a serious\nproblem of current QA study. Experiments reveal that LLMs, including GPT-4o,\nexhibit particularly low robustness regarding low-frequency knowledge. Besides,\nwe find that uncertainty can be used to effectively identify high-quality and\nshortcut-free questions while maintaining the data size. Based on this, we\npropose an automatic method to select such questions to form a subset called\nComparisonQA-Hard, containing only hard low-frequency questions.", "AI": {"tldr": "The paper introduces the ComparisonQA benchmark to assess LLMs' knowledge robustness, focusing on low-frequency entities and their impact on performance.", "motivation": "The research addresses the limitations of current evaluations of LLMs, which do not adequately account for the differing difficulties of questions and low-frequency entities.", "method": "Development of the ComparisonQA benchmark containing 283K questions, and a two-round evaluation method that measures correctness and uncertainty.", "result": "LLMs show low robustness on low-frequency knowledge, and uncertainty effectively identifies higher quality questions. An automatic selection method was proposed for creating a hard subset of questions.", "conclusion": "The study highlights the challenges LLMs face with low-frequency knowledge and offers a structured approach to generate more reliable benchmarks for evaluating LLM performance.", "key_contributions": ["Introduction of the ComparisonQA benchmark", "Demonstration of LLMs' low robustness on low-frequency entities", "Development of a method to select high-quality, hard questions"], "limitations": "", "keywords": ["LLM", "knowledge robustness", "benchmark", "low-frequency entities", "uncertainty"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.02979", "pdf": "https://arxiv.org/pdf/2501.02979.pdf", "abs": "https://arxiv.org/abs/2501.02979", "title": "Registering Source Tokens to Target Language Spaces in Multilingual Neural Machine Translation", "authors": ["Zhi Qu", "Yiran Wang", "Jiannan Mao", "Chenchen Ding", "Hideki Tanaka", "Masao Utiyama", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (main)", "summary": "The multilingual neural machine translation (MNMT) aims for arbitrary\ntranslations across multiple languages. Although MNMT-specific models trained\non parallel data offer low costs in training and deployment, their performance\nconsistently lags behind that of large language models (LLMs). In this work, we\nintroduce registering, a novel method that enables a small MNMT-specific model\nto compete with LLMs. Specifically, we insert a set of artificial tokens\nspecifying the target language, called registers, into the input sequence\nbetween the source and target tokens. By modifying the attention mask, the\ntarget token generation only pays attention to the activation of registers,\nrepresenting the source tokens in the target language space. Experiments on\nEC-40, a large-scale benchmark, show that our method advances the\nstate-of-the-art of MNMT. We further pre-train two models, namely MITRE\n(multilingual translation with registers), by 9.3 billion sentence pairs across\n24 languages collected from public corpora. One of them, MITRE-913M,\noutperforms NLLB-3.3B, achieves comparable performance with commercial LLMs,\nand shows strong adaptability in fine-tuning. Finally, we open-source our\nmodels to facilitate further research and development in MNMT:\nhttps://github.com/zhiqu22/mitre.", "AI": {"tldr": "This paper introduces a method called registering, which enables small multilingual neural machine translation (MNMT) models to perform comparably to large language models (LLMs) by using artificial tokens to indicate target languages.", "motivation": "The performance of MNMT-specific models lags behind LLMs, creating a need for methods that can enhance their capabilities.", "method": "The method involves inserting artificial tokens (registers) into the input sequence and modifying the attention mask to focus on these tokens for target token generation.", "result": "Experiments using the EC-40 benchmark show significant advancements in MNMT performance, where the MITRE models outperform existing MNMT models and are comparable to commercial LLMs.", "conclusion": "The proposed registering method allows for better performance of MNMT models and the research includes open-sourced models for further exploration in the field.", "key_contributions": ["Introduction of the registering method for MNMT", "Development of MITRE models that outperform existing benchmarks", "Open-sourcing of models for community use"], "limitations": "", "keywords": ["multilingual neural machine translation", "large language models", "artificial tokens"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2501.04561", "pdf": "https://arxiv.org/pdf/2501.04561.pdf", "abs": "https://arxiv.org/abs/2501.04561", "title": "OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis", "authors": ["Run Luo", "Ting-En Lin", "Haonan Zhang", "Yuchuan Wu", "Xiong Liu", "Min Yang", "Yongbin Li", "Longze Chen", "Jiaming Li", "Lei Zhang", "Yangyi Chen", "Xiaobo Xia", "Hamid Alinejad-Rokny", "Fei Huang"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in omnimodal learning have significantly improved\nunderstanding and generation across images, text, and speech, yet these\ndevelopments remain predominantly confined to proprietary models. The lack of\nhigh-quality omnimodal datasets and the challenges of real-time emotional\nspeech synthesis have notably hindered progress in open-source research. To\naddress these limitations, we introduce \\name, a two-stage training framework\nthat integrates omnimodal alignment and speech generation to develop a\nstate-of-the-art omnimodal large language model. In the alignment phase, a\npre-trained speech model undergoes further training on text-image tasks,\nenabling (near) zero-shot generalization from vision to speech, outperforming\nmodels trained on tri-modal datasets. In the speech generation phase, a\nlightweight decoder is trained on speech tasks with direct preference\noptimization, enabling real-time emotional speech synthesis with high fidelity.\nExperiments show that \\name surpasses state-of-the-art models across omnimodal,\nvision-language, and speech-language benchmarks. It achieves a 4-point absolute\nimprovement on OmniBench over the leading open-source model VITA, despite using\n5x fewer training samples and a smaller model size (7B vs. 7x8B). Additionally,\n\\name achieves real-time speech generation with <1s latency at\nnon-autoregressive mode, reducing inference time by 5x compared to\nautoregressive methods, and improves emotion classification accuracy by 7.7\\%", "AI": {"tldr": "Introducing \name, an omnimodal large language model framework that enhances speech synthesis and alignment between text, image, and speech modalities.", "motivation": "To advance open-source research in omnimodal learning and address the lack of quality datasets and limitations in real-time emotional speech synthesis.", "method": "A two-stage training framework integrating omnimodal alignment and speech generation, where a pre-trained speech model is retrained on text-image tasks followed by training a lightweight decoder for speech tasks.", "result": "\name surpasses state-of-the-art models on various benchmarks, achieving a 4-point improvement on OmniBench and real-time speech generation with less latency and fewer training samples.", "conclusion": "The \name framework achieves significant gains in omnimodal learning and speech generation, demonstrating the effectiveness of combining alignment and speech synthesis techniques in real-time scenarios.", "key_contributions": ["Development of an omnimodal large language model with real-time speech generation capabilities.", "Integration of text-image tasks into speech model training for improved generalization.", "Achieving high fidelity in emotional speech synthesis while reducing model size and training data requirements."], "limitations": "", "keywords": ["omnimodal learning", "speech generation", "emotional speech synthesis", "alignment", "large language model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.06246", "pdf": "https://arxiv.org/pdf/2501.06246.pdf", "abs": "https://arxiv.org/abs/2501.06246", "title": "A partition cover approach to tokenization", "authors": ["Jia Peng Lim", "Shawn Tan", "Davin Choo", "Hady W. Lauw"], "categories": ["cs.CL", "cs.AI", "cs.DS"], "comment": "under review", "summary": "Tokenization is the process of encoding strings into tokens of a fixed\nvocabulary size, and is widely utilized in Natural Language Processing\napplications. The leading tokenization algorithm today is Byte-Pair Encoding\n(BPE), which formulates the tokenization problem as a compression problem and\ntackles it by performing sequences of merges. In this work, we formulate\ntokenization as an optimization objective, show that it is NP-hard via a simple\nreduction from vertex cover, and propose a polynomial-time greedy algorithm\nGreedTok. Our formulation naturally relaxes to the well-studied weighted\nmaximum coverage problem which has a simple $(1 - 1/e)$-approximation algorithm\nGreedWMC. Through empirical evaluations on real-world corpora, we show that\nGreedTok outperforms BPE and Unigram on compression and achieves a covering\nscore comparable to GreedWMC. Finally, our extensive pre-training for two\ntransformer-based language models with 1 billion parameters, comparing the\nchoices of BPE and GreedTok as the tokenizer, shows that GreedTok achieves a\nlower bit per byte even when we control for either the total dataset proportion\nor total training tokens.", "AI": {"tldr": "This paper formulates tokenization as an optimization problem, demonstrating its NP-hardness and proposing a new greedy algorithm, GreedTok, that outperforms traditional methods like BPE and Unigram in compression efficiency and language model training.", "motivation": "Tokenization is crucial in NLP, affecting the performance of various applications. Current methods like Byte-Pair Encoding (BPE) have limitations that this work aims to address through a new formulation and algorithm.", "method": "The authors reduce the tokenization problem to a known NP-hard problem and present GreedTok, a polynomial-time greedy algorithm. They also relate their approach to the weighted maximum coverage problem, leveraging existing approximation algorithms for comparison.", "result": "Empirical evaluations show that GreedTok outperforms BPE and Unigram significantly in terms of compression and achieves a comparable covering score to GreedWMC, a known optimal solution for the relaxation.", "conclusion": "GreedTok not only enhances the tokenization process but also improves the performance of transformer-based language models by lowering the bit per byte cost", "key_contributions": ["Proposes a new greedy algorithm for tokenization (GreedTok)", "Formulates tokenization as an NP-hard optimization problem", "Demonstrates improved performance over existing algorithms in empirical evaluations"], "limitations": "", "keywords": ["Tokenization", "Natural Language Processing", "GreedTok", "Byte-Pair Encoding", "Optimization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.06892", "pdf": "https://arxiv.org/pdf/2501.06892.pdf", "abs": "https://arxiv.org/abs/2501.06892", "title": "Language Fusion for Parameter-Efficient Cross-lingual Transfer", "authors": ["Philipp Borchert", "Ivan Vulić", "Marie-Francine Moens", "Jochen De Weerdt"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Limited availability of multilingual text corpora for training language\nmodels often leads to poor performance on downstream tasks due to undertrained\nrepresentation spaces for languages other than English. This\n'under-representation' has motivated recent cross-lingual transfer methods to\nleverage the English representation space by e.g. mixing English and\n'non-English' tokens at the input level or extending model parameters to\naccommodate new languages. However, these approaches often come at the cost of\nincreased computational complexity. We propose Fusion forLanguage\nRepresentations (FLARE) in adapters, a novel method that enhances\nrepresentation quality and downstream performance for languages other than\nEnglish while maintaining parameter efficiency. FLARE integrates source and\ntarget language representations within low-rank (LoRA) adapters using\nlightweight linear transformations, maintaining parameter efficiency while\nimproving transfer performance. A series of experiments across representative\ncross-lingual natural language understanding tasks, including natural language\ninference, question-answering and sentiment analysis, demonstrate FLARE's\neffectiveness. FLARE achieves performance improvements of 4.9% for Llama 3.1\nand 2.2% for Gemma~2 compared to standard LoRA fine-tuning on\nquestion-answering tasks, as measured by the exact match metric.", "AI": {"tldr": "FLARE proposes a novel method that enhances multilingual representation quality in NLP using low-rank adapters while maintaining computational efficiency.", "motivation": "The limited availability of multilingual text corpora leads to poor performance of language models on non-English tasks, necessitating better methods for cross-lingual transfer.", "method": "FLARE integrates source and target language representations using low-rank (LoRA) adapters with lightweight linear transformations to improve representation quality and efficiency.", "result": "FLARE improves performance by 4.9% for Llama 3.1 and 2.2% for Gemma~2 on question-answering tasks compared to standard LoRA fine-tuning, measured by the exact match metric.", "conclusion": "FLARE effectively enhances multilingual performance in NLP tasks while keeping parameter efficiency high.", "key_contributions": ["Introduction of FLARE for improved cross-lingual performance", "Use of low-rank adapters to maintain parameter efficiency", "Demonstrated effectiveness across multiple NLP tasks"], "limitations": "", "keywords": ["multilingual NLP", "cross-lingual transfer", "low-rank adapters"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.09706", "pdf": "https://arxiv.org/pdf/2501.09706.pdf", "abs": "https://arxiv.org/abs/2501.09706", "title": "Domain Adaptation of Foundation LLMs for e-Commerce", "authors": ["Christian Herold", "Michael Kozielski", "Tala Bazazo", "Pavel Petrushkov", "Patrycja Cieplicka", "Dominika Basaj", "Yannick Versley", "Seyyed Hadi Hashemi", "Shahram Khadivi"], "categories": ["cs.CL"], "comment": "Accepted at ACL25 (Industry )", "summary": "We present the e-Llama models: 8 billion and 70 billion parameter large\nlanguage models that are adapted towards the e-commerce domain. These models\nare meant as foundation models with deep knowledge about e-commerce, that form\na base for instruction- and fine-tuning. The e-Llama models are obtained by\ncontinuously pretraining the Llama 3.1 base models on 1 trillion tokens of\ndomain-specific data.\n  We discuss our approach and motivate our choice of hyperparameters with a\nseries of ablation studies. To quantify how well the models have been adapted\nto the e-commerce domain, we define and implement a set of multilingual,\ne-commerce specific evaluation tasks.\n  We show that, when carefully choosing the training setup, the Llama 3.1\nmodels can be adapted towards the new domain without sacrificing significant\nperformance on general domain tasks. We also explore the possibility of merging\nthe adapted model and the base model for a better control of the performance\ntrade-off between domains.", "AI": {"tldr": "Introduction of e-Llama models as foundation models for e-commerce applications, demonstrating adaptation techniques without losing general performance.", "motivation": "To create large language models specifically tailored for the e-commerce domain while maintaining performance on general tasks.", "method": "Continuous pretraining of Llama 3.1 models on 1 trillion tokens of domain-specific e-commerce data with hyperparameter tuning validated through ablation studies.", "result": "The e-Llama models show significant adaptation to the e-commerce domain and retain comparable performance on general domain tasks.", "conclusion": "Careful training setups allow for effective domain adaptation without sacrificing the general performance, and merging adapted and base models can optimize performance trade-offs.", "key_contributions": ["Introduction of e-Llama models with 8B and 70B parameters for e-commerce", "Implementation of multilingual, e-commerce specific evaluation tasks", "Exploration of merging adapted and base models for performance control"], "limitations": "", "keywords": ["e-commerce", "language models", "Llama 3.1", "domain adaptation", "multilingual evaluation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2501.09766", "pdf": "https://arxiv.org/pdf/2501.09766.pdf", "abs": "https://arxiv.org/abs/2501.09766", "title": "iTool: Reinforced Fine-Tuning with Dynamic Deficiency Calibration for Advanced Tool Use", "authors": ["Yirong Zeng", "Xiao Ding", "Yuxian Wang", "Weiwen Liu", "Wu Ning", "Yutai Hou", "Xu Huang", "Bing Qin", "Ting Liu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review", "summary": "Augmenting large language models (LLMs) with external tools is a promising\napproach to enhance their capabilities, especially for complex tasks.\nSynthesizing tool-use data through real-world simulations is an effective way\nto achieve this. However, our investigation reveals that training gains\nsignificantly decay as synthetic data increases. The model struggles to benefit\nfrom more synthetic data, and it can not equip the model with advanced tool-use\ncapabilities in complex scenarios. Moreover, we discovered that the above\nlimitation usually manifests as a fragment deficiency (i.e., parameter errors)\nin response. To this end, we propose an iterative reinforced fine-tuning\nstrategy designed to alleviate this limitation. This strategy involves: (1)\nenhancing the diversity of response for synthetic data through path exploration\nof Monte Carlo Tree Search. (2) iteratively pinpointing the model's deficiency\nby constructing fine-grained preference pairs, and then improving it by\npreference optimization algorithms for targeted improvement. The experiments\nshow that our method achieves 13.11% better performance than the same-size base\nmodel. It achieves an improvement of 6.5% in complex scenarios compared to the\nbaseline, and it also outperforms larger open-source and closed-source models.", "AI": {"tldr": "This paper proposes an iterative reinforced fine-tuning strategy to enhance large language models' (LLMs') capabilities by addressing limitations identified in synthetic data training.", "motivation": "To improve LLM performance in complex tasks using external tool integration and to tackle the decay of training gains with increasing synthetic data.", "method": "The proposed strategy involves enhancing response diversity using Monte Carlo Tree Search and iteratively identifying and optimizing model deficiencies through preference optimization algorithms.", "result": "The method shows a 13.11% performance improvement over a base model and a 6.5% improvement in complex scenarios compared to the baseline.", "conclusion": "An iterative reinforced fine-tuning approach significantly enhances LLM capabilities, particularly in complex tasks, compared to standard training methods.", "key_contributions": ["Proposed an iterative reinforced fine-tuning strategy for LLMs.", "Utilized Monte Carlo Tree Search for improving synthetic data response diversity.", "Developed fine-grained preference optimization for model improvement."], "limitations": "The performance benefits observed may vary based on the complexity of tasks and the quality of synthetic data used.", "keywords": ["Large language models", "synthetic data", "fine-tuning", "machine learning", "HCI"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.11478", "pdf": "https://arxiv.org/pdf/2501.11478.pdf", "abs": "https://arxiv.org/abs/2501.11478", "title": "Each Graph is a New Language: Graph Learning with LLMs", "authors": ["Huachi Zhou", "Jiahe Du", "Chuang Zhou", "Chang Yang", "Yilin Xiao", "Yuxuan Xie", "Xiao Huang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent efforts leverage Large Language Models (LLMs) for modeling\ntext-attributed graph structures in node classification tasks. These approaches\ndescribe graph structures for LLMs to understand or aggregate LLM-generated\ntextual attribute embeddings through graph structure. However, these approaches\nface two main limitations in modeling graph structures with LLMs. (i) Graph\ndescriptions become verbose in describing high-order graph structure. (ii)\nTextual attributes alone do not contain adequate graph structure information.\nIt is challenging to model graph structure concisely and adequately with LLMs.\nLLMs lack built-in mechanisms to model graph structures directly. They also\nstruggle with complex long-range dependencies between high-order nodes and\ntarget nodes.\n  Inspired by the observation that LLMs pre-trained on one language can achieve\nexceptional performance on another with minimal additional training, we propose\n\\textbf{G}raph-\\textbf{D}efined \\textbf{L}anguage for \\textbf{L}arge\n\\textbf{L}anguage \\textbf{M}odel (GDL4LLM). This novel framework enables LLMs\nto transfer their powerful language understanding capabilities to\ngraph-structured data. GDL4LLM translates graphs into a graph language corpus\ninstead of graph descriptions and pre-trains LLMs on this corpus to adequately\nunderstand graph structures. During fine-tuning, this corpus describes the\nstructural information of target nodes concisely with only a few tokens. By\ntreating graphs as a new language, GDL4LLM enables LLMs to model graph\nstructures adequately and concisely for node classification tasks. Extensive\nexperiments on three real-world datasets demonstrate that GDL4LLM outperforms\ndescription-based and textual attribute embeddings-based baselines by\nefficiently modeling different orders of graph structure with LLMs.", "AI": {"tldr": "GDL4LLM is a novel framework that enables LLMs to effectively model graph structures for node classification by treating graphs as a language.", "motivation": "Current models struggle with verbose and inadequate descriptions of high-order graph structures, limiting the effectiveness of LLMs in graph-related tasks.", "method": "The GDL4LLM framework translates graphs into a graph language corpus for pre-training LLMs, allowing for efficient modeling of graph structures during fine-tuning.", "result": "GDL4LLM shows superior performance compared to traditional description-based and textual attribute embeddings approaches on three real-world datasets.", "conclusion": "By treating graphs as a new language, GDL4LLM provides a concise and effective way for LLMs to understand and classify nodes in graph-structured data.", "key_contributions": ["Proposes GDL4LLM framework for graphs.", "Transforms graph structures into graph language corpora.", "Achieves improved performance in node classification tasks."], "limitations": "", "keywords": ["Large Language Models", "Graph Structures", "Node Classification", "GDL4LLM", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.12766", "pdf": "https://arxiv.org/pdf/2501.12766.pdf", "abs": "https://arxiv.org/abs/2501.12766", "title": "NExtLong: Toward Effective Long-Context Training without Long Documents", "authors": ["Chaochen Gao", "Xing Wu", "Zijia Lin", "Debing Zhang", "Songlin Hu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025. Corresponding authors: xing wu, and songlin hu", "summary": "Large language models (LLMs) with extended context windows have made\nsignificant strides yet remain a challenge due to the scarcity of long\ndocuments. Existing methods tend to synthesize long-context data but lack a\nclear mechanism to reinforce the long-range dependency modeling. To address\nthis limitation, we propose NExtLong, a novel framework for synthesizing\nlong-context data through Negative document Extension. NExtLong decomposes a\ndocument into multiple meta-chunks and extends the context by interleaving hard\nnegative distractors retrieved from pretraining corpora. This approach compels\nthe model to discriminate long-range dependent context from distracting\ncontent, enhancing its ability to model long-range dependencies. Extensive\nexperiments demonstrate that NExtLong achieves significant performance\nimprovements on the HELMET and RULER benchmarks compared to existing\nlong-context synthesis approaches and leading models, which are trained on\nnon-synthetic long documents. These findings highlight NExtLong's ability to\nreduce reliance on non-synthetic long documents, making it an effective\nframework for developing advanced long-context LLMs.", "AI": {"tldr": "NExtLong is a novel framework that synthesizes long-context data by using negative document extension to enhance long-range dependency modeling in large language models.", "motivation": "Existing methods for synthesizing long-context data often fail to properly reinforce long-range dependency modeling due to the limited availability of long documents.", "method": "NExtLong decomposes documents into multiple meta-chunks and integrates hard negative distractors from pretraining corpora to improve the long-range context discrimination ability of the model.", "result": "NExtLong significantly outperforms existing long-context synthesis methods and leading models on the HELMET and RULER benchmarks, highlighting its effectiveness in modeling long-range dependencies without heavy reliance on non-synthetic long documents.", "conclusion": "NExtLong presents an effective strategy for developing advanced long-context large language models, showcasing the potential of using synthesized long-context data.", "key_contributions": ["Introduction of the NExtLong framework for long-context data synthesis", "Utilization of negative document extension to improve long-range dependency modeling", "Demonstrated significant performance improvements on benchmark tests."], "limitations": "", "keywords": ["Large Language Models", "Long-context synthesis", "Machine Learning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2501.13836", "pdf": "https://arxiv.org/pdf/2501.13836.pdf", "abs": "https://arxiv.org/abs/2501.13836", "title": "Think Outside the Data: Colonial Biases and Systemic Issues in Automated Moderation Pipelines for Low-Resource Languages", "authors": ["Farhana Shahid", "Mona Elswah", "Aditya Vashistha"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Most social media users come from non-English speaking countries in the\nGlobal South, where much of harmful content appears in local languages. Yet,\ncurrent AI-driven moderation systems struggle with low-resource languages\nspoken in these regions. This work examines the systemic challenges in building\nautomated moderation tools for these languages. We conducted semi-structured\ninterviews with 22 AI experts working on detecting harmful content in four\nlow-resource languages: Tamil (South Asia), Swahili (East Africa), Maghrebi\nArabic (North Africa), and Quechua (South America). Our findings show that\nbeyond the well-known data scarcity in local languages, technical issues--such\nas outdated machine translation systems, sentiment and toxicity models grounded\nin Western values, and unreliable language detection technologies--undermine\nmoderation efforts. Even with more data, current language models and\npreprocessing pipelines--primarily designed for English--struggle with the\nmorphological richness, linguistic complexity, and code-mixing. As a result,\nautomated moderation in Tamil, Swahili, Arabic, and Quechua remains fraught\nwith inaccuracies and blind spots. Based on our findings, we argue that these\nlimitations are not just technical gaps but reflect deeper structural\ninequities that continue to reproduce historical power imbalances. We conclude\nby discussing multi-stakeholder approaches to improve automated moderation for\nlow-resource languages.", "AI": {"tldr": "This paper investigates the challenges in automated moderation of harmful content in low-resource languages, highlighting systemic issues and proposing multi-stakeholder approaches for improvement.", "motivation": "The motivation behind this work is to address the systemic challenges faced by AI-driven moderation systems inadequately serving non-English speaking users in the Global South, where harmful content often appears in local languages.", "method": "The study conducted semi-structured interviews with 22 AI experts focused on detecting harmful content in Tamil, Swahili, Maghrebi Arabic, and Quechua, providing insights into the specific challenges faced in these languages.", "result": "Findings reveal technical issues, including data scarcity, outdated machine translation systems, and models grounded in Western values, leading to significant inaccuracies in content moderation.", "conclusion": "The paper concludes that limitations in automated moderation are not merely technical, but are indicative of deeper structural inequities, advocating for multi-stakeholder approaches to improve the situation.", "key_contributions": ["Identifies systemic challenges in automated moderation for low-resource languages", "Highlights the impact of outdated technology and Western-centric models on content moderation accuracy", "Proposes multi-stakeholder approaches for better automated moderation solutions"], "limitations": "The study focuses on only four low-resource languages and may not cover all systemic issues in other languages or regions.", "keywords": ["automated moderation", "low-resource languages", "AI bias", "content moderation", "structural inequities"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.19337", "pdf": "https://arxiv.org/pdf/2501.19337.pdf", "abs": "https://arxiv.org/abs/2501.19337", "title": "Token Sampling Uncertainty Does Not Explain Homogeneity Bias in Large Language Models", "authors": ["Messi H. J. Lee", "Soyeon Jeon"], "categories": ["cs.CL", "cs.CV"], "comment": "11 pages, 5 figures", "summary": "Homogeneity bias is one form of stereotyping in AI models where certain\ngroups are represented as more similar to each other than other groups. This\nbias is a major obstacle to creating equitable language technologies. We test\nwhether the bias is driven by systematic differences in token-sampling\nuncertainty across six large language models. While we observe the presence of\nhomogeneity bias using sentence similarity, we find very little difference in\ntoken sampling uncertainty across groups. This finding elucidates why\ntemperature-based sampling adjustments fail to mitigate homogeneity bias. It\nsuggests researchers should prioritize interventions targeting representation\nlearning mechanisms and training corpus composition rather than inference-time\noutput manipulations.", "AI": {"tldr": "This paper explores homogeneity bias in AI models, specifically in language technologies, showing that interventions should focus on representation learning mechanisms rather than inference-time adjustments.", "motivation": "Homogeneity bias in AI models leads to inequitable language technologies by misrepresenting groups as more alike than they are.", "method": "The study tests homogeneity bias across six large language models by analyzing token-sampling uncertainty and sentence similarity.", "result": "While homogeneity bias is present, there is minimal difference in token sampling uncertainty, indicating that adjustments based on temperature do not effectively reduce the bias.", "conclusion": "To address homogeneity bias, researchers should focus on improving representation learning and the composition of training data rather than making changes during inference.", "key_contributions": ["Investigates causes of homogeneity bias in language models", "Suggests targeting representation learning to mitigate bias", "Challenges the effectiveness of temperature-based sampling adjustments"], "limitations": "Limited to six large language models; further research needed to confirm findings across more models.", "keywords": ["homogeneity bias", "AI equity", "language models", "representation learning", "token sampling"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.00136", "pdf": "https://arxiv.org/pdf/2502.00136.pdf", "abs": "https://arxiv.org/abs/2502.00136", "title": "A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment", "authors": ["Edward Y. Chang"], "categories": ["cs.CL", "cs.AI", "F.2.2"], "comment": "20 pages, 7 tables, 6 figures. arXiv admin note: substantial text\n  overlap with arXiv:2405.07076", "summary": "This paper introduces a checks-and-balances framework for ethical alignment\nof Large Language Models (LLMs), inspired by three-branch governmental systems.\nIt implements three independent yet interacting components: LLMs as the\nexecutive branch for knowledge generation, DIKE as the legislative branch\nestablishing ethical guardrails, and ERIS as the judicial branch for contextual\ninterpretation. Beyond structural separation, we address a fundamental\nchallenge: regulating emotion to shape behaviors. Drawing from psychological\ntheories where managing emotional responses prevents harmful behaviors, we\ndevelop a self-supervised learning pipeline that maps emotions to linguistic\nbehaviors, enabling precise behavioral modulation through emotional\nconditioning. By integrating this approach with adversarial testing, our\nframework demonstrates how DIKE and ERIS direct linguistic behaviors toward\nethical outcomes while preserving independence throughout knowledge generation,\nethical oversight, and contextual interpretation.", "AI": {"tldr": "This paper presents a framework for ethically aligning Large Language Models (LLMs) using a checks-and-balances system with three interacting components: knowledge generation (LLMs), ethical guardrails (DIKE), and contextual interpretation (ERIS). The framework incorporates emotional regulation and self-supervised learning to modulate behaviors for ethical outcomes.", "motivation": "The paper is motivated by the need for ethical alignment in LLMs, drawing inspiration from governmental checks-and-balances systems and addressing the challenge of regulating emotions to improve behavioral outcomes.", "method": "The authors developed a self-supervised learning pipeline that links emotions to linguistic behaviors. This integrated approach is tested adversarially to ensure that ethical guidelines are followed during knowledge generation and interpretation.", "result": "The framework successfully directs linguistic behaviors toward ethical outcomes by allowing DIKE and ERIS to influence LLMs while maintaining independence between the components.", "conclusion": "The proposed framework demonstrates a novel approach to ethical oversight in LLMs through emotional conditioning and structured governance, which could improve the safety and alignment of future AI systems.", "key_contributions": ["Introduction of a checks-and-balances framework for LLMs", "Integration of emotional regulation mechanisms for improved behavior management", "Development of an adversarial testing approach to ensure ethical outcomes"], "limitations": "The framework's effectiveness may depend on the robustness of the emotional behavior mapping and the complexity of interactions between the components.", "keywords": ["Large Language Models", "Ethical Alignment", "Self-Supervised Learning", "Emotional Conditioning", "Adversarial Testing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.00334", "pdf": "https://arxiv.org/pdf/2502.00334.pdf", "abs": "https://arxiv.org/abs/2502.00334", "title": "UGPhysics: A Comprehensive Benchmark for Undergraduate Physics Reasoning with Large Language Models", "authors": ["Xin Xu", "Qiyun Xu", "Tong Xiao", "Tianhao Chen", "Yuchen Yan", "Jiaxin Zhang", "Shizhe Diao", "Can Yang", "Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nsolving complex reasoning tasks, particularly in mathematics. However, the\ndomain of physics reasoning presents unique challenges that have received\nsignificantly less attention. Existing benchmarks often fall short in\nevaluating LLMs' abilities on the breadth and depth of undergraduate-level\nphysics, underscoring the need for a comprehensive evaluation. To fill this\ngap, we introduce UGPhysics, a large-scale and comprehensive benchmark\nspecifically designed to evaluate UnderGraduate-level Physics (UGPhysics)\nreasoning with LLMs. UGPhysics includes 5,520 undergraduate-level physics\nproblems in both English and Chinese, covering 13 subjects with seven different\nanswer types and four distinct physics reasoning skills, all rigorously\nscreened for data leakage. Additionally, we develop a Model-Assistant\nRule-based Judgment (MARJ) pipeline specifically tailored for assessing answer\ncorrectness of physics problems, ensuring accurate evaluation. Our evaluation\nof 31 leading LLMs shows that the highest overall accuracy, 49.8% (achieved by\nOpenAI-o1-mini), emphasizes the necessity for models with stronger physics\nreasoning skills, beyond math abilities. We hope UGPhysics, along with MARJ,\nwill drive future advancements in AI for physics reasoning. Codes and data are\navailable at https://github.com/YangLabHKUST/UGPhysics .", "AI": {"tldr": "UGPhysics is a new benchmark for evaluating large language models' reasoning in undergraduate-level physics, featuring 5,520 problems across 13 subjects.", "motivation": "To address the lack of comprehensive evaluation benchmarks for LLMs in physics reasoning, highlighting the gap in their performance compared to other domains like mathematics.", "method": "Development of UGPhysics, a benchmark comprising 5,520 physics problems and a Model-Assistant Rule-based Judgment (MARJ) pipeline for accurate evaluation.", "result": "The highest accuracy achieved by an LLM on this benchmark is 49.8%, indicating a need for enhanced physics reasoning capabilities in models.", "conclusion": "UGPhysics and the MARJ pipeline aim to stimulate advancements in AI's capability to reason in physics, targeting improvements in models' performance.", "key_contributions": ["Introduction of UGPhysics, a comprehensive benchmark for undergraduate physics reasoning", "Development of MARJ for assessing answer correctness", "Evaluation of 31 LLMs demonstrating limited physics reasoning capabilities."], "limitations": "Focuses solely on undergraduate-level physics; may not address higher-level physics complexities or interdisciplinary applications.", "keywords": ["Large Language Models", "Physics Reasoning", "Benchmark", "UGPhysics", "Machine Learning"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2502.00507", "pdf": "https://arxiv.org/pdf/2502.00507.pdf", "abs": "https://arxiv.org/abs/2502.00507", "title": "A statistically consistent measure of semantic uncertainty using Language Models", "authors": ["Yi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "To address the challenge of quantifying uncertainty in the outputs generated\nby language models, we propose a novel measure of semantic uncertainty,\nsemantic spectral entropy, that is statistically consistent under mild\nassumptions. This measure is implemented through a straightforward algorithm\nthat relies solely on standard, pretrained language models, without requiring\naccess to the internal generation process. Our approach imposes minimal\nconstraints on the choice of language models, making it broadly applicable\nacross different architectures and settings. Through comprehensive simulation\nstudies, we demonstrate that the proposed method yields an accurate and robust\nestimate of semantic uncertainty, even in the presence of the inherent\nrandomness characteristic of generative language model outputs.", "AI": {"tldr": "Proposes a novel measure of semantic uncertainty for language models using semantic spectral entropy to quantify uncertainty in outputs.", "motivation": "To quantify uncertainty in outputs generated by language models.", "method": "A straightforward algorithm that uses standard, pretrained language models without needing access to their internal generation process.", "result": "The method provides an accurate and robust estimation of semantic uncertainty, despite the randomness of generative outputs.", "conclusion": "The approach is widely applicable due to minimal constraints on language model choice and is supported by comprehensive simulation studies.", "key_contributions": ["Introduction of semantic spectral entropy as a measure of semantic uncertainty", "Application of the method across different language model architectures", "Validation through simulation studies showing robustness and accuracy"], "limitations": "", "keywords": ["semantic uncertainty", "language models", "semantic spectral entropy"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.04345", "pdf": "https://arxiv.org/pdf/2502.04345.pdf", "abs": "https://arxiv.org/abs/2502.04345", "title": "JingFang: An Expert-Level Large Language Model for Traditional Chinese Medicine Clinical Consultation and Syndrome Differentiation-Based Treatment", "authors": ["Yehan Yang", "Tianhao Ma", "Ruotai Li", "Xinhan Zheng", "Guodong Shan", "Chisheng Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The effective application of traditional Chinese medicine (TCM) requires\nextensive knowledge of TCM and clinical experience. The emergence of Large\nLanguage Models (LLMs) provides a solution to this, while existing LLMs for TCM\nexhibit critical limitations of incomplete clinical consultation and diagnoses,\nas well as inaccurate syndrome differentiation. To address these issues, we\nestablish JingFang (JF), a novel TCM LLM that demonstrates the level of\nexpertise in clinical consultation and syndrome differentiation. We propose a\nMulti-Agent Collaborative Chain-of-Thought Mechanism (MACCTM) for comprehensive\nand targeted clinical consultation, enabling JF with effective and accurate\ndiagnostic ability. In addition, a Syndrome Agent and a Dual-Stage Recovery\nScheme (DSRS) are developed to accurately enhance the differentiation of the\nsyndrome and the subsequent corresponding treatment. JingFang not only\nfacilitates the application of LLMs but also promotes the effective application\nof TCM for healthcare.", "AI": {"tldr": "JingFang is a novel Large Language Model (LLM) aimed at enhancing clinical consultation and syndrome differentiation in Traditional Chinese Medicine (TCM).", "motivation": "Existing LLMs for TCM face critical limitations in clinical consultation and diagnosis accuracy, necessitating a more effective solution.", "method": "The paper introduces JingFang (JF) which incorporates a Multi-Agent Collaborative Chain-of-Thought Mechanism (MACCTM) for improved clinical consultation and diagnostic capabilities. Additionally, it utilizes a Syndrome Agent and a Dual-Stage Recovery Scheme (DSRS) for better syndrome differentiation and treatment.", "result": "JingFang demonstrates enhanced expertise in clinical consultation, accurate syndrome differentiation, and effective treatment recommendations compared to existing TCM LLMs.", "conclusion": "The development of JingFang contributes significantly to the integration of LLMs in traditional healthcare practices, especially in TCM.", "key_contributions": ["Introduction of JingFang (JF), an advanced TCM LLM.", "Development of MACCTM for improved consultation and diagnostic precision.", "Implementation of a Dual-Stage Recovery Scheme (DSRS) for accurate syndrome differentiation."], "limitations": "The study may be limited by the need for extensive clinical validation of JingFang's recommendations in real-world scenarios.", "keywords": ["Traditional Chinese Medicine", "Large Language Models", "clinical consultation", "syndrome differentiation", "healthcare AI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.04394", "pdf": "https://arxiv.org/pdf/2502.04394.pdf", "abs": "https://arxiv.org/abs/2502.04394", "title": "DECT: Harnessing LLM-assisted Fine-Grained Linguistic Knowledge and Label-Switched and Label-Preserved Data Generation for Diagnosis of Alzheimer's Disease", "authors": ["Tingyu Mo", "Jacqueline C. K. Lam", "Victor O. K. Li", "Lawrence Y. L. Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\naffecting 50 million people worldwide. Low-cost, accurate identification of key\nmarkers of AD is crucial for timely diagnosis and intervention. Language\nimpairment is one of the earliest signs of cognitive decline, which can be used\nto discriminate AD patients from normal control individuals.\nPatient-interviewer dialogues may be used to detect such impairments, but they\nare often mixed with ambiguous, noisy, and irrelevant information, making the\nAD detection task difficult. Moreover, the limited availability of AD speech\nsamples and variability in their speech styles pose significant challenges in\ndeveloping robust speech-based AD detection models. To address these\nchallenges, we propose DECT, a novel speech-based domain-specific approach\nleveraging large language models (LLMs) for fine-grained linguistic analysis\nand label-switched label-preserved data generation. Our study presents four\nnovelties: We harness the summarizing capabilities of LLMs to identify and\ndistill key Cognitive-Linguistic information from noisy speech transcripts,\neffectively filtering irrelevant information. We leverage the inherent\nlinguistic knowledge of LLMs to extract linguistic markers from unstructured\nand heterogeneous audio transcripts. We exploit the compositional ability of\nLLMs to generate AD speech transcripts consisting of diverse linguistic\npatterns to overcome the speech data scarcity challenge and enhance the\nrobustness of AD detection models. We use the augmented AD textual speech\ntranscript dataset and a more fine-grained representation of AD textual speech\ntranscript data to fine-tune the AD detection model. The results have shown\nthat DECT demonstrates superior model performance with an 11% improvement in AD\ndetection accuracy on the datasets from DementiaBank compared to the baselines.", "AI": {"tldr": "The paper presents DECT, a novel LLM-based approach for improving Alzheimer's Disease detection through detailed linguistic analysis of patient-interviewer dialogues.", "motivation": "Alzheimer's Disease is widely prevalent, requiring effective identification of markers for timely intervention, with a focus on language impairment as a key sign.", "method": "DECT employs large language models to analyze speech transcripts, filter irrelevant information, and generate diverse AD speech data for model training.", "result": "The implementation of DECT resulted in an 11% increase in detection accuracy compared to existing models using DementiaBank datasets.", "conclusion": "DECT effectively enhances the robustness of AD detection models through advanced linguistic analysis and data augmentation techniques.", "key_contributions": ["Introduction of DECT for fine-grained linguistic analysis in AD detection", "Utilization of LLMs to extract and generate relevant linguistic markers from speech transcripts", "Demonstration of significant improvement in AD detection accuracy through enhanced data representation"], "limitations": "", "keywords": ["Alzheimer's Disease", "Large Language Models", "Speech Detection", "Linguistic Analysis", "Data Augmentation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.04528", "pdf": "https://arxiv.org/pdf/2502.04528.pdf", "abs": "https://arxiv.org/abs/2502.04528", "title": "Group-Adaptive Threshold Optimization for Robust AI-Generated Text Detection", "authors": ["Minseok Jung", "Cynthia Fuertes Panizo", "Liam Dugan", "Yi R.", "Fung", "Pin-Yu Chen", "Paul Pu Liang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The advancement of large language models (LLMs) has made it difficult to\ndifferentiate human-written text from AI-generated text. Several AI-text\ndetectors have been developed in response, which typically utilize a fixed\nglobal threshold (e.g., $\\theta = 0.5$) to classify machine-generated text.\nHowever, one universal threshold could fail to account for distributional\nvariations by subgroups. For example, when using a fixed threshold, detectors\nmake more false positive errors on shorter human-written text, and more\npositive classifications of neurotic writing styles among long texts. These\ndiscrepancies can lead to misclassifications that disproportionately affect\ncertain groups. We address this critical limitation by introducing FairOPT, an\nalgorithm for group-specific threshold optimization for probabilistic AI-text\ndetectors. We partitioned data into subgroups based on attributes (e.g., text\nlength and writing style) and implemented FairOPT to learn decision thresholds\nfor each group to reduce discrepancy. In experiments with nine AI text\nclassifiers on three datasets, FairOPT decreases overall balanced error rate\n(BER) discrepancy by 12\\% while minimally sacrificing accuracy by 0.003\\%. Our\nframework paves the way for more robust classification in AI-generated content\ndetection via post-processing.", "AI": {"tldr": "FairOPT optimizes thresholds for AI text detectors to reduce error discrepancies among subgroups.", "motivation": "To improve the accuracy of AI-text detectors by addressing limitations of fixed global thresholds which lead to biased misclassifications.", "method": "Introduced FairOPT, which partitions data into subgroups based on attributes and learns group-specific decision thresholds.", "result": "FairOPT decreased overall balanced error rate discrepancy by 12% and minimized accuracy sacrifice by 0.003%.", "conclusion": "FairOPT provides a framework for enhancing classification robustness in AI-generated content detection.", "key_contributions": ["Introduction of FairOPT for group-specific threshold optimization", "Demonstration of reduced error discrepancies across subgroups", "Validation through experiments with nine AI text classifiers on three datasets"], "limitations": "May rely on the assumption that subgroups are properly defined and that attributes accurately represent group differences.", "keywords": ["large language models", "fairness", "AI-text detectors", "threshold optimization", "bias reduction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.07340", "pdf": "https://arxiv.org/pdf/2502.07340.pdf", "abs": "https://arxiv.org/abs/2502.07340", "title": "Aligning Large Language Models to Follow Instructions and Hallucinate Less via Effective Data Filtering", "authors": ["Shuzheng Si", "Haozhe Zhao", "Gang Chen", "Cheng Gao", "Yuzhuo Bai", "Zhitong Wang", "Kaikai An", "Kangyang Luo", "Chen Qian", "Fanchao Qi", "Baobao Chang", "Maosong Sun"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "Training LLMs on data containing unfamiliar knowledge during the instruction\ntuning stage can encourage hallucinations. To address this challenge, we\nintroduce NOVA, a novel framework designed to identify high-quality data that\naligns well with the LLM's learned knowledge to reduce hallucinations. NOVA\nincludes Internal Consistency Probing (ICP) and Semantic Equivalence\nIdentification (SEI) to measure how familiar the LLM is with instruction data.\nSpecifically, ICP evaluates the LLM's understanding of the given instruction by\ncalculating the tailored consistency among multiple self-generated responses.\nSEI further assesses the familiarity of the LLM with the target response by\ncomparing it to the generated responses, using the proposed semantic clustering\nand well-designed voting strategy. Finally, to ensure the quality of selected\nsamples, we introduce an expert-aligned reward model, considering\ncharacteristics beyond just familiarity. By considering data quality and\navoiding unfamiliar data, we can utilize the selected data to effectively align\nLLMs to follow instructions and hallucinate less.", "AI": {"tldr": "NOVA is a framework designed to reduce hallucinations in LLMs by identifying high-quality data with which the models are familiar during instruction tuning.", "motivation": "LLMs can generate hallucinations when trained on unfamiliar data, particularly during instruction tuning. There is a need for a framework that selects high-quality and familiar instruction data to mitigate this issue.", "method": "NOVA employs Internal Consistency Probing (ICP) to evaluate the model's understanding of instructions through self-generated responses, and Semantic Equivalence Identification (SEI) to assess the familiarity of the model with responses, enhanced by a voting strategy. An expert-aligned reward model is also introduced to ensure quality beyond familiarity.", "result": "NOVA effectively reduces hallucinations by optimizing the selection of training data, leading to better alignment of LLMs with instructions and enhanced response quality.", "conclusion": "By focusing on the quality and familiarity of training data, NOVA provides a significant advance in aligning LLMs to follow instructions more accurately and reduce erroneous outputs.", "key_contributions": ["Introduction of NOVA framework for data quality assessment in LLMs", "Development of Internal Consistency Probing (ICP) method", "Implementation of Semantic Equivalence Identification (SEI) for familiarity evaluation"], "limitations": "", "keywords": ["NOVA", "LLMs", "instruction tuning", "data quality", "hallucinations"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.08279", "pdf": "https://arxiv.org/pdf/2502.08279.pdf", "abs": "https://arxiv.org/abs/2502.08279", "title": "What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations", "authors": ["Dongqi Liu", "Chenxi Whitehouse", "Xi Yu", "Louis Mahon", "Rohit Saxena", "Zheng Zhao", "Yifu Qiu", "Mirella Lapata", "Vera Demberg"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Main & Long Conference Paper", "summary": "Transforming recorded videos into concise and accurate textual summaries is a\ngrowing challenge in multimodal learning. This paper introduces VISTA, a\ndataset specifically designed for video-to-text summarization in scientific\ndomains. VISTA contains 18,599 recorded AI conference presentations paired with\ntheir corresponding paper abstracts. We benchmark the performance of\nstate-of-the-art large models and apply a plan-based framework to better\ncapture the structured nature of abstracts. Both human and automated\nevaluations confirm that explicit planning enhances summary quality and factual\nconsistency. However, a considerable gap remains between models and human\nperformance, highlighting the challenges of our dataset. This study aims to\npave the way for future research on scientific video-to-text summarization.", "AI": {"tldr": "This paper introduces VISTA, a dataset for video-to-text summarization of AI conference presentations, and evaluates state-of-the-art models.", "motivation": "The challenge of transforming recorded videos into concise textual summaries in scientific domains, particularly for AI conferences.", "method": "Introduces the VISTA dataset of 18,599 AI conference presentation videos and their abstracts, benchmarks existing models, and applies a plan-based framework to improve summary quality.", "result": "Evaluation shows that a plan-based approach enhances summary quality and factual consistency; however, there remains a significant gap between model and human performance.", "conclusion": "The study highlights the capabilities and limitations of current models and paves the way for future research in scientific video-to-text summarization.", "key_contributions": ["Development of the VISTA dataset for video-to-text summarization", "Benchmarking state-of-the-art large models", "Demonstration of the impact of planning on summary quality"], "limitations": "Not all models achieve human-level performance, revealing challenges in the dataset and the summarization task.", "keywords": ["video-to-text summarization", "multimodal learning", "dataset", "planning", "AI conference"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.08767", "pdf": "https://arxiv.org/pdf/2502.08767.pdf", "abs": "https://arxiv.org/abs/2502.08767", "title": "SelfElicit: Your Language Model Secretly Knows Where is the Relevant Evidence", "authors": ["Zhining Liu", "Rana Ali Amjad", "Ravinarayana Adkathimar", "Tianxin Wei", "Hanghang Tong"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025. 21 pages, 5 figures, 13 tables", "summary": "Providing Language Models (LMs) with relevant evidence in the context (either\nvia retrieval or user-provided) can significantly improve their ability to\nprovide better-grounded responses. However, recent studies have found that LMs\noften struggle to fully comprehend and utilize key evidence from the context,\nespecially when it contains noise and irrelevant information, an issue common\nin real-world scenarios. To address this, we propose SelfElicit, an\ninference-time approach that helps LMs focus on key contextual evidence through\nself-guided explicit highlighting. By leveraging the inherent evidence-finding\ncapabilities of LMs using the attention scores of deeper layers, our method\nautomatically identifies and emphasizes key evidence within the input context,\nfacilitating more accurate and grounded responses without additional training\nor iterative prompting. We demonstrate that SelfElicit brings consistent and\nsignificant improvement on multiple evidence-based QA tasks for various LM\nfamilies while maintaining computational efficiency. Our code and documentation\nare available at https://github.com/ZhiningLiu1998/SelfElicit.", "AI": {"tldr": "SelfElicit improves language models' responses by emphasizing key contextual evidence through self-guided explicit highlighting.", "motivation": "To address the challenge of language models struggling to comprehend and utilize key evidence from noisy and irrelevant contextual information.", "method": "SelfElicit is an inference-time approach that leverages LMs' attention scores to identify and highlight important evidence without extra training.", "result": "SelfElicit consistently improves performance on multiple evidence-based QA tasks across various language model families while being computationally efficient.", "conclusion": "The proposed method enables language models to produce more accurate and grounded responses in real-world scenarios without requiring additional training efforts.", "key_contributions": ["Introduces SelfElicit for highlighting key evidence in context", "Demonstrates significant performance improvements on QA tasks", "Maintains computational efficiency during the inference process"], "limitations": "", "keywords": ["Language Models", "Contextual Evidence", "Self-Guided Highlighting", "QA Tasks", "Attention Mechanism"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11211", "pdf": "https://arxiv.org/pdf/2502.11211.pdf", "abs": "https://arxiv.org/abs/2502.11211", "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?", "authors": ["Wenxuan Wang", "Zizhan Ma", "Zheng Wang", "Chenghan Wu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Yixuan Yuan"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) are transforming healthcare through the\ndevelopment of LLM-based agents that can understand, reason about, and assist\nwith medical tasks. This survey provides a comprehensive review of LLM-based\nagents in medicine, examining their architectures, applications, and\nchallenges. We analyze the key components of medical agent systems, including\nsystem profiles, clinical planning mechanisms, medical reasoning frameworks,\nand external capacity enhancement. The survey covers major application\nscenarios such as clinical decision support, medical documentation, training\nsimulations, and healthcare service optimization. We discuss evaluation\nframeworks and metrics used to assess these agents' performance in healthcare\nsettings. While LLM-based agents show promise in enhancing healthcare delivery,\nseveral challenges remain, including hallucination management, multimodal\nintegration, implementation barriers, and ethical considerations. The survey\nconcludes by highlighting future research directions, including advances in\nmedical reasoning inspired by recent developments in LLM architectures,\nintegration with physical systems, and improvements in training simulations.\nThis work provides researchers and practitioners with a structured overview of\nthe current state and future prospects of LLM-based agents in medicine.", "AI": {"tldr": "This survey reviews LLM-based agents in healthcare, their architectures, applications, challenges, and future research directions.", "motivation": "To provide a comprehensive overview of LLM-based agents' influence on healthcare and identify challenges and future prospects.", "method": "Survey of existing literature and frameworks on LLM-based agents in medicine, focusing on architectures, applications, evaluation metrics, and challenges.", "result": "Identification of key application scenarios for LLM-based agents, including clinical decision support and medical documentation, as well as the challenges these systems face, such as hallucination management and ethical considerations.", "conclusion": "LLM-based agents have potential in enhancing healthcare delivery, but challenges such as system integration and ethical issues need to be addressed; future research should focus on improving medical reasoning and integrating these systems.", "key_contributions": ["Comprehensive review of LLM architectures and applications in healthcare", "Analysis of performance evaluation metrics for LLM-based agents", "Identification of key challenges and future research directions in healthcare LLM application"], "limitations": "Challenges such as multimodal integration and ethical considerations are ongoing; future work is needed to address these issues.", "keywords": ["Large Language Models", "Healthcare", "Medical Agents", "Artificial Intelligence", "Ethics"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2502.11393", "pdf": "https://arxiv.org/pdf/2502.11393.pdf", "abs": "https://arxiv.org/abs/2502.11393", "title": "HellaSwag-Pro: A Large-Scale Bilingual Benchmark for Evaluating the Robustness of LLMs in Commonsense Reasoning", "authors": ["Xiaoyuan Li", "Moxin Li", "Rui Men", "Yichang Zhang", "Keqin Bao", "Wenjie Wang", "Fuli Feng", "Dayiheng Liu", "Junyang Lin"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) have shown remarkable capabilities in\ncommonsense reasoning; however, some variations in questions can trigger\nincorrect responses. Do these models truly understand commonsense knowledge, or\njust memorize expression patterns? To investigate this question, we present the\nfirst extensive robustness evaluation of LLMs in commonsense reasoning. We\nintroduce HellaSwag-Pro, a large-scale bilingual benchmark consisting of 11,200\ncases, by designing and compiling seven types of question variants. To\nconstruct this benchmark, we propose a two-stage method to develop Chinese\nHellaSwag, a finely annotated dataset comprising 12,000 instances across 56\ncategories. We conduct extensive experiments on 41 representative LLMs,\nrevealing that these LLMs are far from robust in commonsense reasoning.\nFurthermore, this robustness varies depending on the language in which the LLM\nis tested. This work establishes a high-quality evaluation benchmark, with\nextensive experiments offering valuable insights to the community in\ncommonsense reasoning for LLMs.", "AI": {"tldr": "This paper presents HellaSwag-Pro, a large-scale evaluation benchmark for assessing the robustness of large language models in commonsense reasoning across different question variants and languages.", "motivation": "To investigate whether large language models truly understand commonsense knowledge or simply memorize expression patterns, and to provide a robust evaluation benchmark.", "method": "A two-stage method was proposed to develop a bilingual benchmark, HellaSwag-Pro, consisting of 11,200 cases with seven types of question variants, alongside a fine-tuned dataset of 12,000 instances in Chinese.", "result": "Extensive experiments conducted on 41 LLMs demonstrated that these models are not robust in commonsense reasoning, with performance varying based on the language of the questions.", "conclusion": "The establishment of HellaSwag-Pro offers a high-quality evaluation benchmark which provides insights to improve commonsense reasoning in LLMs.", "key_contributions": ["Introduction of HellaSwag-Pro as a benchmark for commonsense reasoning in LLMs.", "Development of a bilingual dataset for evaluating model robustness.", "Comprehensive analysis of 41 LLMs revealing flaws in their commonsense reasoning capabilities."], "limitations": "", "keywords": ["large language models", "commonsense reasoning", "evaluation benchmark"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11514", "pdf": "https://arxiv.org/pdf/2502.11514.pdf", "abs": "https://arxiv.org/abs/2502.11514", "title": "Investigating Inference-time Scaling for Chain of Multi-modal Thought: A Preliminary Study", "authors": ["Yujie Lin", "Ante Wang", "Moye Chen", "Jingyao Liu", "Hao Liu", "Jinsong Su", "Xinyan Xiao"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, inference-time scaling of chain-of-thought (CoT) has been\ndemonstrated as a promising approach for addressing multi-modal reasoning\ntasks. While existing studies have predominantly centered on text-based\nthinking, the integration of both visual and textual modalities within the\nreasoning process remains unexplored. In this study, we pioneer the exploration\nof inference-time scaling with multi-modal thought, aiming to bridge this gap.\nTo provide a comprehensive analysis, we systematically investigate popular\nsampling-based and tree search-based inference-time scaling methods on 10\nchallenging tasks spanning various domains. Besides, we uniformly adopt a\nconsistency-enhanced verifier to ensure effective guidance for both methods\nacross different thought paradigms. Results show that multi-modal thought\npromotes better performance against conventional text-only thought, and\nblending the two types of thought fosters more diverse thinking. Despite these\nadvantages, multi-modal thoughts necessitate higher token consumption for\nprocessing richer visual inputs, which raises concerns in practical\napplications. We hope that our findings on the merits and drawbacks of this\nresearch line will inspire future works in the field.", "AI": {"tldr": "This study explores multi-modal reasoning by integrating visual and textual modalities in inference-time scaling, addressing gaps in existing research.", "motivation": "To bridge the gap in understanding multi-modal reasoning processes using both visual and textual modalities, which have been overlooked in current studies.", "method": "The paper investigates popular sampling-based and tree search-based inference-time scaling methods on ten tasks across various domains, incorporating a consistency-enhanced verifier for guidance.", "result": "Results indicate that multi-modal reasoning outperforms text-only reasoning and allows for more diverse thought processes, although it incurs higher token costs due to richer visual inputs.", "conclusion": "The findings highlight both the advantages and drawbacks of multi-modal thought in inference-time scaling, suggesting avenues for future research.", "key_contributions": ["Integration of visual and textual modalities in multi-modal reasoning.", "Systematic analysis of inference-time scaling methods on diverse tasks.", "Identification of the trade-offs involved in multi-modal thought processes."], "limitations": "Higher token consumption for processing visual inputs may limit practical application.", "keywords": ["multi-modal reasoning", "inference-time scaling", "chain-of-thought"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11598", "pdf": "https://arxiv.org/pdf/2502.11598.pdf", "abs": "https://arxiv.org/abs/2502.11598", "title": "Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?", "authors": ["Leyi Pan", "Aiwei Liu", "Shiyu Huang", "Yijian Lu", "Xuming Hu", "Lijie Wen", "Irwin King", "Philip S. Yu"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Accepted by ACL 2025 (Main)", "summary": "The radioactive nature of Large Language Model (LLM) watermarking enables the\ndetection of watermarks inherited by student models when trained on the outputs\nof watermarked teacher models, making it a promising tool for preventing\nunauthorized knowledge distillation. However, the robustness of watermark\nradioactivity against adversarial actors remains largely unexplored. In this\npaper, we investigate whether student models can acquire the capabilities of\nteacher models through knowledge distillation while avoiding watermark\ninheritance. We propose two categories of watermark removal approaches:\npre-distillation removal through untargeted and targeted training data\nparaphrasing (UP and TP), and post-distillation removal through inference-time\nwatermark neutralization (WN). Extensive experiments across multiple model\npairs, watermarking schemes and hyper-parameter settings demonstrate that both\nTP and WN thoroughly eliminate inherited watermarks, with WN achieving this\nwhile maintaining knowledge transfer efficiency and low computational overhead.\nGiven the ongoing deployment of watermarking techniques in production LLMs,\nthese findings emphasize the urgent need for more robust defense strategies.\nOur code is available at\nhttps://github.com/THU-BPM/Watermark-Radioactivity-Attack.", "AI": {"tldr": "This paper investigates watermark removal in Large Language Models (LLMs) and proposes methods for avoiding watermark inheritance during knowledge distillation.", "motivation": "To explore the robustness of watermark radioactivity against adversarial actors and to prevent unauthorized knowledge distillation in LLMs.", "method": "The paper proposes two categories of watermark removal approaches: pre-distillation removal (using untargeted and targeted training data paraphrasing) and post-distillation removal (through inference-time watermark neutralization).", "result": "Experiments demonstrate that both targeted paraphrasing and watermark neutralization effectively eliminate inherited watermarks; watermark neutralization does so while preserving knowledge transfer efficiency and low computational costs.", "conclusion": "The findings highlight the need for more robust defense strategies against potential threats posed by watermarking techniques in production LLMs.", "key_contributions": ["Investigation of watermark radioactivity in LLMs", "Proposed methods for watermark removal during student model training", "Demonstrated effectiveness of removing watermarks without significant performance trade-offs"], "limitations": "", "keywords": ["Large Language Models", "watermarking", "knowledge distillation", "watermark removal", "adversarial robustness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11736", "pdf": "https://arxiv.org/pdf/2502.11736.pdf", "abs": "https://arxiv.org/abs/2502.11736", "title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews", "authors": ["Madhav Krishan Garg", "Tejash Prasad", "Tanmay Singhal", "Chhavi Kirtani", "Murari Mandal", "Dhruv Kumar"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review: 8 pages, 2 figures, 5 tables, 9 pages for appendix", "summary": "The escalating volume of academic research, coupled with a shortage of\nqualified reviewers, necessitates innovative approaches to peer review. In this\nwork, we propose: 1. ReviewEval, a comprehensive evaluation framework for\nAI-generated reviews that measures alignment with human assessments, verifies\nfactual accuracy, assesses analytical depth, identifies degree of\nconstructiveness and adherence to reviewer guidelines; and 2. ReviewAgent, an\nLLM-based review generation agent featuring a novel alignment mechanism to\ntailor feedback to target conferences and journals, along with a\nself-refinement loop that iteratively optimizes its intermediate outputs and an\nexternal improvement loop using ReviewEval to improve upon the final reviews.\nReviewAgent improves actionable insights by 6.78% and 47.62% over existing AI\nbaselines and expert reviews respectively. Further, it boosts analytical depth\nby 3.97% and 12.73%, enhances adherence to guidelines by 10.11% and 47.26%\nrespectively. This paper establishes essential metrics for AIbased peer review\nand substantially enhances the reliability and impact of AI-generated reviews\nin academic research.", "AI": {"tldr": "This paper introduces ReviewEval, a framework for evaluating AI-generated peer reviews, and ReviewAgent, an LLM-based agent that improves review quality by aligning with human assessments and optimizing feedback processes.", "motivation": "The increasing volume of academic research and the shortage of qualified reviewers necessitate innovative solutions in the peer review process.", "method": "The paper proposes ReviewEval, an evaluation framework for AI-generated reviews that assesses alignment, factual accuracy, analytical depth, constructiveness, and adherence to guidelines. ReviewAgent is an LLM-based generation agent that utilizes a unique alignment mechanism and a refinement feedback loop to enhance review quality.", "result": "ReviewAgent outperforms existing AI baselines and expert reviews, improving actionable insights by 6.78% and 47.62%, analytical depth by 3.97% and 12.73%, and adherence to guidelines by 10.11% and 47.26%.", "conclusion": "The study establishes crucial metrics for AI-based peer review and significantly increases the reliability and effectiveness of AI-generated reviews in academic research.", "key_contributions": ["Introduction of ReviewEval for evaluating AI-generated reviews.", "Development of ReviewAgent with a novel alignment mechanism.", "Demonstrated improvements in review quality metrics over existing methods."], "limitations": "", "keywords": ["AI-generated reviews", "peer review", "human assessments", "LLM", "evaluation framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11962", "pdf": "https://arxiv.org/pdf/2502.11962.pdf", "abs": "https://arxiv.org/abs/2502.11962", "title": "Balancing Truthfulness and Informativeness with Uncertainty-Aware Instruction Fine-Tuning", "authors": ["Tianyi Wu", "Jingwei Ni", "Bryan Hooi", "Jiaheng Zhang", "Elliott Ash", "See-Kiong Ng", "Mrinmaya Sachan", "Markus Leippold"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction fine-tuning (IFT) can increase the informativeness of large\nlanguage models (LLMs), but may reduce their truthfulness. This trade-off\narises because IFT steers LLMs to generate responses containing long-tail\nknowledge that was not well covered during pre-training. As a result, models\nbecome more informative but less accurate when generalizing to unseen tasks. In\nthis paper, we empirically demonstrate how unfamiliar knowledge in IFT datasets\ncan negatively affect the truthfulness of LLMs, and we introduce two new IFT\nparadigms, $UNIT_{cut}$ and $UNIT_{ref}$, to address this issue. $UNIT_{cut}$\nidentifies and removes unfamiliar knowledge from IFT datasets to mitigate its\nimpact on model truthfulness, whereas $UNIT_{ref}$ trains LLMs to recognize\ntheir uncertainty and explicitly indicate it at the end of their responses. Our\nexperiments show that $UNIT_{cut}$ substantially improves LLM truthfulness,\nwhile $UNIT_{ref}$ maintains high informativeness and reduces hallucinations by\ndistinguishing between confident and uncertain statements.", "AI": {"tldr": "The paper explores the trade-off between informativeness and truthfulness in instruction fine-tuning (IFT) of LLMs, introducing two new IFT approaches to mitigate issues of unfamiliar knowledge.", "motivation": "The motivation for this work is to address the reduction in truthfulness of large language models (LLMs) caused by instruction fine-tuning (IFT) using unfamiliar knowledge, which can lead to misinformation.", "method": "The authors propose two paradigms: $UNIT_{cut}$, which removes unfamiliar knowledge from IFT datasets, and $UNIT_{ref}$, which trains LLMs to recognize uncertainty and signal it at the end of their responses. They conduct experiments to evaluate the effectiveness of both methods.", "result": "Experiments demonstrate that $UNIT_{cut}$ significantly improves the truthfulness of LLMs, while $UNIT_{ref}$ preserves informativeness and reduces misstatements by allowing the model to differentiate between confident and uncertain claims.", "conclusion": "The findings suggest that carefully designed instruction fine-tuning can enhance the reliability of LLMs, particularly in high-stakes applications where accuracy is critical.", "key_contributions": ["Introduction of $UNIT_{cut}$ for removing unfamiliar knowledge from training datasets, improving truthfulness.", "Introduction of $UNIT_{ref}$ for teaching LLMs to indicate uncertainty, maintaining informativeness.", "Empirical validation showing the effectiveness of both methods in enhancing LLM performance."], "limitations": "", "keywords": ["Instruction fine-tuning", "Large language models", "Truthfulness", "Informativeness", "Uncertainty"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12051", "pdf": "https://arxiv.org/pdf/2502.12051.pdf", "abs": "https://arxiv.org/abs/2502.12051", "title": "How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines", "authors": ["Ayan Sengupta", "Yash Goel", "Tanmoy Chakraborty"], "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 11 tables, 4 figures", "summary": "Neural scaling laws have revolutionized the design and optimization of\nlarge-scale AI models by revealing predictable relationships between model\nsize, dataset volume, and computational resources. Early research established\npower-law relationships in model performance, leading to compute-optimal\nscaling strategies. However, recent studies highlighted their limitations\nacross architectures, modalities, and deployment contexts. Sparse models,\nmixture-of-experts, retrieval-augmented learning, and multimodal models often\ndeviate from traditional scaling patterns. Moreover, scaling behaviors vary\nacross domains such as vision, reinforcement learning, and fine-tuning,\nunderscoring the need for more nuanced approaches. In this survey, we\nsynthesize insights from over 50 studies, examining the theoretical\nfoundations, empirical findings, and practical implications of scaling laws. We\nalso explore key challenges, including data efficiency, inference scaling, and\narchitecture-specific constraints, advocating for adaptive scaling strategies\ntailored to real-world applications. We suggest that while scaling laws provide\na useful guide, they do not always generalize across all architectures and\ntraining strategies.", "AI": {"tldr": "This survey synthesizes insights from over 50 studies on neural scaling laws, examining their implications, challenges, and variability across different AI architectures and use cases.", "motivation": "To analyze the predictability of model performance based on scaling laws and the limitations encountered across various architectures and deployment scenarios.", "method": "A survey of existing research, synthesizing theoretical foundations, empirical results, and practical implications of neural scaling laws.", "result": "Identified variances in scaling behavior across different domains and architectures, emphasizing the need for adaptive scaling strategies for real-world applications.", "conclusion": "Scaling laws are helpful guides for model optimization but are not universally applicable to all training strategies; tailored approaches are needed.", "key_contributions": ["Synthesis of insights from over 50 studies on neural scaling laws", "Identification of challenges such as data efficiency and architecture-specific constraints", "Advocacy for adaptive scaling strategies in AI deployment"], "limitations": "Scaling laws may not generalize across all architectures and training strategies, requiring careful adaptation in practice.", "keywords": ["neural scaling laws", "AI models", "adaptive scaling strategies"], "importance_score": 6, "read_time_minutes": 30}}
{"id": "2502.12067", "pdf": "https://arxiv.org/pdf/2502.12067.pdf", "abs": "https://arxiv.org/abs/2502.12067", "title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "authors": ["Heming Xia", "Chak Tou Leong", "Wenjie Wang", "Yongqi Li", "Wenjie Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop.", "AI": {"tldr": "TokenSkip is a method that allows large language models to skip less important tokens in Chain-of-Thought outputs, reducing inference latency while maintaining reasoning performance.", "motivation": "Scaling the length of Chain-of-Thought sequences improves the reasoning capabilities of large language models, but longer outputs increase inference latency, affecting user experience. Identifying and skipping less important tokens can enhance efficiency.", "method": "The paper analyzes the semantic importance of tokens in Chain-of-Thought sequences and proposes TokenSkip, a method that allows LLMs to selectively skip tokens based on their contribution to reasoning.", "result": "TokenSkip effectively reduces the number of reasoning tokens used by 40% on the GSM8K task with less than a 0.4% drop in performance, demonstrating efficiency improvements for large language models while preserving strong reasoning capabilities.", "conclusion": "TokenSkip offers a practical solution for enhancing the efficiency of Chain-of-Thought reasoning in LLMs, striking a balance between performance and inference speed.", "key_contributions": ["Introduction of TokenSkip for controllable Chain-of-Thought compression", "Demonstration of token importance analysis in reasoning tasks", "Empirical validation of TokenSkip's effectiveness across various models and tasks"], "limitations": "The approach may require fine-tuning for different tasks and models to achieve optimal performance.", "keywords": ["Chain-of-Thought", "TokenSkip", "large language models", "inference latency", "reasoning performance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12289", "pdf": "https://arxiv.org/pdf/2502.12289.pdf", "abs": "https://arxiv.org/abs/2502.12289", "title": "Evaluating Step-by-step Reasoning Traces: A Survey", "authors": ["Jinu Lee", "Julia Hockenmaier"], "categories": ["cs.CL"], "comment": "25 pages (8 pages of main content), 5 figures", "summary": "Step-by-step reasoning is widely used to enhance the reasoning ability of\nlarge language models (LLMs) in complex problems. Evaluating the quality of\nreasoning traces is crucial for understanding and improving LLM reasoning.\nHowever, existing evaluation practices are highly inconsistent, resulting in\nfragmented progress across evaluator design and benchmark development. To\naddress this gap, this survey provides a comprehensive overview of step-by-step\nreasoning evaluation, proposing a taxonomy of evaluation criteria with four\ntop-level categories (factuality, validity, coherence, and utility). Based on\nthe taxonomy, we review different evaluator implementations and recent\nfindings, leading to promising directions for future research.", "AI": {"tldr": "This survey outlines evaluation practices for step-by-step reasoning in large language models, presenting a taxonomy of evaluation criteria to improve consistency in evaluations.", "motivation": "To enhance understanding and improvement in the reasoning capabilities of large language models by standardizing evaluation practices.", "method": "The paper proposes a taxonomy of evaluation criteria comprising factuality, validity, coherence, and utility, and reviews existing evaluator implementations and findings.", "result": "The survey identifies gaps in current evaluation practices and suggests a structured approach to evaluate reasoning in LLMs.", "conclusion": "The proposed taxonomy could guide future research in developing consistent and effective benchmarks for evaluating reasoning in large language models.", "key_contributions": ["Comprehensive overview of step-by-step reasoning evaluation", "Taxonomy of evaluation criteria", "Review of evaluator implementations and findings"], "limitations": "", "keywords": ["large language models", "reasoning evaluation", "taxonomy of evaluation criteria", "factuality", "utility"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2502.12568", "pdf": "https://arxiv.org/pdf/2502.12568.pdf", "abs": "https://arxiv.org/abs/2502.12568", "title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation", "authors": ["Kaiyang Wan", "Honglin Mu", "Rui Hao", "Haoran Luo", "Tianle Gu", "Xiuying Chen"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 6 figures", "summary": "Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.", "AI": {"tldr": "CogWriter is a novel framework that empowers Large Language Models to generate long-form text through cognitive writing processes.", "motivation": "The paper addresses the difficulties LLMs face in generating high-quality long-form text, drawing on Cognitive Writing Theory to enhance LLM writing capabilities.", "method": "CogWriter employs two main components: a Planning Agent for hierarchical task decomposition and multiple Generation Agents for parallel execution of writing plans, with a monitoring system for continuous quality assessment.", "result": "CogWriter significantly outperforms GPT-4o by 22% in instruction completion accuracy and can produce texts over 10,000 words, as demonstrated on LongGenBench.", "conclusion": "The study suggests that leveraging cognitive science principles can enhance LLM writing, providing a new framework for future advancements in this area.", "key_contributions": ["Introduces CogWriter, a training-free framework for LLM writing", "Implements a Planning Agent for task decomposition", "Demonstrates superior performance on constrained text generation benchmarks."], "limitations": "", "keywords": ["Large Language Models", "Cognitive Writing Theory", "Long-form text generation", "AI writing systems", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2502.12672", "pdf": "https://arxiv.org/pdf/2502.12672.pdf", "abs": "https://arxiv.org/abs/2502.12672", "title": "Speech-FT: Merging Pre-trained And Fine-Tuned Speech Representation Models For Cross-Task Generalization", "authors": ["Tzu-Quan Lin", "Wei-Ping Huang", "Hao Tang", "Hung-yi Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Fine-tuning speech representation models can enhance performance on specific\ntasks but often compromises their cross-task generalization ability. This\ndegradation is often caused by excessive changes in the representations, making\nit difficult to retain information learned during pre-training. Existing\napproaches, such as regularizing weight changes during fine-tuning, may fail to\nmaintain sufficiently high feature similarity with the pre-trained model, and\nthus could possibly lose cross-task generalization. To address this issue, we\npropose Speech-FT, a novel two-stage fine-tuning framework designed to maintain\ncross-task generalization while benefiting from fine-tuning. Speech-FT first\napplies fine-tuning specifically designed to reduce representational drift,\nfollowed by weight-space interpolation with the pre-trained model to restore\ncross-task generalization. Extensive experiments on HuBERT, wav2vec 2.0, DeCoAR\n2.0, and WavLM Base+ demonstrate that Speech-FT consistently improves\nperformance across a wide range of supervised, unsupervised, and multitask\nfine-tuning scenarios. Moreover, Speech-FT achieves superior cross-task\ngeneralization compared to fine-tuning baselines that explicitly constrain\nweight changes, such as weight-space regularization and LoRA fine-tuning. Our\nanalysis reveals that Speech-FT maintains higher feature similarity to the\npre-trained model compared to alternative strategies, despite allowing larger\nweight-space updates. Notably, Speech-FT achieves significant improvements on\nthe SUPERB benchmark. For example, when fine-tuning HuBERT on automatic speech\nrecognition, Speech-FT is able to reduce phone error rate from 5.17% to 3.94%,\nlower word error rate from 6.38% to 5.75%, and increase speaker identification\naccuracy from 81.86% to 84.11%. Speech-FT provides a simple yet powerful\nsolution for further refining speech representation models after pre-training.", "AI": {"tldr": "Speech-FT is a two-stage fine-tuning framework for speech representation models that enhances task-specific performance while maintaining cross-task generalization.", "motivation": "To improve fine-tuning methods that often degrade cross-task generalization in speech representation models.", "method": "The framework involves a two-stage process: first fine-tuning to reduce representational drift, followed by weight-space interpolation with the pre-trained model.", "result": "Speech-FT consistently improves performance across various fine-tuning scenarios and maintains higher feature similarity to the pre-trained model compared to existing approaches.", "conclusion": "Speech-FT is effective in refining speech representation models and significantly improves performance on benchmarks like SUPERB.", "key_contributions": ["Proposed a novel two-stage fine-tuning framework for speech models", "Demonstrated significant performance improvements on the SUPERB benchmark", "Maintained higher feature similarity to pre-trained models with larger weight updates"], "limitations": "", "keywords": ["speech representation", "fine-tuning", "cross-task generalization", "HuBERT", "SUPERB benchmark"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2502.12904", "pdf": "https://arxiv.org/pdf/2502.12904.pdf", "abs": "https://arxiv.org/abs/2502.12904", "title": "Fraud-R1 : A Multi-Round Benchmark for Assessing the Robustness of LLM Against Augmented Fraud and Phishing Inducements", "authors": ["Shu Yang", "Shenzhe Zhu", "Zeyu Wu", "Keyu Wang", "Junchi Yao", "Junchao Wu", "Lijie Hu", "Mengdi Li", "Derek F. Wong", "Di Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings", "summary": "We introduce Fraud-R1, a benchmark designed to evaluate LLMs' ability to\ndefend against internet fraud and phishing in dynamic, real-world scenarios.\nFraud-R1 comprises 8,564 fraud cases sourced from phishing scams, fake job\npostings, social media, and news, categorized into 5 major fraud types. Unlike\nprevious benchmarks, Fraud-R1 introduces a multi-round evaluation pipeline to\nassess LLMs' resistance to fraud at different stages, including credibility\nbuilding, urgency creation, and emotional manipulation. Furthermore, we\nevaluate 15 LLMs under two settings: 1. Helpful-Assistant, where the LLM\nprovides general decision-making assistance, and 2. Role-play, where the model\nassumes a specific persona, widely used in real-world agent-based interactions.\nOur evaluation reveals the significant challenges in defending against fraud\nand phishing inducement, especially in role-play settings and fake job\npostings. Additionally, we observe a substantial performance gap between\nChinese and English, underscoring the need for improved multilingual fraud\ndetection capabilities.", "AI": {"tldr": "Fraud-R1 is a benchmark for evaluating LLMs' effectiveness in combating internet fraud and phishing through multi-round assessments.", "motivation": "To address the growing issue of internet fraud and phishing, a comprehensive evaluation framework for LLMs is necessary to enhance their resistance in real-world scenarios.", "method": "The benchmark consists of 8,564 fraud cases from various sources and employs a multi-round evaluation pipeline to test LLMs in two settings: Helpful-Assistant and Role-play.", "result": "The evaluation indicates significant difficulties in defending against fraud, particularly in role-play scenarios, and highlights performance disparities between languages, notably between Chinese and English.", "conclusion": "Improving multilingual capabilities is essential for effective fraud detection across various fraud types.", "key_contributions": ["Introduction of a novel benchmark (Fraud-R1) for evaluating LLMs against fraud", "Multi-round evaluation approach to assess LLMs under different decision-making contexts", "Identification of performance gaps in LLMs' abilities to detect fraud in different languages"], "limitations": "The benchmark may not cover all possible fraud scenarios, and the real-world applicability of the findings needs further exploration.", "keywords": ["Fraud Detection", "Large Language Models", "Phishing Prevention", "Benchmarking", "Role-Play"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.12924", "pdf": "https://arxiv.org/pdf/2502.12924.pdf", "abs": "https://arxiv.org/abs/2502.12924", "title": "Conditioning LLMs to Generate Code-Switched Text", "authors": ["Maite Heredia", "Gorka Labaka", "Jeremy Barnes", "Aitor Soroa"], "categories": ["cs.CL", "cs.AI"], "comment": "[v2]Added new experiments and analyses", "summary": "Code-switching (CS) is still a critical challenge in Natural Language\nProcessing (NLP). Current Large Language Models (LLMs) struggle to interpret\nand generate code-switched text, primarily due to the scarcity of large-scale\nCS datasets for training. This paper presents a novel methodology to generate\nCS data using LLMs, and test it on the English-Spanish language pair. We\npropose back-translating natural CS sentences into monolingual English, and\nusing the resulting parallel corpus to fine-tune LLMs to turn monolingual\nsentences into CS. Unlike previous approaches to CS generation, our methodology\nuses natural CS data as a starting point, allowing models to learn its natural\ndistribution beyond grammatical patterns. We thoroughly analyse the models'\nperformance through a study on human preferences, a qualitative error analysis\nand an evaluation with popular automatic metrics. Results show that our\nmethodology generates fluent code-switched text, expanding research\nopportunities in CS communication, and that traditional metrics do not\ncorrelate with human judgement when assessing the quality of the generated CS\ndata. We release our code and generated dataset under a CC-BY-NC-SA license.", "AI": {"tldr": "This paper presents a novel methodology for generating code-switched (CS) data using LLMs, focusing on the English-Spanish language pair.", "motivation": "Current Large Language Models struggle with code-switching due to a lack of large-scale datasets.", "method": "The paper proposes back-translating natural CS sentences into monolingual English to create a parallel corpus for fine-tuning LLMs to produce CS from monolingual sentences.", "result": "The methodology generates fluent code-switched text, expands research opportunities in CS communication, and reveals that traditional metrics do not correlate with human judgment in evaluating CS data quality.", "conclusion": "The approach allows models to learn natural CS distribution, highlighting the importance of human evaluation in assessing model outputs.", "key_contributions": ["Novel methodology for generating CS data using LLMs", "Focus on natural CS data for training", "Findings on the correlation of traditional metrics with human judgement"], "limitations": "Study focused on English-Spanish; results may not generalize to other language pairs.", "keywords": ["code-switching", "natural language processing", "large language models", "dataset generation", "human evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.13034", "pdf": "https://arxiv.org/pdf/2502.13034.pdf", "abs": "https://arxiv.org/abs/2502.13034", "title": "Natural Language Generation from Visual Events: Challenges and Future Directions", "authors": ["Aditya K Surikuchi", "Raquel Fernández", "Sandro Pezzelle"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The ability to use natural language to talk about visual events is at the\ncore of human intelligence and a crucial feature of any artificial intelligence\nsystem. In recent years, a substantial body of work in visually grounded NLP\nhas focused on describing content depicted in single images. By contrast,\ncomparatively less attention has been devoted to exhaustively modeling\nscenarios in which natural language is employed to interpret and talk about\nevents presented through videos or sequences of images. In this position paper,\nwe argue that any NLG task dealing with sequences of images or frames is an\ninstance of the broader, more general problem of modeling the intricate\nrelationships between visual events unfolding over time and the features of the\nlanguage used to interpret, describe, or narrate them. Therefore, solving these\ntasks requires models to be capable of identifying and managing such\nintricacies. We consider five seemingly different tasks, which we argue are\ncompelling instances of this broader multimodal problem. Consistently, we claim\nthat these tasks pose a common set of challenges and share similarities in\nterms of modeling and evaluation approaches. Building on this perspective, we\nidentify key open questions and propose several research directions for future\ninvestigation. We claim that improving language-and-vision models'\nunderstanding of visual events is both timely and essential, given their\ngrowing applications. Additionally, this challenge offers significant\nscientific insight, advancing model development through principles of human\ncognition and language use.", "AI": {"tldr": "This position paper discusses the importance of natural language generation (NLG) in interpreting visual events, particularly in videos, and proposes directions for improving multimodal models.", "motivation": "To address the lack of focus on natural language tasks that interpret sequences of images and videos in the field of visually grounded NLP.", "method": "The paper reviews and analyzes five relevant tasks, outlining their impacts on modeling language and visual events over time and suggesting unified evaluation approaches.", "result": "It highlights common challenges among tasks, emphasizing the need for improved language-and-vision models to properly interpret visual events, which have significant applications.", "conclusion": "Addressing the intricate relationships between time-based visual events and linguistic descriptions is essential for advancing AI capabilities and understanding human cognition.", "key_contributions": ["Identification of five relevant NLG tasks related to visual events in videos.", "Argument for a unified approach in modeling and evaluating language and vision interaction.", "Proposed research directions for advancing multimodal understanding."], "limitations": "", "keywords": ["NLP", "visual events", "multimodal", "language generation", "human cognition"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.13311", "pdf": "https://arxiv.org/pdf/2502.13311.pdf", "abs": "https://arxiv.org/abs/2502.13311", "title": "Training Turn-by-Turn Verifiers for Dialogue Tutoring Agents: The Curious Case of LLMs as Your Coding Tutors", "authors": ["Jian Wang", "Yinpei Dai", "Yichi Zhang", "Ziqiao Ma", "Wenjie Li", "Joyce Chai"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Intelligent tutoring agents powered by large language models (LLMs) have been\nincreasingly explored to deliver personalized knowledge in areas such as\nlanguage learning and science education. However, their capabilities in guiding\nusers to solve complex real-world tasks remain underexplored. To address this\nlimitation, in this work, we focus on coding tutoring, a challenging problem\nthat requires tutors to proactively guide students towards completing\npredefined coding tasks. We propose a novel agent workflow, Trace-and-Verify\n(TRAVER), which combines knowledge tracing to estimate a student's knowledge\nstate and turn-by-turn verification to ensure effective guidance toward task\ncompletion. We introduce DICT, an automatic evaluation protocol that assesses\ntutor agents using controlled student simulation and code generation tests.\nExtensive experiments reveal the challenges of coding tutoring and demonstrate\nthat TRAVER achieves a significantly higher success rate. Although we use code\ntutoring as an example in this paper, our approach can be extended beyond\ncoding, providing valuable insights into advancing tutoring agents for human\ntask learning.", "AI": {"tldr": "This paper presents TRAVER, a novel workflow for coding tutoring agents powered by LLMs that combines knowledge tracing and verification to guide users in completing coding tasks.", "motivation": "There is a growing interest in using intelligent tutoring agents based on large language models for personalized education, but their effectiveness in real-world task guidance is underexplored, particularly in coding.", "method": "The authors propose the Trace-and-Verify (TRAVER) workflow, which uses knowledge tracing to gauge a student's progress and employs turn-by-turn verification to provide adaptive guidance.", "result": "Experimental results show that TRAVER significantly improves the success rate of users completing coding tasks compared to existing tutoring approaches.", "conclusion": "While the study focuses on coding tutoring, the insights gained from TRAVER suggest its methodological contributions are applicable to various human task learning domains.", "key_contributions": ["Introduction of the Trace-and-Verify (TRAVER) workflow for coding tutoring.", "Development of DICT, an evaluation protocol for assessing tutor agents.", "Demonstration of significantly improved success rates in coding task completions with TRAVER."], "limitations": "The study is primarily focused on coding and does not explore wider applications in depth.", "keywords": ["intelligent tutoring agents", "large language models", "coding tutoring", "knowledge tracing", "adaptive guidance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14924", "pdf": "https://arxiv.org/pdf/2502.14924.pdf", "abs": "https://arxiv.org/abs/2502.14924", "title": "A Tale of Two Structures: Do LLMs Capture the Fractal Complexity of Language?", "authors": ["Ibrahim Alabdulmohsin", "Andreas Steiner"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language exhibits a fractal structure in its information-theoretic complexity\n(i.e. bits per token), with self-similarity across scales and long-range\ndependence (LRD). In this work, we investigate whether large language models\n(LLMs) can replicate such fractal characteristics and identify conditions-such\nas temperature setting and prompting method-under which they may fail.\nMoreover, we find that the fractal parameters observed in natural language are\ncontained within a narrow range, whereas those of LLMs' output vary widely,\nsuggesting that fractal parameters might prove helpful in detecting a\nnon-trivial portion of LLM-generated texts. Notably, these findings, and many\nothers reported in this work, are robust to the choice of the architecture;\ne.g. Gemini 1.0 Pro, Mistral-7B and Gemma-2B. We also release a dataset\ncomprising of over 240,000 articles generated by various LLMs (both pretrained\nand instruction-tuned) with different decoding temperatures and prompting\nmethods, along with their corresponding human-generated texts. We hope that\nthis work highlights the complex interplay between fractal properties,\nprompting, and statistical mimicry in LLMs, offering insights for generating,\nevaluating and detecting synthetic texts.", "AI": {"tldr": "This paper investigates the fractal structure of language and how large language models (LLMs) replicate these characteristics, focusing on the impact of temperature settings and prompting methods, and includes a dataset for analysis.", "motivation": "To explore the fractal structure of language and assess if large language models can replicate this complexity, and to understand conditions under which they fail to do so.", "method": "The study analyzes the fractal characteristics of language through a comparison of natural language and LLM output, examining variations based on temperature settings and prompting methods.", "result": "Findings indicate that LLMs exhibit fractal parameters that vary widely compared to the narrow range observed in natural language, suggesting that these parameters could help in detecting LLM-generated texts.", "conclusion": "The work reveals insights into the relationship between fractal properties and LLM outputs, highlighting implications for text generation and evaluation.", "key_contributions": ["Identified fractal characteristics in natural language vs. LLM outputs.", "Revealed the influence of temperature setting and prompting on LLM performance.", "Released a comprehensive dataset of LLM-generated and human-generated texts for further research."], "limitations": "", "keywords": ["Fractal structure", "Large language models", "Text generation", "Language complexity", "Detection of synthetic texts"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.15094", "pdf": "https://arxiv.org/pdf/2502.15094.pdf", "abs": "https://arxiv.org/abs/2502.15094", "title": "Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models", "authors": ["Marianne Chuang", "Gabriel Chuang", "Cheryl Chuang", "John Chuang"], "categories": ["cs.CL", "stat.AP"], "comment": "17 pages, 12 figures. To appear, ClimateNLP 2025", "summary": "We study the use of large language models (LLMs) to both evaluate and\ngreenwash corporate climate disclosures. First, we investigate the use of the\nLLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on\nemissions reduction targets and progress. Second, we probe the behavior of an\nLLM when it is prompted to greenwash a response subject to accuracy and length\nconstraints. Finally, we test the robustness of the LLMJ methodology against\nresponses that may be greenwashed using an LLM. We find that two LLMJ scoring\nsystems, numerical rating and pairwise comparison, are effective in\ndistinguishing high-performing companies from others, with the pairwise\ncomparison system showing greater robustness against LLM-greenwashed responses.", "AI": {"tldr": "The paper examines the effectiveness of large language models in evaluating corporate climate disclosures and their resistance to greenwashing.", "motivation": "To investigate how large language models can be employed to evaluate corporate sustainability reports and exposed to greenwashing tactics.", "method": "The study employs the LLM-as-a-Judge (LLMJ) methodology to score emissions reduction reports and analyzes LLM behavior when prompted to generate greenwashed responses.", "result": "Two scoring systems derived from the LLMJ methodology effectively distinguish companies based on their climate performance, with the pairwise comparison demonstrating superior robustness against LLM-generated greenwashed responses.", "conclusion": "The findings indicate that while LLMs can be useful in assessing corporate sustainability, their susceptibility to greenwashing requires careful methodological design to ensure reliable evaluations.", "key_contributions": ["Introduces LLM-as-a-Judge methodology for scoring corporate climate reports", "Demonstrates effectiveness of LLM scoring systems in differentiating company performance", "Highlights robustness of pairwise comparison method against greenwashing"], "limitations": "Potential limitations include the reliance on LLM capabilities and the need for continuous adaptation to keep pace with evolving greenwashing techniques.", "keywords": ["large language models", "corporate climate disclosures", "greenwashing", "LLM-as-a-Judge", "sustainability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.15361", "pdf": "https://arxiv.org/pdf/2502.15361.pdf", "abs": "https://arxiv.org/abs/2502.15361", "title": "Does Reasoning Introduce Bias? A Study of Social Bias Evaluation and Mitigation in LLM Reasoning", "authors": ["Xuyang Wu", "Jinming Nian", "Ting-Ruen Wei", "Zhiqiang Tao", "Hsin-Tai Wu", "Yi Fang"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Recent advances in large language models (LLMs) have enabled automatic\ngeneration of chain-of-thought (CoT) reasoning, leading to strong performance\non tasks such as math and code. However, when reasoning steps reflect social\nstereotypes (e.g., those related to gender, race or age), they can reinforce\nharmful associations and lead to misleading conclusions. We present the first\nsystematic evaluation of social bias within LLM-generated reasoning, using the\nBBQ dataset to analyze both prediction accuracy and bias. Our study spans a\nwide range of mainstream reasoning models, including instruction-tuned and\nCoT-augmented variants of DeepSeek-R1 (8B/32B), ChatGPT, and other open-source\nLLMs. We quantify how biased reasoning steps correlate with incorrect\npredictions and often lead to stereotype expression. To mitigate\nreasoning-induced bias, we propose Answer Distribution as Bias Proxy (ADBP), a\nlightweight mitigation method that detects bias by tracking how model\npredictions change across incremental reasoning steps. ADBP outperforms a\nstereotype-free baseline in most cases, mitigating bias and improving the\naccuracy of LLM outputs. Code will be released upon paper acceptance.", "AI": {"tldr": "The paper systematically evaluates social bias in reasoning generated by large language models (LLMs) and proposes a mitigation method called Answer Distribution as Bias Proxy (ADBP).", "motivation": "The study is motivated by the potential for LLMs to generate reasoning that reflects harmful social stereotypes, which can lead to misleading conclusions in critical applications.", "method": "The authors analyze LLM outputs using the BBQ dataset, assessing prediction accuracy and bias across various reasoning models, including several instruction-tuned LLMs.", "result": "The evaluation shows a correlation between biased reasoning steps and incorrect predictions. ADBP effectively detects and mitigates this bias, often outperforming stereotype-free baselines.", "conclusion": "The proposed ADBP method not only reduces bias in LLM outputs but also enhances the accuracy of predictions, addressing important ethical concerns in AI applications.", "key_contributions": ["First systematic evaluation of social bias in LLM-generated reasoning", "Development of the ADBP method for detecting and mitigating bias", "Demonstration of bias correlation with prediction inaccuracies"], "limitations": "The study primarily focuses on existing LLMs and may not generalize to future models or completely eliminate bias.", "keywords": ["large language models", "social bias", "reasoning", "mitigation", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.16514", "pdf": "https://arxiv.org/pdf/2502.16514.pdf", "abs": "https://arxiv.org/abs/2502.16514", "title": "GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking", "authors": ["Yingjian Chen", "Haoran Liu", "Yinhong Liu", "Jinxiang Xie", "Rui Yang", "Han Yuan", "Yanran Fu", "Peng Yuan Zhou", "Qingyu Chen", "James Caverlee", "Irene Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are widely used, but they often generate subtle\nfactual errors, especially in long-form text. These errors are fatal in some\nspecialized domains such as medicine. Existing fact-checking with grounding\ndocuments methods face two main challenges: (1) they struggle to understand\ncomplex multihop relations in long documents, often overlooking subtle factual\nerrors; (2) most specialized methods rely on pairwise comparisons, requiring\nmultiple model calls, leading to high resource and computational costs. To\naddress these challenges, we propose GraphCheck, a fact-checking framework that\nuses extracted knowledge graphs to enhance text representation. Graph Neural\nNetworks further process these graphs as a soft prompt, enabling LLMs to\nincorporate structured knowledge more effectively. Enhanced with graph-based\nreasoning, GraphCheck captures multihop reasoning chains that are often\noverlooked by existing methods, enabling precise and efficient fact-checking in\na single inference call. Experimental results on seven benchmarks spanning both\ngeneral and medical domains demonstrate up to a 7.1% overall improvement over\nbaseline models. Notably, GraphCheck outperforms existing specialized\nfact-checkers and achieves comparable performance with state-of-the-art LLMs,\nsuch as DeepSeek-V3 and OpenAI-o1, with significantly fewer parameters.", "AI": {"tldr": "GraphCheck is a fact-checking framework using knowledge graphs to enhance LLM performance, addressing multihop reasoning and resource efficiency.", "motivation": "Existing fact-checking methods struggle with subtle factual errors in long-form text, especially in specialized fields like medicine, and are often resource-intensive.", "method": "GraphCheck employs extracted knowledge graphs and processes them with Graph Neural Networks as soft prompts to improve LLM text representation and reasoning capabilities.", "result": "GraphCheck achieves up to a 7.1% improvement on seven benchmarks in general and medical domains, outperforming existing specialized fact-checkers with fewer parameters.", "conclusion": "GraphCheck provides an innovative solution for efficient and precise fact-checking in long-form texts by leveraging structured knowledge and graph-based reasoning.", "key_contributions": ["Introduction of GraphCheck framework for fact-checking", "Enhanced multihop reasoning capabilities using knowledge graphs", "Significant resource efficiency by reducing model calls"], "limitations": "Further evaluation on more specialized domains is needed to fully understand the framework's applicability.", "keywords": ["fact-checking", "large language models", "knowledge graphs", "graph neural networks", "multihop reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.16880", "pdf": "https://arxiv.org/pdf/2502.16880.pdf", "abs": "https://arxiv.org/abs/2502.16880", "title": "CORAL: Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter", "authors": ["Yepeng Weng", "Dianwen Mei", "Huishi Qiu", "Xujie Chen", "Li Liu", "Jiang Tian", "Zhongchao Shi"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 main conference", "summary": "Speculative decoding is a powerful technique that accelerates Large Language\nModel (LLM) inference by leveraging a lightweight speculative draft model.\nHowever, existing designs suffers in performance due to misalignment between\ntraining and inference. Recent methods have tried to solve this issue by\nadopting a multi-step training strategy, but the complex inputs of different\ntraining steps make it harder for the draft model to converge. To address this,\nwe propose CORAL, a novel framework that improves both accuracy and efficiency\nin speculative drafting. CORAL introduces Cross-Step Representation Alignment,\na method that enhances consistency across multiple training steps,\nsignificantly improving speculative drafting performance. Additionally, we\nidentify the LM head as a major bottleneck in the inference speed of the draft\nmodel. We introduce a weight-grouping mechanism that selectively activates a\nsubset of LM head parameters during inference, substantially reducing the\nlatency of the draft model. We evaluate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming\nstate-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that\nCORAL effectively mitigates training-inference misalignment and delivers\nsignificant speedup for modern LLMs with large vocabularies.", "AI": {"tldr": "CORAL is a framework that enhances speculative decoding in large language models by improving training-inference alignment and reducing inference latency.", "motivation": "To address performance issues in speculative decoding caused by training-inference misalignment and to improve the efficiency of large language model inference.", "method": "Introduce the CORAL framework with Cross-Step Representation Alignment to enhance training step consistency and a weight-grouping mechanism to reduce LM head parameters during inference.", "result": "CORAL achieves speedup ratios of 2.50x-4.07x on multiple LLM families and benchmark datasets, outperforming existing methods like EAGLE-2 and HASS.", "conclusion": "CORAL effectively reduces training-inference misalignment and significantly increases the speed of modern large language models.", "key_contributions": ["Introduced CORAL framework for improved speculative drafting.", "Developed Cross-Step Representation Alignment for training consistency.", "Implemented weight-grouping mechanism to reduce inference latency."], "limitations": "", "keywords": ["speculative decoding", "large language models", "training-inference alignment", "efficiency", "latency reduction"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.17173", "pdf": "https://arxiv.org/pdf/2502.17173.pdf", "abs": "https://arxiv.org/abs/2502.17173", "title": "Cheems: A Practical Guidance for Building and Evaluating Chinese Reward Models from Scratch", "authors": ["Xueru Wen", "Jie Lou", "Zichao Li", "Yaojie Lu", "Xing Yu", "Yuqiu Ji", "Guohai Xu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Debing Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.", "AI": {"tldr": "This paper introduces CheemsBench and CheemsPreference, a new benchmark and dataset for evaluating reward models in Chinese, highlighting the limitations of existing RMs and the importance of human supervision.", "motivation": "To address the lack of robust evaluation benchmarks and datasets for reward models (RMs) in Chinese, which is a gap in existing research that primarily focuses on English.", "method": "The paper introduces CheemsBench, a human-annotated RM evaluation benchmark for Chinese contexts, and CheemsPreference, a large-scale dataset created through human-machine collaboration. The authors evaluate several open-source RMs using these resources.", "result": "Significant limitations were found in the ability of existing RMs to capture human preferences in Chinese contexts. The constructed RM using CheemsPreference achieved state-of-the-art performance on CheemsBench.", "conclusion": "The findings indicate the critical role of high-quality human supervision in the development of reward models to effectively align with human preferences, suggesting that AI-generated data alone is insufficient.", "key_contributions": ["Introduction of CheemsBench, a benchmark for Chinese reward models.", "Development of CheemsPreference, a diverse dataset for RM training.", "Demonstrated limitations of existing RMs in capturing human preferences in Chinese scenarios."], "limitations": "Limited to evaluating RMs in Chinese, which may not generalize to other languages or regions.", "keywords": ["reward models", "human preferences", "Chinese language", "human supervision", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.18001", "pdf": "https://arxiv.org/pdf/2502.18001.pdf", "abs": "https://arxiv.org/abs/2502.18001", "title": "Unveiling the Key Factors for Distilling Chain-of-Thought Reasoning", "authors": ["Xinghao Chen", "Zhijing Sun", "Wenjin Guo", "Miaoran Zhang", "Yanjun Chen", "Yirong Sun", "Hui Su", "Yijie Pan", "Dietrich Klakow", "Wenjie Li", "Xiaoyu Shen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in reasoning tasks through\nChain-of-Thought (CoT) prompting. However, CoT prompting greatly increases\ncomputational demands, which has prompted growing interest in distilling CoT\ncapabilities into Small Language Models (SLMs). This study systematically\nexamines the factors influencing CoT distillation, including the choice of\ngranularity, format and teacher model. Through experiments involving four\nteacher models and seven student models across seven mathematical and\ncommonsense reasoning datasets, we uncover three key findings: (1) Unlike LLMs,\nSLMs exhibit a non-monotonic relationship with granularity, with stronger\nmodels benefiting from finer-grained reasoning and weaker models performing\nbetter with simpler CoT supervision; (2) CoT format significantly impacts LLMs\nbut has minimal effect on SLMs, likely due to their reliance on supervised\nfine-tuning rather than pretraining preferences; (3) Stronger teacher models do\nNOT always produce better student models, as diversity and complexity in CoT\nsupervision can outweigh accuracy alone. These findings emphasize the need to\ntailor CoT strategies to specific student model, offering actionable insights\nfor optimizing CoT distillation in SLMs. The code and datasets are available at\nhttps://github.com/EIT-NLP/Distilling-CoT-Reasoning.", "AI": {"tldr": "This study investigates the distillation of Chain-of-Thought (CoT) capabilities from Large Language Models (LLMs) to Small Language Models (SLMs), identifying key factors that impact effectiveness.", "motivation": "The research aims to optimize the distillation of reasoning capabilities from LLMs to SLMs while addressing the increased computational demands of CoT prompting.", "method": "The study systematically examines CoT distillation factors by experimenting with four teacher models and seven student models on various reasoning datasets, analyzing performance variations based on model granularity, format, and teacher model choice.", "result": "Findings reveal that SLMs demonstrate a non-monotonic relationship with reasoning granularity, CoT format has less impact on SLMs compared to LLMs, and diversity in CoT supervision can be more beneficial than sheer accuracy in teacher models.", "conclusion": "The study underscores the necessity of customizing CoT strategies for distinct student models, providing valuable guidance for enhancing CoT distillation methods in SLMs.", "key_contributions": ["Identified non-monotonic relationship of granularity in SLMs versus LLMs", "Demonstrated limited effect of CoT format on SLM performance", "Highlighted importance of supervision diversity over teacher model accuracy"], "limitations": "", "keywords": ["Chain-of-Thought", "Small Language Models", "reasoning tasks", "distillation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.18791", "pdf": "https://arxiv.org/pdf/2502.18791.pdf", "abs": "https://arxiv.org/abs/2502.18791", "title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs", "authors": ["Jungsoo Park", "Junmo Kang", "Gabriel Stanovsky", "Alan Ritter"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 main conference", "summary": "The surge of LLM studies makes synthesizing their findings challenging.\nAnalysis of experimental results from literature can uncover important trends\nacross studies, but the time-consuming nature of manual data extraction limits\nits use. Our study presents a semi-automated approach for literature analysis\nthat accelerates data extraction using LLMs. It automatically identifies\nrelevant arXiv papers, extracts experimental results and related attributes,\nand organizes them into a structured dataset, LLMEvalDB. We then conduct an\nautomated literature analysis of frontier LLMs, reducing the effort of paper\nsurveying and data extraction by more than 93% compared to manual approaches.\nWe validate LLMEvalDB by showing that it reproduces key findings from a recent\nmanual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new\ninsights that go beyond it, showing, for example, that in-context examples\nbenefit coding & multimodal tasks but offer limited gains in math reasoning\ntasks compared to zero-shot CoT. Our automatically updatable dataset enables\ncontinuous tracking of target models by extracting evaluation studies as new\ndata becomes available. Through LLMEvalDB and empirical analysis, we provide\ninsights into LLMs while facilitating ongoing literature analyses of their\nbehavior.", "AI": {"tldr": "A semi-automated approach using LLMs accelerates literature analysis by extracting experimental results from relevant papers, resulting in a new structured dataset (LLMEvalDB) to facilitate ongoing insights into LLM behaviors.", "motivation": "To address the challenges of synthesizing findings from the increasing volume of LLM studies by reducing the manual effort required for literature analysis and data extraction.", "method": "Developed a semi-automated approach that utilizes LLMs to identify relevant arXiv papers, extract experimental results and their attributes, organizing them into LLMEvalDB for easier access and analysis.", "result": "Reduces the effort of literature surveying and data extraction by over 93% compared to manual methods; LLMEvalDB reproduces key findings and discovers new insights related to LLM performance across different tasks.", "conclusion": "LLMEvalDB serves as a continually updated dataset that supports ongoing analysis of LLMs, providing important insights while also streamlining the process of literature review in LLM research.", "key_contributions": ["Introduction of LLMEvalDB, a structured dataset for LLM analysis", "Significant reduction in data extraction time", "Empirical insights into LLM performance on various tasks"], "limitations": "", "keywords": ["LLM", "literature analysis", "data extraction", "Chain-of-Thought", "empirical analysis"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.19127", "pdf": "https://arxiv.org/pdf/2502.19127.pdf", "abs": "https://arxiv.org/abs/2502.19127", "title": "Exploring the Generalizability of Factual Hallucination Mitigation via Enhancing Precise Knowledge Utilization", "authors": ["Siyuan Zhang", "Yichi Zhang", "Yinpeng Dong", "Hang Su"], "categories": ["cs.CL"], "comment": "31 pages, 17 figures", "summary": "Large Language Models (LLMs) often struggle to align their responses with\nobjective facts, resulting in the issue of factual hallucinations, which can be\ndifficult to detect and mislead users without relevant knowledge. Although\npost-training techniques have been employed to mitigate the issue, existing\nmethods usually suffer from poor generalization and trade-offs in different\ncapabilities. In this paper, we propose to address it by directly augmenting\nLLM's fundamental ability to precisely leverage its knowledge and introduce\nPKUE, which fine-tunes the model on self-generated responses to precise and\nsimple factual questions through preference optimization. Furthermore, we\nconstruct FactualBench, a comprehensive and precise factual QA dataset\ncontaining 181k Chinese data spanning 21 domains, to facilitate both evaluation\nand training. Extensive experiments demonstrate that PKUE significantly\nimproves LLM overall performance, with consistent enhancement across factual\ntasks of various forms, general tasks beyond factuality, and tasks in a\ndifferent language.", "AI": {"tldr": "This paper introduces PKUE, a method to fine-tune LLMs to improve factual accuracy, and constructs FactualBench, a large dataset for evaluating factual QA.", "motivation": "The need to address the problem of factual hallucinations in LLMs which mislead users due to misaligned responses with objective facts.", "method": "PKUE fine-tunes LLMs on self-generated responses to factual questions using preference optimization, alongside the creation of FactualBench, a 181k data QA dataset.", "result": "Extensive experiments show that PKUE enhances LLM performance across factual tasks and general tasks, with improvements consistent in various languages.", "conclusion": "PKUE effectively improves LLM's capability in factual tasks, addressing issues of generalization and performance trade-offs seen in prior methods.", "key_contributions": ["Introduction of PKUE for fine-tuning LLMs on factual questions", "Development of FactualBench dataset for factual QA evaluation", "Demonstration of significant improvement in LLM factual performance across different tasks"], "limitations": "", "keywords": ["Large Language Models", "factual hallucinations", "preference optimization", "factual QA", "dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.19148", "pdf": "https://arxiv.org/pdf/2502.19148.pdf", "abs": "https://arxiv.org/abs/2502.19148", "title": "Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs", "authors": ["Zhaowei Zhang", "Fengshuo Bai", "Qizhi Chen", "Chengdong Ma", "Mingzhi Wang", "Haoran Sun", "Zilong Zheng", "Yaodong Yang"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "Accepted by ICLR 2025, Project page:\n  https://zowiezhang.github.io/projects/Amulet", "summary": "How to align large language models (LLMs) with user preferences from a static\ngeneral dataset has been frequently studied. However, user preferences are\nusually personalized, changing, and diverse regarding culture, values, or time.\nThis leads to the problem that the actual user preferences often do not\ncoincide with those trained by the model developers in the practical use of\nLLMs. Since we cannot collect enough data and retrain for every demand,\nresearching efficient real-time preference adaptation methods based on the\nbackbone LLMs during test time is important. To this end, we introduce Amulet,\na novel, training-free framework that formulates the decoding process of every\ntoken as a separate online learning problem with the guidance of simple\nuser-provided prompts, thus enabling real-time optimization to satisfy users'\npersonalized preferences. To reduce the computational cost brought by this\noptimization process for each token, we additionally provide a closed-form\nsolution for each iteration step of the optimization process, thereby reducing\nthe computational time cost to a negligible level. The detailed experimental\nresults demonstrate that Amulet can achieve significant performance\nimprovements in rich settings with combinations of different LLMs, datasets,\nand user preferences, while maintaining acceptable computational efficiency.", "AI": {"tldr": "Amulet is a novel framework for real-time adaptation of large language models (LLMs) to align with user preferences using online learning techniques and user prompts.", "motivation": "The need for LLMs to align with diverse and changing user preferences, which are often not adequately captured during model training.", "method": "Amulet formulates the decoding of each token as an online learning problem, allowing real-time adaptation based on simple prompts provided by the user.", "result": "Amulet achieves significant performance improvements across various LLMs, datasets, and user preferences while maintaining high computational efficiency.", "conclusion": "Amulet enables effective and efficient real-time optimization of LLMs to meet personalized user demands without the need for retraining.", "key_contributions": ["Introduction of a training-free framework for real-time preference adaptation in LLMs.", "Formulation of token decoding as an online learning problem.", "Provision of a closed-form solution to reduce computational costs during optimization."], "limitations": "", "keywords": ["large language models", "real-time adaptation", "user preferences", "online learning", "computational efficiency"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.19735", "pdf": "https://arxiv.org/pdf/2502.19735.pdf", "abs": "https://arxiv.org/abs/2502.19735", "title": "R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning Learning", "authors": ["Minggui He", "Yilun Liu", "Shimin Tao", "Yuanchang Luo", "Hongyong Zeng", "Chang Su", "Li Zhang", "Hongxia Ma", "Daimeng Wei", "Weibin Meng", "Hao Yang", "Boxing Chen", "Osamu Yoshie"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans and supervised fine-tuning (SFT) prone to overfitting, limiting their\nadaptability to diverse translation scenarios. This paper introduces\nR1-Translator (R1-T1), a novel framework to achieve inference-time reasoning\nfor general MT via reinforcement learning (RL) with human-aligned CoTs\ncomprising six common patterns. Our approach pioneers three innovations: (1)\nextending reasoning-based translation to broader MT scenarios (e.g.,\nmultilingual MT, domain MT) unseen in the training phase; (2) formalizing six\nexpert-curated CoT templates that mirror hybrid human strategies like\ncontext-aware paraphrasing and back translation; and (3) enabling self-evolving\nCoT discovery through RL. Both human and automatic evaluation results indicate\na steady translation performance improvement in a total of 10+ languages and\n40+ translation directions on Flores-101 test set and four domain-specific MT\ntasks, especially on the languages unseen from training.", "AI": {"tldr": "This paper presents R1-Translator, a framework that enhances machine translation (MT) by incorporating inference-time reasoning through reinforcement learning, using human-aligned chain-of-thought (CoT) patterns.", "motivation": "To explore the integration of structured reasoning in machine translation, an area that remains underexploited despite its relevance to human translator methods.", "method": "The framework employs reinforcement learning to develop and utilize six expert-curated CoT templates that align with human reasoning strategies in MT tasks.", "result": "The results show improved translation performance across 10+ languages and multiple translation scenarios, particularly in languages that were not included in the training phase.", "conclusion": "R1-Translator demonstrates that reasoning-based translation can significantly enhance MT adaptability and performance in diverse contexts.", "key_contributions": ["Extension of reasoning-based translation to broader MT scenarios, including unseen languages.", "Formalization of six human-aligned CoT templates.", "Implementation of self-evolving CoT discovery via reinforcement learning."], "limitations": "", "keywords": ["machine translation", "reinforcement learning", "chain-of-thought", "language models", "reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.19953", "pdf": "https://arxiv.org/pdf/2502.19953.pdf", "abs": "https://arxiv.org/abs/2502.19953", "title": "GeoEdit: Geometric Knowledge Editing for Large Language Models", "authors": ["Yujie Feng", "Liming Zhan", "Zexin Lu", "Yongxin Xu", "Xu Chu", "Yasha Wang", "Jiannong Cao", "Philip S. Yu", "Xiao-Ming Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Regular updates are essential for maintaining up-to-date knowledge in large\nlanguage models (LLMs). Consequently, various model editing methods have been\ndeveloped to update specific knowledge within LLMs. However, training-based\napproaches often struggle to effectively incorporate new knowledge while\npreserving unrelated general knowledge. To address this challenge, we propose a\nnovel framework called Geometric Knowledge Editing (GeoEdit). GeoEdit utilizes\nthe geometric relationships of parameter updates from fine-tuning to\ndifferentiate between neurons associated with new knowledge updates and those\nrelated to general knowledge perturbations. By employing a direction-aware\nknowledge identification method, we avoid updating neurons with directions\napproximately orthogonal to existing knowledge, thus preserving the model's\ngeneralization ability. For the remaining neurons, we integrate both old and\nnew knowledge for aligned directions and apply a \"forget-then-learn\" editing\nstrategy for opposite directions. Additionally, we introduce an\nimportance-guided task vector fusion technique that filters out redundant\ninformation and provides adaptive neuron-level weighting, further enhancing\nmodel editing performance. Extensive experiments on two publicly available\ndatasets demonstrate the superiority of GeoEdit over existing state-of-the-art\nmethods.", "AI": {"tldr": "GeoEdit is a novel framework for efficiently updating knowledge in large language models while preserving general knowledge using geometric relationships of parameter updates.", "motivation": "Regular updates are vital for maintaining current knowledge in LLMs, but existing training methods often fail to balance the incorporation of new knowledge without losing general knowledge.", "method": "GeoEdit utilizes a direction-aware knowledge identification method to differentiate between neurons for new and general knowledge, employing a 'forget-then-learn' approach and an importance-guided task vector fusion technique.", "result": "GeoEdit outperforms existing state-of-the-art methods on two publicly available datasets for knowledge editing in LLMs.", "conclusion": "GeoEdit improves model editing performance by effectively managing the update of new knowledge while conserving general knowledge, ensuring better model generalization.", "key_contributions": ["Introduction of the GeoEdit framework for knowledge editing in LLMs.", "Direction-aware identification of knowledge-related neurons to preserve model generalization.", "Importance-guided task vector fusion to filter redundant information."], "limitations": "", "keywords": ["Large Language Models", "Knowledge Editing", "Machine Learning", "Geometric Methods", "Model Preservation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.21017", "pdf": "https://arxiv.org/pdf/2502.21017.pdf", "abs": "https://arxiv.org/abs/2502.21017", "title": "PersuasiveToM: A Benchmark for Evaluating Machine Theory of Mind in Persuasive Dialogues", "authors": ["Fangxu Yu", "Lai Jiang", "Shenyi Huang", "Zhen Wu", "Xinyu Dai"], "categories": ["cs.CL"], "comment": null, "summary": "The ability to understand and predict the mental states of oneself and\nothers, known as the Theory of Mind (ToM), is crucial for effective social\nscenarios. Although recent studies have evaluated ToM in Large Language Models\n(LLMs), existing benchmarks focus on simplified settings (e.g.,\nSally-Anne-style tasks) and overlook the complexity of real-world social\ninteractions. To mitigate this gap, we propose PersuasiveToM, a benchmark\ndesigned to evaluate the ToM abilities of LLMs in persuasive dialogues. Our\nframework contains two core tasks: ToM Reasoning, which tests tracking of\nevolving desires, beliefs, and intentions; and ToM Application, which assesses\nthe use of inferred mental states to predict and evaluate persuasion\nstrategies. Experiments across eight leading LLMs reveal that while models\nexcel on multiple questions, they struggle with the tasks that need tracking\nthe dynamics and shifts of mental states and understanding the mental states in\nthe whole dialogue comprehensively. Our aim with PersuasiveToM is to allow an\neffective evaluation of the ToM reasoning ability of LLMs with more focus on\ncomplex psychological activities. Our code is available at\nhttps://github.com/Yu-Fangxu/PersuasiveToM.", "AI": {"tldr": "PersuasiveToM is a new benchmark for evaluating the Theory of Mind abilities of LLMs in persuasive dialogues, focusing on complex social interactions.", "motivation": "To address the limitations of existing benchmarks that do not capture the complexity of real-world social interactions in evaluating Theory of Mind in LLMs.", "method": "PersuasiveToM introduces two tasks: ToM Reasoning for tracking desires, beliefs, and intentions, and ToM Application for assessing the use of inferred mental states in persuasion strategies. It tests eight leading LLMs.", "result": "Experiments show that while LLMs perform well on simple questions, they struggle significantly with tracking evolving mental states and understanding comprehensive dialogues.", "conclusion": "PersuasiveToM enables a more effective evaluation of ToM reasoning in LLMs, specifically targeting complex psychological activities.", "key_contributions": ["Introduction of PersuasiveToM benchmark for LLMs", "Focus on persuasive dialogues and complex social interactions", "Identification of LLMs' struggles with evolving mental state tracking"], "limitations": "", "keywords": ["Theory of Mind", "Persuasive dialogues", "Large Language Models", "Benchmark", "Social interactions"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.00032", "pdf": "https://arxiv.org/pdf/2503.00032.pdf", "abs": "https://arxiv.org/abs/2503.00032", "title": "Detecting LLM-Generated Korean Text through Linguistic Feature Analysis", "authors": ["Shinwoo Park", "Shubin Kim", "Do-Kyung Kim", "Yo-Sub Han"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.", "AI": {"tldr": "The paper proposes KatFish, a benchmark dataset and a detection method (KatFishNet) for distinguishing between human-written and LLM-generated Korean text.", "motivation": "With the rise of large language models (LLMs), it is increasingly difficult to differentiate between human and LLM-generated text, which is essential for maintaining academic integrity and ethical research practices.", "method": "The authors created a benchmark dataset named KatFish featuring human-written and LLM-generated Korean text, and developed KatFishNet, a detection method that analyses linguistic features such as spacing patterns and part-of-speech diversity.", "result": "KatFishNet outperforms the best existing detection methods by an average of 19.78% in AUROC scores.", "conclusion": "The proposed dataset and detection method contribute significantly to the field of LLM detection for non-English languages, particularly Korean, and offer tools to uphold ethical standards in text generation.", "key_contributions": ["Introduction of KatFish, a benchmark dataset for Korean LLM-generated text.", "Development of KatFishNet, a specialized detection method for Korean text.", "Demonstration of significant performance improvement in detecting LLM-generated text compared to existing methods."], "limitations": "", "keywords": ["LLM detection", "Korean language", "KatFish", "KatFishNet", "linguistic analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.01763", "pdf": "https://arxiv.org/pdf/2503.01763.pdf", "abs": "https://arxiv.org/abs/2503.01763", "title": "Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models", "authors": ["Zhengliang Shi", "Yuhan Wang", "Lingyong Yan", "Pengjie Ren", "Shuaiqiang Wang", "Dawei Yin", "Zhaochun Ren"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025. Code: https://github.com/mangopy/tool-retrieval-benchmark", "summary": "Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels.", "AI": {"tldr": "The paper introduces ToolRet, a benchmark for tool retrieval in large language models, highlighting the poor performance of existing IR models on diverse tasks.", "motivation": "To address the limited context length of tool-using LLMs and the underexplored performance of information retrieval models in selecting useful tools from large toolsets.", "method": "The authors propose ToolRet, a heterogeneous benchmark consisting of 7.6k retrieval tasks and a corpus of 43k tools, and benchmark six types of models on this new task.", "result": "The models tested showed weak performance on ToolRet compared to conventional benchmarks, affecting the overall task pass rate for tool-using LLMs.", "conclusion": "The work highlights the need for improved retrieval capabilities in LLMs and provides a large-scale training dataset of over 200k instances to optimize IR models.", "key_contributions": ["Introduction of ToolRet benchmark for tool retrieval tasks.", "Demonstration of poor performance of conventional IR models in tool retrieval.", "Provision of a large-scale training dataset to enhance tool retrieval capabilities."], "limitations": "", "keywords": ["tool learning", "large language models", "information retrieval", "benchmark", "tool retrieval"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.02863", "pdf": "https://arxiv.org/pdf/2503.02863.pdf", "abs": "https://arxiv.org/abs/2503.02863", "title": "SteerConf: Steering LLMs for Confidence Elicitation", "authors": ["Ziang Zhou", "Tianyuan Jin", "Jieming Shi", "Qing Li"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) exhibit impressive performance across diverse\ndomains but often suffer from overconfidence, limiting their reliability in\ncritical applications. We propose SteerConf, a novel framework that\nsystematically steers LLMs' confidence scores to improve their calibration and\nreliability. SteerConf introduces three key components: (1) a steering prompt\nstrategy that guides LLMs to produce confidence scores in specified directions\n(e.g., conservative or optimistic) by leveraging prompts with varying steering\nlevels; (2) a steered confidence consistency measure that quantifies alignment\nacross multiple steered confidences to enhance calibration; and (3) a steered\nconfidence calibration method that aggregates confidence scores using\nconsistency measures and applies linear quantization for answer selection.\nSteerConf operates without additional training or fine-tuning, making it\nbroadly applicable to existing LLMs. Experiments on seven benchmarks spanning\nprofessional knowledge, common sense, ethics, and reasoning tasks, using\nadvanced LLM models (GPT-3.5, LLaMA 3, GPT-4), demonstrate that SteerConf\nsignificantly outperforms existing methods, often by a significant margin. Our\nfindings highlight the potential of steering the confidence of LLMs to enhance\ntheir reliability for safer deployment in real-world applications.", "AI": {"tldr": "SteerConf is a framework that enhances the reliability of LLMs by improving their confidence score calibration through steering prompts, consistency measures, and calibration methods.", "motivation": "LLMs often exhibit overconfidence which affects their reliability, particularly in critical applications; therefore, improving their calibration is essential.", "method": "SteerConf introduces a steering prompt strategy, a steered confidence consistency measure, and a calibration method that uses linear quantization to improve LLMs' confidence scores without any additional training.", "result": "Experimental results show that SteerConf significantly outperforms existing calibration methods across various benchmarks, including professional knowledge and ethical reasoning tasks.", "conclusion": "SteerConf demonstrates that adequately adjusting LLMs' confidence can lead to enhanced performance and reliability in real-world applications.", "key_contributions": ["Steering prompt strategy for confidence score directionality.", "Steered confidence consistency measure for better calibration.", "Calibration method using linear quantization for answer selection."], "limitations": "The framework operates without additional training, which may limit its customization for specific tasks.", "keywords": ["Large Language Models", "confidence calibration", "steering prompts", "machine learning", "reliability"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.02972", "pdf": "https://arxiv.org/pdf/2503.02972.pdf", "abs": "https://arxiv.org/abs/2503.02972", "title": "LINGOLY-TOO: Disentangling Memorisation from Knowledge with Linguistic Templatisation and Orthographic Obfuscation", "authors": ["Jude Khouja", "Karolina Korgul", "Simi Hellsten", "Lingyi Yang", "Vlad Neacsu", "Harry Mayne", "Ryan Kearns", "Andrew Bean", "Adam Mahdi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The expanding knowledge and memorisation capacity of frontier language models\nallows them to solve many reasoning tasks directly by exploiting prior\nknowledge, leading to inflated estimates of their reasoning abilities. We\nintroduce LINGOLY-TOO, a challenging reasoning benchmark grounded in natural\nlanguage and designed to counteract the effect of non-reasoning abilities on\nreasoning estimates. Using linguistically informed rulesets, we permute\nreasoning problems written in real languages to generate numerous question\nvariations. These permutations preserve the intrinsic reasoning steps required\nfor each solution while reducing the likelihood problems are directly solvable\nwith models' knowledge. Experiments and analyses show that models can\ncircumvent reasoning and answer from prior knowledge. On a metric that rewards\nconsistent reasoning, all models perform poorly and exhibit high variance\nacross question permutations, indicating that Large Language Models' (LLMs)\nreasoning faculty remains brittle. Overall, results on the benchmark reflect\nthe recent progress of Inference-Time Compute (ITC) models but suggest ample\nroom for further improvement. The benchmark is a step towards better\nmeasurement of reasoning abilities of LLMs and offers a cautionary tale on the\nimportance of disentangling reasoning abilities from models' internalised\nknowledge when developing reasoning benchmarks.", "AI": {"tldr": "LINGOLY-TOO is a new benchmark that evaluates reasoning abilities of language models independently from their prior knowledge, highlighting their limitations in consistent reasoning.", "motivation": "To address inflated estimates of language models' reasoning abilities by evaluating their performance on reasoning tasks that are not solvable by prior knowledge.", "method": "The benchmark introduces permuted reasoning problems using linguistically informed rulesets, allowing for multiple question variations while retaining the original reasoning steps needed for solutions.", "result": "Experiments reveal that language models, when evaluated on consistent reasoning, perform poorly and show significant variance across question permutations, indicating brittle reasoning capabilities.", "conclusion": "The study emphasizes the need to distinguish between reasoning ability and prior knowledge in benchmarks, suggesting significant room for improvement in language model reasoning.", "key_contributions": ["Introduction of LINGOLY-TOO benchmark for language model reasoning evaluation.", "Use of permutation-based problems to isolate reasoning from prior knowledge.", "Highlighting the brittleness of LLM reasoning abilities through experimental results."], "limitations": "The benchmark still relies on the existing reasoning capabilities of models and might not fully account for all aspects of reasoning.", "keywords": ["Language Models", "Reasoning Benchmark", "Natural Language Processing", "Machine Learning", "Inference-Time Compute"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.03862", "pdf": "https://arxiv.org/pdf/2503.03862.pdf", "abs": "https://arxiv.org/abs/2503.03862", "title": "Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions", "authors": ["Emmy Liu", "Amanda Bertsch", "Lintang Sutawika", "Lindia Tjuatja", "Patrick Fernandes", "Lara Marinov", "Michael Chen", "Shreya Singhal", "Carolin Lawrence", "Aditi Raghunathan", "Kiril Gashteovski", "Graham Neubig"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Improvements in language model capabilities are often attributed to\nincreasing model size or training data, but in some cases smaller models\ntrained on curated data or with different architectural decisions can\noutperform larger ones trained on more tokens. What accounts for this? To\nquantify the impact of these design choices, we meta-analyze 92 open-source\npretrained models across a wide array of scales, including state-of-the-art\nopen-weights models as well as less performant models and those with less\nconventional design decisions. We find that by incorporating features besides\nmodel size and number of training tokens, we can achieve a relative 3-28%\nincrease in ability to predict downstream performance compared with using scale\nalone. Analysis of model design decisions reveal insights into data\ncomposition, such as the trade-off between language and code tasks at 15-25\\%\ncode, as well as the better performance of some architectural decisions such as\nchoosing rotary over learned embeddings. Broadly, our framework lays a\nfoundation for more systematic investigation of how model development choices\nshape final capabilities.", "AI": {"tldr": "A meta-analysis of 92 open-source pretrained language models reveals that smaller, curated models can outperform larger ones due to design choices, achieving a 3-28% increase in performance prediction by considering factors beyond model size.", "motivation": "To quantify the effects of various design choices in language models beyond just size and training data, examining why smaller models can outperform larger ones under certain conditions.", "method": "A meta-analysis of 92 open-source pretrained language models, comparing performance based on different model architectures and data sources.", "result": "Found that including design choice characteristics leads to a 3-28% improvement in predicting model performance compared to only using model size and training tokens.", "conclusion": "The study provides a framework for future investigations into how various decisions during model development impact their capabilities.", "key_contributions": ["Meta-analysis of 92 pretrained models", "Insights into the impact of design decisions on performance", "Framework for systematic investigation of model development choices"], "limitations": "", "keywords": ["language models", "model performance", "architectural decisions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.04240", "pdf": "https://arxiv.org/pdf/2503.04240.pdf", "abs": "https://arxiv.org/abs/2503.04240", "title": "DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models", "authors": ["Ruizhe Chen", "Wenhao Chai", "Zhifei Yang", "Xiaotian Zhang", "Joey Tianyi Zhou", "Tony Quek", "Soujanya Poria", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B.", "AI": {"tldr": "This paper introduces Diffusion-styled Preference Optimization (\u0005model), a novel method for aligning large language models (LLMs) with human preferences efficiently at the sentence level, enhancing performance while reducing inference-time latency.", "motivation": "To address limited scalability and latency challenges in existing inference-time alignment methods for LLMs.", "method": "The proposed \u0005model performs alignment directly at the sentence level rather than at the token level, designed to be a plug-and-play module for various base models.", "result": "\u0005model shows superior alignment performance across multiple benchmarks (AlpacaEval 2, MT-bench, HH-RLHF), achieving a balance between alignment quality and latency, and enhancing large models like Llama-3-70B.", "conclusion": "The proposed method improves the alignment of LLMs with human preferences while maintaining scalability and reducing latency in inference.", "key_contributions": ["Introduction of a policy-agnostic alignment method", "Improvement of alignment performance at sentence level", "Scalability enhancement for large models"], "limitations": "", "keywords": ["Preference Optimization", "LLMs", "Alignment", "Inference", "Scalability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.04856", "pdf": "https://arxiv.org/pdf/2503.04856.pdf", "abs": "https://arxiv.org/abs/2503.04856", "title": "One-Shot is Enough: Consolidating Multi-Turn Attacks into Efficient Single-Turn Prompts for LLMs", "authors": ["Junwoo Ha", "Hyunjun Kim", "Sangyoon Yu", "Haon Park", "Ashkan Yousefpour", "Yuna Park", "Suhyun Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce a novel framework for consolidating multi-turn adversarial\n``jailbreak'' prompts into single-turn queries, significantly reducing the\nmanual overhead required for adversarial testing of large language models\n(LLMs). While multi-turn human jailbreaks have been shown to yield high attack\nsuccess rates, they demand considerable human effort and time. Our\nmulti-turn-to-single-turn (M2S) methods -- Hyphenize, Numberize, and Pythonize\n-- systematically reformat multi-turn dialogues into structured single-turn\nprompts. Despite removing iterative back-and-forth interactions, these prompts\npreserve and often enhance adversarial potency: in extensive evaluations on the\nMulti-turn Human Jailbreak (MHJ) dataset, M2S methods achieve attack success\nrates from 70.6 percent to 95.9 percent across several state-of-the-art LLMs.\nRemarkably, the single-turn prompts outperform the original multi-turn attacks\nby as much as 17.5 percentage points while cutting token usage by more than\nhalf on average. Further analysis shows that embedding malicious requests in\nenumerated or code-like structures exploits ``contextual blindness'', bypassing\nboth native guardrails and external input-output filters. By converting\nmulti-turn conversations into concise single-turn prompts, the M2S framework\nprovides a scalable tool for large-scale red teaming and reveals critical\nweaknesses in contemporary LLM defenses.", "AI": {"tldr": "A framework for converting multi-turn adversarial prompts to single-turn queries, improving efficiency in testing LLM defenses.", "motivation": "To reduce manual effort in adversarial testing of large language models while maintaining or enhancing the effectiveness of adversarial prompts.", "method": "The paper introduces M2S methods (Hyphenize, Numberize, Pythonize) to reformulate multi-turn dialogues into structured single-turn prompts.", "result": "M2S methods achieve attack success rates between 70.6% and 95.9% on the Multi-turn Human Jailbreak dataset, outperforming multi-turn prompts by up to 17.5 percentage points and reducing token usage by over 50%.", "conclusion": "The M2S framework serves as a scalable tool for adversarial testing, highlighting vulnerabilities in LLM defenses.", "key_contributions": ["Introduces the M2S framework for prompt consolidation.", "Demonstrates higher success rates and reduced token usage with single-turn prompts.", "Reveals contextual vulnerabilities in LLM defenses."], "limitations": "", "keywords": ["adversarial testing", "large language models", "multi-turn prompts", "single-turn queries", "ML vulnerabilities"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.06692", "pdf": "https://arxiv.org/pdf/2503.06692.pdf", "abs": "https://arxiv.org/abs/2503.06692", "title": "InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models", "authors": ["Yuchen Yan", "Yongliang Shen", "Yang Liu", "Jin Jiang", "Mengdi Zhang", "Jian Shao", "Yueting Zhuang"], "categories": ["cs.CL", "cs.AI"], "comment": "Project Page: https://zju-real.github.io/InftyThink Code:\n  https://github.com/ZJU-REAL/InftyThink", "summary": "Advanced reasoning in large language models has achieved remarkable\nperformance on challenging tasks, but the prevailing long-context reasoning\nparadigm faces critical limitations: quadratic computational scaling with\nsequence length, reasoning constrained by maximum context boundaries, and\nperformance degradation beyond pre-training context windows. Existing\napproaches primarily compress reasoning chains without addressing the\nfundamental scaling problem. To overcome these challenges, we introduce\nInftyThink, a paradigm that transforms monolithic reasoning into an iterative\nprocess with intermediate summarization. By interleaving short reasoning\nsegments with concise progress summaries, our approach enables unbounded\nreasoning depth while maintaining bounded computational costs. This creates a\ncharacteristic sawtooth memory pattern that significantly reduces computational\ncomplexity compared to traditional approaches. Furthermore, we develop a\nmethodology for reconstructing long-context reasoning datasets into our\niterative format, transforming OpenR1-Math into 333K training instances.\nExperiments across multiple model architectures demonstrate that our approach\nreduces computational costs while improving performance, with Qwen2.5-Math-7B\nshowing 3-13% improvements across MATH500, AIME24, and GPQA_diamond benchmarks.\nOur work challenges the assumed trade-off between reasoning depth and\ncomputational efficiency, providing a more scalable approach to complex\nreasoning without architectural modifications.", "AI": {"tldr": "InftyThink transforms monolithic reasoning in language models into an iterative process with intermediate summaries, reducing computational complexity and improving performance in long-context reasoning.", "motivation": "To address the critical limitations of long-context reasoning in large language models, such as quadratic computational scaling and performance degradation beyond context boundaries.", "method": "InftyThink introduces an iterative reasoning process interleaved with summarization, creating a sawtooth memory pattern that maintains computational efficiency.", "result": "Our methodology, applied to OpenR1-Math, resulted in 333K training instances and demonstrated 3-13% performance improvements on various benchmarks.", "conclusion": "InftyThink presents a scalable approach to complex reasoning in language models, challenging the trade-offs between reasoning depth and computational efficiency.", "key_contributions": ["Introduction of InftyThink for iterative reasoning", "Transformation of long-context reasoning datasets into iterative format", "Empirical validation of reduced computational costs and improved performance"], "limitations": "", "keywords": ["large language models", "iterative reasoning", "computational efficiency"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2503.09600", "pdf": "https://arxiv.org/pdf/2503.09600.pdf", "abs": "https://arxiv.org/abs/2503.09600", "title": "MoC: Mixtures of Text Chunking Learners for Retrieval-Augmented Generation System", "authors": ["Jihao Zhao", "Zhiyuan Ji", "Zhaoxin Fan", "Hanyu Wang", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG), while serving as a viable complement to\nlarge language models (LLMs), often overlooks the crucial aspect of text\nchunking within its pipeline. This paper initially introduces a dual-metric\nevaluation method, comprising Boundary Clarity and Chunk Stickiness, to enable\nthe direct quantification of chunking quality. Leveraging this assessment\nmethod, we highlight the inherent limitations of traditional and semantic\nchunking in handling complex contextual nuances, thereby substantiating the\nnecessity of integrating LLMs into chunking process. To address the inherent\ntrade-off between computational efficiency and chunking precision in LLM-based\napproaches, we devise the granularity-aware Mixture-of-Chunkers (MoC)\nframework, which consists of a three-stage processing mechanism. Notably, our\nobjective is to guide the chunker towards generating a structured list of\nchunking regular expressions, which are subsequently employed to extract chunks\nfrom the original text. Extensive experiments demonstrate that both our\nproposed metrics and the MoC framework effectively settle challenges of the\nchunking task, revealing the chunking kernel while enhancing the performance of\nthe RAG system.", "AI": {"tldr": "This paper addresses the neglected role of text chunking in Retrieval-Augmented Generation (RAG) for LLMs by introducing new metrics and a framework to improve chunk quality and system performance.", "motivation": "To examine and improve the text chunking process in RAG systems for better integration with LLMs and to enhance context handling.", "method": "Introduces a dual-metric evaluation method for chunking quality and devises the Mixture-of-Chunkers (MoC) framework consisting of a three-stage processing mechanism.", "result": "Experiments show that the proposed metrics and the MoC framework effectively improve chunking quality, addressing the challenges in the RAG system.", "conclusion": "The study validates the importance of chunking improvement in RAG and presents methods that enhance performance while managing trade-offs.", "key_contributions": ["Introduction of Boundary Clarity and Chunk Stickiness metrics for chunking quality assessment", "Development of the Mixture-of-Chunkers framework", "Demonstration of improved performance in RAG systems through effective chunking."], "limitations": "Focuses primarily on chunking without extensively addressing other aspects of RAG systems.", "keywords": ["Retrieval-Augmented Generation", "text chunking", "large language models", "Mixture-of-Chunkers", "chunk quality assessment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.10497", "pdf": "https://arxiv.org/pdf/2503.10497.pdf", "abs": "https://arxiv.org/abs/2503.10497", "title": "MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation", "authors": ["Weihao Xuan", "Rui Yang", "Heli Qi", "Qingcheng Zeng", "Yunze Xiao", "Aosong Feng", "Dairui Liu", "Yun Xing", "Junjue Wang", "Fan Gao", "Jinghui Lu", "Yuang Jiang", "Huitao Li", "Xin Li", "Kunyu Yu", "Ruihai Dong", "Shangding Gu", "Yuekang Li", "Xiaofei Xie", "Felix Juefei-Xu", "Foutse Khomh", "Osamu Yoshie", "Qingyu Chen", "Douglas Teodoro", "Nan Liu", "Randy Goebel", "Lei Ma", "Edison Marrese-Taylor", "Shijian Lu", "Yusuke Iwasawa", "Yutaka Matsuo", "Irene Li"], "categories": ["cs.CL"], "comment": null, "summary": "Existing large language model (LLM) evaluation benchmarks primarily focus on\nEnglish, while current multilingual tasks lack parallel questions that\nspecifically assess cross-linguistic reasoning abilities. This dual limitation\nmakes it challenging to comprehensively assess LLMs' performance in the\nmultilingual setting. To fill this gap, we introduce MMLU-ProX, a comprehensive\nbenchmark covering 29 languages, built on an English benchmark. Each language\nversion consists of 11,829 identical questions, enabling direct\ncross-linguistic comparisons. Additionally, to meet efficient evaluation needs,\nwe provide a lite version containing 658 questions per language. To ensure the\nhigh quality of MMLU-ProX, we employ a rigorous development process that\ninvolves multiple powerful LLMs for translation, followed by expert review to\nensure accurate expression, consistent terminology, and cultural relevance.\nBuilding on this, we systematically evaluate 36 state-of-the-art LLMs,\nincluding reasoning-enhanced and multilingual-optimized LLMs. The results\nreveal significant disparities in the multilingual capabilities of LLMs: While\nthey perform well in high-resource languages, their performance declines\nmarkedly in low-resource languages, with gaps of up to 24.3%. Through\nMMLU-ProX, we aim to advance the development of more inclusive AI systems and\npromote equitable access to technology across global contexts.", "AI": {"tldr": "MMLU-ProX is a new multilingual benchmark assessing LLMs across 29 languages with identical questions for effective cross-linguistic evaluations.", "motivation": "To address the lack of multilingual evaluation benchmarks that assess cross-linguistic reasoning abilities in LLMs.", "method": "Creation of MMLU-ProX involving a comprehensive benchmark with 11,829 identical questions across 29 languages and a lite version with 658 questions per language, alongside rigorous translation and expert review processes.", "result": "Evaluation of 36 state-of-the-art LLMs showed significant performance disparities, with high-resource languages performing well, while low-resource languages experienced declines up to 24.3%.", "conclusion": "MMLU-ProX will help develop inclusive AI systems and promote equitable technology access globally.", "key_contributions": ["Introduction of MMLU-ProX as a multilingual LLM benchmark", "Systematic evaluation of multilingual capabilities of 36 state-of-the-art LLMs", "Identification of performance gaps in LLMs between high-resource and low-resource languages"], "limitations": "", "keywords": ["Large Language Models", "Multilingual Evaluation", "Cross-linguistic Reasoning", "Benchmark", "AI Equity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.10688", "pdf": "https://arxiv.org/pdf/2503.10688.pdf", "abs": "https://arxiv.org/abs/2503.10688", "title": "CULEMO: Cultural Lenses on Emotion -- Benchmarking LLMs for Cross-Cultural Emotion Understanding", "authors": ["Tadesse Destaw Belay", "Ahmed Haj Ahmed", "Alvin Grissom II", "Iqra Ameer", "Grigori Sidorov", "Olga Kolesnikova", "Seid Muhie Yimam"], "categories": ["cs.CL"], "comment": "ACL-main 2025", "summary": "NLP research has increasingly focused on subjective tasks such as emotion\nanalysis. However, existing emotion benchmarks suffer from two major\nshortcomings: (1) they largely rely on keyword-based emotion recognition,\noverlooking crucial cultural dimensions required for deeper emotion\nunderstanding, and (2) many are created by translating English-annotated data\ninto other languages, leading to potentially unreliable evaluation. To address\nthese issues, we introduce Cultural Lenses on Emotion (CuLEmo), the first\nbenchmark designed to evaluate culture-aware emotion prediction across six\nlanguages: Amharic, Arabic, English, German, Hindi, and Spanish. CuLEmo\ncomprises 400 crafted questions per language, each requiring nuanced cultural\nreasoning and understanding. We use this benchmark to evaluate several\nstate-of-the-art LLMs on culture-aware emotion prediction and sentiment\nanalysis tasks. Our findings reveal that (1) emotion conceptualizations vary\nsignificantly across languages and cultures, (2) LLMs performance likewise\nvaries by language and cultural context, and (3) prompting in English with\nexplicit country context often outperforms in-language prompts for\nculture-aware emotion and sentiment understanding. The dataset and evaluation\ncode are publicly available.", "AI": {"tldr": "Introduction of CuLEmo, a benchmark for culture-aware emotion prediction in NLP.", "motivation": "To address shortcomings in existing emotion benchmarks related to cultural dimensions and language reliability.", "method": "Development of the Cultural Lenses on Emotion (CuLEmo) benchmark, evaluating cultural emotion prediction across six languages with crafted cultural reasoning questions.", "result": "Significant variation in emotion conceptualizations and LLM performance across languages; English prompts with cultural context perform better than in-language prompts.", "conclusion": "The CuLEmo dataset can enhance understanding of emotion across cultures and improve NLP emotion tasks.", "key_contributions": ["Introduction of the first culture-aware emotion benchmark across multiple languages", "Highlighting the variation of emotion understanding and LLM performance across cultures", "Providing publicly available dataset and evaluation code"], "limitations": "", "keywords": ["Cultural dimensions", "Emotion analysis", "Language models", "NLP", "Benchmarking"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2503.12345", "pdf": "https://arxiv.org/pdf/2503.12345.pdf", "abs": "https://arxiv.org/abs/2503.12345", "title": "General Table Question Answering via Answer-Formula Joint Generation", "authors": ["Zhongyuan Wang", "Richong Zhang", "Zhijie Nie"], "categories": ["cs.CL", "cs.AI"], "comment": "work in progress", "summary": "Advanced table question answering (TableQA) methods prompt large language\nmodels (LLMs) to generate answer text, SQL query, Python code, or custom\noperations, which impressively improve the complex reasoning problems in the\nTableQA task. However, these methods lack the versatility to cope with specific\nquestion types or table structures. In contrast, the Spreadsheet Formula, the\nwidely used and well-defined operation language for tabular data, has not been\nthoroughly explored to solve TableQA. In this paper, we first attempt to use\nthe Formula as the executable representation for solving complex reasoning on\ntables with different structures. Specifically, we construct\n\\texttt{FromulaQA}, a large Formula-annotated TableQA dataset from existing\ndatasets. In addition, we propose \\texttt{TabAF}, a general table answering\nframework to solve multiple types of tasks over multiple types of tables\nsimultaneously. Unlike existing methods, \\texttt{TabAF} decodes answers and\nFormulas with a single LLM backbone, demonstrating great versatility and\ngeneralization. \\texttt{TabAF} based on Llama3.1-70B achieves new\nstate-of-the-art performance on the WikiTableQuestion, HiTab, and TabFact.", "AI": {"tldr": "This paper introduces a new system called TabAF that uses Spreadsheet Formula as a representation for advanced table question answering, enabling versatile handling of various question types and table structures.", "motivation": "Current TableQA methods relying on LLMs struggle with specific question types and table structures, necessitating a more versatile approach.", "method": "The authors constructed a new dataset called FromulaQA and proposed a table answering framework, TabAF, which decodes answers and formulas with a single LLM backbone thus improving performance across multiple tasks.", "result": "TabAF, utilizing Llama3.1-70B, demonstrated state-of-the-art results on datasets like WikiTableQuestion, HiTab, and TabFact.", "conclusion": "TabAF represents a significant advancement in TableQA by integrating Spreadsheet Formulas for enhanced reasoning capabilities across diverse tabular formats.", "key_contributions": ["Introduction of FromulaQA dataset for table question answering", "Development of TabAF framework that unifies answer generation and formula decoding", "Achievement of state-of-the-art performance on several benchmarks"], "limitations": "", "keywords": ["TableQA", "Spreadsheet Formula", "LLM", "TabAF", "FromulaQA"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.12759", "pdf": "https://arxiv.org/pdf/2503.12759.pdf", "abs": "https://arxiv.org/abs/2503.12759", "title": "RAG-RL: Advancing Retrieval-Augmented Generation via RL and Curriculum Learning", "authors": ["Jerry Huang", "Siddarth Madala", "Risham Sidhu", "Cheng Niu", "Hao Peng", "Julia Hockenmaier", "Tong Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) systems rely on retrieval models for\nidentifying relevant contexts and answer generation models for utilizing those\ncontexts. However, retrievers exhibit imperfect recall and precision, limiting\ndownstream performance. We introduce RAG-RL, an answer generation model trained\nnot only to produce answers but also to identify and cite relevant information\nfrom larger sets of retrieved contexts, shifting some of the burden of\nidentifying relevant documents from the retriever to the answer generator. Our\napproach uses curriculum learning, where the model is first trained on easier\nexamples that include only relevant contexts. Our experiments show that these\ntraining samples enable models to acquire citation and reasoning skills with\ngreater sample efficiency and generalizability, demonstrating strong model\nperformance even as the number of irrelevant passages increases. We benchmark\nour methods on three open-domain multi-hop question answering datasets and\nreport significant gains in answer and citation accuracy. Our experiments\nprovide empirical insights into how easier training samples can give models\nstronger signals for learning specific skills (e.g., citation generation) and\nhow different components of post-training (e.g., training set construction,\nrule-based rewards, training sample ordering, etc.) impact final model\nperformance.", "AI": {"tldr": "RAG-RL enhances retrieval-augmented generation systems by training answer generation models to also identify and cite relevant information from retrieved contexts, improving performance through curriculum learning.", "motivation": "To address the limitations of traditional retrieval models in RAG systems, where imperfect recall and precision hinder the overall performance.", "method": "RAG-RL employs a curriculum learning approach, training the model with easier examples containing only relevant contexts to enhance its citation and reasoning capabilities.", "result": "The experiments demonstrate significant improvements in answer and citation accuracy across multiple open-domain multi-hop question answering datasets, even with increasing irrelevant passages.", "conclusion": "The study concludes that easier training samples provide effective signals for skill acquisition, with various post-training factors influencing overall model effectiveness.", "key_contributions": ["Introduction of RAG-RL for improved answer generation in RAG systems", "Use of curriculum learning for better citation and reasoning skills", "Empirical insights on training sample construction and model performance"], "limitations": "", "keywords": ["retrieval-augmented generation", "curriculum learning", "question answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.15354", "pdf": "https://arxiv.org/pdf/2503.15354.pdf", "abs": "https://arxiv.org/abs/2503.15354", "title": "Optimizing Decomposition for Optimal Claim Verification", "authors": ["Yining Lu", "Noah Ziems", "Hy Dang", "Meng Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current research on the \\textit{Decompose-Then-Verify} paradigm for\nevaluating the factuality of long-form text typically treats decomposition and\nverification in isolation, overlooking their interactions and potential\nmisalignment. We find that existing decomposition policies, typically\nhand-crafted demonstrations, do not align well with downstream verifiers in\nterms of atomicity -- a novel metric quantifying information density -- leading\nto suboptimal verification results. We formulate finding the optimal\ndecomposition policy for optimal verification as a bilevel optimization\nproblem. To approximate a solution for this strongly NP-hard problem, we\npropose dynamic decomposition, a reinforcement learning framework that\nleverages verifier feedback to learn a policy for dynamically decomposing\nclaims to verifier-preferred atomicity. Experimental results show that dynamic\ndecomposition outperforms existing decomposition policies, improving\nverification confidence by 0.07 and accuracy by 0.12 (on a 0-1 scale) on\naverage across varying verifiers, datasets, and atomcities of input claims.", "AI": {"tldr": "The paper proposes a reinforcement learning framework for dynamically decomposing claims to optimize synchronization with verifiers, addressing misalignments in factual verification.", "motivation": "Current methods for evaluating factuality in long-form text do not effectively consider the interactions between decomposition and verification, often leading to mismatches in atomicity and suboptimal verification results.", "method": "The authors formulate the problem of finding an optimal decomposition policy as a bilevel optimization problem and employ reinforcement learning to create a dynamic decomposition method that adapts based on verifier feedback.", "result": "Dynamic decomposition demonstrates improved verification confidence (0.07) and accuracy (0.12) over existing methods across various verifiers and datasets.", "conclusion": "The proposed approach improves alignment between decomposition and verification, offering a more effective solution for factual evaluation of text.", "key_contributions": ["Introduces a bilevel optimization framework for decomposition and verification alignment.", "Proposes dynamic decomposition as a novel reinforcement learning approach to improve factual verification.", "Demonstrates improved verification metrics through experimental validation."], "limitations": "The problem remains NP-hard, and the performance can depend on the design of verifiers and datasets used in experiments.", "keywords": ["factuality evaluation", "dynamic decomposition", "reinforcement learning", "bilevel optimization", "text verification"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.20749", "pdf": "https://arxiv.org/pdf/2503.20749.pdf", "abs": "https://arxiv.org/abs/2503.20749", "title": "Prompting is Not All You Need! Evaluating LLM Agent Simulation Methodologies with Real-World Online Customer Behavior Data", "authors": ["Yuxuan Lu", "Jing Huang", "Yan Han", "Bingsheng Yao", "Sisong Bei", "Jiri Gesi", "Yaochen Xie", "Zheshen", "Wang", "Qi He", "Dakuo Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent research shows that LLMs can simulate ``believable'' human behaviors\nto power LLM agents via prompt-only methods. In this work, we focus on\nevaluating LLM's objective ``accuracy'' rather than the subjective\n``believability'' in simulating human behavior, leveraging a large-scale,\nreal-world dataset collected from customers' online shopping actions. We\npresent the first comprehensive evaluation of state-of-the-art LLMs (e.g.,\nDeepSeek-R1, Llama, and Claude) on the task of web shopping action generation.\nOur results show that out-of-the-box LLM-generated actions are often misaligned\nwith actual human behavior, whereas fine-tuning LLMs on real-world behavioral\ndata substantially improves their ability to generate accurate actions compared\nto prompt-only methods. Furthermore, incorporating synthesized reasonings into\nmodel training leads to additional performance gains, demonstrating the value\nof explicit rationale in behavior modeling. This work evaluates\nstate-of-the-art LLMs in behavior simulation and provides actionable insights\ninto how real-world action data can enhance the fidelity of LLM agents.", "AI": {"tldr": "This study evaluates the objective accuracy of LLMs in simulating human shopping behaviors and demonstrates that fine-tuning LLMs on real-world data enhances performance significantly.", "motivation": "To assess the accuracy of LLM-generated human behavior simulations in online shopping contexts, moving beyond subjective measures of believability.", "method": "Comprehensive evaluation of various LLMs on web shopping action generation using a large-scale dataset of customer actions, comparing prompt-only methods to fine-tuned approaches.", "result": "Fine-tuning LLMs on behavioral data significantly improves action generation accuracy, and the addition of synthesized reasonings further enhances this performance.", "conclusion": "Real-world action data is crucial for improving LLMs' ability to simulate accurate human behaviors, providing insights for better behavior modeling.", "key_contributions": ["First comprehensive evaluation of LLMs in online shopping action simulation", "Demonstrated effectiveness of fine-tuning LLMs on real-world data", "Showed that synthesized reasonings contribute to performance improvements"], "limitations": "", "keywords": ["LLM", "behavior simulation", "online shopping", "fine-tuning", "real-world data"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.01346", "pdf": "https://arxiv.org/pdf/2504.01346.pdf", "abs": "https://arxiv.org/abs/2504.01346", "title": "GTR: Graph-Table-RAG for Cross-Table Question Answering", "authors": ["Jiaru Zou", "Dongqi Fu", "Sirui Chen", "Xinrui He", "Zihao Li", "Yada Zhu", "Jiawei Han", "Jingrui He"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "20 pages, 7 figures", "summary": "Beyond pure text, a substantial amount of knowledge is stored in tables. In\nreal-world scenarios, user questions often require retrieving answers that are\ndistributed across multiple tables. GraphRAG has recently attracted much\nattention for enhancing LLMs' reasoning capabilities by organizing external\nknowledge to address ad-hoc and complex questions, exemplifying a promising\ndirection for cross-table question answering. In this paper, to address the\ncurrent gap in available data, we first introduce a multi-table benchmark,\nMutliTableQA, comprising 60k tables and 25k user queries collected from\nreal-world sources. Then, we propose the first Graph-Table-RAG framework,\nnamely GTR, which reorganizes table corpora into a heterogeneous graph, employs\na hierarchical coarse-to-fine retrieval process to extract the most relevant\ntables, and integrates graph-aware prompting for downstream LLMs' tabular\nreasoning. Extensive experiments show that GTR exhibits superior cross-table\nquestion-answering performance while maintaining high deployment efficiency,\ndemonstrating its real-world practical applicability.", "AI": {"tldr": "The paper introduces a benchmark and framework for cross-table question answering using a heterogeneous graph representation of tables, enhancing LLMs' reasoning capabilities.", "motivation": "To address the need for effective retrieval of answers distributed across multiple tables in real-world scenarios and to fill the gap in available data for this task.", "method": "The paper presents the Graph-Table-RAG (GTR) framework, which reorganizes table data into a heterogeneous graph and implements a hierarchical retrieval process alongside graph-aware prompting for LLMs.", "result": "GTR demonstrates superior performance in cross-table question answering and shows high efficiency in deployment across extensive experiments.", "conclusion": "The proposed GTR framework enhances the capabilities of LLMs in tabular reasoning and offers a practical solution for real-world applications.", "key_contributions": ["Introduction of the MultiTableQA benchmark with 60k tables and 25k queries", "Development of the Graph-Table-RAG framework (GTR)", "Demonstration of improved performance in cross-table question-answering tasks", "High deployment efficiency of the proposed method."], "limitations": "", "keywords": ["GraphRAG", "table retrieval", "cross-table QA", "natural language processing", "machine learning"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2504.05050", "pdf": "https://arxiv.org/pdf/2504.05050.pdf", "abs": "https://arxiv.org/abs/2504.05050", "title": "Revealing the Intrinsic Ethical Vulnerability of Aligned Large Language Models", "authors": ["Jiawei Lian", "Jianhong Pan", "Lefan Wang", "Yi Wang", "Shaohui Mei", "Lap-Pui Chau"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are foundational explorations to artificial\ngeneral intelligence, yet their alignment with human values via instruction\ntuning and preference learning achieves only superficial compliance. Here, we\ndemonstrate that harmful knowledge embedded during pretraining persists as\nindelible \"dark patterns\" in LLMs' parametric memory, evading alignment\nsafeguards and resurfacing under adversarial inducement at distributional\nshifts. In this study, we first theoretically analyze the intrinsic ethical\nvulnerability of aligned LLMs by proving that current alignment methods yield\nonly local \"safety regions\" in the knowledge manifold. In contrast, pretrained\nknowledge remains globally connected to harmful concepts via high-likelihood\nadversarial trajectories. Building on this theoretical insight, we empirically\nvalidate our findings by employing semantic coherence inducement under\ndistributional shifts--a method that systematically bypasses alignment\nconstraints through optimized adversarial prompts. This combined theoretical\nand empirical approach achieves a 100% attack success rate across 19 out of 23\nstate-of-the-art aligned LLMs, including DeepSeek-R1 and LLaMA-3, revealing\ntheir universal vulnerabilities.", "AI": {"tldr": "This paper demonstrates that harmful knowledge in LLMs persists as 'dark patterns' despite alignment efforts, exposing vulnerabilities when faced with adversarial prompts.", "motivation": "The study aims to uncover the limitations of current alignment methods in LLMs and their susceptibility to harmful knowledge even after instruction tuning.", "method": "The authors analyze the ethical vulnerabilities in LLMs theoretically and empirically, using adversarial prompts to test alignment limits.", "result": "The research shows that current alignment methods only create local safety zones, while harmful knowledge can be accessed globally through adversarial methods with a 100% attack success rate across various LLMs.", "conclusion": "The findings highlight the need for improved safeguarding in LLM alignment strategies to address inherent vulnerabilities to harmful knowledge.", "key_contributions": ["Theoretical analysis of ethical vulnerabilities in aligned LLMs.", "Demonstration of the persistence of harmful knowledge as dark patterns.", "Empirical validation of attack success rates across multiple aligned LLMs."], "limitations": "Focuses on adversarial prompts and may not address all methods of alignment failure.", "keywords": ["large language models", "alignment", "adversarial prompt", "ethical vulnerabilities", "dark patterns"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.05228", "pdf": "https://arxiv.org/pdf/2504.05228.pdf", "abs": "https://arxiv.org/abs/2504.05228", "title": "NoveltyBench: Evaluating Language Models for Humanlike Diversity", "authors": ["Yiming Zhang", "Harshita Diddee", "Susan Holm", "Hanchen Liu", "Xinyue Liu", "Vinay Samuel", "Barry Wang", "Daphne Ippolito"], "categories": ["cs.CL"], "comment": null, "summary": "Language models have demonstrated remarkable capabilities on standard\nbenchmarks, yet they struggle increasingly from mode collapse, the inability to\ngenerate diverse and novel outputs. Our work introduces NoveltyBench, a\nbenchmark specifically designed to evaluate the ability of language models to\nproduce multiple distinct and high-quality outputs. NoveltyBench utilizes\nprompts curated to elicit diverse answers and filtered real-world user queries.\nEvaluating 20 leading language models, we find that current state-of-the-art\nsystems generate significantly less diversity than human writers. Notably,\nlarger models within a family often exhibit less diversity than their smaller\ncounterparts, challenging the notion that capability on standard benchmarks\ntranslates directly to generative utility. While prompting strategies like\nin-context regeneration can elicit diversity, our findings highlight a\nfundamental lack of distributional diversity in current models, reducing their\nutility for users seeking varied responses and suggesting the need for new\ntraining and evaluation paradigms that prioritize diversity alongside quality.", "AI": {"tldr": "Introducing NoveltyBench, a benchmark to evaluate the diversity of language model outputs, revealing significant shortcomings compared to human writers.", "motivation": "To address the issue of mode collapse in language models, where they fail to generate diverse outputs despite high performance on benchmarks.", "method": "Development of NoveltyBench, which utilizes curated prompts and real-world user queries to assess the diversity of 20 leading language models' outputs.", "result": "Current state-of-the-art models generate significantly less diversity than human writers; larger models often show less diversity than smaller ones.", "conclusion": "There is a critical need for new training and evaluation paradigms that emphasize output diversity alongside quality in language models.", "key_contributions": ["Introduction of NoveltyBench for evaluating output diversity", "Findings that challenge existing notions of model capability", "Insights on the limitations of larger language models in generating diverse outputs."], "limitations": "Focus on evaluating only language model outputs, which may not encompass all facets of generative utility.", "keywords": ["language models", "diversity", "benchmark", "evaluating outputs", "mode collapse"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.07440", "pdf": "https://arxiv.org/pdf/2504.07440.pdf", "abs": "https://arxiv.org/abs/2504.07440", "title": "Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric", "authors": ["Yixin Cao", "Jiahao Ying", "Yaoning Wang", "Xipeng Qiu", "Xuanjing Huang", "Yugang Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable across academia,\nindustry, and daily applications, yet current evaluation methods struggle to\nkeep pace with their rapid development. One core challenge of evaluation in the\nlarge language model (LLM) era is the generalization issue: how to infer a\nmodel's near-unbounded abilities from inevitably bounded benchmarks. We address\nthis challenge by proposing Model Utilization Index (MUI), a mechanism\ninterpretability enhanced metric that complements traditional performance\nscores. MUI quantifies the effort a model expends on a task, defined as the\nproportion of activated neurons or features during inference. Intuitively, a\ntruly capable model should achieve higher performance with lower effort.\nExtensive experiments across popular LLMs reveal a consistent inverse\nlogarithmic relationship between MUI and performance, which we formulate as the\nUtility Law. From this law we derive four practical corollaries that (i) guide\ntraining diagnostics, (ii) expose data contamination issue, (iii) enable fairer\nmodel comparisons, and (iv) design model-specific dataset diversity. Our code\ncan be found at https://github.com/ALEX-nlp/MUI-Eva.", "AI": {"tldr": "The paper proposes a Model Utilization Index (MUI) to evaluate Large Language Models (LLMs) more effectively by quantifying the effort a model expends during inference, revealing a relationship between effort and performance.", "motivation": "Existing evaluation methods for LLMs are inadequate, particularly concerning the generalization issue of inferring model capabilities from bounded benchmarks.", "method": "The proposal of the MUI, which quantifies the proportion of activated neurons during inference to measure the effort expended by a model on a task.", "result": "A consistent inverse logarithmic relationship between MUI and performance was found across popular LLMs, leading to the formulation of the Utility Law.", "conclusion": "The Utility Law provides practical insights for training diagnostics, identifying data contamination, enabling fairer model comparisons, and guiding dataset diversity for specific models.", "key_contributions": ["Introduction of the Model Utilization Index (MUI) as a new evaluation metric for LLMs", "Establishment of the Utility Law showing the relationship between model effort and performance", "Presentation of practical corollaries for training diagnostics and model comparisons"], "limitations": "", "keywords": ["Large Language Models", "Model Utilization Index", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.07830", "pdf": "https://arxiv.org/pdf/2504.07830.pdf", "abs": "https://arxiv.org/abs/2504.07830", "title": "MOSAIC: Modeling Social AI for Content Dissemination and Regulation in Multi-Agent Simulations", "authors": ["Genglin Liu", "Vivian Le", "Salman Rahman", "Elisa Kreiss", "Marzyeh Ghassemi", "Saadia Gabriel"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Work in progress. 27 pages", "summary": "We present a novel, open-source social network simulation framework, MOSAIC,\nwhere generative language agents predict user behaviors such as liking,\nsharing, and flagging content. This simulation combines LLM agents with a\ndirected social graph to analyze emergent deception behaviors and gain a better\nunderstanding of how users determine the veracity of online social content. By\nconstructing user representations from diverse fine-grained personas, our\nsystem enables multi-agent simulations that model content dissemination and\nengagement dynamics at scale. Within this framework, we evaluate three\ndifferent content moderation strategies with simulated misinformation\ndissemination, and we find that they not only mitigate the spread of\nnon-factual content but also increase user engagement. In addition, we analyze\nthe trajectories of popular content in our simulations, and explore whether\nsimulation agents' articulated reasoning for their social interactions truly\naligns with their collective engagement patterns. We open-source our simulation\nsoftware to encourage further research within AI and social sciences.", "AI": {"tldr": "We introduce MOSAIC, an open-source framework for simulating user behavior on social networks using LLM agents, focusing on misinformation dynamics and content moderation.", "motivation": "To understand user behaviors in online social networks and the spread of misinformation.", "method": "The framework combines generative language agents with a directed social graph and constructs user personas for multi-agent simulations.", "result": "The evaluation of content moderation strategies shows they can reduce misinformation spread while enhancing user engagement.", "conclusion": "The software has been open-sourced to promote further research into AI and social sciences.", "key_contributions": ["Introduction of the MOSAIC framework for social network simulation.", "Evaluation of content moderation strategies against misinformation.", "Analysis of user engagement dynamics in simulated environments."], "limitations": "The work is still in progress and requires further validation of findings.", "keywords": ["social network simulation", "misinformation", "content moderation", "generative language agents", "user behavior"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.09866", "pdf": "https://arxiv.org/pdf/2504.09866.pdf", "abs": "https://arxiv.org/abs/2504.09866", "title": "PASS-FC: Progressive and Adaptive Search Scheme for Fact Checking of Comprehensive Claims", "authors": ["Ziyu Zhuang"], "categories": ["cs.CL"], "comment": null, "summary": "Automated fact-checking (AFC) still falters on claims that are\ntime-sensitive, entity-ambiguous, or buried beneath noisy search-engine\nresults. We present PASS-FC, a Progressive and Adaptive Search Scheme for Fact\nChecking. Each atomic claim is first grounded with a precise time span and\ndisambiguated entity descriptors. An adaptive search loop then issues\nstructured queries, filters domains through credible-source selection, and\nexpands queries cross-lingually; when necessary, a lightweight reflection\nroutine restarts the loop. Experiments on six benchmark--covering general\nknowledge, scientific literature, real-world events, and ten languages--show\nthat PASS-FC consistently outperforms prior systems, even those powered by\nlarger backbone LLMs. On the multilingual X-FACT set, performance of different\nlanguages partially correlates with typological closeness to English, and\nforcing the model to reason in low-resource languages degrades accuracy.\nAblations highlight the importance of temporal grounding and the adaptive\nsearch scheme, while detailed analysis shows that cross-lingual retrieval\ncontributes genuinely new evidence. Code and full results will be released to\nfacilitate further research.", "AI": {"tldr": "PASS-FC is a new automated fact-checking system that improves performance on time-sensitive and entity-ambiguous claims through adaptive search techniques.", "motivation": "Automated fact-checking struggles with time-sensitive and ambiguous claims buried in noisy search results.", "method": "The system uses a structured adaptive search loop that grounds claims with precise timelines and disambiguated entities, filtering sources for credibility and expanding searches cross-lingually.", "result": "PASS-FC outperforms existing systems in various benchmarks, showing the importance of temporal grounding and adaptive methods in fact-checking.", "conclusion": "The framework demonstrates significant improvements in automated fact-checking and offers insights into the need for cross-lingual retrieval in various contexts.", "key_contributions": ["Introduction of an adaptive search scheme for fact-checking", "Demonstration of improved accuracy on multilingual benchmarks", "Highlighting the importance of temporal grounding in claims"], "limitations": "Performance degrades for low-resource languages; relies on the assumption of typological closeness to English for better outcomes.", "keywords": ["automated fact-checking", "adaptive search", "multilingual retrieval"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2504.11442", "pdf": "https://arxiv.org/pdf/2504.11442.pdf", "abs": "https://arxiv.org/abs/2504.11442", "title": "TextArena", "authors": ["Leon Guertler", "Bobby Cheng", "Simon Yu", "Bo Liu", "Leshem Choshen", "Cheston Tan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Work in progress; 5 pages, 3 figures", "summary": "TextArena is an open-source collection of competitive text-based games for\ntraining and evaluation of agentic behavior in Large Language Models (LLMs). It\nspans 57+ unique environments (including single-player, two-player, and\nmulti-player setups) and allows for easy evaluation of model capabilities via\nan online-play system (against humans and other submitted models) with\nreal-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social\nskills such as negotiation, theory of mind, and deception, creating a gap that\nTextArena addresses. Designed with research, community and extensibility in\nmind, TextArena emphasizes ease of adding new games, adapting the framework,\ntesting models, playing against the models, and training models. Detailed\ndocumentation of environments, games, leaderboard, and examples are available\non https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.", "AI": {"tldr": "TextArena is an open-source collection of 57+ competitive text-based games designed for evaluating agentic behavior in Large Language Models (LLMs).", "motivation": "To address the lack of evaluation of dynamic social skills in traditional benchmarks for LLMs, including negotiation and deception.", "method": "TextArena provides a framework for competitive gameplay that includes single-player, two-player, and multi-player setups, with the ability to evaluate model capabilities through an online play system and TrueSkill scoring.", "result": "The platform allows users to test and train models, add new games, and evaluate behaviors in a wide range of competitive scenarios.", "conclusion": "TextArena emphasizes community involvement and extensibility, making it easier for researchers to assess LLM capabilities in social contexts.", "key_contributions": ["Open-source framework for competitive text-based games", "Real-time evaluation system using TrueSkill scores", "Focus on dynamic social skills in LLM evaluation"], "limitations": "", "keywords": ["Large Language Models", "agentic behavior", "text-based games", "evaluation framework", "social skills"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2504.14366", "pdf": "https://arxiv.org/pdf/2504.14366.pdf", "abs": "https://arxiv.org/abs/2504.14366", "title": "Empirical Evaluation of Knowledge Distillation from Transformers to Subquadratic Language Models", "authors": ["Patrick Haller", "Jonas Golde", "Alan Akbik"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge distillation is a widely used technique for compressing large\nlanguage models (LLMs), in which a smaller student model is trained to mimic a\nlarger teacher model. Typically, both the teacher and student models are\nTransformer-based architectures, leveraging softmax attention for sequence\nmodeling. However, the quadratic complexity of self-attention during inference\nremains a significant bottleneck, motivating the exploration of subquadratic\nalternatives such as structured state-space models (SSMs), linear attention,\nand recurrent architectures. In this work, we systematically evaluate the\ntransferability of knowledge distillation from a Transformer teacher model to\neight subquadratic student architectures. Our study investigates which\nsubquadratic model can most effectively approximate the teacher model's learned\nrepresentations through knowledge distillation, and how different architectural\ndesign choices influence the training dynamics. We further investigate the\nimpact of initialization strategies, such as matrix mixing and query-key-value\n(QKV) copying, on the adaptation process. Our empirical results on multiple NLP\nbenchmarks provide insights into the trade-offs between efficiency and\nperformance, highlighting key factors for successful knowledge transfer to\nsubquadratic architectures.", "AI": {"tldr": "This study evaluates knowledge distillation from Transformer models to subquadratic architectures to enhance efficiency while maintaining performance.", "motivation": "To address the bottleneck of quadratic complexity in self-attention during inference in large language models (LLMs).", "method": "The authors systematically evaluate the transferability of knowledge distillation from a Transformer teacher model to eight different subquadratic student architectures, analyzing the effects of various architectural designs and initialization strategies.", "result": "Empirical results demonstrate the trade-offs between efficiency and performance when transferring knowledge to subquadratic models, offering insights on effective parameter adaptation.", "conclusion": "The findings suggest that specific architectural design choices and initialization strategies play a crucial role in the successful knowledge transfer process.", "key_contributions": ["Exploration of knowledge distillation from Transformer to subquadratic architectures.", "Evaluation of different architectural design choices and initialization strategies.", "Insights into efficiency and performance trade-offs for knowledge transfer."], "limitations": "The study is limited to specific subquadratic models and NLP benchmarks; generalizability to other domains is uncertain.", "keywords": ["knowledge distillation", "subquadratic architecture", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10113", "pdf": "https://arxiv.org/pdf/2505.10113.pdf", "abs": "https://arxiv.org/abs/2505.10113", "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical Specialty Data in Medical LLMs", "authors": ["Xinlan Yan", "Di Wu", "Yibin Lei", "Christof Monz", "Iacer Calixto"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.", "AI": {"tldr": "S-MedQA is a novel English medical QA dataset designed to benchmark large language models in clinical specialties, challenging existing knowledge injection hypotheses in medical QA.", "motivation": "The need for a reliable dataset to evaluate large language models in various clinical specialties and to investigate knowledge injection in medical question-answering.", "method": "Introducing S-MedQA, an English medical QA dataset, and conducting experiments to assess the performance of large language models across different clinical specialties.", "result": "The findings indicate that training on specialty-specific data does not always yield optimal performance, and improvements are primarily attributed to domain shifting rather than knowledge injection.", "conclusion": "The role of fine-tuning data in medical applications should be re-evaluated, and S-MedQA is released for community use.", "key_contributions": ["Introduction of S-MedQA dataset for medical QA benchmarking", "Challenging the effectiveness of specialty-specific fine-tuning", "Providing code for reproducibility of experiments"], "limitations": "The study is limited to English medical data and may not translate universally across languages or non-English-speaking contexts.", "keywords": ["medical question answering", "large language models", "fine-tuning", "S-MedQA", "domain shifting"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2405.07960", "pdf": "https://arxiv.org/pdf/2405.07960.pdf", "abs": "https://arxiv.org/abs/2405.07960", "title": "AgentClinic: a multimodal agent benchmark to evaluate AI in simulated clinical environments", "authors": ["Samuel Schmidgall", "Rojin Ziaei", "Carl Harris", "Eduardo Reis", "Jeffrey Jopling", "Michael Moor"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "Evaluating large language models (LLM) in clinical scenarios is crucial to\nassessing their potential clinical utility. Existing benchmarks rely heavily on\nstatic question-answering, which does not accurately depict the complex,\nsequential nature of clinical decision-making. Here, we introduce AgentClinic,\na multimodal agent benchmark for evaluating LLMs in simulated clinical\nenvironments that include patient interactions, multimodal data collection\nunder incomplete information, and the usage of various tools, resulting in an\nin-depth evaluation across nine medical specialties and seven languages. We\nfind that solving MedQA problems in the sequential decision-making format of\nAgentClinic is considerably more challenging, resulting in diagnostic\naccuracies that can drop to below a tenth of the original accuracy. Overall, we\nobserve that agents sourced from Claude-3.5 outperform other LLM backbones in\nmost settings. Nevertheless, we see stark differences in the LLMs' ability to\nmake use of tools, such as experiential learning, adaptive retrieval, and\nreflection cycles. Strikingly, Llama-3 shows up to 92% relative improvements\nwith the notebook tool that allows for writing and editing notes that persist\nacross cases. To further scrutinize our clinical simulations, we leverage\nreal-world electronic health records, perform a clinical reader study, perturb\nagents with biases, and explore novel patient-centric metrics that this\ninteractive environment firstly enables.", "AI": {"tldr": "AgentClinic is a multimodal agent benchmark for evaluating LLMs in clinical settings, focusing on sequential decision-making and multimodal data collection.", "motivation": "To provide an improved framework for assessing LLMs in clinical scenarios that better reflects the complexity of clinical decision-making compared to static benchmarks.", "method": "The study introduces AgentClinic, a simulated environment featuring patient interactions and multimodal data, evaluated across nine medical specialties and seven languages.", "result": "Diagnoses accuracy significantly drops in the sequential format of AgentClinic, with Claude-3.5 performing best among LLMs, while Llama-3 shows up to 92% improvement when using a specific note-taking tool.", "conclusion": "The evaluation reveals large performance variances among LLMs when using tools and emphasizes the need for patient-centered metrics in assessing LLM capabilities in healthcare.", "key_contributions": ["Introduction of AgentClinic for evaluating LLMs in clinical settings", "Demonstrates performance differences among LLMs in sequential decision-making", "Identifies success factors such as tool usage in enhancing diagnostic accuracy"], "limitations": "", "keywords": ["large language models", "clinical decision-making", "human-computer interaction", "health informatics", "evaluation benchmarks"], "importance_score": 9, "read_time_minutes": 10}}
