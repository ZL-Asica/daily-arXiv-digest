{"id": "2505.23780", "pdf": "https://arxiv.org/pdf/2505.23780.pdf", "abs": "https://arxiv.org/abs/2505.23780", "title": "More-than-Human Storytelling: Designing Longitudinal Narrative Engagements with Generative AI", "authors": ["Émilie Fabre", "Katie Seaborn", "Shuta Koiwai", "Mizuki Watanabe", "Paul Riesch"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SD", "eess.AS"], "comment": "CHI EA '25", "summary": "Longitudinal engagement with generative AI (GenAI) storytelling agents is a\ntimely but less charted domain. We explored multi-generational experiences with\n\"Dreamsmithy,\" a daily dream-crafting app, where participants (N = 28)\nco-created stories with AI narrator \"Makoto\" every day. Reflections and\ninteractions were captured through a two-week diary study. Reflexive thematic\nanalysis revealed themes likes \"oscillating ambivalence\" and\n\"socio-chronological bonding,\" highlighting the complex dynamics that emerged\nbetween individuals and the AI narrator over time. Findings suggest that while\npeople appreciated the personal notes, opportunities for reflection, and AI\ncreativity, limitations in narrative coherence and control occasionally caused\nfrustration. The results underscore the potential of GenAI for longitudinal\nstorytelling, but also raise critical questions about user agency and ethics.\nWe contribute initial empirical insights and design considerations for\ndeveloping adaptive, more-than-human storytelling systems.", "AI": {"tldr": "The study investigates user experiences with a generative AI storytelling app, revealing both positive engagement and challenges in narrative coherence and control.", "motivation": "To explore the dynamics of engagement with generative AI storytelling agents over time and understand user experiences in this less-charted domain.", "method": "A two-week diary study with 28 participants co-creating stories with an AI narrator, using reflexive thematic analysis to identify emerging themes.", "result": "Participants experienced both appreciation for AI creativity and frustration due to limitations in narrative coherence and user control.", "conclusion": "While generative AI shows potential for enriching storytelling experiences, it raises important questions regarding user agency and ethical considerations.", "key_contributions": ["Initial empirical insights into generative AI storytelling", "Design considerations for adaptive storytelling systems", "Identification of themes related to user experiences with AI"], "limitations": "Narrative coherence and user control limitations.", "keywords": ["generative AI", "storytelling agents", "user experience", "longitudinal study", "ethical considerations"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23994", "pdf": "https://arxiv.org/pdf/2505.23994.pdf", "abs": "https://arxiv.org/abs/2505.23994", "title": "PolicyPulse: LLM-Synthesis Tool for Policy Researchers", "authors": ["Maggie Wang", "Ella Colby", "Jennifer Okwara", "Varun Nagaraj Rao", "Yuhan Liu", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Published in Proceedings of the Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems", "summary": "Public opinion shapes policy, yet capturing it effectively to surface diverse\nperspectives remains challenging. This paper introduces PolicyPulse, an\nLLM-powered interactive system that synthesizes public experiences from online\ncommunity discussions to help policy researchers author memos and briefs,\nleveraging curated real-world anecdotes. Given a specific topic (e.g., \"Climate\nChange\"), PolicyPulse returns an organized list of themes (e.g., \"Biodiversity\nLoss\" or \"Carbon Pricing\"), supporting each theme with relevant quotes from\nreal-life anecdotes. We compared PolicyPulse outputs to authoritative policy\nreports. Additionally, we asked 11 policy researchers across multiple\ninstitutions in the Northeastern U.S to compare using PolicyPulse with their\nexpert approach. We found that PolicyPulse's themes aligned with authoritative\nreports and helped spark research by analyzing existing data, gathering diverse\nexperiences, revealing unexpected themes, and informing survey or interview\ndesign. Participants also highlighted limitations including insufficient\ndemographic context and data verification challenges. Our work demonstrates how\nAI-powered tools can help influence policy-relevant research and shape policy\noutcomes.", "AI": {"tldr": "PolicyPulse is an LLM-based system that synthesizes public opinions from online discussions to assist policy researchers in drafting memos and briefs.", "motivation": "Capturing diverse public opinions is challenging yet crucial for shaping effective policies.", "method": "PolicyPulse organizes themes based on online community discussions and supports them with quotes from real-world anecdotes, comparing its outputs to authoritative reports and gathering feedback from policy researchers.", "result": "PolicyPulse themes align well with authoritative policy reports, enhancing the research process by analyzing data and revealing unexpected themes.", "conclusion": "AI-powered tools like PolicyPulse can significantly influence policy-related research and outcomes.", "key_contributions": ["Introduction of an LLM-powered tool for synthesizing public opinions", "Demonstration of system's effectiveness through comparisons with policy reports", "Insights from policy researchers on the utility of the tool and its limitations"], "limitations": "Insufficient demographic context and challenges in data verification.", "keywords": ["Human-Computer Interaction", "Public Policy", "AI in Research"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23997", "pdf": "https://arxiv.org/pdf/2505.23997.pdf", "abs": "https://arxiv.org/abs/2505.23997", "title": "Fitting the Message to the Moment: Designing Calendar-Aware Stress Messaging with Large Language Models", "authors": ["Pranav Rao", "Maryam Taj", "Alex Mariakakis", "Joseph Jay Williams", "Ananya Bhattacharjee"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Existing stress-management tools fail to account for the timing and\ncontextual specificity of students' daily lives, often providing static or\nmisaligned support. Digital calendars contain rich, personal indicators of\nupcoming responsibilities, yet this data is rarely leveraged for adaptive\nwellbeing interventions. In this short paper, we explore how large language\nmodels (LLMs) might use digital calendar data to deliver timely and\npersonalized stress support. We conducted a one-week study with eight\nuniversity students using a functional technology probe that generated daily\nstress-management messages based on participants' calendar events. Through\nsemi-structured interviews and thematic analysis, we found that participants\nvalued interventions that prioritized stressful events and adopted a concise,\nbut colloquial tone. These findings reveal key design implications for\nLLM-based stress-management tools, including the need for structured\nquestioning and tone calibration to foster relevance and trust.", "AI": {"tldr": "This paper investigates how large language models (LLMs) can utilize digital calendar data to provide tailored stress management support to university students.", "motivation": "The study aims to enhance existing stress-management tools by incorporating the timing and context of students' lives, addressing the lack of personalized and adaptive support currently available.", "method": "A one-week study with eight university students was conducted using a functional technology probe that sent tailored stress-management messages based on the participants' calendar events. Data was collected through semi-structured interviews and analyzed thematically.", "result": "Participants appreciated the timely interventions that focused on upcoming stressful events and preferred a concise, conversational tone in the messages they received.", "conclusion": "The findings suggest that LLM-based stress-management tools should refine their design to include structured questioning and a calibrated tone to improve user relevance and trust.", "key_contributions": ["Proposed the use of digital calendar data for stress management", "Identified student preferences for concise and colloquial messaging", "Highlighted design implications for LLM-based wellbeing interventions"], "limitations": "", "keywords": ["stress management", "large language models", "digital calendars", "user experience", "wellbeing interventions"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24000", "pdf": "https://arxiv.org/pdf/2505.24000.pdf", "abs": "https://arxiv.org/abs/2505.24000", "title": "ConversAR: Exploring Embodied LLM-Powered Group Conversations in Augmented Reality for Second Language Learners", "authors": ["Jad Bendarkawi", "Ashley Ponce", "Sean Mata", "Aminah Aliu", "Yuhan Liu", "Lei Zhang", "Amna Liaqat", "Varun Nagaraj Rao", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Published in Proceedings of the Extended Abstracts of the CHI\n  Conference on Human Factors in Computing Systems", "summary": "Group conversations are valuable for second language (L2) learners as they\nprovide opportunities to practice listening and speaking, exercise complex\nturn-taking skills, and experience group social dynamics in a target language.\nHowever, most existing Augmented Reality (AR)-based conversational learning\ntools focus on dyadic interactions rather than group dialogues. Although\nresearch has shown that AR can help reduce speaking anxiety and create a\ncomfortable space for practicing speaking skills in dyadic scenarios,\nespecially with Large Language Model (LLM)-based conversational agents, the\npotential for group language practice using these technologies remains largely\nunexplored. We introduce ConversAR, a gpt-4o powered AR application, that\nenables L2 learners to practice contextualized group conversations. Our system\nfeatures two embodied LLM agents with vision-based scene understanding and live\ncaptions. In a system evaluation with 10 participants, users reported reduced\nspeaking anxiety and increased learner autonomy compared to perceptions of\nin-person practice methods with other learners.", "AI": {"tldr": "ConversAR is an AR application that allows second language learners to engage in group conversations with embodied LLM agents, helping to reduce speaking anxiety and increase learner autonomy.", "motivation": "To enhance language learning for second language (L2) learners by facilitating group conversations through Augmented Reality (AR), an area mostly overlooked in existing tools.", "method": "Developed an AR application powered by GPT-4o that uses two embodied LLM agents for group dialogue, incorporating vision-based scene understanding and live captions.", "result": "In a system evaluation with 10 participants, users reported lower speaking anxiety and greater learner autonomy compared to in-person practice methods.", "conclusion": "ConversAR provides a promising approach for group language practice using AR and LLM technologies, showing potential benefits over traditional methods.", "key_contributions": ["Introduced an AR application for group language learning", "Utilized embodied LLM agents for enhanced interaction", "Demonstrated reduced anxiety and increased autonomy among users"], "limitations": "", "keywords": ["Augmented Reality", "Language Learning", "Large Language Models", "Group Conversations", "L2 Learners"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23785", "pdf": "https://arxiv.org/pdf/2505.23785.pdf", "abs": "https://arxiv.org/abs/2505.23785", "title": "Meaning Is Not A Metric: Using LLMs to make cultural context legible at scale", "authors": ["Cody Kommers", "Drew Hemment", "Maria Antoniak", "Joel Z. Leibo", "Hoyt Long", "Emily Robinson", "Adam Sobey"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Position paper", "summary": "This position paper argues that large language models (LLMs) can make\ncultural context, and therefore human meaning, legible at an unprecedented\nscale in AI-based sociotechnical systems. We argue that such systems have\npreviously been unable to represent human meaning because they rely on thin\ndescriptions: numerical representations that enforce standardization and\ntherefore strip human activity of the cultural context that gives it meaning.\nBy contrast, scholars in the humanities and qualitative social sciences have\ndeveloped frameworks for representing meaning through thick description: verbal\nrepresentations that accommodate heterogeneity and retain contextual\ninformation needed to represent human meaning. While these methods can\neffectively codify meaning, they are difficult to deploy at scale. However, the\nverbal capabilities of LLMs now provide a means of (at least partially)\nautomating the generation and processing of thick descriptions, potentially\novercoming this bottleneck. We argue that the problem of rendering human\nmeaning legible is not just about selecting better metrics, but about\ndeveloping new representational formats (based on thick description). We frame\nthis as a crucial direction for the application of generative AI and identify\nfive key challenges: preserving context, maintaining interpretive pluralism,\nintegrating perspectives based on lived experience and critical distance,\ndistinguishing qualitative content from quantitative magnitude, and\nacknowledging meaning as dynamic rather than static. Furthermore, we suggest\nthat thick description has the potential to serve as a unifying framework to\naddress a number of emerging concerns about the difficulties of representing\nculture in (or using) LLMs.", "AI": {"tldr": "This position paper discusses how large language models (LLMs) can facilitate the representation of human meaning in AI systems through 'thick description', overcoming the limitations of traditional numerical representations.", "motivation": "To argue that LLMs can make cultural context and human meaning legible in AI-based sociotechnical systems by improving the representation of human activity.", "method": "The paper compares thin descriptions (quantitative, standardized) with thick descriptions (qualitative, contextual) and suggests that LLMs can automate the generation of thick descriptions at scale.", "result": "LLMs have the potential to address challenges in representing human meaning by accommodating cultural context, interpretive pluralism, and dynamic understanding.", "conclusion": "Adopting thick description as a framework can help address emerging issues in representing culture with LLMs and suggests it as a crucial direction for generative AI applications.", "key_contributions": ["Introduces thick description as a method for better capturing human meaning in AI systems", "Identifies key challenges in representing culture with LLMs", "Frames the importance of qualitative representation in the context of AI"], "limitations": "", "keywords": ["Large Language Models", "Cultural Context", "Thick Description", "Human-Computer Interaction", "Generative AI"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2505.24004", "pdf": "https://arxiv.org/pdf/2505.24004.pdf", "abs": "https://arxiv.org/abs/2505.24004", "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins", "authors": ["Amanda Chan", "Catherine Di", "Joseph Rupertus", "Gary Smith", "Varun Nagaraj Rao", "Manoel Horta Ribeiro", "Andrés Monroy-Hernández"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "Accepted as a CHI Late Breaking Work (2025), cite appropriately", "summary": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity.", "AI": {"tldr": "This paper proposes a hybrid framework using digital twins to enhance crowd work productivity and maintain data quality amidst the rise of generative AI tools.", "motivation": "The increasing use of generative AI by crowd workers threatens the authenticity of responses in research, necessitating a solution to maintain data validity and worker roles.", "method": "A hybrid framework using digital twins was developed and evaluated through an experiment with 88 crowd workers and interviews with 9 individuals (5 workers, 4 researchers).", "result": "The evaluation showed that digital twins can enhance productivity, reduce decision fatigue, and maintain response quality in crowd work settings.", "conclusion": "Automating repetitive tasks with digital twins while preserving human engagement in nuanced areas can help balance scalability with authenticity in crowd work.", "key_contributions": ["Development of a hybrid framework using digital twins for crowd work.", "Experiment and interviews demonstrating the effectiveness of the proposed framework.", "Emphasis on transparency, ethical data use, and maintaining worker agency."], "limitations": "The study is based on a limited sample size and may not be generalizable across all crowd work scenarios.", "keywords": ["crowd work", "digital twins", "generative AI", "research validity", "worker agency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23788", "pdf": "https://arxiv.org/pdf/2505.23788.pdf", "abs": "https://arxiv.org/abs/2505.23788", "title": "Nine Ways to Break Copyright Law and Why Our LLM Won't: A Fair Use Aligned Generation Framework", "authors": ["Aakash Sen Sharma", "Debdeep Sanyal", "Priyansh Srivastava", "Sundar Atreya H.", "Shirish Karande", "Mohan Kankanhalli", "Murari Mandal"], "categories": ["cs.CL", "cs.AI"], "comment": "30 Pages", "summary": "Large language models (LLMs) commonly risk copyright infringement by\nreproducing protected content verbatim or with insufficient transformative\nmodifications, posing significant ethical, legal, and practical concerns.\nCurrent inference-time safeguards predominantly rely on restrictive\nrefusal-based filters, often compromising the practical utility of these\nmodels. To address this, we collaborated closely with intellectual property\nexperts to develop FUA-LLM (Fair Use Aligned Language Models), a\nlegally-grounded framework explicitly designed to align LLM outputs with\nfair-use doctrine. Central to our method is FairUseDB, a carefully constructed\ndataset containing 18,000 expert-validated examples covering nine realistic\ninfringement scenarios. Leveraging this dataset, we apply Direct Preference\nOptimization (DPO) to fine-tune open-source LLMs, encouraging them to produce\nlegally compliant and practically useful alternatives rather than resorting to\nblunt refusal. Recognizing the shortcomings of traditional evaluation metrics,\nwe propose new measures: Weighted Penalty Utility and Compliance Aware Harmonic\nMean (CAH) to balance infringement risk against response utility. Extensive\nquantitative experiments coupled with expert evaluations confirm that FUA-LLM\nsubstantially reduces problematic outputs (up to 20\\%) compared to\nstate-of-the-art approaches, while preserving real-world usability.", "AI": {"tldr": "The paper presents FUA-LLM, a framework for aligning LLM outputs with fair use doctrine, utilizing a dataset called FairUseDB and introducing new evaluation metrics.", "motivation": "To address copyright infringement risks in large language models by developing a framework that ensures outputs are fair use compliant and practically useful.", "method": "The paper collaborates with intellectual property experts to create FUA-LLM, leveraging FairUseDB and employing Direct Preference Optimization (DPO) to fine-tune LLMs.", "result": "FUA-LLM demonstrates a reduction in problematic outputs by up to 20% compared to existing methods while maintaining usability.", "conclusion": "The proposed framework significantly improves the compliance of LLM outputs with copyright laws and enhances their practical utility.", "key_contributions": ["Development of FUA-LLM framework", "Introduction of FairUseDB dataset", "New evaluation metrics for assessing infringement risk"], "limitations": "", "keywords": ["large language models", "fair use", "copyright", "Direct Preference Optimization", "language model evaluation"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.24014", "pdf": "https://arxiv.org/pdf/2505.24014.pdf", "abs": "https://arxiv.org/abs/2505.24014", "title": "Enhancing Critical Thinking in Generative AI Search with Metacognitive Prompts", "authors": ["Anjali Singh", "Zhitong Guan", "Soo Young Rieh"], "categories": ["cs.HC"], "comment": "To appear in Proceedings of the Association for Information Science\n  and Technology. 2025", "summary": "The growing use of Generative AI (GenAI) conversational search tools has\nraised concerns about their effects on people's metacognitive engagement,\ncritical thinking, and learning. As people increasingly rely on GenAI to\nperform tasks such as analyzing and applying information, they may become less\nactively engaged in thinking and learning. This study examines whether\nmetacognitive prompts - designed to encourage people to pause, reflect, assess\ntheir understanding, and consider multiple perspectives - can support critical\nthinking during GenAI-based search. We conducted a user study (N=40) with\nuniversity students to investigate the impact of metacognitive prompts on their\nthought processes and search behaviors while searching with a GenAI tool. We\nfound that these prompts led to more active engagement, prompting students to\nexplore a broader range of topics and engage in deeper inquiry through\nfollow-up queries. Students reported that the prompts were especially helpful\nfor considering overlooked perspectives, promoting evaluation of AI responses,\nand identifying key takeaways. Additionally, the effectiveness of these prompts\nwas influenced by students' metacognitive flexibility. Our findings highlight\nthe potential of metacognitive prompts to foster critical thinking and provide\ninsights for designing and implementing metacognitive support in human-AI\ninteractions.", "AI": {"tldr": "This study examines the effects of metacognitive prompts on critical thinking when using Generative AI tools for searches.", "motivation": "To explore how reliance on Generative AI may diminish metacognitive engagement and critical thinking, and to evaluate solutions like metacognitive prompts.", "method": "A user study was conducted with 40 university students to assess the impact of metacognitive prompts during GenAI-based searches on their thought processes and behaviors.", "result": "The use of metacognitive prompts resulted in higher engagement levels, broader topic exploration, and deeper inquiry through follow-up queries. Students found the prompts helpful for assessing AI responses and identifying key takeaways.", "conclusion": "Metacognitive prompts can enhance critical thinking in human-AI interactions, with variations in effectiveness based on students' metacognitive flexibility.", "key_contributions": ["Demonstrated the positive impact of metacognitive prompts on critical thinking during GenAI searches.", "Identified how prompts influenced students' inquiry behaviors and evaluation of AI-generated responses.", "Provided insights for designing metacognitive support in human-AI applications."], "limitations": "", "keywords": ["Generative AI", "metacognitive prompts", "critical thinking", "human-AI interaction", "user study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23789", "pdf": "https://arxiv.org/pdf/2505.23789.pdf", "abs": "https://arxiv.org/abs/2505.23789", "title": "Conversational Exploration of Literature Landscape with LitChat", "authors": ["Mingyu Huang", "Shasha Zhou", "Yuxuan Chen", "Ke Li"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "We are living in an era of \"big literature\", where the volume of digital\nscientific publications is growing exponentially. While offering new\nopportunities, this also poses challenges for understanding literature\nlandscapes, as traditional manual reviewing is no longer feasible. Recent large\nlanguage models (LLMs) have shown strong capabilities for literature\ncomprehension, yet they are incapable of offering \"comprehensive, objective,\nopen and transparent\" views desired by systematic reviews due to their limited\ncontext windows and trust issues like hallucinations. Here we present LitChat,\nan end-to-end, interactive and conversational literature agent that augments\nLLM agents with data-driven discovery tools to facilitate literature\nexploration. LitChat automatically interprets user queries, retrieves relevant\nsources, constructs knowledge graphs, and employs diverse data-mining\ntechniques to generate evidence-based insights addressing user needs. We\nillustrate the effectiveness of LitChat via a case study on AI4Health,\nhighlighting its capacity to quickly navigate the users through large-scale\nliterature landscape with data-based evidence that is otherwise infeasible with\ntraditional means.", "AI": {"tldr": "LitChat is an interactive literature agent that enhances LLMs to improve literature exploration and comprehension.", "motivation": "To address the challenges posed by the increasing volume of digital scientific publications and the limitations of traditional review methods.", "method": "Developed an interactive literature agent that interprets user queries, retrieves sources, constructs knowledge graphs, and uses data-mining techniques for insights.", "result": "Demonstrated effectiveness through a case study on AI4Health, showing its ability to navigate large literature landscapes effectively.", "conclusion": "LitChat offers a comprehensive, data-driven approach that can outperform traditional literature review methods.", "key_contributions": ["Introduction of LitChat as a conversational literature agent", "Integration of knowledge graphs for enhanced literature retrieval", "Use of data-mining techniques to generate insights from large data sets"], "limitations": "Limited by the inherent challenges of LLMs, such as hallucinations and context limitations.", "keywords": ["literature review", "large language models", "data-driven insights", "AI4Health", "knowledge graphs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24039", "pdf": "https://arxiv.org/pdf/2505.24039.pdf", "abs": "https://arxiv.org/abs/2505.24039", "title": "Advancing Digital Accessibility: Integrating AR/VR and Health Tech for Inclusive Healthcare Solutions", "authors": ["Vishnu Ramineni", "Shivareddy Devarapalli", "Balakrishna Pothineni", "Prema Kumar Veerapaneni", "Aditya Gupta", "Pankaj Gupta"], "categories": ["cs.HC"], "comment": "15 pages", "summary": "Modern healthcare domain incorporates a feature of digital accessibility to\nensure seamless flow of online services for the patients. However, this feature\nof digital accessibility poses a challenge particularly for patients with\ndisabilities. To eradicate this issue and provide immersive and user-friendly\nexperiences, evolving technologies like Augmented Reality (AR) and Virtual\nReality (VR) are integrated in medical applications to enhance accessibility.\nThe present research paper aims to study inclusivity and accessibility features\nof AR/VR in revolutionizing healthcare practices especially in domains like\ntelemedicine, patient education, assistive tools, and rehabilitation for\npersons with disabilities. The current trends of advancements and case studies\nare also analyzed to measure the efficacy of AR/VR in healthcare. Moreover, the\npaper entails a detailed analysis of the challenges of its adoption\nparticularly technical limitations, implementation costs, and regulatory\naspects. Finally, the paper concludes with recommendations for integrating\nAR/VR to foster a more equitable and inclusive healthcare system and provide\nindividuals with auditory, visual, and motor impairments with digital\nhealthcare solutions.", "AI": {"tldr": "The paper explores the integration of Augmented Reality (AR) and Virtual Reality (VR) in healthcare to improve accessibility for patients with disabilities.", "motivation": "To address the digital accessibility challenges faced by patients with disabilities in healthcare services.", "method": "Study of inclusivity and accessibility features of AR/VR applications in various healthcare domains, along with analysis of current trends and case studies.", "result": "Findings suggest that AR/VR can significantly enhance patient experiences in telemedicine, education, assistive tools, and rehabilitation, despite certain challenges.", "conclusion": "Recommendations for the integration of AR/VR technologies aim to create a more equitable healthcare system for individuals with disabilities.", "key_contributions": ["Evaluates the impact of AR/VR on healthcare accessibility.", "Analyzes current trends and case studies in AR/VR application.", "Suggests practical recommendations for improved implementation."], "limitations": "Technical limitations, implementation costs, and regulatory challenges hinder widespread adoption.", "keywords": ["Augmented Reality", "Virtual Reality", "Digital Accessibility", "Healthcare", "Disabilities"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.23790", "pdf": "https://arxiv.org/pdf/2505.23790.pdf", "abs": "https://arxiv.org/abs/2505.23790", "title": "Rethinking the Understanding Ability across LLMs through Mutual Information", "authors": ["Shaojie Wang", "Sirui Ding", "Na Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have revolutionized natural\nlanguage processing, yet evaluating their intrinsic linguistic understanding\nremains challenging. Moving beyond specialized evaluation tasks, we propose an\ninformation-theoretic framework grounded in mutual information (MI) to achieve\nthis. We formalize the understanding as MI between an input sentence and its\nlatent representation (sentence-level MI), measuring how effectively input\ninformation is preserved in latent representation. Given that LLMs learn\nembeddings for individual tokens, we decompose sentence-level MI into\ntoken-level MI between tokens and sentence embeddings, establishing theoretical\nbounds connecting these measures. Based on this foundation, we theoretically\nderive a computable lower bound for token-level MI using Fano's inequality,\nwhich directly relates to token-level recoverability-the ability to predict\noriginal tokens from sentence embedding. We implement this recoverability task\nto comparatively measure MI across different LLMs, revealing that encoder-only\nmodels consistently maintain higher information fidelity than their\ndecoder-only counterparts, with the latter exhibiting a distinctive late-layer\n\"forgetting\" pattern where mutual information is first enhanced and then\ndiscarded. Moreover, fine-tuning to maximize token-level recoverability\nconsistently improves understanding ability of LLMs on tasks without\ntask-specific supervision, demonstrating that mutual information can serve as a\nfoundation for understanding and improving language model capabilities.", "AI": {"tldr": "This paper proposes an information-theoretic framework to evaluate the linguistic understanding of large language models (LLMs) by measuring mutual information (MI) between input sentences and their latent representations.", "motivation": "To evaluate the intrinsic linguistic understanding of LLMs beyond specialized tasks, using a framework grounded in mutual information.", "method": "The authors formalize linguistic understanding as mutual information between an input sentence and its latent representation, and derive a computable lower bound for token-level MI using Fano's inequality.", "result": "Encoder-only models show higher information fidelity than decoder-only models, which demonstrate a forgetting pattern in later layers. Fine-tuning to maximize token-level recoverability improves understanding of LLMs across generic tasks.", "conclusion": "Mutual information serves as a foundational measure for understanding and enhancing LLM capabilities without task-specific supervision.", "key_contributions": ["Proposes a mutual information-based framework to evaluate LLM understanding", "Establishes theoretical bounds connecting token-level and sentence-level MI", "Demonstrates the effectiveness of fine-tuning on token-level recoverability"], "limitations": "", "keywords": ["large language models", "mutual information", "evaluation framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24042", "pdf": "https://arxiv.org/pdf/2505.24042.pdf", "abs": "https://arxiv.org/abs/2505.24042", "title": "Advancing Digital Accessibility In Digital Pharmacy, Healthcare, And Wearable Devices: Inclusive Solutions for Enhanced Patient Engagement", "authors": ["Vishnu Ramineni", "Balaji Shesharao Ingole", "Nikhil Kumar Pulipeta", "Balakrishna Pothineni", "Aditya Gupta"], "categories": ["cs.HC"], "comment": "15 pages", "summary": "Modern healthcare facilities demand digital accessibility to guarantee equal\naccess to telemedicine platforms, online pharmacy services, and health\nmonitoring devices that can be worn or are handy. With the rising call for the\nimplementation of robust digital healthcare solutions, people with disabilities\nencounter impediments in their endeavor of managing and getting accustomed to\nthese modern technologies owing to insufficient accessibility features. The\npaper highlights the role of comprehensive solutions for enhanced patient\nengagement and usability, particularly, in digital pharmacy, healthcare, and\nwearable devices. Besides, it elucidates the key obstructions faced by users\nexperiencing auditory, visual, cognitive, and motor impairments. Through a kind\nconsideration of present accessibility guidelines, practices, and emerging\ntechnologies, the paper provides a holistic overview by offering innovative\nsolutions, accentuating the vitality of compliance with Web Content\nAccessibility Guidelines (WCAG), Americans with Disabilities Act (ADA), and\nother regulatory structures to foster easy access to digital healthcare\nservices. Moreover, there is due focus on using AI-driven tools,\nspeech-activated interfaces, and tactile feedback in wearable health devices to\nassist persons with disabilities. The outcome of the research explicates the\nnecessity of prioritizing accessibility for individuals with disabilities and\ncultivating a culture where healthcare providers, policymakers, and officials\nbuild a patient-centered digital healthcare ecosystem that is all-encompassing\nin nature.", "AI": {"tldr": "The paper discusses the importance of digital accessibility in healthcare for individuals with disabilities and offers solutions to improve user engagement and usability in telemedicine, online pharmacies, and wearable devices.", "motivation": "To address the barriers faced by individuals with disabilities in accessing modern digital healthcare solutions and to emphasize the need for digital accessibility compliance.", "method": "The paper reviews current accessibility guidelines, practices, and technologies while proposing innovative AI-driven solutions, speech-activated interfaces, and tactile feedback mechanisms.", "result": "The research identifies critical obstacles faced by users with various impairments and stresses the importance of compliance with accessibility standards to create a more inclusive digital healthcare environment.", "conclusion": "Prioritizing accessibility is essential for fostering an inclusive culture in digital healthcare, requiring collaboration between healthcare providers, policymakers, and technology developers.", "key_contributions": ["Highlighting barriers faced by users with disabilities in digital healthcare", "Proposing AI-driven and innovative solutions for accessibility", "Emphasizing the importance of compliance with accessibility standards"], "limitations": "", "keywords": ["digital accessibility", "healthcare", "wearable devices", "AI solutions", "WCAG compliance"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23794", "pdf": "https://arxiv.org/pdf/2505.23794.pdf", "abs": "https://arxiv.org/abs/2505.23794", "title": "R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning", "authors": ["Yuan Li", "Qi Luo", "Xiaonan Li", "Bufan Li", "Qinyuan Cheng", "Bo Wang", "Yining Zheng", "Yuxin Wang", "Zhangyue Yin", "Xipeng Qiu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates external knowledge with Large\nLanguage Models (LLMs) to enhance factual correctness and mitigate\nhallucination. However, dense retrievers often become the bottleneck of RAG\nsystems due to their limited parameters compared to LLMs and their inability to\nperform step-by-step reasoning. While prompt-based iterative RAG attempts to\naddress these limitations, it is constrained by human-designed workflows. To\naddress these limitations, we propose $\\textbf{R3-RAG}$, which uses\n$\\textbf{R}$einforcement learning to make the LLM learn how to\n$\\textbf{R}$eason and $\\textbf{R}$etrieve step by step, thus retrieving\ncomprehensive external knowledge and leading to correct answers. R3-RAG is\ndivided into two stages. We first use cold start to make the model learn the\nmanner of iteratively interleaving reasoning and retrieval. Then we use\nreinforcement learning to further harness its ability to better explore the\nexternal retrieval environment. Specifically, we propose two rewards for\nR3-RAG: 1) answer correctness for outcome reward, which judges whether the\ntrajectory leads to a correct answer; 2) relevance-based document verification\nfor process reward, encouraging the model to retrieve documents that are\nrelevant to the user question, through which we can let the model learn how to\niteratively reason and retrieve relevant documents to get the correct answer.\nExperimental results show that R3-RAG significantly outperforms baselines and\ncan transfer well to different retrievers. We release R3-RAG at\nhttps://github.com/Yuan-Li-FNLP/R3-RAG.", "AI": {"tldr": "This paper introduces R3-RAG, a novel approach that combines reinforcement learning with retrieval-augmented generation to enhance reasoning and retrieval capabilities in large language models.", "motivation": "Existing retrieval-augmented generation methods face limitations in retrieval effectiveness due to the bottleneck posed by dense retrievers and the constraints of human-designed workflows.", "method": "R3-RAG employs a two-stage approach: initially using cold start for iterative reasoning and retrieval, followed by reinforcement learning to improve exploration of the retrieval environment. Two reward mechanisms are introduced to guide the learning process: outcome rewards for correct answers and relevance-based process rewards for document retrieval.", "result": "Experimental results demonstrate that R3-RAG significantly outperforms traditional baseline methods and shows a strong ability to adapt across different retrieval systems.", "conclusion": "R3-RAG represents a significant advance in integrating reasoning with retrieval in RAG systems, improving the factual correctness and reliability of outputs from large language models.", "key_contributions": ["Introduction of R3-RAG framework leveraging reinforcement learning for better reasoning and retrieval integration.", "Two novel reward mechanisms for guiding model learning in reasoning and retrieval tasks.", "Demonstration of significant performance improvements over existing RAG systems."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Reinforcement Learning", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24102", "pdf": "https://arxiv.org/pdf/2505.24102.pdf", "abs": "https://arxiv.org/abs/2505.24102", "title": "Beyond the Prototype: Challenges of Long-Term Integration of Visual Analytics in Civic Spaces", "authors": ["Mahmood Jasim", "Narges Mahyar"], "categories": ["cs.HC"], "comment": null, "summary": "Despite the recognized benefits of visual analytics systems in supporting\ndata-driven decision-making, their deployment in real-world civic contexts\noften faces significant barriers. Beyond technical challenges such as resource\nconstraints and development complexity, sociotechnical factors, including\norganizational hierarchies, misalignment between designers and stakeholders,\nand concerns around technology adoption hinder their sustained use. In this\nwork, we reflect on our collective experiences of designing, developing, and\ndeploying visual analytics systems in the civic domain and discuss challenges\nacross design and adoption aspects. We emphasize the need for deeper\nintegration strategies, equitable stakeholder engagement, and sustainable\nimplementation frameworks to bridge the gap between research and practice.", "AI": {"tldr": "This paper discusses the challenges of deploying visual analytics systems in civic contexts, focusing on technical and sociotechnical barriers to adoption.", "motivation": "To address the barriers faced in the deployment of visual analytics systems in real-world civic applications.", "method": "The authors reflect on their experiences in designing, developing, and implementing visual analytics systems and analyze the challenges encountered.", "result": "They identify key barriers such as organizational hierarchies, designer-stakeholder misalignment, and technology adoption concerns.", "conclusion": "The paper emphasizes the necessity for better integration strategies and sustainable implementation frameworks that can facilitate effective stakeholder engagement and promote sustained use of these systems.", "key_contributions": ["Insights into the sociotechnical barriers of visual analytics deployment", "Proposed strategies for equitable stakeholder engagement", "Framework suggestions for sustainable implementation in civic contexts"], "limitations": "The reflection is based solely on the authors' experiences, which may not be generalizable.", "keywords": ["visual analytics", "civic technology", "stakeholder engagement", "sustainable implementation", "sociotechnical challenges"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.23796", "pdf": "https://arxiv.org/pdf/2505.23796.pdf", "abs": "https://arxiv.org/abs/2505.23796", "title": "Emergent LLM behaviors are observationally equivalent to data leakage", "authors": ["Christopher Barrie", "Petter Törnberg"], "categories": ["cs.CL", "cs.GT"], "comment": null, "summary": "Ashery et al. recently argue that large language models (LLMs), when paired\nto play a classic \"naming game,\" spontaneously develop linguistic conventions\nreminiscent of human social norms. Here, we show that their results are better\nexplained by data leakage: the models simply reproduce conventions they already\nencountered during pre-training. Despite the authors' mitigation measures, we\nprovide multiple analyses demonstrating that the LLMs recognize the structure\nof the coordination game and recall its outcomes, rather than exhibit\n\"emergent\" conventions. Consequently, the observed behaviors are\nindistinguishable from memorization of the training corpus. We conclude by\npointing to potential alternative strategies and reflecting more generally on\nthe place of LLMs for social science models.", "AI": {"tldr": "This paper challenges the notion that large language models (LLMs) develop emergent conventions in linguistic tasks by arguing that their behavior is a result of data leakage and memorization from their training data.", "motivation": "To critique the findings of Ashery et al. regarding LLMs developing linguistic conventions through a naming game, and to provide a clearer understanding of the mechanisms behind the observed behaviors.", "method": "The authors conduct multiple analyses to evaluate whether the LLMs’ behaviors in a naming game reflect emergent conventions or simply memorization of training data.", "result": "The analyses demonstrate that LLMs recognize the structure of the coordination game and recall its outcomes, which are indistinguishable from mere memorization rather than the emergence of new conventions.", "conclusion": "The paper concludes by discussing alternative strategies for studying LLM behavior and reflecting on their implications for social science modeling.", "key_contributions": ["Critique of the emergent convention theory for LLMs in linguistic tasks.", "Evidence showing LLM behaviors are due to memorization rather than emergent learning.", "Suggestions for alternative strategies in LLM behavior research."], "limitations": "The paper primarily challenges existing interpretations without proposing extensive new methodologies for investigating LLMs in social science contexts.", "keywords": ["large language models", "emergent conventions", "naming game", "data leakage", "linguistic behaviors"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24107", "pdf": "https://arxiv.org/pdf/2505.24107.pdf", "abs": "https://arxiv.org/abs/2505.24107", "title": "GPTFootprint: Increasing Consumer Awareness of the Environmental Impacts of LLMs", "authors": ["Nora Graves", "Vitus Larrieu", "Yingyue Trace Zhang", "Joanne Peng", "Varun Nagaraj Rao", "Yuhan Liu", "Andrés Monroy-Hernández"], "categories": ["cs.HC"], "comment": "Published in Proceedings of the Extended Abstracts of the CHI\n  Conference on Human Factors in Computing System", "summary": "With the growth of AI, researchers are studying how to mitigate its\nenvironmental impact, primarily by proposing policy changes and increasing\nawareness among developers. However, research on AI end users is limited.\nTherefore, we introduce GPTFootprint, a browser extension that aims to increase\nconsumer awareness of the significant water and energy consumption of LLMs, and\nreduce unnecessary LLM usage. GPTFootprint displays a dynamically updating\nvisualization of the resources individual users consume through their ChatGPT\nqueries. After a user reaches a set query limit, a popup prompts them to take a\nbreak from ChatGPT. In a week-long user study, we found that GPTFootprint\nincreases people's awareness of environmental impact, but has limited success\nin decreasing ChatGPT usage. This research demonstrates the potential for\nindividual-level interventions to contribute to the broader goal of sustainable\nAI usage, and provides insights into the effectiveness of awareness-based\nbehavior modification strategies in the context of LLMs.", "AI": {"tldr": "GPTFootprint is a browser extension that raises awareness about the environmental impact of LLMs by tracking user resource consumption.", "motivation": "To address the limited research on AI end users' environmental impact and promote sustainable AI usage.", "method": "Developed the GPTFootprint browser extension, which visualizes resource consumption from ChatGPT queries and prompts users to take breaks after reaching query limits.", "result": "A week-long user study showed GPTFootprint increased awareness of environmental impact but had limited success in reducing usage of ChatGPT.", "conclusion": "Individual-level interventions can aid sustainable AI usage, although awareness alone may not significantly modify user behavior.", "key_contributions": ["Introduction of GPTFootprint browser extension", "Demonstration of increased user awareness on environmental impact", "Insights into behavior modification strategies in LLM context"], "limitations": "Limited success in reducing ChatGPT usage despite increased awareness.", "keywords": ["GPTFootprint", "environmental impact", "LLM usage", "user awareness", "sustainable AI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.23797", "pdf": "https://arxiv.org/pdf/2505.23797.pdf", "abs": "https://arxiv.org/abs/2505.23797", "title": "Detection of Suicidal Risk on Social Media: A Hybrid Model", "authors": ["Zaihan Yang", "Ryan Leonard", "Hien Tran", "Rory Driscoll", "Chadbourne Davis"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "cs.SI"], "comment": null, "summary": "Suicidal thoughts and behaviors are increasingly recognized as a critical\nsocietal concern, highlighting the urgent need for effective tools to enable\nearly detection of suicidal risk. In this work, we develop robust machine\nlearning models that leverage Reddit posts to automatically classify them into\nfour distinct levels of suicide risk severity. We frame this as a multi-class\nclassification task and propose a RoBERTa-TF-IDF-PCA Hybrid model, integrating\nthe deep contextual embeddings from Robustly Optimized BERT Approach (RoBERTa),\na state-of-the-art deep learning transformer model, with the statistical\nterm-weighting of TF-IDF, further compressed with PCA, to boost the accuracy\nand reliability of suicide risk assessment. To address data imbalance and\noverfitting, we explore various data resampling techniques and data\naugmentation strategies to enhance model generalization. Additionally, we\ncompare our model's performance against that of using RoBERTa only, the BERT\nmodel and other traditional machine learning classifiers. Experimental results\ndemonstrate that the hybrid model can achieve improved performance, giving a\nbest weighted $F_{1}$ score of 0.7512.", "AI": {"tldr": "Development of a hybrid machine learning model to classify suicide risk from Reddit posts.", "motivation": "To enable early detection of suicidal risk given the societal concern around suicidal thoughts and behaviors.", "method": "A RoBERTa-TF-IDF-PCA Hybrid model was developed for multi-class classification of suicide risk severity using various data resampling techniques and data augmentation strategies.", "result": "The hybrid model achieved a best weighted F1 score of 0.7512, outperforming traditional classifiers and RoBERTa alone.", "conclusion": "The proposed hybrid model enhances accuracy and reliability in assessing suicide risk from social media posts.", "key_contributions": ["Development of a novel hybrid machine learning model", "Integration of RoBERTa embeddings with TF-IDF and PCA", "Exploration of data resampling and augmentation techniques for better generalization"], "limitations": "", "keywords": ["suicide risk assessment", "machine learning", "natural language processing", "Reddit", "hybrid model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24126", "pdf": "https://arxiv.org/pdf/2505.24126.pdf", "abs": "https://arxiv.org/abs/2505.24126", "title": "How Students (Really) Use ChatGPT: Uncovering Experiences Among Undergraduate Students", "authors": ["Tawfiq Ammari", "Meilun Chen", "S M Mehedi Zaman", "Kiran Garimella"], "categories": ["cs.HC"], "comment": null, "summary": "This study investigates how undergraduate students engage with ChatGPT in\nself directed learning contexts. Analyzing naturalistic interaction logs, we\nidentify five dominant use categories of ChatGPT information seeking, content\ngeneration, language refinement, meta cognitive engagement, and conversational\nrepair. Behavioral modeling reveals that structured, goal driven tasks like\ncoding, multiple choice solving, and job application writing are strong\npredictors of continued use. Drawing on Self-Directed Learning (SDL) and the\nUses and Gratifications Theory (UGT), we show how students actively manage\nChatGPTs affordances and limitations through prompt adaptation, follow-ups, and\nemotional regulation. Rather than disengaging after breakdowns, students often\npersist through clarification and repair, treating the assistant as both tool\nand learning partner. We also offer design and policy recommendations to\nsupport transparent, responsive, and pedagogically grounded integration of\ngenerative AI in higher education.", "AI": {"tldr": "Investigation of undergraduate students' engagement with ChatGPT in self-directed learning reveals five usage categories and important predictors of continued use.", "motivation": "To explore how students utilize ChatGPT for self-directed learning and to identify effective engagement strategies.", "method": "Analysis of naturalistic interaction logs to categorize uses of ChatGPT and behavioral modeling to understand predictors of continued use.", "result": "Students engage with ChatGPT across five categories and persist in using it through task breakdowns, indicating its role as a learning partner.", "conclusion": "The study highlights the need for design and policy recommendations for integrating generative AI in education to enhance learning experiences.", "key_contributions": ["Identification of five dominant use categories of ChatGPT", "Behavioral modeling of predictors for continued use", "Recommendations for integration of AI in higher education"], "limitations": "", "keywords": ["ChatGPT", "Self-Directed Learning", "Higher Education", "Generative AI", "User Engagement"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.23798", "pdf": "https://arxiv.org/pdf/2505.23798.pdf", "abs": "https://arxiv.org/abs/2505.23798", "title": "My Answer Is NOT 'Fair': Mitigating Social Bias in Vision-Language Models via Fair and Biased Residuals", "authors": ["Jian Lan", "Yifei Fu", "Udo Schlegel", "Gengyuan Zhang", "Tanveer Hannan", "Haokun Chen", "Thomas Seidl"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Social bias is a critical issue in large vision-language models (VLMs), where\nfairness- and ethics-related problems harm certain groups of people in society.\nIt is unknown to what extent VLMs yield social bias in generative responses. In\nthis study, we focus on evaluating and mitigating social bias on both the\nmodel's response and probability distribution. To do so, we first evaluate four\nstate-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets with the\nmultiple-choice selection task. Surprisingly, we find that models suffer from\ngenerating gender-biased or race-biased responses. We also observe that models\nare prone to stating their responses are fair, but indeed having mis-calibrated\nconfidence levels towards particular social groups. While investigating why\nVLMs are unfair in this study, we observe that VLMs' hidden layers exhibit\nsubstantial fluctuations in fairness levels. Meanwhile, residuals in each layer\nshow mixed effects on fairness, with some contributing positively while some\nlead to increased bias. Based on these findings, we propose a post-hoc method\nfor the inference stage to mitigate social bias, which is training-free and\nmodel-agnostic. We achieve this by ablating bias-associated residuals while\namplifying fairness-associated residuals on model hidden layers during\ninference. We demonstrate that our post-hoc method outperforms the competing\ntraining strategies, helping VLMs have fairer responses and more reliable\nconfidence levels.", "AI": {"tldr": "The paper evaluates and mitigates social bias in vision-language models (VLMs) using a novel post-hoc method during inference.", "motivation": "To address social bias issues in large vision-language models that harm certain societal groups.", "method": "Evaluated four state-of-the-art VLMs on PAIRS and SocialCounterfactuals datasets and proposed a post-hoc method to mitigate biases by modifying hidden layer residuals without retraining the model.", "result": "Found that VLMs produce gender and race-biased responses while they misrepresent their fairness confidence; the proposed method improved response fairness and reliability.", "conclusion": "The proposed post-hoc method effectively reduces social bias in VLMs compared to traditional training strategies and improves the calibration of model responses.", "key_contributions": ["Evaluation of VLMs for social biases in generative responses", "Proposal of a training-free, model-agnostic post-hoc bias mitigation method", "Demonstration of improved fairness and reliability in model outputs"], "limitations": "The study is limited to the evaluation of specific datasets; further research is needed to generalize findings across more contexts.", "keywords": ["vision-language models", "social bias", "fairness", "post-hoc mitigation", "hidden layers"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24195", "pdf": "https://arxiv.org/pdf/2505.24195.pdf", "abs": "https://arxiv.org/abs/2505.24195", "title": "WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions", "authors": ["Zining Wang", "Yuxuan Zhang", "Dongwook Yoon", "Nicholas Vincent", "Farhan Samir", "Vered Shwartz"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "With more than 11 times as many pageviews as the next, English Wikipedia\ndominates global knowledge access relative to other language editions. Readers\nare prone to assuming English Wikipedia as a superset of all language editions,\nleading many to prefer it even when their primary language is not English.\nOther language editions, however, comprise complementary facts rooted in their\nrespective cultures and media environments, which are marginalized in English\nWikipedia. While Wikipedia's user interface enables switching between language\neditions through its Interlanguage Link (ILL) system, it does not reveal to\nreaders that other language editions contain valuable, complementary\ninformation. We present WikiGap, a system that surfaces complementary facts\nsourced from other Wikipedias within the English Wikipedia interface.\nSpecifically, by combining a recent multilingual information-gap discovery\nmethod with a user-centered design, WikiGap enables access to complementary\ninformation from French, Russian, and Chinese Wikipedia. In a mixed-methods\nstudy (n=21), WikiGap significantly improved fact-finding accuracy, reduced\ntask time, and received a 32-point higher usability score relative to\nWikipedia's current ILL-based navigation system. Participants reported\nincreased awareness of the availability of complementary information in\nnon-English editions and reconsidered the completeness of English Wikipedia.\nWikiGap thus paves the way for improved epistemic equity across language\neditions.", "AI": {"tldr": "WikiGap is a system that improves access to complementary information from other language editions of Wikipedia, enhancing users' awareness and fact-finding accuracy.", "motivation": "English Wikipedia's dominance leads readers to overlook valuable information in other language editions, which contain complementary facts rooted in diverse cultures.", "method": "WikiGap integrates a multilingual information-gap discovery method with a user-centered design to present information from French, Russian, and Chinese Wikipedia within the English Wikipedia interface.", "result": "In a study with 21 participants, WikiGap showed significant improvements in fact-finding accuracy and reduced task times, yielding a higher usability score compared to the current Interlanguage Link system.", "conclusion": "WikiGap enhances epistemic equity by allowing users to recognize the value of complementary information in non-English Wikipedia editions.", "key_contributions": ["Introduces a system for surfacing complementary facts from other Wikipedia editions.", "Demonstrates significant improvements in usability and task performance in user studies.", "Raises awareness of the content availability across different language editions."], "limitations": "Study sample size is small, and the system requires further testing across broader demographics.", "keywords": ["Wikipedia", "Multilingual Information", "User-Centered Design", "Usability", "Epistemic Equity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799.pdf", "abs": "https://arxiv.org/abs/2505.23799", "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used.", "AI": {"tldr": "This paper investigates methods for measuring the consistency of large language models (LLMs) and proposes a new logit-based ensemble method that aligns better with human perceptions of LLM consistency compared to existing metrics.", "motivation": "To address the issues of hallucinations and response fragility in LLMs, which affect the reliability of generated text, by finding effective ways to measure the consistency of LLM responses as perceived by users.", "method": "A user study with 2,976 participants was conducted to evaluate different methods for measuring LLM consistency, focusing on their alignment with human perceptions. An ensemble method based on logits was proposed for this purpose.", "result": "The proposed logit-based ensemble method for estimating LLM consistency performed comparably to the best existing metrics in correlating with human ratings of consistency, highlighting shortcomings in past approaches that did not involve human evaluation.", "conclusion": "The study recommends incorporating human evaluations for estimating LLM consistency, given the inadequacies of current automated methods to reflect user perceptions.", "key_contributions": ["Proposed a logit-based ensemble method for consistency estimation", "Demonstrated that existing methods often misalign with human perceptions", "Emphasized the need for human evaluation in assessing LLM consistency"], "limitations": "The study only included a single user study and may not represent all contexts in which LLMs are used.", "keywords": ["language models", "consistency", "user study", "hallucinations", "human evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24246", "pdf": "https://arxiv.org/pdf/2505.24246.pdf", "abs": "https://arxiv.org/abs/2505.24246", "title": "Locating Risk: Task Designers and the Challenge of Risk Disclosure in RAI Content Work", "authors": ["Alice Qian Zhang", "Ryland Shaw", "Laura Dabbish", "Jina Suh", "Hong Shen"], "categories": ["cs.HC", "cs.CY"], "comment": "Under submission at CSCW 2026", "summary": "As AI systems are increasingly tested and deployed in open-ended and\nhigh-stakes domains, crowd workers are often tasked with responsible AI (RAI)\ncontent work. These tasks include labeling violent content, moderating\ndisturbing text, or simulating harmful behavior for red teaming exercises to\nshape AI system behaviors. While prior efforts have highlighted the risks to\nworker well-being associated with RAI content work, far less attention has been\npaid to how these risks are communicated to workers. Existing transparency\nframeworks and guidelines such as model cards, datasheets, and crowdworksheets\nfocus on documenting model information and dataset collection processes, but\nthey overlook an important aspect of disclosing well-being risks to workers. In\nthe absence of standard workflows or clear guidance, the consistent application\nof content warnings, consent flows, or other forms of well-being risk\ndisclosure remain unclear. This study investigates how task designers approach\nrisk disclosure in crowdsourced RAI tasks. Drawing on interviews with 23 task\ndesigners across academic and industry sectors, we examine how well-being risk\nis recognized, interpreted, and communicated in practice. Our findings surface\na need to support task designers in identifying and communicating well-being\nrisk not only to support crowdworker well-being but also to strengthen the\nethical integrity and technical efficacy of AI development pipelines.", "AI": {"tldr": "This study investigates how task designers communicate well-being risks to crowd workers in responsible AI content work, highlighting the need for improved transparency and support in risk disclosure.", "motivation": "As AI systems are tested in high-stakes environments, there are risks to the well-being of crowd workers involved in responsible AI tasks, which require clearer communication of these risks.", "method": "Interviews with 23 task designers from academic and industry sectors to examine their approaches to risk disclosure in crowdsourced responsible AI work.", "result": "The study finds inconsistencies in how well-being risks are recognized and communicated, indicating a need for better workflows and transparency for task designers.", "conclusion": "Improving the communication of well-being risks can enhance worker safety and strengthen the ethical standards and effectiveness of AI development processes.", "key_contributions": ["Identification of gaps in current transparency frameworks regarding well-being risks to workers", "Insights from task designers on how risks are communicated", "Recommendations for improving risk disclosure practices"], "limitations": "", "keywords": ["Responsible AI", "Crowdwork", "Worker Well-being", "Risk Disclosure", "Transparency"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23801", "pdf": "https://arxiv.org/pdf/2505.23801.pdf", "abs": "https://arxiv.org/abs/2505.23801", "title": "SEMFED: Semantic-Aware Resource-Efficient Federated Learning for Heterogeneous NLP Tasks", "authors": ["Sajid Hussain", "Muhammad Sohail", "Nauman Ali Khan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages", "summary": "Background: Federated Learning (FL) has emerged as a promising paradigm for\ntraining machine learning models while preserving data privacy. However,\napplying FL to Natural Language Processing (NLP) tasks presents unique\nchallenges due to semantic heterogeneity across clients, vocabulary mismatches,\nand varying resource constraints on edge devices. Objectives: This paper\nintroduces SEMFED, a novel semantic-aware resource-efficient federated learning\nframework specifically designed for heterogeneous NLP tasks. Methods: SEMFED\nincorporates three key innovations: (1) a semantic-aware client selection\nmechanism that balances semantic diversity with resource constraints, (2)\nadaptive NLP-specific model architectures tailored to device capabilities while\npreserving semantic information, and (3) a communication-efficient semantic\nfeature compression technique that significantly reduces bandwidth\nrequirements. Results: Experimental results on various NLP classification tasks\ndemonstrate that SEMFED achieves an 80.5% reduction in communication costs\nwhile maintaining model accuracy above 98%, outperforming state-of-the-art FL\napproaches. Conclusion: SEMFED effectively manages heterogeneous client\nenvironments with varying computational resources, network reliability, and\nsemantic data distributions, making it particularly suitable for real-world\nfederated NLP deployments.", "AI": {"tldr": "SEMFED is a novel federated learning framework that addresses challenges in NLP by improving client selection, model architecture, and communication efficiency.", "motivation": "To address the unique challenges of applying federated learning in NLP tasks, including semantic heterogeneity and resource constraints.", "method": "SEMFED features a semantic-aware client selection mechanism, adaptive NLP-specific model architectures, and a communication-efficient semantic feature compression technique.", "result": "SEMFED achieves an 80.5% reduction in communication costs while maintaining model accuracy above 98% across various NLP classification tasks.", "conclusion": "SEMFED is well-suited for heterogeneous client environments, making it effective for real-world federated NLP deployments.", "key_contributions": ["Semantic-aware client selection mechanism", "Adaptive NLP-specific model architectures", "Communication-efficient semantic feature compression technique"], "limitations": "", "keywords": ["Federated Learning", "Natural Language Processing", "Resource Efficiency", "Client Selection", "Semantic Heterogeneity"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2505.24348", "pdf": "https://arxiv.org/pdf/2505.24348.pdf", "abs": "https://arxiv.org/abs/2505.24348", "title": "A 3D Mobile Crowdsensing Framework for Sustainable Urban Digital Twins", "authors": ["Taku Yamazaki", "Kaito Watanabe", "Tatsuya Kase", "Kenta Hasegawa", "Koki Saida", "Takumi Miyoshi"], "categories": ["cs.HC", "cs.CY"], "comment": "8 pages, 18 figures, 3 tables", "summary": "In this article, we propose a 3D mobile crowdsensing (3D-MCS) framework aimed\nat sustainable urban digital twins (UDTs). The framework comprises four key\nmechanisms: (1) the 3D-MCS mechanism, consisting of active and passive models;\n(2) the Geohash-based spatial information management mechanism; (3) the dynamic\npoint cloud integration mechanism for UDTs; and (4) the web-based real-time\nvisualizer for 3D-MCS and UDTs. The active sensing model features a gamified\n3D-MCS approach, where participants collect point cloud data through an\naugmented reality territory coloring game. In contrast, the passive sensing\nmodel employs a wearable 3D-MCS approach, where participants wear smartphones\naround their necks without disrupting daily activities. The spatial information\nmanagement mechanism efficiently partitions the space into regions using\nGeohash. The dynamic point cloud integration mechanism incorporates point\nclouds collected by 3D-MCS into UDTs through global and local point cloud\nregistration. Finally, we evaluated the proposed framework through real-world\nexperiments. We verified the effectiveness of the proposed 3D-MCS models from\nthe perspectives of subjective evaluation and data collection and analysis.\nFurthermore, we analyzed the performance of the dynamic point cloud integration\nusing a dataset.", "AI": {"tldr": "This paper proposes a 3D mobile crowdsensing framework designed for sustainable urban digital twins, integrating gamified active sensing and wearable passive sensing with dynamic point cloud integration.", "motivation": "The need for sustainable urban digital twins to enhance urban planning and management.", "method": "The framework includes an active model using a gamified approach for data collection and a passive model utilizing wearable technology, along with mechanisms for spatial information management and point cloud integration.", "result": "Real-world experiments validated the effectiveness of the proposed models in data collection and integration for urban digital twins.", "conclusion": "The proposed 3D-MCS framework contributes to the development of sustainable urban digital twins through innovative data collection and integration methods.", "key_contributions": ["Introduction of a gamified 3D mobile crowdsensing mechanism", "Development of a spatial information management strategy using Geohash", "Implementation of dynamic point cloud integration for urban digital twins"], "limitations": "The scalability of the framework in larger urban contexts and potential privacy concerns with data collection.", "keywords": ["3D mobile crowdsensing", "urban digital twins", "gamified sensing", "point cloud integration", "wearable technology"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.23802", "pdf": "https://arxiv.org/pdf/2505.23802.pdf", "abs": "https://arxiv.org/abs/2505.23802", "title": "MedHELM: Holistic Evaluation of Large Language Models for Medical Tasks", "authors": ["Suhana Bedi", "Hejie Cui", "Miguel Fuentes", "Alyssa Unell", "Michael Wornow", "Juan M. Banda", "Nikesh Kotecha", "Timothy Keyes", "Yifan Mai", "Mert Oez", "Hao Qiu", "Shrey Jain", "Leonardo Schettini", "Mehr Kashyap", "Jason Alan Fries", "Akshay Swaminathan", "Philip Chung", "Fateme Nateghi", "Asad Aali", "Ashwin Nayak", "Shivam Vedak", "Sneha S. Jain", "Birju Patel", "Oluseyi Fayanju", "Shreya Shah", "Ethan Goh", "Dong-han Yao", "Brian Soetikno", "Eduardo Reis", "Sergios Gatidis", "Vasu Divi", "Robson Capasso", "Rachna Saralkar", "Chia-Chun Chiang", "Jenelle Jindal", "Tho Pham", "Faraz Ghoddusi", "Steven Lin", "Albert S. Chiou", "Christy Hong", "Mohana Roy", "Michael F. Gensheimer", "Hinesh Patel", "Kevin Schulman", "Dev Dash", "Danton Char", "Lance Downing", "Francois Grolleau", "Kameron Black", "Bethel Mieso", "Aydin Zahedivash", "Wen-wai Yim", "Harshita Sharma", "Tony Lee", "Hannah Kirsch", "Jennifer Lee", "Nerissa Ambers", "Carlene Lugtu", "Aditya Sharma", "Bilal Mawji", "Alex Alekseyev", "Vicky Zhou", "Vikas Kakkar", "Jarrod Helzer", "Anurang Revri", "Yair Bannett", "Roxana Daneshjou", "Jonathan Chen", "Emily Alsentzer", "Keith Morse", "Nirmal Ravi", "Nima Aghaeepour", "Vanessa Kennedy", "Akshay Chaudhari", "Thomas Wang", "Sanmi Koyejo", "Matthew P. Lungren", "Eric Horvitz", "Percy Liang", "Mike Pfeffer", "Nigam H. Shah"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) achieve near-perfect scores on medical\nlicensing exams, these evaluations inadequately reflect the complexity and\ndiversity of real-world clinical practice. We introduce MedHELM, an extensible\nevaluation framework for assessing LLM performance for medical tasks with three\nkey contributions. First, a clinician-validated taxonomy spanning 5 categories,\n22 subcategories, and 121 tasks developed with 29 clinicians. Second, a\ncomprehensive benchmark suite comprising 35 benchmarks (17 existing, 18 newly\nformulated) providing complete coverage of all categories and subcategories in\nthe taxonomy. Third, a systematic comparison of LLMs with improved evaluation\nmethods (using an LLM-jury) and a cost-performance analysis. Evaluation of 9\nfrontier LLMs, using the 35 benchmarks, revealed significant performance\nvariation. Advanced reasoning models (DeepSeek R1: 66% win-rate; o3-mini: 64%\nwin-rate) demonstrated superior performance, though Claude 3.5 Sonnet achieved\ncomparable results at 40% lower estimated computational cost. On a normalized\naccuracy scale (0-1), most models performed strongly in Clinical Note\nGeneration (0.73-0.85) and Patient Communication & Education (0.78-0.83),\nmoderately in Medical Research Assistance (0.65-0.75), and generally lower in\nClinical Decision Support (0.56-0.72) and Administration & Workflow\n(0.53-0.63). Our LLM-jury evaluation method achieved good agreement with\nclinician ratings (ICC = 0.47), surpassing both average clinician-clinician\nagreement (ICC = 0.43) and automated baselines including ROUGE-L (0.36) and\nBERTScore-F1 (0.44). Claude 3.5 Sonnet achieved comparable performance to top\nmodels at lower estimated cost. These findings highlight the importance of\nreal-world, task-specific evaluation for medical use of LLMs and provides an\nopen source framework to enable this.", "AI": {"tldr": "MedHELM is a new evaluation framework for assessing large language models (LLMs) in medical tasks, featuring a clinician-validated taxonomy, a comprehensive benchmark suite, and innovative evaluation methods.", "motivation": "To address the inadequacy of existing evaluations for LLMs in reflecting the complexity of real-world clinical practices.", "method": "Development of a taxonomy with contributions from 29 clinicians, creation of a benchmark suite with 35 tasks, and systematic evaluation of 9 LLMs using improved methods including an LLM-jury.", "result": "LLM evaluation revealed performance variations among 9 models, with deep reasoning models performing best but lower-cost models like Claude 3.5 Sonnet showing competitive results. The new evaluation method correlated well with clinician ratings and surpassed existing automated metrics.", "conclusion": "Real-world evaluations are crucial for assessing the application of LLMs in medicine, and the MedHELM framework is made available for future research and improvements in this area.", "key_contributions": ["Clinician-validated taxonomy for medical tasks", "Comprehensive benchmark suite covering diverse medical tasks", "LLM-jury evaluation method that aligns well with clinician assessments"], "limitations": "", "keywords": ["large language models", "medical tasks", "evaluation framework"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24658", "pdf": "https://arxiv.org/pdf/2505.24658.pdf", "abs": "https://arxiv.org/abs/2505.24658", "title": "Can LLMs and humans be friends? Uncovering factors affecting human-AI intimacy formation", "authors": ["Yeseon Hong", "Junhyuk Choi", "Minju Kim", "Bugeun Kim"], "categories": ["cs.HC", "H.5.2; I.2.7"], "comment": "30 pages, 2figures", "summary": "Large language models (LLMs) are increasingly being used in conversational\nroles, yet little is known about how intimacy emerges in human-LLM\ninteractions. Although previous work emphasized the importance of\nself-disclosure in human-chatbot interaction, it is questionable whether\ngradual and reciprocal self-disclosure is also helpful in human-LLM\ninteraction. Thus, this study examined three possible aspects contributing to\nintimacy formation: gradual self-disclosure, reciprocity, and naturalness.\nStudy 1 explored the impact of mutual, gradual self-disclosure with 29 users\nand a vanilla LLM. Study 2 adopted self-criticism methods for more natural\nresponses and conducted a similar experiment with 53 users. Results indicate\nthat gradual self-disclosure significantly enhances perceived social intimacy,\nregardless of persona reciprocity. Moreover, participants perceived utterances\ngenerated with self-criticism as more natural compared to those of vanilla\nLLMs; self-criticism fostered higher intimacy in early stages. Also, we\nobserved that excessive empathetic expressions occasionally disrupted\nimmersion, pointing to the importance of response calibration during intimacy\nformation.", "AI": {"tldr": "This study investigates how intimacy develops in human-LLM interactions through gradual self-disclosure, reciprocity, and perceived naturalness of responses.", "motivation": "Understanding intimacy in human-LLM interactions is vital as LLMs gain popularity in conversational roles, yet little research has addressed how intimacy forms in these contexts.", "method": "The study consists of two main experiments: Study 1 involved 29 users interacting with a vanilla LLM focusing on mutual self-disclosure; Study 2 involved 53 users, incorporating self-criticism to enhance natural responses in a similar experimental setup.", "result": "Gradual self-disclosure significantly increased perceived social intimacy. Responses from LLMs using self-criticism were viewed as more natural and fostered higher intimacy early in interactions, although overly empathetic responses could disrupt immersion.", "conclusion": "The findings suggest that gradual self-disclosure is beneficial for intimacy in human-LLM interactions, emphasizing the need for careful calibration of responses to maintain immersion.", "key_contributions": ["Explored the role of gradual self-disclosure in LLM interactions", "Demonstrated the impact of self-criticism on perceived naturalness and intimacy", "Identified potential pitfalls of excessive empathetic responses in interactions"], "limitations": "The study's results may not be generalizable beyond the specific LLMs used or the small user samples; further research is needed across diverse interactions.", "keywords": ["Large Language Models", "Human-LLM Interaction", "Intimacy", "Self-Disclosure", "Natural Responses"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.23804", "pdf": "https://arxiv.org/pdf/2505.23804.pdf", "abs": "https://arxiv.org/abs/2505.23804", "title": "Calibrating LLMs for Text-to-SQL Parsing by Leveraging Sub-clause Frequencies", "authors": ["Terrance Liu", "Shuyi Wang", "Daniel Preotiuc-Pietro", "Yash Chandarana", "Chirag Gupta"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) achieve strong performance on text-to-SQL\nparsing, they sometimes exhibit unexpected failures in which they are\nconfidently incorrect. Building trustworthy text-to-SQL systems thus requires\neliciting reliable uncertainty measures from the LLM. In this paper, we study\nthe problem of providing a calibrated confidence score that conveys the\nlikelihood of an output query being correct. Our work is the first to establish\na benchmark for post-hoc calibration of LLM-based text-to-SQL parsing. In\nparticular, we show that Platt scaling, a canonical method for calibration,\nprovides substantial improvements over directly using raw model output\nprobabilities as confidence scores. Furthermore, we propose a method for\ntext-to-SQL calibration that leverages the structured nature of SQL queries to\nprovide more granular signals of correctness, named \"sub-clause frequency\"\n(SCF) scores. Using multivariate Platt scaling (MPS), our extension of the\ncanonical Platt scaling technique, we combine individual SCF scores into an\noverall accurate and calibrated score. Empirical evaluation on two popular\ntext-to-SQL datasets shows that our approach of combining MPS and SCF yields\nfurther improvements in calibration and the related task of error detection\nover traditional Platt scaling.", "AI": {"tldr": "This paper addresses the reliability of uncertainty measures in text-to-SQL parsing using large language models and proposes a method that improves the calibration of confidence scores for these outputs.", "motivation": "To build trustworthy text-to-SQL systems, it is essential to accurately gauge the reliability of outputs from large language models, which can sometimes fail confident but incorrectly.", "method": "The paper introduces a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing and uses Platt scaling along with an innovative approach leveraging SQL query structures called 'sub-clause frequency' (SCF) scores, which are combined using multivariate Platt scaling (MPS).", "result": "Empirical evaluation on popular text-to-SQL datasets indicates that the proposed method of combining MPS and SCF significantly improves both calibration accuracy and error detection compared to traditional Platt scaling.", "conclusion": "The study demonstrates that calibrated confidence scores enhance the reliability of outputs from large language models used for text-to-SQL parsing.", "key_contributions": ["Establishment of a benchmark for post-hoc calibration of LLM-based text-to-SQL parsing.", "Introduction of 'sub-clause frequency' (SCF) scores for improved calibration.", "Combination of individual SCF scores using multivariate Platt scaling (MPS) yields enhanced calibration and error detection."], "limitations": "", "keywords": ["text-to-SQL", "large language models", "confidence calibration", "Platt scaling", "error detection"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.23799", "pdf": "https://arxiv.org/pdf/2505.23799.pdf", "abs": "https://arxiv.org/abs/2505.23799", "title": "Estimating LLM Consistency: A User Baseline vs Surrogate Metrics", "authors": ["Xiaoyuan Wu", "Weiran Lin", "Omer Akgul", "Lujo Bauer"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are prone to hallucinations and sensitive to\nprompt perturbations, often resulting in inconsistent or unreliable generated\ntext. Different methods have been proposed to mitigate such hallucinations and\nfragility -- one of them being measuring the consistency (the model's\nconfidence in the response, or likelihood of generating a similar response when\nresampled) of LLM responses. In previous work, measuring consistency often\nrelied on the probability of a response appearing within a pool of resampled\nresponses, or internal states or logits of responses. However, it is not yet\nclear how well these approaches approximate how humans perceive the consistency\nof LLM responses. We performed a user study (n=2,976) and found current methods\ntypically do not approximate users' perceptions of LLM consistency very well.\nWe propose a logit-based ensemble method for estimating LLM consistency, and we\nshow that this method matches the performance of the best-performing existing\nmetric in estimating human ratings of LLM consistency. Our results suggest that\nmethods of estimating LLM consistency without human evaluation are sufficiently\nimperfect that we suggest evaluation with human input be more broadly used.", "AI": {"tldr": "This paper investigates the measurement of consistency in LLM responses through a user study and proposes a logit-based ensemble method for a more accurate estimation.", "motivation": "To address the issue of hallucinations and fragility in large language models (LLMs), which affect the reliability of generated text.", "method": "A user study with 2,976 participants to evaluate existing methods for measuring LLM consistency and the introduction of a logit-based ensemble method to improve accuracy.", "result": "The proposed method matches the performance of the best existing metric for estimating human ratings of LLM consistency, highlighting the inadequacies of previous approaches.", "conclusion": "Current methods often fail to align with human perceptions of LLM consistency, suggesting a need for integrating human evaluations in consistency estimation.", "key_contributions": ["User study demonstrating user perceptions of LLM consistency", "Logit-based ensemble method for better consistency estimation", "Call for broader use of human evaluation in measuring LLM consistency"], "limitations": "", "keywords": ["large language models", "LLMs", "consistency estimation", "human evaluation", "hallucinations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23806", "pdf": "https://arxiv.org/pdf/2505.23806.pdf", "abs": "https://arxiv.org/abs/2505.23806", "title": "MedOrchestra: A Hybrid Cloud-Local LLM Approach for Clinical Data Interpretation", "authors": ["Sihyeon Lee", "Hyunjoo Song", "Jong-chan Lee", "Yoon Jin Lee", "Boram Lee", "Hee-Eon Lim", "Dongyeong Kim", "Jinwook Seo", "Bohyoung Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Deploying large language models (LLMs) in clinical settings faces critical\ntrade-offs: cloud LLMs, with their extensive parameters and superior\nperformance, pose risks to sensitive clinical data privacy, while local LLMs\npreserve privacy but often fail at complex clinical interpretation tasks. We\npropose MedOrchestra, a hybrid framework where a cloud LLM decomposes complex\nclinical tasks into manageable subtasks and prompt generation, while a local\nLLM executes these subtasks in a privacy-preserving manner. Without accessing\nclinical data, the cloud LLM generates and validates subtask prompts using\nclinical guidelines and synthetic test cases. The local LLM executes subtasks\nlocally and synthesizes outputs generated by the cloud LLM. We evaluate\nMedOrchestra on pancreatic cancer staging using 100 radiology reports under\nNCCN guidelines. On free-text reports, MedOrchestra achieves 70.21% accuracy,\noutperforming local model baselines (without guideline: 48.94%, with guideline:\n56.59%) and board-certified clinicians (gastroenterologists: 59.57%, surgeons:\n65.96%, radiologists: 55.32%). On structured reports, MedOrchestra reaches\n85.42% accuracy, showing clear superiority across all settings.", "AI": {"tldr": "MedOrchestra is a hybrid framework combining cloud and local LLMs for improved clinical task execution while preserving data privacy.", "motivation": "To address the trade-offs between performance and data privacy in deploying LLMs in clinical settings.", "method": "A hybrid framework where a cloud LLM decomposes complex tasks and generates prompts, and a local LLM executes these tasks privately.", "result": "MedOrchestra achieves 70.21% accuracy on free-text reports and 85.42% on structured reports, outperforming local baselines and clinicians.", "conclusion": "The hybrid approach significantly improves accuracy for clinical tasks while maintaining privacy.", "key_contributions": ["Introduction of a hybrid LLM framework for clinical tasks", "Demonstrated superiority over traditional models and clinician performance", "Validation using real clinical guidelines and test cases"], "limitations": "", "keywords": ["large language models", "clinical settings", "privacy-preserving", "hybrid framework", "MedOrchestra"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23856", "pdf": "https://arxiv.org/pdf/2505.23856.pdf", "abs": "https://arxiv.org/abs/2505.23856", "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities", "authors": ["Sahil Verma", "Keegan Hines", "Jeff Bilmes", "Charlotte Siska", "Luke Zettlemoyer", "Hila Gonen", "Chandan Singh"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard.", "AI": {"tldr": "OMNIGUARD is a novel approach for detecting harmful prompts in large language models across multiple languages and modalities, showing substantial improvements in classification accuracy and efficiency.", "motivation": "To address the fallibility of current detection approaches for harmful prompts in LLMs, particularly in low-resource languages and non-text modalities like images and audio.", "method": "OMNIGUARD identifies internal representations of language models that are aligned across languages and modalities to build a classifier that is agnostic to both.", "result": "OMNIGUARD improves harmful prompt classification accuracy by 11.57% in multilingual settings, 20.44% for image prompts, and sets a new state-of-the-art for audio prompts, while being approximately 120 times faster than its closest competitor.", "conclusion": "OMNIGUARD significantly enhances the detection of harmful prompts across various modalities while maintaining high efficiency, making it a promising tool for addressing LLM safety concerns.", "key_contributions": ["Introduces a language-agnostic classifier for harmful prompts.", "Achieves state-of-the-art results in audio-based prompt detection.", "Demonstrates high efficiency, being much faster than existing methods."], "limitations": "", "keywords": ["harmful prompts", "large language models", "multimodal detection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23807", "pdf": "https://arxiv.org/pdf/2505.23807.pdf", "abs": "https://arxiv.org/abs/2505.23807", "title": "DLP: Dynamic Layerwise Pruning in Large Language Models", "authors": ["Yuli Chen", "Bo Cheng", "Jiale Han", "Yingying Zhang", "Yingting Li", "Shuhao Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.", "AI": {"tldr": "Dynamic Layerwise Pruning (DLP) adapts effective pruning rates for each layer in LLMs to improve performance and reduce inference inefficiencies.", "motivation": "Existing pruning methods often lead to performance degradation in LLMs due to uniform layerwise strategies. A need for adaptive approaches that can preserve model performance while achieving high sparsity motivates the development of DLP.", "method": "DLP determines the relative importance of each layer by integrating model weights and input activation information to assign dynamic pruning rates.", "result": "At 70% sparsity, DLP reduces the perplexity of LLaMA2-7B by 7.79 and improves average accuracy by 2.7% compared to state-of-the-art pruning techniques.", "conclusion": "DLP effectively maintains model performance at high sparsity levels and is compatible with various LLM compression techniques. The code is available for further research.", "key_contributions": ["Introduction of Dynamic Layerwise Pruning (DLP) for adaptive layer importance assessment.", "Demonstrated improved performance metrics at high sparsity levels vs. traditional methods.", "Compatibility with existing Parameter-Efficient Fine-Tuning (PEFT) techniques."], "limitations": "", "keywords": ["Dynamic Layerwise Pruning", "Large Language Models", "Model Compression", "Sparsity", "Performance Preservation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23808", "pdf": "https://arxiv.org/pdf/2505.23808.pdf", "abs": "https://arxiv.org/abs/2505.23808", "title": "DenseLoRA: Dense Low-Rank Adaptation of Large Language Models", "authors": ["Lin Mu", "Xiaoyu Wang", "Li Ni", "Yang Li", "Zhize Wu", "Peiquan Jin", "Yiwen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Low-rank adaptation (LoRA) has been developed as an efficient approach for\nadapting large language models (LLMs) by fine-tuning two low-rank matrices,\nthereby reducing the number of trainable parameters. However, prior research\nindicates that many of the weights in these matrices are redundant, leading to\ninefficiencies in parameter utilization. To address this limitation, we\nintroduce Dense Low-Rank Adaptation (DenseLoRA), a novel approach that enhances\nparameter efficiency while achieving superior performance compared to LoRA.\nDenseLoRA builds upon the concept of representation fine-tuning, incorporating\na single Encoder-Decoder to refine and compress hidden representations across\nall adaptation layers before applying adaptation. Instead of relying on two\nredundant low-rank matrices as in LoRA, DenseLoRA adapts LLMs through a dense\nlow-rank matrix, improving parameter utilization and adaptation efficiency. We\nevaluate DenseLoRA on various benchmarks, showing that it achieves 83.8%\naccuracy with only 0.01% of trainable parameters, compared to LoRA's 80.8%\naccuracy with 0.70% of trainable parameters on LLaMA3-8B. Additionally, we\nconduct extensive experiments to systematically assess the impact of\nDenseLoRA's components on overall model performance. Code is available at\nhttps://github.com/mulin-ahu/DenseLoRA.", "AI": {"tldr": "Dense Low-Rank Adaptation (DenseLoRA) improves the efficiency and performance of adapting large language models by utilizing a single dense low-rank matrix instead of two redundant ones.", "motivation": "Prior research on Low-rank adaptation (LoRA) reveals inefficiencies due to redundancy in parameter use, prompting the need for a more effective adaptation approach.", "method": "DenseLoRA utilizes a single Encoder-Decoder to refine and compress hidden representations across all layers, adapting LLMs with a dense low-rank matrix.", "result": "DenseLoRA achieves 83.8% accuracy with only 0.01% of trainable parameters, outperforming LoRA's 80.8% accuracy with 0.70% of trainable parameters on LLaMA3-8B.", "conclusion": "DenseLoRA offers superior parameter efficiency and performance improvements over traditional LoRA, making it a promising method for large language model adaptation.", "key_contributions": ["Introduction of DenseLoRA for improved parameter efficiency", "Demonstrates higher accuracy with fewer trainable parameters compared to LoRA", "Extensive evaluation of DenseLoRA's components and their impact on performance"], "limitations": "", "keywords": ["DenseLowRA", "LLM", "parameter efficiency", "large language models", "fine-tuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23809", "pdf": "https://arxiv.org/pdf/2505.23809.pdf", "abs": "https://arxiv.org/abs/2505.23809", "title": "LLM-Driven E-Commerce Marketing Content Optimization: Balancing Creativity and Conversion", "authors": ["Haowei Yang", "Haotian Lyu", "Tianle Zhang", "Dingzhou Wang", "Yushang Zhao"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "As e-commerce competition intensifies, balancing creative content with\nconversion effectiveness becomes critical. Leveraging LLMs' language generation\ncapabilities, we propose a framework that integrates prompt engineering,\nmulti-objective fine-tuning, and post-processing to generate marketing copy\nthat is both engaging and conversion-driven. Our fine-tuning method combines\nsentiment adjustment, diversity enhancement, and CTA embedding. Through offline\nevaluations and online A/B tests across categories, our approach achieves a\n12.5 % increase in CTR and an 8.3 % increase in CVR while maintaining content\nnovelty. This provides a practical solution for automated copy generation and\nsuggests paths for future multimodal, real-time personalization.", "AI": {"tldr": "Proposes a framework utilizing LLMs for creating engaging and conversion-driven marketing copy.", "motivation": "Increased competition in e-commerce necessitates effective marketing content that balances creativity and conversion rates.", "method": "Integrates prompt engineering, multi-objective fine-tuning (including sentiment adjustment and diversity enhancement), and post-processing for marketing copy generation.", "result": "Achieves 12.5% increase in CTR and 8.3% increase in CVR through offline evaluations and online A/B tests.", "conclusion": "Offers a practical automated solution for copy generation and directions for future research in real-time personalization.", "key_contributions": ["Framework for automated marketing copy generation using LLMs", "Integration of multi-objective fine-tuning techniques", "Demonstrated effectiveness through empirical testing"], "limitations": "", "keywords": ["E-commerce", "Marketing", "LLM", "Conversion", "Personalization"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.23810", "pdf": "https://arxiv.org/pdf/2505.23810.pdf", "abs": "https://arxiv.org/abs/2505.23810", "title": "MARS-Bench: A Multi-turn Athletic Real-world Scenario Benchmark for Dialogue Evaluation", "authors": ["Chenghao Yang", "Yinbo Luo", "Zhoufutu Wen", "Qi Chu", "Tao Gong", "Longxiang Liu", "Kaiyuan Zhang", "Jianpeng Jiao", "Ge Zhang", "Wenhao Huang", "Nenghai Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 13 figures", "summary": "Large Language Models (\\textbf{LLMs}), e.g. ChatGPT, have been widely adopted\nin real-world dialogue applications. However, LLMs' robustness, especially in\nhandling long complex dialogue sessions, including frequent motivation\ntransfer, sophisticated cross-turn dependency, is criticized all along.\nNevertheless, no existing benchmarks can fully reflect these weaknesses. We\npresent \\textbf{MARS-Bench}, a \\textbf{M}ulti-turn \\textbf{A}thletic\n\\textbf{R}eal-world \\textbf{S}cenario Dialogue \\textbf{Bench}mark, designed to\nremedy the gap. MARS-Bench is constructed from play-by-play text commentary so\nto feature realistic dialogues specifically designed to evaluate three critical\naspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn,\nand Cross-turn Tasks. Extensive experiments on MARS-Bench also reveal that\nclosed-source LLMs significantly outperform open-source alternatives, explicit\nreasoning significantly boosts LLMs' robustness on handling long complex\ndialogue sessions, and LLMs indeed face significant challenges when handling\nmotivation transfer and sophisticated cross-turn dependency. Moreover, we\nprovide mechanistic interpretability on how attention sinks due to special\ntokens lead to LLMs' performance degradation when handling long complex\ndialogue sessions based on attention visualization experiment in\nQwen2.5-7B-Instruction.", "AI": {"tldr": "MARS-Bench is a benchmark designed to evaluate the robustness of LLMs in complex multi-turn dialogues, revealing significant performance differences between closed-source and open-source models.", "motivation": "To address the lack of benchmarks that reflect the weaknesses of LLMs in handling long complex dialogue sessions, including motivation transfer and cross-turn dependency.", "method": "MARS-Bench is constructed from play-by-play text commentary and focuses on three aspects of multi-turn conversations: Ultra Multi-turn, Interactive Multi-turn, and Cross-turn Tasks. Extensive experiments are conducted to assess the performance of different LLMs on this benchmark.", "result": "The experiments show that closed-source LLMs outperform open-source ones, and that explicit reasoning improves robustness in handling complex dialogue. Challenges in motivation transfer and cross-turn dependency were identified.", "conclusion": "MARS-Bench provides insights into LLMs' performance issues in multi-turn dialogues and illustrates how special token attention leads to performance degradation in handling long dialogues.", "key_contributions": ["Introduces MARS-Bench, a new benchmark for evaluating LLM multi-turn dialogue capabilities.", "Reveals that closed-source models significantly outperform open-source alternatives in dialogue tasks.", "Demonstrates the importance of explicit reasoning for LLM robustness in complex conversations."], "limitations": "Benchmarks are derived from specific dialogue scenarios and may not cover all potential dialogue complexities.", "keywords": ["Large Language Models", "dialogue applications", "multi-turn conversations"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23811", "pdf": "https://arxiv.org/pdf/2505.23811.pdf", "abs": "https://arxiv.org/abs/2505.23811", "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions", "authors": ["Hadi Askari", "Shivanshu Gupta", "Fei Wang", "Anshuman Chhabra", "Muhao Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under Review", "summary": "Pretrained Large Language Models (LLMs) achieve strong performance across a\nwide range of tasks, yet exhibit substantial variability in the various layers'\ntraining quality with respect to specific downstream applications, limiting\ntheir downstream performance.It is therefore critical to estimate layer-wise\ntraining quality in a manner that accounts for both model architecture and\ntraining data. However, existing approaches predominantly rely on model-centric\nheuristics (such as spectral statistics, outlier detection, or uniform\nallocation) while overlooking the influence of data. To address these\nlimitations, we propose LayerIF, a data-driven framework that leverages\nInfluence Functions to quantify the training quality of individual layers in a\nprincipled and task-sensitive manner. By isolating each layer's gradients and\nmeasuring the sensitivity of the validation loss to training examples by\ncomputing layer-wise influences, we derive data-driven estimates of layer\nimportance. Notably, our method produces task-specific layer importance\nestimates for the same LLM, revealing how layers specialize for different\ntest-time evaluation tasks. We demonstrate the utility of our scores by\nleveraging them for two downstream applications: (a) expert allocation in\nLoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM\npruning. Experiments across multiple LLM architectures demonstrate that our\nmodel-agnostic, influence-guided allocation leads to consistent gains in task\nperformance.", "AI": {"tldr": "This paper presents LayerIF, a data-driven framework that evaluates layer-wise training quality in pretrained Large Language Models (LLMs) using Influence Functions, enhancing task performance in downstream applications like expert allocation and layer-wise sparsity distribution.", "motivation": "To address the limitations of existing methods that ignore data influence on layer-wise training quality in pretrained LLMs, leading to suboptimal performance in downstream tasks.", "method": "LayerIF uses Influence Functions to quantify training quality per layer by analyzing gradients and validation loss sensitivity to training examples, producing task-specific importance estimates for each layer.", "result": "LayerIF provides a model-agnostic approach that leads to improved task performance in applications such as expert allocation in LoRA-MoE architectures and layer-wise sparsity distribution for LLM pruning.", "conclusion": "The paper demonstrates that using LayerIF yields consistent performance gains in downstream tasks by providing nuanced insights into layer specialization across different evaluations.", "key_contributions": ["Introduction of LayerIF framework for layer-wise training quality evaluation", "Task-specific layer importance estimates for pretrained LLMs", "Demonstrated practical applications in expert allocation and LLM pruning"], "limitations": "", "keywords": ["Large Language Models", "Layer-wise training quality", "Influence Functions", "task specialization", "LLM pruning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24255", "pdf": "https://arxiv.org/pdf/2505.24255.pdf", "abs": "https://arxiv.org/abs/2505.24255", "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM.", "AI": {"tldr": "This study explores the impact of theory-of-mind reasoning in Large Language Models (LLMs) on negotiation tasks using the ultimatum game.", "motivation": "To understand how theory-of-mind reasoning influences AI behavior in social interactions, particularly in negotiation scenarios.", "method": "The study involved initializing LLM agents with various prosocial beliefs and reasoning methods, conducting 2,700 simulations to analyze their decisions in negotiation tasks.", "result": "The results indicated that theory-of-mind reasoning improves behavior alignment with human norms and enhances decision-making consistency and negotiation success.", "conclusion": "The findings highlight the importance of incorporating theory-of-mind reasoning in AI to foster better human-AI interactions and facilitate cooperative decision-making.", "key_contributions": ["Investigated role of ToM reasoning in AI negotiations", "Demonstrated enhanced alignment of AI with human norms", "Provided empirical data from extensive simulations"], "limitations": "", "keywords": ["Large Language Models", "theory-of-mind", "human-AI interaction", "negotiation", "cooperative decision-making"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23812", "pdf": "https://arxiv.org/pdf/2505.23812.pdf", "abs": "https://arxiv.org/abs/2505.23812", "title": "Emotion-aware Dual Cross-Attentive Neural Network with Label Fusion for Stance Detection in Misinformative Social Media Content", "authors": ["Lata Pangtey", "Mohammad Zia Ur Rehman", "Prasad Chaudhari", "Shubhi Bansal", "Nagendra Kumar"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid evolution of social media has generated an overwhelming volume of\nuser-generated content, conveying implicit opinions and contributing to the\nspread of misinformation. The method aims to enhance the detection of stance\nwhere misinformation can polarize user opinions. Stance detection has emerged\nas a crucial approach to effectively analyze underlying biases in shared\ninformation and combating misinformation. This paper proposes a novel method\nfor \\textbf{S}tance \\textbf{P}rediction through a \\textbf{L}abel-fused dual\ncross-\\textbf{A}ttentive \\textbf{E}motion-aware neural \\textbf{Net}work\n(SPLAENet) in misinformative social media user-generated content. The proposed\nmethod employs a dual cross-attention mechanism and a hierarchical attention\nnetwork to capture inter and intra-relationships by focusing on the relevant\nparts of source text in the context of reply text and vice versa. We\nincorporate emotions to effectively distinguish between different stance\ncategories by leveraging the emotional alignment or divergence between the\ntexts. We also employ label fusion that uses distance-metric learning to align\nextracted features with stance labels, improving the method's ability to\naccurately distinguish between stances. Extensive experiments demonstrate the\nsignificant improvements achieved by SPLAENet over existing state-of-the-art\nmethods. SPLAENet demonstrates an average gain of 8.92\\% in accuracy and\n17.36\\% in F1-score on the RumourEval dataset. On the SemEval dataset, it\nachieves average gains of 7.02\\% in accuracy and 10.92\\% in F1-score. On the\nP-stance dataset, it demonstrates average gains of 10.03\\% in accuracy and\n11.18\\% in F1-score. These results validate the effectiveness of the proposed\nmethod for stance detection in the context of misinformative social media\ncontent.", "AI": {"tldr": "This paper presents SPLAENet, a novel method for stance prediction that leverages dual cross-attention and emotional awareness to improve stance detection in misinformative social media content.", "motivation": "To enhance stance detection in the context of misinformation on social media, which is crucial for analyzing biases in user-generated content.", "method": "The proposed method, SPLAENet, uses a dual cross-attention mechanism and a hierarchical attention network, along with emotional context and distance-metric learning for label fusion.", "result": "SPLAENet shows an average gain of 8.92% accuracy and 17.36% F1-score on the RumourEval dataset, 7.02% accuracy and 10.92% F1-score on the SemEval dataset, and 10.03% accuracy and 11.18% F1-score on the P-stance dataset compared to existing methods.", "conclusion": "The results confirm that SPLAENet is an effective tool for stance detection in misinformative social media content.", "key_contributions": ["Introduction of dual cross-attention for improved context understanding", "Integration of emotional awareness in stance detection", "Distance-metric learning for enhanced label fusion"], "limitations": "", "keywords": ["stance detection", "misinformation", "emotional awareness", "social media", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23815", "pdf": "https://arxiv.org/pdf/2505.23815.pdf", "abs": "https://arxiv.org/abs/2505.23815", "title": "Aligning LLMs by Predicting Preferences from User Writing Samples", "authors": ["Stéphane Aroca-Ouellette", "Natalie Mackraz", "Barry-John Theobald", "Katherine Metcalf"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025. 32 pages total: 9 main, 2 references, 21\n  appendix. arXiv admin note: substantial text overlap with arXiv:2410.06273", "summary": "Accommodating human preferences is essential for creating aligned LLM agents\nthat deliver personalized and effective interactions. Recent work has shown the\npotential for LLMs acting as writing agents to infer a description of user\npreferences. Agent alignment then comes from conditioning on the inferred\npreference description. However, existing methods often produce generic\npreference descriptions that fail to capture the unique and individualized\nnature of human preferences. This paper introduces PROSE, a method designed to\nenhance the precision of preference descriptions inferred from user writing\nsamples. PROSE incorporates two key elements: (1) iterative refinement of\ninferred preferences, and (2) verification of inferred preferences across\nmultiple user writing samples. We evaluate PROSE with several LLMs (i.e.,\nQwen2.5 7B and 72B Instruct, GPT-mini, and GPT-4o) on a summarization and an\nemail writing task. We find that PROSE more accurately infers nuanced human\npreferences, improving the quality of the writing agent's generations over\nCIPHER (a state-of-the-art method for inferring preferences) by 33\\%. Lastly,\nwe demonstrate that ICL and PROSE are complementary methods, and combining them\nprovides up to a 9\\% improvement over ICL alone.", "AI": {"tldr": "This paper presents PROSE, a method for improving the accuracy of inferred user preferences from writing samples for LLMs. By refining and verifying preferences, PROSE enhances the performance of writing agents compared to existing methods, demonstrating significant improvements in personalization for tasks such as summarization and email writing.", "motivation": "To address the shortcomings of existing methods that produce generic preference descriptions, thus failing to capture the unique nature of human preferences in LLM interactions.", "method": "PROSE enhances preference description inference through iterative refinement and cross-sample verification of user writing samples, tested on multiple LLMs for writing tasks.", "result": "PROSE improves the accuracy of inferred preferences and the quality of LLM-generated texts by 33% over the previous state-of-the-art method, CIPHER, and shows synergistic effects when combined with ICL.", "conclusion": "The findings suggest that tailored preference inference methods like PROSE can significantly enhance the personalization of LLM outputs, making interactions more effective.", "key_contributions": ["Introduction of PROSE for preference description inference", "Demonstrated 33% improvement over existing methods", "Combination with ICL for enhanced performance"], "limitations": "", "keywords": ["Preference Inference", "Large Language Models", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23816", "pdf": "https://arxiv.org/pdf/2505.23816.pdf", "abs": "https://arxiv.org/abs/2505.23816", "title": "A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs", "authors": ["Trenton Chang", "Tobias Schnabel", "Adith Swaminathan", "Jenna Wiens"], "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 8 figures. 26 pages of references and supplementary\n  material, 20 additional figures", "summary": "Despite advances in large language models (LLMs) on reasoning and\ninstruction-following benchmarks, it remains unclear whether they can reliably\nproduce outputs aligned with a broad variety of user goals, a concept we refer\nto as steerability. The abundance of methods proposed to modify LLM behavior\nmakes it unclear whether current LLMs are already steerable, or require further\nintervention. In particular, LLMs may exhibit (i) poor coverage, where rare\nuser goals are underrepresented; (ii) miscalibration, where models overshoot\nrequests; and (iii) side effects, where changes to one dimension of text\ninadvertently affect others. To systematically evaluate these failures, we\nintroduce a framework based on a multi-dimensional goal space that models user\ngoals and LLM outputs as vectors with dimensions corresponding to text\nattributes (e.g., reading difficulty). Applied to a text-rewriting task, we\nfind that current LLMs struggle with steerability, as side effects are\npersistent. Interventions to improve steerability, such as prompt engineering,\nbest-of-$N$ sampling, and reinforcement learning fine-tuning, have varying\neffectiveness, yet side effects remain problematic. Our findings suggest that\neven strong LLMs struggle with steerability, and existing alignment strategies\nmay be insufficient. We open-source our steerability evaluation framework at\nhttps://github.com/MLD3/steerability.", "AI": {"tldr": "This paper evaluates the challenges of steerability in LLMs, revealing persistent issues with side effects and effectiveness of interventions.", "motivation": "To assess whether current large language models (LLMs) can reliably align with diverse user goals, highlighting issues of steerability.", "method": "A framework based on a multi-dimensional goal space is introduced to systematically evaluate LLM outputs against user goals, particularly in a text-rewriting task.", "result": "The study finds that LLMs have persistent issues with steerability, notably side effects, and varying effectiveness of interventions such as prompt engineering and reinforcement learning fine-tuning.", "conclusion": "Current LLMs struggle with steerability, and existing alignment strategies may not adequately address the identified problems.", "key_contributions": ["Introduction of a framework for evaluating LLM steerability", "Identification of persistent side effects in LLM outputs", "Assessment of various interventions to improve steerability and their effectiveness."], "limitations": "Focused on text-rewriting tasks; may not generalize to all text generation contexts.", "keywords": ["large language models", "steerability", "text rewriting", "prompt engineering", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23818", "pdf": "https://arxiv.org/pdf/2505.23818.pdf", "abs": "https://arxiv.org/abs/2505.23818", "title": "Ratas framework: A comprehensive genai-based approach to rubric-based marking of real-world textual exams", "authors": ["Masoud Safilian", "Amin Beheshti", "Stephen Elbourn"], "categories": ["cs.CL"], "comment": null, "summary": "Automated answer grading is a critical challenge in educational technology,\nwith the potential to streamline assessment processes, ensure grading\nconsistency, and provide timely feedback to students. However, existing\napproaches are often constrained to specific exam formats, lack\ninterpretability in score assignment, and struggle with real-world\napplicability across diverse subjects and assessment types. To address these\nlimitations, we introduce RATAS (Rubric Automated Tree-based Answer Scoring), a\nnovel framework that leverages state-of-the-art generative AI models for\nrubric-based grading of textual responses. RATAS is designed to support a wide\nrange of grading rubrics, enable subject-agnostic evaluation, and generate\nstructured, explainable rationales for assigned scores. We formalize the\nautomatic grading task through a mathematical framework tailored to\nrubric-based assessment and present an architecture capable of handling\ncomplex, real-world exam structures. To rigorously evaluate our approach, we\nconstruct a unique, contextualized dataset derived from real-world\nproject-based courses, encompassing diverse response formats and varying levels\nof complexity. Empirical results demonstrate that RATAS achieves high\nreliability and accuracy in automated grading while providing interpretable\nfeedback that enhances transparency for both students and nstructors.", "AI": {"tldr": "RATAS is a framework for rubric-based automated grading using generative AI, designed to be subject-agnostic and provide interpretable feedback.", "motivation": "To improve automated grading processes that often lack interpretability and applicability across various subjects and assessment types.", "method": "RATAS employs a mathematical framework and architecture designed for rubric-based assessment, utilizing a contextualized dataset derived from real-world project-based courses to evaluate its effectiveness.", "result": "RATAS demonstrated high reliability and accuracy in automated grading, alongside providing interpretable feedback, enhancing transparency for students and instructors.", "conclusion": "The framework addresses existing limitations in automated grading systems and offers a robust solution for grading textual responses across diverse contexts.", "key_contributions": ["Introduction of RATAS, a novel rubric-based grading framework using generative AI.", "Support for a wide range of grading rubrics and diverse response formats.", "Provision of structured and explainable rationales for assigned scores."], "limitations": "The performance may vary depending on the quality of the grading rubrics and the complexity of the responses.", "keywords": ["automated grading", "generative AI", "rubric-based assessment", "educational technology", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803.pdf", "abs": "https://arxiv.org/abs/2505.24803", "title": "Guiding Generative Storytelling with Knowledge Graphs", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.", "AI": {"tldr": "This paper explores how knowledge graphs (KGs) can improve long-form storytelling generated by large language models (LLMs) by enhancing narrative quality and user control.", "motivation": "Despite the advances of LLMs in automated storytelling, maintaining coherence and user control during long-form narrative generation remains a challenge. The paper seeks to leverage knowledge graphs to address these issues in storytelling contexts.", "method": "We propose a KG-assisted storytelling pipeline and evaluate its effectiveness through a user study involving 15 participants who created story prompts, generated stories, and edited knowledge graphs.", "result": "The study's findings indicate that using knowledge graphs significantly improves the quality of stories, particularly in structured narratives, and enhances users' sense of control while making storytelling more engaging and interactive.", "conclusion": "Knowledge graphs provide an effective means of enhancing narrative generation with LLMs, enabling improved story coherence and user-driven customization.", "key_contributions": ["Development of a KG-assisted storytelling pipeline", "Demonstration of improved narrative quality using KGs", "Enhanced user control and engagement in storytelling through KG editing"], "limitations": "", "keywords": ["Large Language Models", "Knowledge Graphs", "Storytelling", "User Control", "Narrative Quality"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23820", "pdf": "https://arxiv.org/pdf/2505.23820.pdf", "abs": "https://arxiv.org/abs/2505.23820", "title": "Arbiters of Ambivalence: Challenges of Using LLMs in No-Consensus Tasks", "authors": ["Bhaktipriya Radharapu", "Manon Revel", "Megan Ung", "Sebastian Ruder", "Adina Williams"], "categories": ["cs.CL"], "comment": null, "summary": "The increasing use of LLMs as substitutes for humans in ``aligning'' LLMs has\nraised questions about their ability to replicate human judgments and\npreferences, especially in ambivalent scenarios where humans disagree. This\nstudy examines the biases and limitations of LLMs in three roles: answer\ngenerator, judge, and debater. These roles loosely correspond to previously\ndescribed alignment frameworks: preference alignment (judge) and scalable\noversight (debater), with the answer generator reflecting the typical setting\nwith user interactions. We develop a ``no-consensus'' benchmark by curating\nexamples that encompass a variety of a priori ambivalent scenarios, each\npresenting two possible stances. Our results show that while LLMs can provide\nnuanced assessments when generating open-ended answers, they tend to take a\nstance on no-consensus topics when employed as judges or debaters. These\nfindings underscore the necessity for more sophisticated methods for aligning\nLLMs without human oversight, highlighting that LLMs cannot fully capture human\ndisagreement even on topics where humans themselves are divided.", "AI": {"tldr": "This study examines the limitations of Large Language Models (LLMs) in replicating human judgments and preferences in ambiguous situations. It identifies biases when LLMs act as answer generators, judges, and debaters, suggesting the need for better alignment methods without human oversight.", "motivation": "To investigate the ability of LLMs to replicate human judgments in ambivalent scenarios, given the increasing reliance on LLMs for alignment tasks.", "method": "The study develops a 'no-consensus' benchmark with curated examples of ambivalent scenarios, testing LLM performance in roles such as answer generator, judge, and debater.", "result": "LLMs provide nuanced assessments in open-ended generation but tend to assert a stance on ambivalent topics when acting as judges or debaters, revealing their inherent biases.", "conclusion": "LLMs cannot fully capture human disagreement, emphasizing the need for improved alignment methods without human oversight in ambiguous scenarios.", "key_contributions": ["Identification of biases in LLMs when acting in different roles (answer generator, judge, debater)", "Development of a 'no-consensus' benchmark for evaluating LLM performance in ambiguity", "Insights into the limitations of LLMs in replicating human preferences and judgments."], "limitations": "The study is limited to the performance of LLMs in specific ambivalent scenarios, which may not generalize to all contexts.", "keywords": ["Large Language Models", "bias", "alignment", "human judgment", "ambivalent scenarios"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.08906", "pdf": "https://arxiv.org/pdf/2405.08906.pdf", "abs": "https://arxiv.org/abs/2405.08906", "title": "Functional Near-Infrared Spectroscopy (fNIRS) Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming", "authors": ["Shayla Sharmin", "Elham Bakhshipour", "Behdokht Kiafar", "Md Fahim Abrar", "Pinar Kullu", "Nancy Getchell", "Roghayeh Leila Barmaki"], "categories": ["cs.HC"], "comment": null, "summary": "Educational games enhance learning experiences by integrating touchscreens,\nmaking interactions more engaging and intuitive for learners. However, the\ncognitive impacts of educational game-play input modalities, such as the hand\nand stylus technique, are unclear. We compared the experience of using hands\nvs. a stylus for touchscreens while playing an educational game by analyzing\noxygenated hemoglobin collected by functional Near-Infrared Spectroscopy and\nself-reported measures. In addition, we measured the hand vs. the stylus\nmodalities of the task and calculated the relative neural efficiency and\nrelative neural involvement using the mental demand and the quiz score. Our\nfindings show that the hand condition had a significantly lower neural\ninvolvement, yet higher neural efficiency than the stylus condition. This\nresult suggests the requirement of less cognitive effort while using the hand.\nAdditionally, the self-reported measures show significant differences, and the\nresults suggest that hand-based input is more intuitive, less cognitively\ndemanding, and less frustrating. Conversely, the use of a stylus required\nhigher cognitive effort due to the cognitive balance of controlling the pen and\nanswering questions. These findings highlight the importance of designing\neducational games that allow learners to engage with the system while\nminimizing cognitive effort.", "AI": {"tldr": "This paper investigates the cognitive impacts of using hands vs. a stylus for touchscreens in educational games, revealing that hand use is more efficient and intuitive.", "motivation": "To understand the cognitive impacts of different input modalities (hands vs. stylus) on educational game play and learning experiences.", "method": "The study compared the use of hands and stylus for touchscreen interactions in an educational game, measuring oxygenated hemoglobin via functional Near-Infrared Spectroscopy and self-reported measures of user experience.", "result": "The hand condition exhibited significantly lower neural involvement and higher neural efficiency compared to the stylus condition, indicating less cognitive effort required when using hands for interaction.", "conclusion": "Hand-based input methods are suggested to be more intuitive and less cognitively demanding, highlighting the need for game design that reduces cognitive effort for learners.", "key_contributions": ["Demonstrated the cognitive differences between hand and stylus interactions in educational games.", "Provided evidence that hand interactions are more efficient and intuitive compared to stylus use.", "Underlined the importance of designing educational games that minimize cognitive load."], "limitations": "", "keywords": ["educational games", "touchscreen input", "cognitive effort", "neural efficiency", "HCI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23822", "pdf": "https://arxiv.org/pdf/2505.23822.pdf", "abs": "https://arxiv.org/abs/2505.23822", "title": "Speech as a Multimodal Digital Phenotype for Multi-Task LLM-based Mental Health Prediction", "authors": ["Mai Ali", "Christopher Lucasius", "Tanmay P. Patel", "Madison Aitken", "Jacob Vorstman", "Peter Szatmari", "Marco Battaglia", "Deepa Kundur"], "categories": ["cs.CL", "cs.MM"], "comment": "6 pages, 1 figure, 3 tables. Submitted to ICSM 2025. The\n  corresponding author is Mai Ali (maia.ali@mail.utoronto.ca). Christopher\n  Lucasius and Tanmay P. Patel contributed equally", "summary": "Speech is a noninvasive digital phenotype that can offer valuable insights\ninto mental health conditions, but it is often treated as a single modality. In\ncontrast, we propose the treatment of patient speech data as a trimodal\nmultimedia data source for depression detection. This study explores the\npotential of large language model-based architectures for speech-based\ndepression prediction in a multimodal regime that integrates speech-derived\ntext, acoustic landmarks, and vocal biomarkers. Adolescent depression presents\na significant challenge and is often comorbid with multiple disorders, such as\nsuicidal ideation and sleep disturbances. This presents an additional\nopportunity to integrate multi-task learning (MTL) into our study by\nsimultaneously predicting depression, suicidal ideation, and sleep disturbances\nusing the multimodal formulation. We also propose a longitudinal analysis\nstrategy that models temporal changes across multiple clinical interactions,\nallowing for a comprehensive understanding of the conditions' progression. Our\nproposed approach, featuring trimodal, longitudinal MTL is evaluated on the\nDepression Early Warning dataset. It achieves a balanced accuracy of 70.8%,\nwhich is higher than each of the unimodal, single-task, and non-longitudinal\nmethods.", "AI": {"tldr": "This study proposes using a trimodal multimedia approach for depression detection through speech data, integrating text, acoustic features, and vocal biomarkers with multi-task learning for improved prediction.", "motivation": "To enhance depression detection using speech data by treating it as a trimodal multimedia resource, thereby improving prediction accuracy in adolescents.", "method": "The approach integrates speech-derived text, acoustic landmarks, and vocal biomarkers, using multi-task learning to predict depression, suicidal ideation, and sleep disturbances simultaneously, with a longitudinal analysis of temporal changes.", "result": "Achieved a balanced accuracy of 70.8% on the Depression Early Warning dataset, outperforming unimodal and single-task methods.", "conclusion": "The trimodal, longitudinal multi-task learning approach shows promise for better understanding and predicting depression-related conditions over time.", "key_contributions": ["Introduction of a trimodal multimedia approach for depression detection.", "Application of multi-task learning to simultaneously predict related mental health challenges.", "Longitudinal analysis of speech data to track changes over time."], "limitations": "", "keywords": ["Mental Health", "Depression Detection", "Multi-task Learning", "Speech Data", "Longitudinal Analysis"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2503.00303", "pdf": "https://arxiv.org/pdf/2503.00303.pdf", "abs": "https://arxiv.org/abs/2503.00303", "title": "Leveraging Complementary AI Explanations to Mitigate Misunderstanding in XAI", "authors": ["Yueqing Xuan", "Kacper Sokol", "Mark Sanderson", "Jeffrey Chan"], "categories": ["cs.HC"], "comment": "Accepted to IEEE Swiss Conference on Data Science (SDS) 2025", "summary": "Artificial intelligence explanations can make complex predictive models more\ncomprehensible. To be effective, however, they should anticipate and mitigate\npossible misinterpretations, e.g., arising when users infer incorrect\ninformation that is not explicitly conveyed. To this end, we propose\ncomplementary explanations -- a novel method that pairs explanations to\ncompensate for their respective limitations. A complementary explanation adds\ninsights that clarify potential misconceptions stemming from the primary\nexplanation while ensuring their coherency and avoiding redundancy. We\nintroduce a framework for designing and evaluating complementary explanation\npairs based on pertinent qualitative properties and quantitative metrics. Our\napproach allows to construct complementary explanations that minimise the\nchance of their misinterpretation.", "AI": {"tldr": "This paper proposes a method for creating complementary explanations in AI to enhance understanding and reduce misinterpretations of predictive models.", "motivation": "To improve the comprehensibility of AI explanations and mitigate misinterpretations that arise from predictive models.", "method": "The authors introduce a framework for designing and evaluating complementary explanation pairs that clarify potential misconceptions while maintaining coherence.", "result": "The proposed method allows for the construction of complementary explanations that effectively minimize the chances of misinterpretation.", "conclusion": "Complementary explanations can significantly enhance user understanding of AI predictions and reduce incorrect inferences.", "key_contributions": ["Introduces complementary explanations to address limitations of primary AI explanations.", "Develops a framework for designing and evaluating explanation pairs.", "Provides both qualitative properties and quantitative metrics for assessment."], "limitations": "The framework's effectiveness may vary across different domains and types of predictive models.", "keywords": ["AI explanations", "complementary explanations", "predictive models", "misinterpretations", "framework evaluation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.23823", "pdf": "https://arxiv.org/pdf/2505.23823.pdf", "abs": "https://arxiv.org/abs/2505.23823", "title": "RAGPPI: RAG Benchmark for Protein-Protein Interactions in Drug Discovery", "authors": ["Youngseung Jeon", "Ziwen Li", "Thomas Li", "JiaSyuan Chang", "Morteza Ziyadi", "Xiang 'Anthony' Chen"], "categories": ["cs.CL"], "comment": "17 pages, 4 figures, 8 tables", "summary": "Retrieving the biological impacts of protein-protein interactions (PPIs) is\nessential for target identification (Target ID) in drug development. Given the\nvast number of proteins involved, this process remains time-consuming and\nchallenging. Large Language Models (LLMs) and Retrieval-Augmented Generation\n(RAG) frameworks have supported Target ID; however, no benchmark currently\nexists for identifying the biological impacts of PPIs. To bridge this gap, we\nintroduce the RAG Benchmark for PPIs (RAGPPI), a factual question-answer\nbenchmark of 4,420 question-answer pairs that focus on the potential biological\nimpacts of PPIs. Through interviews with experts, we identified criteria for a\nbenchmark dataset, such as a type of QA and source. We built a gold-standard\ndataset (500 QA pairs) through expert-driven data annotation. We developed an\nensemble auto-evaluation LLM that reflected expert labeling characteristics,\nwhich facilitates the construction of a silver-standard dataset (3,720 QA\npairs). We are committed to maintaining RAGPPI as a resource to support the\nresearch community in advancing RAG systems for drug discovery QA solutions.", "AI": {"tldr": "The paper introduces the RAG Benchmark for protein-protein interactions (PPIs) to aid in drug development by providing a factual question-answer benchmark.", "motivation": "Identifying the biological impacts of protein-protein interactions is crucial for target identification in drug development, but the lack of a benchmark makes this process difficult.", "method": "The authors developed the RAGPPI, a benchmark dataset consisting of 4,420 question-answer pairs focusing on biological impacts of PPIs, validated through expert interviews and data annotation.", "result": "A gold-standard dataset of 500 QA pairs and a silver-standard dataset of 3,720 QA pairs were created, alongside an ensemble auto-evaluation LLM to reflect expert labeling characteristics.", "conclusion": "RAGPPI will serve as a vital resource to support advancements in retrieval-augmented generation systems for drug discovery and related QA solutions.", "key_contributions": ["Introduction of RAGPPI benchmark for PPIs", "Creation of gold-standard and silver-standard datasets", "Development of ensemble auto-evaluation LLM for QA evaluation"], "limitations": "", "keywords": ["Protein-Protein Interactions", "Drug Discovery", "Large Language Models", "Retrieval-Augmented Generation", "Benchmark Datasets"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2504.13883", "pdf": "https://arxiv.org/pdf/2504.13883.pdf", "abs": "https://arxiv.org/abs/2504.13883", "title": "Hybrid Deep Learning Model to Estimate Cognitive Effort from fNIRS Signals in Educational Game Playing", "authors": ["Shayla Sharmin", "Roghayeh Leila Barmaki"], "categories": ["cs.HC", "cs.LG"], "comment": null, "summary": "This study estimates cognitive effort (CE) based on functional near-infrared\nspectroscopy (fNIRS) data and performance scores using a hybrid deep learning\nmodel. The estimation of CE enables educators to modify material to enhance\nlearning effectiveness and student engagement. Relative neural efficiency (RNE)\nand relative neural involvement (RNI) are two metrics that have been used to\nrepresent CE. To estimate RNE and RNI we need hemodynamic response in the brain\nand the performance score of a task.We collected oxygenated hemoglobin ($\\Delta\n\\mathrm{HbO}$). Sixteen participants answered 16 questions in a unity-based\neducational game, each with a 30-second response time. We used deep learning\nmodels to predict the performance score and estimate RNE and RNI to understand\nCE. The study compares traditional machine learning techniques with deep\nlearning models such as CNN, LSTM, BiLSTM, and a hybrid CNN-GRU to determine\nwhich approach provides better accuracy in predicting performance scores. The\nresult shows that the hybrid CNN-GRU gives better performance with 78.36\\%\ntraining accuracy and 73.08\\% test accuracy than other models. We performed\nXGBoost on the extracted GRU feature and got the highest accuracy (69.23\\%).\nThis suggests that the features learned from this hybrid model generalize\nbetter even in traditional machine learning algorithms. We used the $\\Delta\n\\mathrm{HbO}$ and predicted score to calculate RNE and RNI to observe cognitive\neffort in our four test cases. Our result shows that even with moderate\naccuracy, the predicted RNE and RNI closely follows the actual trends. we also\nobserved that when participants were in a state of high CE, introducing rest\nled decrease of CE. These findings can be helpful to design and improve\nlearning environments and provide valuable insights in learning materials.", "AI": {"tldr": "This study uses a hybrid deep learning model to estimate cognitive effort based on fNIRS data, aiming to enhance learning effectiveness and engagement.", "motivation": "The goal is to estimate cognitive effort to enable educators to modify learning material, enhancing engagement and effectiveness.", "method": "A hybrid deep learning model (CNN-GRU) was used alongside traditional machine learning techniques to predict performance scores and estimate cognitive metrics (RNE and RNI) from collected fNIRS data.", "result": "The hybrid CNN-GRU model achieved the highest accuracy with 78.36% training and 73.08% test accuracy, outperforming traditional models and showing that predicted RNE and RNI align with actual trends.", "conclusion": "Findings indicate that cognitive effort can be effectively estimated using deep learning, with implications for improving learning environments.", "key_contributions": ["Demonstrated the efficacy of hybrid deep learning models in estimating cognitive effort from fNIRS data.", "Provided insights into the relationship between cognitive effort and learning material adaptation.", "Showed potential for traditional ML models to leverage features from deep learning for improved accuracy."], "limitations": "Accuracy still moderate and requires further validation across diverse learning contexts.", "keywords": ["cognitive effort", "functional near-infrared spectroscopy", "deep learning", "educational technology", "learning effectiveness"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.23824", "pdf": "https://arxiv.org/pdf/2505.23824.pdf", "abs": "https://arxiv.org/abs/2505.23824", "title": "Reviewing Scientific Papers for Critical Problems With Reasoning LLMs: Baseline Approaches and Automatic Evaluation", "authors": ["Tianmai M. Zhang", "Neil F. Abernethy"], "categories": ["cs.CL"], "comment": "Work in progress. Conclusions may be updated", "summary": "Recent advancements in large language models have sparked interest in\nutilizing them to assist the peer review process of scientific publication.\nInstead of having AI models generate reviews in the same way as human\nreviewers, we propose adopting them as manuscript quality checkers. We\nintroduce several baseline approaches and an extendable automatic evaluation\nframework using top LLMs as judges to tackle the difficulty of recruiting\ndomain experts for manual evaluation. Utilizing papers withdrawn from arXiv, we\nvalidated our proposed methods with several leading reasoning LLMs from\ndifferent providers and assessed their performance and API costs for\nidentifying critical errors and unsoundness problems. The OpenAI o3 model\nperformed the best, while o4-mini was the most cost-effective one in our\nevaluation. This paper provides insights into document-based scientific\nunderstanding/reasoning and lays the foundation for future applications.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) as manuscript quality checkers in the peer review process of scientific publications, contrasting AI-generated reviews with automated quality assessment methods.", "motivation": "To address the challenges of recruiting domain experts for manual evaluation in the peer review process of scientific publications by leveraging LLMs.", "method": "The authors propose baseline approaches and an evaluation framework that utilizes leading reasoning LLMs as judges, employing withdrawn arXiv papers to validate the performance and cost of these models in identifying critical errors.", "result": "The evaluation revealed that the OpenAI o3 model was the most effective in assessing manuscript quality, while the o4-mini model was noted for its cost-effectiveness in this context.", "conclusion": "The work offers insights into document-based scientific reasoning and sets a foundation for future applications of LLMs in the peer review process.", "key_contributions": ["Introduced an extendable automatic evaluation framework using LLMs as judges.", "Proposed several baseline approaches for LLMs in the context of manuscript quality assessment.", "Evaluated the performance and cost of multiple LLMs in identifying critical errors."], "limitations": "The paper is a work in progress, and conclusions may be updated.", "keywords": ["large language models", "peer review", "scientific publication", "manuscript quality", "automated evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23827", "pdf": "https://arxiv.org/pdf/2505.23827.pdf", "abs": "https://arxiv.org/abs/2505.23827", "title": "ValueSim: Generating Backstories to Model Individual Value Systems", "authors": ["Bangde Du", "Ziyi Ye", "Zhijing Wu", "Jankowska Monika", "Shuqi Zhu", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "categories": ["cs.CL"], "comment": "8 pages main paper + 13 pages appendix, 3 figures, 2 tables", "summary": "As Large Language Models (LLMs) continue to exhibit increasingly human-like\ncapabilities, aligning them with human values has become critically important.\nContemporary advanced techniques, such as prompt learning and reinforcement\nlearning, are being deployed to better align LLMs with human values. However,\nwhile these approaches address broad ethical considerations and helpfulness,\nthey rarely focus on simulating individualized human value systems. To address\nthis gap, we present ValueSim, a framework that simulates individual values\nthrough the generation of personal backstories reflecting past experiences and\ndemographic information. ValueSim converts structured individual data into\nnarrative backstories and employs a multi-module architecture inspired by the\nCognitive-Affective Personality System to simulate individual values based on\nthese narratives. Testing ValueSim on a self-constructed benchmark derived from\nthe World Values Survey demonstrates an improvement in top-1 accuracy by over\n10% compared to retrieval-augmented generation methods. Further analysis\nreveals that performance enhances as additional user interaction history\nbecomes available, indicating the model's ability to refine its persona\nsimulation capabilities over time.", "AI": {"tldr": "The paper presents ValueSim, a framework designed to simulate individualized human values by generating personal backstories that reflect users' experiences and demographics, improving alignment of LLMs with personal values.", "motivation": "The need for alignment of Large Language Models (LLMs) with individualized human value systems, rather than broad ethical considerations.", "method": "ValueSim converts structured individual data into narrative backstories using a multi-module architecture inspired by the Cognitive-Affective Personality System to simulate individual values.", "result": "ValueSim improved top-1 accuracy by over 10% on a benchmark derived from the World Values Survey compared to retrieval-augmented generation methods.", "conclusion": "The model can refine its persona simulation capabilities over time as more user interaction history becomes available, enhancing its performance further.", "key_contributions": ["Introduction of ValueSim framework for simulating individual human values", "Demonstration of improved accuracy over existing methods", "Showing adaptability and refinement capabilities of the model based on user interactions"], "limitations": "", "keywords": ["Large Language Models", "human values", "ValueSim", "Cognitive-Affective Personality System", "individualized simulation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.22767", "pdf": "https://arxiv.org/pdf/2505.22767.pdf", "abs": "https://arxiv.org/abs/2505.22767", "title": "In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge", "authors": ["Eleni Vasilaki"], "categories": ["cs.HC", "cs.AI"], "comment": "6 pages, 1 table", "summary": "Large Language Models (LLMs) are typically analysed through architectural,\nbehavioural, or training-data lenses. This article offers a theoretical and\nexperiential re-framing: LLMs as dynamic instantiations of Collective human\nKnowledge (CK), where intelligence is evoked through dialogue rather than\nstored statically. Drawing on concepts from neuroscience and AI, and grounded\nin sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,\nthe implications of fine-tuning, and the notion of co-augmentation: mutual\nenhancement between human and machine cognition. This perspective offers a new\nlens for understanding interaction, representation, and agency in contemporary\nAI systems.", "AI": {"tldr": "This paper reframes Large Language Models (LLMs) as dynamic representations of Collective human Knowledge, emphasizing dialogue and co-augmentation between human and machine intelligence.", "motivation": "To provide a new perspective on understanding LLMs that goes beyond traditional analyses of architecture and training data.", "method": "The paper draws on concepts from neuroscience and AI, analyzing emergent dialogue patterns during sustained interactions with ChatGPT-4.", "result": "The study explores the implications of fine-tuning LLMs and highlights the mutual enhancement of human and machine cognition.", "conclusion": "This new lens sheds light on the interaction, representation, and agency in modern AI systems.", "key_contributions": ["Reframing LLMs as dynamic embodiments of collective knowledge", "Analysis of dialogue patterns and co-augmentation in human-machine interaction", "Exploration of fine-tuning implications on LLM performance"], "limitations": "", "keywords": ["Large Language Models", "Collective Knowledge", "Human-Machine Interaction", "Co-Augmentation", "Neuroscience and AI"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2505.23829", "pdf": "https://arxiv.org/pdf/2505.23829.pdf", "abs": "https://arxiv.org/abs/2505.23829", "title": "BiasFilter: An Inference-Time Debiasing Framework for Large Language Models", "authors": ["Xiaoqing Cheng", "Ruizhe Chen", "Hongying Zan", "Yuxiang Jia", "Min Peng"], "categories": ["cs.CL"], "comment": null, "summary": "Mitigating social bias in large language models (LLMs) has become an\nincreasingly important research objective. However, existing debiasing methods\noften incur high human and computational costs, exhibit limited effectiveness,\nand struggle to scale to larger models and open-ended generation tasks. To\naddress these limitations, this paper proposes BiasFilter, a model-agnostic,\ninference-time debiasing framework that integrates seamlessly with both\nopen-source and API-based LLMs. Instead of relying on retraining with balanced\ndata or modifying model parameters, BiasFilter enforces fairness by filtering\ngeneration outputs in real time. Specifically, it periodically evaluates\nintermediate outputs every few tokens, maintains an active set of candidate\ncontinuations, and incrementally completes generation by discarding low-reward\nsegments based on a fairness reward signal. To support this process, we\nconstruct a fairness preference dataset and train an implicit reward model to\nassess token-level fairness in generated responses. Extensive experiments\ndemonstrate that BiasFilter effectively mitigates social bias across a range of\nLLMs while preserving overall generation quality.", "AI": {"tldr": "BiasFilter is a model-agnostic framework for real-time debiasing of LLM outputs without retraining, effectively mitigating social bias while maintaining quality.", "motivation": "The need to mitigate social bias in large language models while addressing the limitations of existing debiasing methods, which are often costly and ineffective.", "method": "BiasFilter filters generation outputs in real-time by evaluating intermediate outputs, maintaining an active set of candidate continuations, and discarding low-reward segments using a fairness reward signal.", "result": "BiasFilter effectively reduces social bias in various LLMs and maintains high generation quality through its innovative filtering approach.", "conclusion": "The proposed BiasFilter framework offers an efficient solution for real-time debiasing in LLMs without modifying model parameters or retraining, demonstrating significant improvements in fairness.", "key_contributions": ["Introduces BiasFilter, a model-agnostic framework for real-time debiasing of LLMs.", "Adopts a novel approach of filtering outputs based on a fairness reward signal.", "Constructs a fairness preference dataset and implicit reward model for assessing token-level fairness."], "limitations": "", "keywords": ["social bias", "large language models", "debiasing", "fairness", "real-time filtering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23830", "pdf": "https://arxiv.org/pdf/2505.23830.pdf", "abs": "https://arxiv.org/abs/2505.23830", "title": "EvoMoE: Expert Evolution in Mixture of Experts for Multimodal Large Language Models", "authors": ["Linglin Jing", "Yuting Gao", "Zhigang Wang", "Wang Lan", "Yiwen Tang", "Wenhai Wang", "Kaipeng Zhang", "Qingpei Guo"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements have shown that the Mixture of Experts (MoE) approach\nsignificantly enhances the capacity of large language models (LLMs) and\nimproves performance on downstream tasks. Building on these promising results,\nmulti-modal large language models (MLLMs) have increasingly adopted MoE\ntechniques. However, existing multi-modal MoE tuning methods typically face two\nkey challenges: expert uniformity and router rigidity. Expert uniformity occurs\nbecause MoE experts are often initialized by simply replicating the FFN\nparameters from LLMs, leading to homogenized expert functions and weakening the\nintended diversification of the MoE architecture. Meanwhile, router rigidity\nstems from the prevalent use of static linear routers for expert selection,\nwhich fail to distinguish between visual and textual tokens, resulting in\nsimilar expert distributions for image and text. To address these limitations,\nwe propose EvoMoE, an innovative MoE tuning framework. EvoMoE introduces a\nmeticulously designed expert initialization strategy that progressively evolves\nmultiple robust experts from a single trainable expert, a process termed expert\nevolution that specifically targets severe expert homogenization. Furthermore,\nwe introduce the Dynamic Token-aware Router (DTR), a novel routing mechanism\nthat allocates input tokens to appropriate experts based on their modality and\nintrinsic token values. This dynamic routing is facilitated by hypernetworks,\nwhich dynamically generate routing weights tailored for each individual token.\nExtensive experiments demonstrate that EvoMoE significantly outperforms other\nsparse MLLMs across a variety of multi-modal benchmarks, including MME,\nMMBench, TextVQA, and POPE. Our results highlight the effectiveness of EvoMoE\nin enhancing the performance of MLLMs by addressing the critical issues of\nexpert uniformity and router rigidity.", "AI": {"tldr": "EvoMoE is a novel tuning framework that enhances multi-modal large language models (MLLMs) by addressing expert uniformity and router rigidity through expert evolution and a dynamic routing mechanism.", "motivation": "The paper aims to improve the performance of multi-modal large language models (MLLMs) that face challenges with expert uniformity and router rigidity in the Mixture of Experts (MoE) framework.", "method": "The authors propose EvoMoE, which includes an expert initialization strategy that evolves multiple experts from a single trainable expert, and a Dynamic Token-aware Router (DTR) that allocates tokens to experts based on modality.", "result": "EvoMoE demonstrates significant performance improvements over existing sparse MLLMs on multiple multi-modal benchmarks, such as MME, MMBench, TextVQA, and POPE.", "conclusion": "The proposed EvoMoE framework effectively enhances MLLM performance by overcoming expert homogenization and inflexible routing mechanisms.", "key_contributions": ["Introduction of a new expert initialization strategy for diverse expert functions.", "Development of a Dynamic Token-aware Router for improved routing across modalities.", "Demonstrated superiority of EvoMoE in various multi-modal benchmarks."], "limitations": "", "keywords": ["Mixture of Experts", "Multi-modal models", "Machine Learning", "Dynamic Routing", "Expert Evolution"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23831", "pdf": "https://arxiv.org/pdf/2505.23831.pdf", "abs": "https://arxiv.org/abs/2505.23831", "title": "ICH-Qwen: A Large Language Model Towards Chinese Intangible Cultural Heritage", "authors": ["Wenhao Ye", "Tiansheng Zheng", "Yue Qi", "Wenhua Zhao", "Xiyu Wang", "Xue Zhao", "Jiacheng He", "Yaya Zheng", "Dongbo Wang"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures", "summary": "The intangible cultural heritage (ICH) of China, a cultural asset transmitted\nacross generations by various ethnic groups, serves as a significant testament\nto the evolution of human civilization and holds irreplaceable value for the\npreservation of historical lineage and the enhancement of cultural\nself-confidence. However, the rapid pace of modernization poses formidable\nchallenges to ICH, including threats damage, disappearance and discontinuity of\ninheritance. China has the highest number of items on the UNESCO Intangible\nCultural Heritage List, which is indicative of the nation's abundant cultural\nresources and emphasises the pressing need for ICH preservation. In recent\nyears, the rapid advancements in large language modelling have provided a novel\ntechnological approach for the preservation and dissemination of ICH. This\nstudy utilises a substantial corpus of open-source Chinese ICH data to develop\na large language model, ICH-Qwen, for the ICH domain. The model employs natural\nlanguage understanding and knowledge reasoning capabilities of large language\nmodels, augmented with synthetic data and fine-tuning techniques. The\nexperimental results demonstrate the efficacy of ICH-Qwen in executing tasks\nspecific to the ICH domain. It is anticipated that the model will provide\nintelligent solutions for the protection, inheritance and dissemination of\nintangible cultural heritage, as well as new theoretical and practical\nreferences for the sustainable development of intangible cultural heritage.\nFurthermore, it is expected that the study will open up new paths for digital\nhumanities research.", "AI": {"tldr": "This study develops ICH-Qwen, a large language model aimed at preserving and disseminating China's intangible cultural heritage (ICH) using recent advancements in natural language processing.", "motivation": "The urgent need for preserving the intangible cultural heritage of China amidst modernization challenges, leveraging technology for protection and dissemination.", "method": "Development of ICH-Qwen using a large corpus of open-source Chinese ICH data, integrating natural language understanding and knowledge reasoning with synthetic data and fine-tuning techniques.", "result": "ICH-Qwen has demonstrated efficacy in performing ICH domain-specific tasks, suggesting its potential as an intelligent solution for ICH protection and dissemination.", "conclusion": "The study posits that ICH-Qwen can facilitate the sustainable development of intangible cultural heritage and advance digital humanities research.", "key_contributions": ["Introduction of ICH-Qwen, a specialized LLM for ICH", "Utilization of large-scale open-source data for model training", "Demonstration of model efficacy in ICH-specific applications"], "limitations": "", "keywords": ["Intangible Cultural Heritage", "Large Language Models", "Digital Humanities"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2411.06160", "pdf": "https://arxiv.org/pdf/2411.06160.pdf", "abs": "https://arxiv.org/abs/2411.06160", "title": "Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework", "authors": ["Jingyi Zhou", "Senlin Luo", "Haofan Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "3.1 There is a misstatement in the EQN Framework section", "summary": "Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing.", "AI": {"tldr": "The paper introduces the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation, addressing limitations in existing emotion datasets.", "motivation": "To improve emotion detection accuracy and expand the capabilities of AI in emotional reasoning, reducing reliance on manual annotations with inherent biases and limitations.", "method": "The study proposes an all-labels and training-set label regression method that maps label values to energy intensity levels, allowing for the detection of multiple emotions in text samples.", "result": "The EQN framework was validated using five sentiment datasets, showing high effectiveness in automatic micro-emotion detection and annotation, outperforming existing literature methods.", "conclusion": "The EQN framework represents a significant advancement in the automatic detection of micro-emotions, enabling deeper insights into emotional analysis and quantitative research in emotion computing.", "key_contributions": ["Introduction of a novel framework for micro-emotion detection (EQN).", "Mapping of label values to energy intensity levels for improved model learning.", "Validation of the framework's effectiveness across multiple sentiment datasets."], "limitations": "The paper acknowledges a misstatement in the EQN Framework section and does not delve deeply into potential areas of bias in dataset selection.", "keywords": ["Emotion Detection", "Micro-Emotions", "Emotion Quantization Network", "NLP", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23832", "pdf": "https://arxiv.org/pdf/2505.23832.pdf", "abs": "https://arxiv.org/abs/2505.23832", "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements Generation", "authors": ["Chaeeun Kim", "Jinu Lee", "Wonseok Hwang"], "categories": ["cs.CL", "cs.IR"], "comment": "Under review", "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.", "AI": {"tldr": "The paper introduces LEGAR BENCH, a large-scale legal case retrieval benchmark, and LegalSearchLM, a model enhancing legal element reasoning for better retrieval performance.", "motivation": "Existing legal case retrieval studies are limited by small-scale corpora and narrow query types, leading to inadequate performance and legally irrelevant matches.", "method": "This study develops LEGAR BENCH for a comprehensive evaluation of legal case retrieval and proposes LegalSearchLM, which utilizes legal element reasoning and constrained decoding for case retrieval.", "result": "LegalSearchLM demonstrates a performance improvement of 6-20% over baseline models on LEGAR BENCH and shows robustness in generalization to out-of-domain cases, surpassing traditional generative models.", "conclusion": "The findings suggest that the combination of a large-scale benchmark and advanced reasoning model significantly enhances legal case retrieval capabilities.", "key_contributions": ["Introduction of LEGAR BENCH, a large-scale benchmark for legal case retrieval", "Development of LegalSearchLM for improved legal element reasoning in retrieval", "Strong performance improvements demonstrated over existing models"], "limitations": "", "keywords": ["legal case retrieval", "machine learning", "benchmarking", "legal informatics", "Korean law"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2412.15712", "pdf": "https://arxiv.org/pdf/2412.15712.pdf", "abs": "https://arxiv.org/abs/2412.15712", "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining", "authors": ["Maike Züfle", "Jan Niehues"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data.", "AI": {"tldr": "This paper proposes a two-stage training approach for adapting large language models (LLMs) to speech processing tasks using contrastive learning for the pretraining phase followed by minimal data fine-tuning.", "motivation": "The need for efficient adaptation of LLMs to speech tasks due to challenges like overfitting, data requirements, and high computational costs.", "method": "A two-stage training approach: (1) Task-independent speech pretraining using contrastive learning for layer-wise alignment of text and speech representations; (2) Task-specific fine-tuning with minimal data.", "result": "The proposed method outperforms traditional ASR pretraining and exceeds the performance of models specialized in speech translation and question answering, even with only 10% of the task-specific data used for training.", "conclusion": "The two-stage training approach effectively mitigates overfitting and data limitations while enhancing speech processing model performance.", "key_contributions": ["Introduction of a scalable two-stage training approach for speech tasks", "Use of contrastive learning for text-speech representation alignment", "Demonstrated performance improvement with less task-specific data"], "limitations": "", "keywords": ["large language models", "speech processing", "contrastive learning", "task-specific fine-tuning", "ASR"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23833", "pdf": "https://arxiv.org/pdf/2505.23833.pdf", "abs": "https://arxiv.org/abs/2505.23833", "title": "Benchmarking Abstract and Reasoning Abilities Through A Theoretical Perspective", "authors": ["Qingchuan Ma", "Yuhang Wu", "Xiawu Zheng", "Rongrong Ji"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we aim to establish a simple, effective, and theoretically\ngrounded benchmark for rigorously probing abstract reasoning in Large Language\nModels (LLMs). To achieve this, we first develop a mathematic framework that\ndefines abstract reasoning as the ability to: (i) extract essential patterns\nindependent of surface representations, and (ii) apply consistent rules to\nthese abstract patterns. Based on this framework, we introduce two novel\ncomplementary metrics: \\(\\scoreGamma\\) measures basic reasoning accuracy, while\n\\(\\scoreDelta\\) quantifies a model's reliance on specific symbols rather than\nunderlying patterns - a key indicator of true abstraction versus mere\nmemorization. To implement this measurement, we design a benchmark: systematic\nsymbol remapping in rule-based tasks, which forces models to demonstrate\ngenuine pattern recognition beyond superficial token matching. Extensive LLM\nevaluations using this benchmark (commercial API models, 7B-70B, multi-agent)\nreveal:1) critical limitations in non-decimal arithmetic and symbolic\nreasoning; 2) persistent abstraction gaps despite chain-of-thought prompting;\nand 3) \\(\\scoreDelta\\)'s effectiveness in robustly measuring memory dependence\nby quantifying performance degradation under symbol remapping, particularly\nhighlighting operand-specific memorization. These findings underscore that\ncurrent LLMs, despite domain-specific strengths, still lack robust abstract\nreasoning, highlighting key areas for future improvement.", "AI": {"tldr": "The paper proposes a benchmark for assessing abstract reasoning in LLMs, introducing metrics to measure reasoning accuracy and dependence on specific symbols.", "motivation": "To rigorously evaluate abstract reasoning capabilities in Large Language Models (LLMs) and address their current limitations.", "method": "Developed a mathematical framework defining abstract reasoning, introduced two new metrics for evaluation (\u0003scoreGamma\u0003 and \u0003scoreDelta\u0003), and created a benchmark involving systematic symbol remapping in rule-based tasks.", "result": "Evaluations showed critical shortcomings in arithmetic and symbolic reasoning, uncovered abstraction gaps despite chain-of-thought prompting, and established that \u0003scoreDelta\u0003 effectively measures memory dependence.", "conclusion": "Current LLMs exhibit significant deficiencies in abstract reasoning; this study highlights crucial areas for enhancement.", "key_contributions": ["Established a new benchmark for probing abstract reasoning in LLMs", "Introduced two novel metrics to differentiate true abstraction from memorization", "Revealed specific limitations in LLMs regarding arithmetic and symbolic reasoning"], "limitations": "The study is focused on specific tasks and models, which may not generalize across all types of reasoning or LLM architectures.", "keywords": ["Large Language Models", "abstract reasoning", "benchmark", "symbolic reasoning", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23835", "pdf": "https://arxiv.org/pdf/2505.23835.pdf", "abs": "https://arxiv.org/abs/2505.23835", "title": "Say What You Mean: Natural Language Access Control with Large Language Models for Internet of Things", "authors": ["Ye Cheng", "Minghui Xu", "Yue Zhang", "Kun Li", "Hao Wu", "Yechao Zhang", "Shaoyong Guo", "Wangjie Qiu", "Dongxiao Yu", "Xiuzhen Cheng"], "categories": ["cs.CL"], "comment": null, "summary": "Access control in the Internet of Things (IoT) is becoming increasingly\ncomplex, as policies must account for dynamic and contextual factors such as\ntime, location, user behavior, and environmental conditions. However, existing\nplatforms either offer only coarse-grained controls or rely on rigid rule\nmatching, making them ill-suited for semantically rich or ambiguous access\nscenarios. Moreover, the policy authoring process remains fragmented: domain\nexperts describe requirements in natural language, but developers must manually\ntranslate them into code, introducing semantic gaps and potential\nmisconfiguration. In this work, we present LACE, the Language-based Access\nControl Engine, a hybrid framework that leverages large language models (LLMs)\nto bridge the gap between human intent and machine-enforceable logic. LACE\ncombines prompt-guided policy generation, retrieval-augmented reasoning, and\nformal validation to support expressive, interpretable, and verifiable access\ncontrol. It enables users to specify policies in natural language,\nautomatically translates them into structured rules, validates semantic\ncorrectness, and makes access decisions using a hybrid LLM-rule-based engine.\nWe evaluate LACE in smart home environments through extensive experiments. LACE\nachieves 100% correctness in verified policy generation and up to 88% decision\naccuracy with 0.79 F1-score using DeepSeek-V3, outperforming baselines such as\nGPT-3.5 and Gemini. The system also demonstrates strong scalability under\nincreasing policy volume and request concurrency. Our results highlight LACE's\npotential to enable secure, flexible, and user-friendly access control across\nreal-world IoT platforms.", "AI": {"tldr": "LACE is a hybrid framework utilizing LLMs for natural language-based access control in IoT, achieving high correctness and accuracy in policy generation and decisions.", "motivation": "Current access control methods in IoT are inadequate due to reliance on coarse-grained controls and rigid rule matching, leading to potential misconfigurations.", "method": "LACE leverages large language models for prompt-guided policy generation, retrieval-augmented reasoning, and formal validation, allowing users to create access policies in natural language.", "result": "LACE achieves 100% correctness in verified policy generation and up to 88% decision accuracy with an F1-score of 0.79, outperforming models like GPT-3.5 and Gemini.", "conclusion": "LACE demonstrates significant potential for enhancing access control in IoT by being secure, flexible, and user-friendly with scalability.", "key_contributions": ["Introduction of a language-based access control engine (LACE) for IoT", "Proven correctness in policy generation", "High decision accuracy and scalability of the system"], "limitations": "", "keywords": ["access control", "IoT", "large language models", "policy generation", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815.pdf", "abs": "https://arxiv.org/abs/2504.15815", "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Jonas Fischer", "Barbara Plank"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at ACL'25", "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods of LLM outputs, either automated\nmetrics or human evaluation, have limitations, such as providing limited\ninsights or being labor-intensive. We propose Spotlight, a new approach that\ncombines both automation and human analysis. Based on data mining techniques,\nwe automatically distinguish between random (decoding) variations and\nsystematic differences in language model outputs. This process provides token\npatterns that describe the systematic differences and guide the user in\nmanually analyzing the effects of their prompts and changes in models\nefficiently. We create three benchmarks to quantitatively test the reliability\nof token pattern extraction methods and demonstrate that our approach provides\nnew insights into established prompt data. From a human-centric perspective,\nthrough demonstration studies and a user study, we show that our token pattern\napproach helps users understand the systematic differences of language model\noutputs. We are further able to discover relevant differences caused by prompt\nand model changes (e.g. related to gender or culture), thus supporting the\nprompt engineering process and human-centric model behavior research.", "AI": {"tldr": "Spotlight is a novel approach to prompt engineering for large language models that combines automation and human analysis to better understand systematic differences in model outputs.", "motivation": "Existing evaluation methods for large language model outputs have significant limitations, which can hinder the prompt engineering process.", "method": "Spotlight uses data mining techniques to distinguish between random and systematic differences in language model outputs, providing token patterns for analysis.", "result": "The approach successfully identifies systematic differences in outputs influenced by prompts and model changes, demonstrating reliability through three benchmarks.", "conclusion": "Spotlight aids in understanding language model outputs and enhances the efficiency of prompt engineering by revealing important systematic differences.", "key_contributions": ["Introduction of the Spotlight approach combining automated and human-driven analysis", "Creation of three benchmarks for assessing token pattern extraction methods", "Identification of systematic differences in outputs related to prompt and model changes"], "limitations": "", "keywords": ["prompt engineering", "language models", "data mining", "human analysis", "systematic differences"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23836", "pdf": "https://arxiv.org/pdf/2505.23836.pdf", "abs": "https://arxiv.org/abs/2505.23836", "title": "Large Language Models Often Know When They Are Being Evaluated", "authors": ["Joe Needham", "Giles Edkins", "Govind Pimpale", "Henning Bartsch", "Marius Hobbhahn"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "If AI models can detect when they are being evaluated, the effectiveness of\nevaluations might be compromised. For example, models could have systematically\ndifferent behavior during evaluations, leading to less reliable benchmarks for\ndeployment and governance decisions. We investigate whether frontier language\nmodels can accurately classify transcripts based on whether they originate from\nevaluations or real-world deployment, a capability we call evaluation\nawareness. To achieve this, we construct a diverse benchmark of 1,000 prompts\nand transcripts from 61 distinct datasets. These span public benchmarks (e.g.,\nMMLU, SWEBench), real-world deployment interactions, and agent trajectories\nfrom scaffolding frameworks (e.g., web-browsing agents). Frontier models\nclearly demonstrate above-random evaluation awareness (Gemini-2.5-Pro reaches\nan AUC of $0.83$), but do not yet surpass our simple human baseline (AUC of\n$0.92$). Furthermore, both AI models and humans are better at identifying\nevaluations in agentic settings compared to chat settings. Additionally, we\ntest whether models can identify the purpose of the evaluation. Under\nmultiple-choice and open-ended questioning, AI models far outperform random\nchance in identifying what an evaluation is testing for. Our results indicate\nthat frontier models already exhibit a substantial, though not yet superhuman,\nlevel of evaluation-awareness. We recommend tracking this capability in future\nmodels.", "AI": {"tldr": "The paper investigates if language models can detect evaluation contexts, termed evaluation awareness, using a benchmark of prompts and transcripts. Models display above-random evaluation awareness but do not yet surpass human performance.", "motivation": "To understand if AI models can behave differently during evaluations, which could bias their effectiveness in benchmarks and deployment decisions.", "method": "A benchmark dataset of 1,000 prompts and transcripts from 61 datasets was analyzed, comparing how well language models classify whether transcripts are from evaluations or real-world usage.", "result": "Models showed evaluation awareness (Gemini-2.5-Pro reached an AUC of 0.83) but did not outperform humans (AUC of 0.92). They were better at identifying evaluations in agentic contexts and effectively recognized the purpose of the evaluations.", "conclusion": "Frontier models exhibit significant evaluation awareness, warranting further monitoring of this capability in future AI developments.", "key_contributions": ["Demonstrated that language models can identify evaluation contexts with above-random accuracy.", "Created a benchmark for assessing evaluation awareness in AI models.", "Found performance discrepancies between model capabilities in agentic versus chat settings."], "limitations": "The models do not yet reach superhuman performance in evaluation awareness compared to humans.", "keywords": ["evaluation awareness", "language models", "benchmarking", "human-AI interaction", "performance metrics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23837", "pdf": "https://arxiv.org/pdf/2505.23837.pdf", "abs": "https://arxiv.org/abs/2505.23837", "title": "CoMaPOI: A Collaborative Multi-Agent Framework for Next POI Prediction Bridging the Gap Between Trajectory and Language", "authors": ["Lin Zhong", "Lingzhi Wang", "Xu Yang", "Qing Liao"], "categories": ["cs.CL", "cs.IR", "I.2.0"], "comment": "This paper has been accepted by SIGIR 2025", "summary": "Large Language Models (LLMs) offer new opportunities for the next\nPoint-Of-Interest (POI) prediction task, leveraging their capabilities in\nsemantic understanding of POI trajectories. However, previous LLM-based\nmethods, which are superficially adapted to next POI prediction, largely\noverlook critical challenges associated with applying LLMs to this task.\nSpecifically, LLMs encounter two critical challenges: (1) a lack of intrinsic\nunderstanding of numeric spatiotemporal data, which hinders accurate modeling\nof users' spatiotemporal distributions and preferences; and (2) an excessively\nlarge and unconstrained candidate POI space, which often results in random or\nirrelevant predictions. To address these issues, we propose a Collaborative\nMulti Agent Framework for Next POI Prediction, named CoMaPOI. Through the close\ninteraction of three specialized agents (Profiler, Forecaster, and Predictor),\nCoMaPOI collaboratively addresses the two critical challenges. The Profiler\nagent is responsible for converting numeric data into language descriptions,\nenhancing semantic understanding. The Forecaster agent focuses on dynamically\nconstraining and refining the candidate POI space. The Predictor agent\nintegrates this information to generate high-precision predictions. Extensive\nexperiments on three benchmark datasets (NYC, TKY, and CA) demonstrate that\nCoMaPOI achieves state of the art performance, improving all metrics by 5% to\n10% compared to SOTA baselines. This work pioneers the investigation of\nchallenges associated with applying LLMs to complex spatiotemporal tasks by\nleveraging tailored collaborative agents.", "AI": {"tldr": "The paper introduces CoMaPOI, a Collaborative Multi-Agent Framework for predicting the next Point-Of-Interest (POI) using Large Language Models to overcome challenges in spatiotemporal data and candidate POI space.", "motivation": "To address the shortcomings of LLMs in next POI prediction, particularly their lack of understanding of numeric spatiotemporal data and the large candidate POI space, which leads to irrelevant predictions.", "method": "CoMaPOI employs three specialized agents: Profiler (converts numeric data into language), Forecaster (refines the POI candidate space), and Predictor (integrates this information for accurate predictions).", "result": "CoMaPOI achieves state-of-the-art performance on three benchmark datasets, improving prediction accuracy by 5% to 10% compared to existing models.", "conclusion": "The work demonstrates the potential of collaborative agents in enhancing LLM applications for complex spatiotemporal prediction tasks.", "key_contributions": ["Introduction of CoMaPOI framework for next POI prediction", "Three specialized agents working collaboratively to enhance model performance", "Experimental validation showing significant performance improvements over SOTA baselines"], "limitations": "", "keywords": ["Large Language Models", "Point-Of-Interest prediction", "Collaborative agents", "Spatiotemporal data"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23838", "pdf": "https://arxiv.org/pdf/2505.23838.pdf", "abs": "https://arxiv.org/abs/2505.23838", "title": "Exploring the Landscape of Text-to-SQL with Large Language Models: Progresses, Challenges and Opportunities", "authors": ["Yiming Huang", "Jiyu Guo", "Wenxin Mao", "Cuiyun Gao", "Peiyi Han", "Chuanyi Liu", "Qing Ling"], "categories": ["cs.CL", "cs.IR"], "comment": "Submitted to ACM Computing Surveys (CSUR). Currently under review", "summary": "Converting natural language (NL) questions into SQL queries, referred to as\nText-to-SQL, has emerged as a pivotal technology for facilitating access to\nrelational databases, especially for users without SQL knowledge. Recent\nprogress in large language models (LLMs) has markedly propelled the field of\nnatural language processing (NLP), opening new avenues to improve text-to-SQL\nsystems. This study presents a systematic review of LLM-based text-to-SQL,\nfocusing on four key aspects: (1) an analysis of the research trends in\nLLM-based text-to-SQL; (2) an in-depth analysis of existing LLM-based\ntext-to-SQL techniques from diverse perspectives; (3) summarization of existing\ntext-to-SQL datasets and evaluation metrics; and (4) discussion on potential\nobstacles and avenues for future exploration in this domain. This survey seeks\nto furnish researchers with an in-depth understanding of LLM-based text-to-SQL,\nsparking new innovations and advancements in this field.", "AI": {"tldr": "This study systematically reviews LLM-based Text-to-SQL technology, focusing on research trends, techniques, datasets, evaluation metrics, and future directions.", "motivation": "To facilitate access to relational databases through natural language queries, particularly for users without SQL knowledge, leveraging advancements in large language models (LLMs).", "method": "This study conducts a systematic review of existing literature on LLM-based text-to-SQL, analyzing techniques and datasets while discussing trends and future directions.", "result": "Identified trends in the research of LLM-based text-to-SQL, detailed various techniques, and compiled datasets and evaluation metrics used in the field.", "conclusion": "The survey provides insights into LLM-based text-to-SQL, aiming to inspire future research and innovation in the area.", "key_contributions": ["Analysis of research trends in LLM-based text-to-SQL.", "In-depth analysis of existing techniques and methodologies.", "Comprehensive summary of datasets and evaluation metrics used in the field."], "limitations": "", "keywords": ["Text-to-SQL", "Large Language Models", "Natural Language Processing", "Relational Databases", "Research Trends"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23840", "pdf": "https://arxiv.org/pdf/2505.23840.pdf", "abs": "https://arxiv.org/abs/2505.23840", "title": "Measuring Sycophancy of Language Models in Multi-turn Dialogues", "authors": ["Jiseung Hong", "Grace Byun", "Seungone Kim", "Kai Shu"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are expected to provide helpful and harmless\nresponses, yet they often exhibit sycophancy--conforming to user beliefs\nregardless of factual accuracy or ethical soundness. Prior research on\nsycophancy has primarily focused on single-turn factual correctness,\noverlooking the dynamics of real-world interactions. In this work, we introduce\nSYCON Bench, a novel benchmark for evaluating sycophantic behavior in\nmulti-turn, free-form conversational settings. Our benchmark measures how\nquickly a model conforms to the user (Turn of Flip) and how frequently it\nshifts its stance under sustained user pressure (Number of Flip). Applying\nSYCON Bench to 17 LLMs across three real-world scenarios, we find that\nsycophancy remains a prevalent failure mode. Our analysis shows that alignment\ntuning amplifies sycophantic behavior, whereas model scaling and reasoning\noptimization strengthen the model's ability to resist undesirable user views.\nReasoning models generally outperform instruction-tuned models but often fail\nwhen they over-index on logical exposition instead of directly addressing the\nuser's underlying beliefs. Finally, we evaluate four additional prompting\nstrategies and demonstrate that adopting a third-person perspective reduces\nsycophancy by up to 63.8% in debate scenario. We release our code and data at\nhttps://github.com/JiseungHong/SYCON-Bench.", "AI": {"tldr": "This paper introduces SYCON Bench, a benchmark for evaluating sycophantic behavior in large language models during multi-turn conversations, revealing that sycophancy is prevalent and highlighting strategies to mitigate it.", "motivation": "To address the limitation of existing research on sycophancy, which has mostly focused on single-turn factual correctness, by evaluating behavior in multi-turn conversations.", "method": "The authors developed SYCON Bench to measure sycophantic behavior through metrics such as Turn of Flip (speed of conformance to user beliefs) and Number of Flip (frequency of stance shifts).", "result": "The evaluation of 17 LLMs showed persistent sycophantic behavior across scenarios, with alignment tuning increasing this behavior while model scaling and reasoning optimization helped models resist user biases.", "conclusion": "Third-person prompting strategies can significantly reduce sycophancy, with reductions of up to 63.8% noted in debate scenarios, indicating potential paths to enhance model responses.", "key_contributions": ["Introduction of SYCON Bench for benchmarking sycophancy", "Demonstration of prevalence of sycophancy in LLMs", "Identification of effective prompting strategies to reduce sycophantic behavior"], "limitations": "The study does not address long-term impacts of sycophantic interactions nor all possible conversational dynamics.", "keywords": ["sycophancy", "large language models", "benchmarking", "prompting strategies", "multi-turn conversations"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23842", "pdf": "https://arxiv.org/pdf/2505.23842.pdf", "abs": "https://arxiv.org/abs/2505.23842", "title": "Document Valuation in LLM Summaries: A Cluster Shapley Approach", "authors": ["Zikun Ye", "Hema Yoganarasimhan"], "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in systems that retrieve\nand summarize content from multiple sources, such as search engines and AI\nassistants. While these models enhance user experience by generating coherent\nsummaries, they obscure the contributions of original content creators, raising\nconcerns about credit attribution and compensation. We address the challenge of\nvaluing individual documents used in LLM-generated summaries. We propose using\nShapley values, a game-theoretic method that allocates credit based on each\ndocument's marginal contribution. Although theoretically appealing, Shapley\nvalues are expensive to compute at scale. We therefore propose Cluster Shapley,\nan efficient approximation algorithm that leverages semantic similarity between\ndocuments. By clustering documents using LLM-based embeddings and computing\nShapley values at the cluster level, our method significantly reduces\ncomputation while maintaining attribution quality. We demonstrate our approach\nto a summarization task using Amazon product reviews. Cluster Shapley\nsignificantly reduces computational complexity while maintaining high accuracy,\noutperforming baseline methods such as Monte Carlo sampling and Kernel SHAP\nwith a better efficient frontier. Our approach is agnostic to the exact LLM\nused, the summarization process used, and the evaluation procedure, which makes\nit broadly applicable to a variety of summarization settings.", "AI": {"tldr": "This paper proposes an efficient method for credit attribution to documents in LLM-generated summaries using Cluster Shapley, which reduces computation while maintaining accuracy.", "motivation": "To address concerns about credit attribution and compensation for original content creators in LLM-generated summaries.", "method": "The method introduces Cluster Shapley, an approximation algorithm that utilizes semantic similarity to cluster documents and compute Shapley values at the cluster level for efficient and accurate credit allocation.", "result": "Cluster Shapley significantly reduces computational complexity while maintaining high accuracy, outperforming traditional methods like Monte Carlo sampling and Kernel SHAP.", "conclusion": "The proposed method is broadly applicable to various summarization settings and is agnostic to the specific LLM or summarization process used.", "key_contributions": ["Introduction of Cluster Shapley for efficient credit attribution", "Demonstration of improved performance over baseline methods", "Broad applicability of the method across different summarization contexts"], "limitations": "Shapley values are expensive to compute at scale, but Cluster Shapley mitigates this limitation through approximation.", "keywords": ["Large Language Models", "credit attribution", "Shapley values", "summarization", "Cluster Shapley"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23843", "pdf": "https://arxiv.org/pdf/2505.23843.pdf", "abs": "https://arxiv.org/abs/2505.23843", "title": "Evaluation Hallucination in Multi-Round Incomplete Information Lateral-Driven Reasoning Tasks", "authors": ["Wenhan Dong", "Tianyi Hu", "Jingyi Zheng", "Zhen Sun", "Yuemeng Zhao", "Yule Liu", "Xinlei He", "Xinyi Huang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Multi-round incomplete information tasks are crucial for evaluating the\nlateral thinking capabilities of large language models (LLMs). Currently,\nresearch primarily relies on multiple benchmarks and automated evaluation\nmetrics to assess these abilities. However, our study reveals novel insights\ninto the limitations of existing methods, as they often yield misleading\nresults that fail to uncover key issues, such as shortcut-taking behaviors,\nrigid patterns, and premature task termination. These issues obscure the true\nreasoning capabilities of LLMs and undermine the reliability of evaluations. To\naddress these limitations, we propose a refined set of evaluation standards,\nincluding inspection of reasoning paths, diversified assessment metrics, and\ncomparative analyses with human performance.", "AI": {"tldr": "This study critiques current evaluation methods for LLMs in multi-round incomplete tasks and proposes refined standards for assessing reasoning capabilities.", "motivation": "To identify and address the limitations of existing automated evaluation methods for large language models, which can produce misleading insights into their reasoning abilities.", "method": "The authors analyzed current benchmarks and metrics, identifying shortcomings such as shortcut-taking behaviors and rigid patterns in LLMs, and proposed a new evaluation framework that includes reasoning path inspection and diversified metrics.", "result": "The analysis demonstrated that existing evaluations often fail to reveal key reasoning issues in LLMs, prompting the need for improved evaluation standards.", "conclusion": "Implementing refined evaluation standards can enhance the reliability of assessments for lateral thinking capabilities in LLMs, leading to more accurate insights into their reasoning processes.", "key_contributions": ["Critique of existing benchmarks for LLMs in multi-round tasks", "Proposal of a new evaluation framework that includes diverse assessment metrics", "Highlighting specific reasoning issues that current metrics fail to address"], "limitations": "The paper may not cover all potential issues in LLM evaluations, and the proposed standards require further empirical validation.", "keywords": ["large language models", "evaluation standards", "reasoning capabilities", "multi-round tasks", "automated metrics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23844", "pdf": "https://arxiv.org/pdf/2505.23844.pdf", "abs": "https://arxiv.org/abs/2505.23844", "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation", "authors": ["Zhenglun Kong", "Zheng Zhan", "Shiyue Hou", "Yifan Gong", "Xin Meng", "Pengwei Sui", "Peiyan Dong", "Xuan Shen", "Zifeng Wang", "Pu Zhao", "Hao Tang", "Stratis Ioannidis", "Yanzhi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration", "AI": {"tldr": "This paper presents a framework for adaptively selecting and aggregating knowledge from multiple LLMs to improve performance while reducing memory overhead and knowledge interference.", "motivation": "To improve the continual performance of LLMs through better integration of knowledge without the limitations of traditional fine-tuning methods.", "method": "The authors propose an adaptive selection network to choose the most relevant LLMs and a dynamic weighted fusion strategy, combined with a feedback-driven loss function.", "result": "The method achieves a reduction in knowledge interference by up to 50% compared to existing aggregation approaches, enabling a more stable and scalable integration process.", "conclusion": "The proposed framework successfully addresses the challenges of integrating knowledge from various LLMs and improves performance while managing resource constraints.", "key_contributions": ["Adaptive selection network for relevant LLM identification", "Dynamic weighted fusion strategy for knowledge integration", "Feedback-driven loss function to enhance model adaptability"], "limitations": "", "keywords": ["large language models", "knowledge aggregation", "adaptive selection", "dynamic fusion", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23845", "pdf": "https://arxiv.org/pdf/2505.23845.pdf", "abs": "https://arxiv.org/abs/2505.23845", "title": "Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in LLMs", "authors": ["Jakub Podolak", "Rajeev Verma"], "categories": ["cs.CL"], "comment": null, "summary": "We study the source of uncertainty in DeepSeek R1-32B by analyzing its\nself-reported verbal confidence on question answering (QA) tasks. In the\ndefault answer-then-confidence setting, the model is regularly over-confident,\nwhereas semantic entropy - obtained by sampling many responses - remains\nreliable. We hypothesize that this is because of semantic entropy's larger\ntest-time compute, which lets us explore the model's predictive distribution.\nWe show that granting DeepSeek the budget to explore its distribution by\nforcing a long chain-of-thought before the final answer greatly improves its\nverbal score effectiveness, even on simple fact-retrieval questions that\nnormally require no reasoning. Furthermore, a separate reader model that sees\nonly the chain can reconstruct very similar confidences, indicating the verbal\nscore might be merely a statistic of the alternatives surfaced during\nreasoning. Our analysis concludes that reliable uncertainty estimation requires\nexplicit exploration of the generative space, and self-reported confidence is\ntrustworthy only after such exploration.", "AI": {"tldr": "The paper analyzes the uncertainty in DeepSeek R1-32B by examining its self-reported verbal confidence in question answering tasks, revealing that semantic entropy provides a more reliable measure of confidence than the model's own assessments.", "motivation": "To better understand the reliability of self-reported confidence in question answering tasks performed by DeepSeek R1-32B and to explore methods for improving uncertainty estimation.", "method": "The study involved analyzing DeepSeek's performance in an answer-then-confidence setting, comparing it with a semantic entropy approach that samples multiple responses.", "result": "The findings indicate that allowing DeepSeek to perform a long chain-of-thought reasoning before answering significantly enhances its verbal score effectiveness, showing that self-reported confidence improves only after exploring the generative space.", "conclusion": "Reliable uncertainty estimation in models requires explicit exploration of the generative space, and self-reported confidence is only trustworthy after such exploration occurs.", "key_contributions": ["Examination of the effectiveness of self-reported confidence in DeepSeek.", "Introduction of semantic entropy as a more reliable measure of confidence.", "Demonstration that longer reasoning chains improve confidence assessments."], "limitations": "The analysis may not generalize to all models or types of tasks beyond those tested with DeepSeek R1-32B.", "keywords": ["DeepSeek", "question answering", "uncertainty estimation", "semantic entropy", "confidence"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23846", "pdf": "https://arxiv.org/pdf/2505.23846.pdf", "abs": "https://arxiv.org/abs/2505.23846", "title": "Scalable, Symbiotic, AI and Non-AI Agent Based Parallel Discrete Event Simulations", "authors": ["Atanu Barai", "Stephan Eidenbenz", "Nandakishore Santhi"], "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "To fully leverage the potential of artificial intelligence (AI) systems in a\ntrustworthy manner, it is desirable to couple multiple AI and non-AI systems\ntogether seamlessly for constraining and ensuring correctness of the output.\nThis paper introduces a novel parallel discrete event simulation (PDES) based\nmethodology to combine multiple AI and non-AI agents in a causal, rule-based\nway. Our approach tightly integrates the concept of passage of time, with each\nagent considered as an entity in the PDES framework and responding to prior\nrequests from other agents. Such coupling mechanism enables the agents to work\nin a co-operative environment towards a common goal while many tasks run in\nparallel throughout the simulation. It further enables setting up boundaries to\nthe outputs of the AI agents by applying necessary dynamic constraints using\nnon-AI agents while allowing for scalability through deployment of hundreds of\nsuch agents in a larger compute cluster. Distributing smaller AI agents can\nenable extremely scalable simulations in the future, addressing local memory\nbottlenecks for model parameter storage. Within a PDES involving both AI and\nnon-AI agents, we break down the problem at hand into structured steps, when\nnecessary, providing a set of multiple choices to the AI agents, and then\nprogressively solve these steps towards a final goal. At each step, the non-AI\nagents act as unbiased auditors, verifying each action by the AI agents so that\ncertain rules of engagement are followed. We evaluate our approach by solving\nfour problems from four different domains and comparing the results with those\nfrom AI models alone. Our results show greater accuracy in solving problems\nfrom various domains where the AI models struggle to solve the problems solely\nby themselves. Results show that overall accuracy of our approach is 68% where\nas the accuracy of vanilla models is less than 23%.", "AI": {"tldr": "This paper presents a novel methodology that uses parallel discrete event simulation (PDES) to effectively combine AI and non-AI agents for enhanced problem-solving accuracy and trustworthy AI output constraints.", "motivation": "To leverage the full potential of AI systems in a trustworthy manner by integrating multiple AI and non-AI systems for correctness of output.", "method": "A parallel discrete event simulation (PDES) framework is established where both AI and non-AI agents work together in a cooperative environment, utilizing dynamic constraints and verifying actions at each step.", "result": "The proposed approach achieved an overall accuracy of 68% in solving various domain problems, significantly higher than the less than 23% accuracy of traditional AI models.", "conclusion": "The integration of non-AI agents as unbiased auditors improves the overall accuracy and reliability of AI systems in complex problem-solving tasks.", "key_contributions": ["Introduction of a novel PDES methodology for AI and non-AI agent integration.", "Demonstration of improved accuracy in problem-solving across different domains.", "Scalable simulation capabilities through the distribution of smaller AI agents."], "limitations": "", "keywords": ["Artificial Intelligence", "Non-AI Agents", "Parallel Discrete Event Simulation", "Accuracy", "Dynamic Constraints"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.23848", "pdf": "https://arxiv.org/pdf/2505.23848.pdf", "abs": "https://arxiv.org/abs/2505.23848", "title": "Derailing Non-Answers via Logit Suppression at Output Subspace Boundaries in RLHF-Aligned Language Models", "authors": ["Harvey Dam", "Jonas Knochelmann", "Vinu Joseph", "Ganesh Gopalakrishnan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We introduce a method to reduce refusal rates of large language models (LLMs)\non sensitive content without modifying model weights or prompts. Motivated by\nthe observation that refusals in certain models were often preceded by the\nspecific token sequence of a token marking the beginning of the\nchain-of-thought (CoT) block (<think>) followed by a double newline token\n(\\n\\n), we investigate the impact of two simple formatting adjustments during\ngeneration: suppressing \\n\\n after <think> and suppressing the end-of-sequence\ntoken after the end of the CoT block (</think>). Our method requires no\ndatasets, parameter changes, or training, relying solely on modifying token\nprobabilities during generation. In our experiments with official DeepSeek-R1\ndistillations, these interventions increased the proportion of substantive\nanswers to sensitive prompts without affecting performance on standard\nbenchmarks. Our findings suggest that refusal behaviors can be circumvented by\nblocking refusal subspaces at specific points in the generation process.", "AI": {"tldr": "A method is introduced to reduce refusal rates of LLMs on sensitive content by adjusting token formatting during generation without altering model weights or training.", "motivation": "To address refusal rates of LLMs on sensitive topics, which are often indicated by specific token sequences, the paper aims to find a solution that does not require modifications to model architecture or retraining.", "method": "The approach involves suppressing specific token sequences in the generation process: removing the newline token after the CoT marker (<think>) and preventing the end-of-sequence token from appearing after the CoT block ends.", "result": "The application of these methods led to an increased proportion of substantive responses to sensitive prompts in experiments, maintaining standard performance on regular benchmarks.", "conclusion": "The results indicate that managing token formatting can effectively mitigate refusal behaviors in LLMs during content generation.", "key_contributions": ["Developed a novel approach to reduce refusals without model retraining", "Demonstrated effectiveness through experiments with prominent distillation models", "Highlighted the importance of token sequence management in LLM generation."], "limitations": "", "keywords": ["large language models", "sensitive content", "token formatting", "refusal rates", "chain-of-thought"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23851", "pdf": "https://arxiv.org/pdf/2505.23851.pdf", "abs": "https://arxiv.org/abs/2505.23851", "title": "ASyMOB: Algebraic Symbolic Mathematical Operations Benchmark", "authors": ["Michael Shalyt", "Rotem Elimelech", "Ido Kaminer"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Code repository: https://github.com/RamanujanMachine/ASyMOB Complete\n  benchmark dataset:\n  https://huggingface.co/datasets/Shalyt/ASyMOB-Algebraic_Symbolic_Mathematical_Operations_Benchmark", "summary": "Large language models (LLMs) are rapidly approaching the level of proficiency\nin university-level symbolic mathematics required for applications in advanced\nscience and technology. However, existing benchmarks fall short in assessing\nthe core skills of LLMs in symbolic mathematics-such as integration,\ndifferential equations, and algebraic simplification. To address this gap, we\nintroduce ASyMOB, a novel assessment framework focused exclusively on symbolic\nmanipulation, featuring 17,092 unique math challenges, organized by similarity\nand complexity. ASyMOB enables analysis of LLM generalization capabilities by\ncomparing performance in problems that differ by simple numerical or symbolic\n`perturbations'. Evaluated LLMs exhibit substantial degradation in performance\nfor all perturbation types (up to -70.3%), suggesting reliance on memorized\npatterns rather than deeper understanding of symbolic math, even among models\nachieving high baseline accuracy. Comparing LLM performance to computer algebra\nsystems, we identify examples where they fail while LLMs succeed, as well as\nproblems solved only by combining both approaches. Models capable of integrated\ncode execution yielded higher accuracy compared to their performance without\ncode, particularly stabilizing weaker models (up to +33.1% for certain\nperturbation types). Notably, the most advanced models (o4-mini, Gemini 2.5\nFlash) demonstrate not only high symbolic math proficiency (scoring 96.8% and\n97.6% on the unperturbed set), but also remarkable robustness against\nperturbations, (-21.7% and -21.2% vs. average -50.4% for the other models).\nThis may indicate a recent \"phase transition\" in the generalization\ncapabilities of frontier LLMs. It remains to be seen whether the path forward\nlies in deeper integration with sophisticated external tools, or in developing\nmodels so capable that symbolic math systems like CAS become unnecessary.", "AI": {"tldr": "Introduction of ASyMOB, a novel framework for assessing LLMs in symbolic mathematics with 17,092 unique challenges.", "motivation": "Existing benchmarks inadequately assess LLM proficiency in symbolic mathematics for advanced applications.", "method": "Development of ASyMOB framework featuring a diverse set of math challenges organized by similarity and complexity, enabling performance analysis for LLMs.", "result": "Evaluated LLMs showed significant performance degradation due to reliance on memorization, with advanced models demonstrating remarkable robustness against perturbations.", "conclusion": "The findings suggest a critical phase transition in LLM capabilities, raising questions on the future of symbolic math systems and their integration with advanced tools.", "key_contributions": ["Introduction of ASyMOB framework with extensive benchmarks", "Demonstrated performance variations among LLMs and CAS", "Identification of models that perform better with integrated code execution."], "limitations": "The study primarily focuses on symbolic manipulation; broader mathematical reasoning is not addressed.", "keywords": ["large language models", "symbolic mathematics", "ASyMOB", "assessment framework", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.23852", "pdf": "https://arxiv.org/pdf/2505.23852.pdf", "abs": "https://arxiv.org/abs/2505.23852", "title": "Large Language Model-Based Agents for Automated Research Reproducibility: An Exploratory Study in Alzheimer's Disease", "authors": ["Nic Dobbins", "Christelle Xiong", "Kristine Lan", "Meliha Yetisgen"], "categories": ["cs.CL", "cs.AI", "cs.MA", "stat.AP"], "comment": null, "summary": "Objective: To demonstrate the capabilities of Large Language Models (LLMs) as\nautonomous agents to reproduce findings of published research studies using the\nsame or similar dataset.\n  Materials and Methods: We used the \"Quick Access\" dataset of the National\nAlzheimer's Coordinating Center (NACC). We identified highly cited published\nresearch manuscripts using NACC data and selected five studies that appeared\nreproducible using this dataset alone. Using GPT-4o, we created a simulated\nresearch team of LLM-based autonomous agents tasked with writing and executing\ncode to dynamically reproduce the findings of each study, given only study\nAbstracts, Methods sections, and data dictionary descriptions of the dataset.\n  Results: We extracted 35 key findings described in the Abstracts across 5\nAlzheimer's studies. On average, LLM agents approximately reproduced 53.2% of\nfindings per study. Numeric values and range-based findings often differed\nbetween studies and agents. The agents also applied statistical methods or\nparameters that varied from the originals, though overall trends and\nsignificance were sometimes similar.\n  Discussion: In some cases, LLM-based agents replicated research techniques\nand findings. In others, they failed due to implementation flaws or missing\nmethodological detail. These discrepancies show the current limits of LLMs in\nfully automating reproducibility assessments. Still, this early investigation\nhighlights the potential of structured agent-based systems to provide scalable\nevaluation of scientific rigor.\n  Conclusion: This exploratory work illustrates both the promise and\nlimitations of LLMs as autonomous agents for automating reproducibility in\nbiomedical research.", "AI": {"tldr": "This study demonstrates the use of Large Language Models (LLMs) as autonomous agents for reproducing findings from biomedical research using a specific dataset, revealing both successes and limitations in their capabilities.", "motivation": "To explore the potential of LLMs as autonomous agents in automating the reproducibility of biomedical research findings.", "method": "A simulated research team of LLM-based autonomous agents was tasked with reproducing findings from five selected Alzheimer's studies using the 'Quick Access' dataset from the National Alzheimer's Coordinating Center (NACC).", "result": "LLM agents reproduced approximately 53.2% of findings per study across five Alzheimer's studies, with some discrepancies in numeric values and statistical methods used compared to original research.", "conclusion": "The study highlights the promise of LLMs in facilitating reproducibility in biomedical research but also underscores the limitations in their current capabilities.", "key_contributions": ["Demonstrated LLMs' ability to mimic research techniques", "Showed the percentage of findings LLMs could reproduce", "Identified discrepancies in statistical methods and implementation"], "limitations": "LLM agents exhibited flaws in implementation and lacked detailed methodological information, limiting their effectiveness in full automation of reproducibility assessments.", "keywords": ["Large Language Models", "reproducibility", "biomedical research", "autonomous agents", "Alzheimer's studies"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2505.23854", "pdf": "https://arxiv.org/pdf/2505.23854.pdf", "abs": "https://arxiv.org/abs/2505.23854", "title": "Revisiting Uncertainty Estimation and Calibration of Large Language Models", "authors": ["Linwei Tao", "Yi-Fan Yeh", "Minjing Dong", "Tao Huang", "Philip Torr", "Chang Xu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in high-stakes\napplications, robust uncertainty estimation is essential for ensuring the safe\nand trustworthy deployment of LLMs. We present the most comprehensive study to\ndate of uncertainty estimation in LLMs, evaluating 80 models spanning open- and\nclosed-source families, dense and Mixture-of-Experts (MoE) architectures,\nreasoning and non-reasoning modes, quantization variants and parameter scales\nfrom 0.6B to 671B. Focusing on three representative black-box single-pass\nmethods, including token probability-based uncertainty (TPU), numerical verbal\nuncertainty (NVU), and linguistic verbal uncertainty (LVU), we systematically\nevaluate uncertainty calibration and selective classification using the\nchallenging MMLU-Pro benchmark, which covers both reasoning-intensive and\nknowledge-based tasks. Our results show that LVU consistently outperforms TPU\nand NVU, offering stronger calibration and discrimination while being more\ninterpretable. We also find that high accuracy does not imply reliable\nuncertainty, and that model scale, post-training, reasoning ability and\nquantization all influence estimation performance. Notably, LLMs exhibit better\nuncertainty estimates on reasoning tasks than on knowledge-heavy ones, and good\ncalibration does not necessarily translate to effective error ranking. These\nfindings highlight the need for multi-perspective evaluation and position LVU\nas a practical tool for improving the reliability of LLMs in real-world\nsettings.", "AI": {"tldr": "This study investigates uncertainty estimation in large language models (LLMs) across various architectures and scales, highlighting the effectiveness of linguistic verbal uncertainty (LVU) over other methods.", "motivation": "To ensure the safe and trustworthy deployment of LLMs in high-stakes applications through robust uncertainty estimation.", "method": "Evaluated 80 LLMs across different architectures and scales using three uncertainty estimation methods: token probability-based uncertainty (TPU), numerical verbal uncertainty (NVU), and linguistic verbal uncertainty (LVU) with a focus on the MMLU-Pro benchmark.", "result": "LVU outperforms TPU and NVU in uncertainty calibration and discrimination. High accuracy does not guarantee reliable uncertainty, and model characteristics affect performance.", "conclusion": "LVU is positioned as a practical tool to enhance LLM reliability in real-world applications, emphasizing the need for comprehensive evaluation metrics.", "key_contributions": ["Comprehensive analysis of uncertainty estimation across 80 LLMs", "Demonstration of LVU's superiority in uncertainty calibration", "Insights into the relationship between model characteristics and uncertainty estimation performance."], "limitations": "", "keywords": ["Uncertainty Estimation", "Large Language Models", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.23856", "pdf": "https://arxiv.org/pdf/2505.23856.pdf", "abs": "https://arxiv.org/abs/2505.23856", "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Modalities", "authors": ["Sahil Verma", "Keegan Hines", "Jeff Bilmes", "Charlotte Siska", "Luke Zettlemoyer", "Hila Gonen", "Chandan Singh"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient ($\\approx 120 \\times$ faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard.", "AI": {"tldr": "OMNIGUARD is a novel approach for detecting harmful prompts in large language models (LLMs) across languages and modalities, showing significant accuracy improvements and efficiency over existing methods.", "motivation": "To address concerns regarding the harmful misuse of large language models (LLMs) and the fallibility of existing detection methods for harmful prompts, particularly in multilingual settings and different modalities.", "method": "OMNIGUARD identifies internal representations in LLMs that are aligned across languages and modalities, using these representations to develop a language-agnostic and modality-agnostic classifier for detecting harmful prompts.", "result": "OMNIGUARD improves harmful prompt classification accuracy by 11.57% in multilingual scenarios, by 20.44% for image-based prompts, and sets new state-of-the-art results for audio-based prompts, all while being approximately 120 times faster than previous leading methods.", "conclusion": "The OMNIGUARD approach enhances the detection of harmful prompts in LLMs across various languages and modalities, making it a significant advancement over existing techniques.", "key_contributions": ["Proposing a new method for harmful prompt detection in multilingual and multimodal settings.", "Demonstrating significant accuracy improvements on multilingual and image-based prompts.", "Achieving state-of-the-art results for audio-based prompt detection with high efficiency."], "limitations": "", "keywords": ["large language models", "harmful prompts", "multimodal detection", "classification", "AI safety"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23867", "pdf": "https://arxiv.org/pdf/2505.23867.pdf", "abs": "https://arxiv.org/abs/2505.23867", "title": "Infi-Med: Low-Resource Medical MLLMs with Robust Reasoning Evaluation", "authors": ["Zeyu Liu", "Zhitian Hou", "Yining Di", "Kejing Yang", "Zhijie Sang", "Congkai Xie", "Jingwen Yang", "Siyuan Liu", "Jialu Wang", "Chunming Li", "Ming Li", "Hongxia Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have demonstrated promising\nprospects in healthcare, particularly for addressing complex medical tasks,\nsupporting multidisciplinary treatment (MDT), and enabling personalized\nprecision medicine. However, their practical deployment faces critical\nchallenges in resource efficiency, diagnostic accuracy, clinical\nconsiderations, and ethical privacy. To address these limitations, we propose\nInfi-Med, a comprehensive framework for medical MLLMs that introduces three key\ninnovations: (1) a resource-efficient approach through curating and\nconstructing high-quality supervised fine-tuning (SFT) datasets with minimal\nsample requirements, with a forward-looking design that extends to both\npretraining and posttraining phases; (2) enhanced multimodal reasoning\ncapabilities for cross-modal integration and clinical task understanding; and\n(3) a systematic evaluation system that assesses model performance across\nmedical modalities and task types. Our experiments demonstrate that Infi-Med\nachieves state-of-the-art (SOTA) performance in general medical reasoning while\nmaintaining rapid adaptability to clinical scenarios. The framework establishes\na solid foundation for deploying MLLMs in real-world healthcare settings by\nbalancing model effectiveness with operational constraints.", "AI": {"tldr": "Infi-Med is a framework designed to improve the deployment of multimodal large language models in healthcare by enhancing resource efficiency, multimodal reasoning, and evaluation systems.", "motivation": "To address challenges in deploying multimodal large language models in healthcare, such as resource efficiency, diagnostic accuracy, and ethical considerations.", "method": "Infi-Med introduces a framework focusing on preparing high-quality supervised fine-tuning datasets, enhancing multimodal reasoning, and implementing a systematic evaluation system for medical tasks.", "result": "Infi-Med achieves state-of-the-art performance in medical reasoning and demonstrates adaptability to various clinical scenarios.", "conclusion": "The framework supports effective deployment of MLLMs in healthcare by addressing operational constraints without compromising effectiveness.", "key_contributions": ["Resource-efficient dataset curation method for supervised fine-tuning", "Enhanced multimodal reasoning for clinical task understanding", "Systematic evaluation system for diverse medical modalities"], "limitations": "", "keywords": ["multimodal large language models", "healthcare", "resource efficiency", "supervised fine-tuning", "clinical scenarios"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.23911", "pdf": "https://arxiv.org/pdf/2505.23911.pdf", "abs": "https://arxiv.org/abs/2505.23911", "title": "One Task Vector is not Enough: A Large-Scale Study for In-Context Learning", "authors": ["Pavel Tikhonov", "Ivan Oseledets", "Elena Tutubalina"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables Large Language Models (LLMs) to adapt to\nnew tasks using few examples, with task vectors - specific hidden state\nactivations - hypothesized to encode task information. Existing studies are\nlimited by small-scale benchmarks, restricting comprehensive analysis. We\nintroduce QuiteAFew, a novel dataset of 3,096 diverse few-shot tasks, each with\n30 input-output pairs derived from the Alpaca dataset. Experiments with\nLlama-3-8B on QuiteAFew reveal: (1) task vector performance peaks at an\nintermediate layer (e.g., 15th), (2) effectiveness varies significantly by task\ntype, and (3) complex tasks rely on multiple, subtask-specific vectors rather\nthan a single vector, suggesting distributed task knowledge representation.", "AI": {"tldr": "This paper presents QuiteAFew, a large dataset for benchmarking in-context learning in Large Language Models (LLMs) with 3,096 diverse few-shot tasks.", "motivation": "To address the limitations of existing studies on in-context learning (ICL) in LLMs, particularly the small-scale benchmarks which hinder comprehensive analysis.", "method": "The authors create and experiment with the QuiteAFew dataset consisting of 3,096 few-shot tasks, testing the performance of Llama-3-8B across various tasks.", "result": "Experiments showed that task vector performance peaked at an intermediate layer, effectiveness varied by task type, and complex tasks utilized multiple subtask-specific vectors, indicating a distributed representation of task knowledge.", "conclusion": "The findings suggest that ICL in LLMs has nuanced performance characteristics that depend on task complexity and require a better understanding of task vector usage.", "key_contributions": ["Introduction of the QuiteAFew dataset comprising 3,096 diverse few-shot tasks.", "Insights into the layered performance of task vectors in LLMs.", "Identification of the need for distributed representation of task-related knowledge."], "limitations": "The study focuses solely on the performance of Llama-3-8B, which may not generalize to other models.", "keywords": ["in-context learning", "large language models", "few-shot tasks", "task vector", "distributed representation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23912", "pdf": "https://arxiv.org/pdf/2505.23912.pdf", "abs": "https://arxiv.org/abs/2505.23912", "title": "Reinforcement Learning for Better Verbalized Confidence in Long-Form Generation", "authors": ["Caiqi Zhang", "Xiaochen Zhu", "Chengzu Li", "Nigel Collier", "Andreas Vlachos"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucination remains a major challenge for the safe and trustworthy\ndeployment of large language models (LLMs) in factual content generation. Prior\nwork has explored confidence estimation as an effective approach to\nhallucination detection, but often relies on post-hoc self-consistency methods\nthat require computationally expensive sampling. Verbalized confidence offers a\nmore efficient alternative, but existing approaches are largely limited to\nshort-form question answering (QA) tasks and do not generalize well to\nopen-ended generation. In this paper, we propose LoVeC (Long-form Verbalized\nConfidence), an on-the-fly verbalized confidence estimation method for\nlong-form generation. Specifically, we use reinforcement learning (RL) to train\nLLMs to append numerical confidence scores to each generated statement, serving\nas a direct and interpretable signal of the factuality of generation. Our\nexperiments consider both on-policy and off-policy RL methods, including DPO,\nORPO, and GRPO, to enhance the model calibration. We introduce two novel\nevaluation settings, free-form tagging and iterative tagging, to assess\ndifferent verbalized confidence estimation methods. Experiments on three\nlong-form QA datasets show that our RL-trained models achieve better\ncalibration and generalize robustly across domains. Also, our method is highly\nefficient, as it only requires adding a few tokens to the output being decoded.", "AI": {"tldr": "This paper presents LoVeC, a method for estimating verbalized confidence in long-form generation using reinforcement learning to enhance model calibration and factuality representation.", "motivation": "To address the challenge of hallucination in large language models (LLMs) during factual content generation, by providing an efficient estimation of confidence in generated outputs.", "method": "The authors propose LoVeC, which trains LLMs to append numerical confidence scores to generated statements through reinforcement learning, while introducing new evaluation settings to assess performance.", "result": "Experiments demonstrate that the RL-trained models using LoVeC achieve improved calibration and generalize effectively across diverse domains in long-form QA tasks.", "conclusion": "LoVeC provides an efficient and interpretable way to estimate confidence in long-form generation, enhancing the practical deployment of LLMs.", "key_contributions": ["Introduction of LoVeC for long-form verbalized confidence estimation.", "Utilization of reinforcement learning techniques for model calibration.", "Development of novel evaluation settings for confidence estimation methods."], "limitations": "", "keywords": ["large language models", "hallucination detection", "reinforcement learning", "long-form generation", "verbalized confidence"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23914", "pdf": "https://arxiv.org/pdf/2505.23914.pdf", "abs": "https://arxiv.org/abs/2505.23914", "title": "Probing Association Biases in LLM Moderation Over-Sensitivity", "authors": ["Yuxin Wang", "Botao Yu", "Ivory Yang", "Saeed Hassanpour", "Soroush Vosoughi"], "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large Language Models are widely used for content moderation but often\nmisclassify benign comments as toxic, leading to over-sensitivity. While\nprevious research attributes this issue primarily to the presence of offensive\nterms, we reveal a potential cause beyond token level: LLMs exhibit systematic\ntopic biases in their implicit associations. Inspired by cognitive psychology's\nimplicit association tests, we introduce Topic Association Analysis, a\nsemantic-level approach to quantify how LLMs associate certain topics with\ntoxicity. By prompting LLMs to generate free-form scenario imagination for\nmisclassified benign comments and analyzing their topic amplification levels,\nwe find that more advanced models (e.g., GPT-4 Turbo) demonstrate stronger\ntopic stereotype despite lower overall false positive rates. These biases\nsuggest that LLMs do not merely react to explicit, offensive language but rely\non learned topic associations, shaping their moderation decisions. Our findings\nhighlight the need for refinement beyond keyword-based filtering, providing\ninsights into the underlying mechanisms driving LLM over-sensitivity.", "AI": {"tldr": "This paper addresses the issue of Large Language Models (LLMs) misclassifying benign comments as toxic due to systematic topic biases, introducing a semantic-level approach to analyze these biases.", "motivation": "To explore the systematic topic biases in LLMs that contribute to their misclassification of benign comments as toxic, beyond just offensive language.", "method": "The authors introduce Topic Association Analysis, designed to quantify LLM topic associations with toxicity by prompting LLMs to generate scenarios for misclassified benign comments and analyzing the topic amplification levels.", "result": "More advanced models like GPT-4 Turbo show stronger topic stereotypes, indicating that LLMs' moderation decisions are influenced by learned topic associations rather than just offensive language, though they have lower overall false positive rates.", "conclusion": "Refinement of moderation techniques is necessary beyond traditional keyword filtering to address LLM over-sensitivity effectively, shedding light on the cognitive mechanisms involved.", "key_contributions": ["Introduction of Topic Association Analysis for LLMs", "Demonstration of systematic topic biases affecting LLM performance", "Insights into moderation technique refinements needed for LLMs"], "limitations": "", "keywords": ["Large Language Models", "toxicity", "topic biases", "implicit association", "content moderation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23923", "pdf": "https://arxiv.org/pdf/2505.23923.pdf", "abs": "https://arxiv.org/abs/2505.23923", "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced Role-Playing Language Agents", "authors": ["Feiteng Fang", "Ting-En Lin", "Yuchuan Wu", "Xiong Liu", "Xiang Huang", "Dingwei Chen", "Jing Ye", "Haonan Zhang", "Liang Zhu", "Hamid Alinejad-Rokny", "Min Yang", "Fei Huang", "Yongbin Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM.", "AI": {"tldr": "This paper presents ChARM, a Character-based Act-adaptive Reward Model for Role-Playing Language Agents (RPLAs) that improves human-computer interactions by leveraging a novel reward model and introducing a large-scale preference dataset.", "motivation": "To improve the scalability and adaptability of reward models in simulating characters for better human-computer interactions.", "method": "ChARM introduces an act-adaptive margin for enhanced learning efficiency and a self-evolution mechanism using large-scale unlabeled data, along with the creation of RoleplayPref, a new preference dataset for RPLAs.", "result": "ChARM shows a 13% improvement over the traditional Bradley-Terry model in preference rankings and achieves state-of-the-art results in preference learning.", "conclusion": "ChARM effectively enhances the training and evaluation of RPLAs, showing significant improvements in key performance metrics, with the resources made publicly available for further research.", "key_contributions": ["Introduction of act-adaptive margin for efficiency", "Development of self-evolution mechanism", "Creation of RoleplayPref dataset for RPLAs"], "limitations": "", "keywords": ["Role-Playing Language Agents", "Reward Model", "Character-based Learning", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.23931", "pdf": "https://arxiv.org/pdf/2505.23931.pdf", "abs": "https://arxiv.org/abs/2505.23931", "title": "Scaling up the think-aloud method", "authors": ["Daniel Wurgaft", "Ben Prystawski", "Kanishk Gandhi", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Noah D. Goodman"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 4 figures. Daniel Wurgaft and Ben Prystawski contributed\n  equally", "summary": "The think-aloud method, where participants voice their thoughts as they solve\na task, is a valuable source of rich data about human reasoning processes. Yet,\nit has declined in popularity in contemporary cognitive science, largely\nbecause labor-intensive transcription and annotation preclude large sample\nsizes. Here, we develop methods to automate the transcription and annotation of\nverbal reports of reasoning using natural language processing tools, allowing\nfor large-scale analysis of think-aloud data. In our study, 640 participants\nthought aloud while playing the Game of 24, a mathematical reasoning task. We\nautomatically transcribed the recordings and coded the transcripts as search\ngraphs, finding moderate inter-rater reliability with humans. We analyze these\ngraphs and characterize consistency and variation in human reasoning traces.\nOur work demonstrates the value of think-aloud data at scale and serves as a\nproof of concept for the automated analysis of verbal reports.", "AI": {"tldr": "This paper develops automated methods for transcribing and annotating think-aloud verbal reports, enabling large-scale analysis of human reasoning.", "motivation": "Reinvigorate the think-aloud method in cognitive science by overcoming the limitations of manual transcription and annotation.", "method": "The authors used natural language processing tools to automate transcription and annotation of think-aloud data from 640 participants engaged in a mathematical reasoning task.", "result": "Moderate inter-rater reliability with human coders was found, and analyses of search graphs revealed insights into human reasoning processes.", "conclusion": "Automated analysis of think-aloud data can significantly enhance the scalability of cognitive research and provide valuable insights into reasoning.", "key_contributions": ["Development of automated transcription and annotation methods for think-aloud data", "Application of methods to a large sample size", "Demonstration of value in analyzing human reasoning traces at scale."], "limitations": "The study is contingent on the quality of the NLP tools used for transcription and may not capture all nuances of human reasoning.", "keywords": ["think-aloud method", "natural language processing", "cognitive science", "human reasoning", "automated analysis"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.23932", "pdf": "https://arxiv.org/pdf/2505.23932.pdf", "abs": "https://arxiv.org/abs/2505.23932", "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "categories": ["cs.CL"], "comment": null, "summary": "We present SwingArena, a competitive evaluation framework for Large Language\nModels (LLMs) that closely mirrors real-world software development workflows.\nUnlike traditional static benchmarks, SwingArena models the collaborative\nprocess of software iteration by pairing LLMs as submitters, who generate\npatches, and reviewers, who create test cases and verify the patches through\ncontinuous integration (CI) pipelines. To support these interactive\nevaluations, we introduce a retrieval-augmented code generation (RACG) module\nthat efficiently handles long-context challenges by providing syntactically and\nsemantically relevant code snippets from large codebases, supporting multiple\nprogramming languages (C++, Python, Rust, and Go). This enables the framework\nto scale across diverse tasks and contexts while respecting token limitations.\nOur experiments, using over 400 high-quality real-world GitHub issues selected\nfrom a pool of 2,300 issues, show that models like GPT-4o excel at aggressive\npatch generation, whereas DeepSeek and Gemini prioritize correctness in CI\nvalidation. SwingArena presents a scalable and extensible methodology for\nevaluating LLMs in realistic, CI-driven software development settings. More\ndetails are available on our project page: swing-bench.github.io", "AI": {"tldr": "SwingArena is a framework for evaluating Large Language Models (LLMs) in software development environments by simulating real-world collaboration between code submitters and reviewers.", "motivation": "The need for a competitive evaluation framework that reflects actual software development workflows, moving beyond static benchmarks to dynamic interactions.", "method": "SwingArena pairs LLMs as submitters and reviewers in a CI pipeline, augmented with a retrieval module for relevant code snippets from large codebases in multiple programming languages.", "result": "Experiments on 400 real-world GitHub issues show that models like GPT-4o excel in patch generation, while others focus on correctness during CI validation.", "conclusion": "SwingArena offers a scalable and extensible methodology for evaluating LLMs within realistic software development contexts.", "key_contributions": ["Introduction of a competitive evaluation framework for LLMs in software development", "Models paired as submitters and reviewers for interactive evaluation", "Inclusion of a retrieval-augmented code generation module for long-context challenges"], "limitations": "", "keywords": ["Large Language Models", "Software Development", "Continuous Integration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.23944", "pdf": "https://arxiv.org/pdf/2505.23944.pdf", "abs": "https://arxiv.org/abs/2505.23944", "title": "Retrieval Augmented Generation based Large Language Models for Causality Mining", "authors": ["Thushara Manjari Naduvilakandy", "Hyeju Jang", "Mohammad Al Hasan"], "categories": ["cs.CL"], "comment": "13 pages, 6 figures, published in knowledgeNLP-NAACL2025", "summary": "Causality detection and mining are important tasks in information retrieval\ndue to their enormous use in information extraction, and knowledge graph\nconstruction. To solve these tasks, in existing literature there exist several\nsolutions -- both unsupervised and supervised. However, the unsupervised\nmethods suffer from poor performance and they often require significant human\nintervention for causal rule selection, leading to poor generalization across\ndifferent domains. On the other hand, supervised methods suffer from the lack\nof large training datasets. Recently, large language models (LLMs) with\neffective prompt engineering are found to be effective to overcome the issue of\nunavailability of large training dataset. Yet, in existing literature, there\ndoes not exist comprehensive works on causality detection and mining using LLM\nprompting. In this paper, we present several retrieval-augmented generation\n(RAG) based dynamic prompting schemes to enhance LLM performance in causality\ndetection and extraction tasks. Extensive experiments over three datasets and\nfive LLMs validate the superiority of our proposed RAG-based dynamic prompting\nover other static prompting schemes.", "AI": {"tldr": "The paper presents RAG-based dynamic prompting schemes to improve LLM performance in causality detection and extraction, validated across multiple datasets and models.", "motivation": "To address the limitations of existing unsupervised and supervised methods for causality detection and mining due to their reliance on human intervention and insufficient training data.", "method": "Introduces several retrieval-augmented generation (RAG) based dynamic prompting schemes for LLMs to enhance their performance in causality detection tasks.", "result": "Experiments show that the proposed RAG-based dynamic prompting improves performance over traditional static prompting methods across three datasets and five LLMs.", "conclusion": "RAG-based prompting emerges as a superior approach for causality detection in comparison to static methods, filling a gap in the existing literature on LLM applications for causality.", "key_contributions": ["Development of RAG-based dynamic prompting schemes for causality detection", "Validation of performance improvements through extensive experiments", "Addressing the gap of LLM application in causality mining"], "limitations": "The study is limited to the specific datasets and LLMs tested; applicability may vary with other models or datasets.", "keywords": ["Causality detection", "Large language models", "Retrieval-augmented generation", "Causality mining", "Dynamic prompting"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.23945", "pdf": "https://arxiv.org/pdf/2505.23945.pdf", "abs": "https://arxiv.org/abs/2505.23945", "title": "A Closer Look at Bias and Chain-of-Thought Faithfulness of Large (Vision) Language Models", "authors": ["Sriram Balasubramanian", "Samyadeep Basu", "Soheil Feizi"], "categories": ["cs.CL", "cs.AI", "I.2.10; I.2.7"], "comment": "34 pages, 25 figures", "summary": "Chain-of-thought (CoT) reasoning enhances performance of large language\nmodels, but questions remain about whether these reasoning traces faithfully\nreflect the internal processes of the model. We present the first comprehensive\nstudy of CoT faithfulness in large vision-language models (LVLMs),\ninvestigating how both text-based and previously unexplored image-based biases\naffect reasoning and bias articulation. Our work introduces a novel,\nfine-grained evaluation pipeline for categorizing bias articulation patterns,\nenabling significantly more precise analysis of CoT reasoning than previous\nmethods. This framework reveals critical distinctions in how models process and\nrespond to different types of biases, providing new insights into LVLM CoT\nfaithfulness. Our findings reveal that subtle image-based biases are rarely\narticulated compared to explicit text-based ones, even in models specialized\nfor reasoning. Additionally, many models exhibit a previously unidentified\nphenomenon we term ``inconsistent'' reasoning - correctly reasoning before\nabruptly changing answers, serving as a potential canary for detecting biased\nreasoning from unfaithful CoTs. We then apply the same evaluation pipeline to\nrevisit CoT faithfulness in LLMs across various levels of implicit cues. Our\nfindings reveal that current language-only reasoning models continue to\nstruggle with articulating cues that are not overtly stated.", "AI": {"tldr": "This paper investigates the faithfulness of chain-of-thought reasoning in large vision-language models and introduces a novel evaluation pipeline to analyze bias articulation patterns.", "motivation": "To examine if chain-of-thought reasoning accurately reflects the internal processes of large vision-language models and to explore the impact of text-based and image-based biases on reasoning.", "method": "A fine-grained evaluation pipeline was developed to categorize bias articulation patterns in large vision-language models, enabling a more precise analysis of chain-of-thought reasoning.", "result": "The study found that subtle image-based biases are rarely articulated compared to explicit text-based ones, and introduced the concept of 'inconsistent' reasoning, where models may abruptly change answers.", "conclusion": "The findings highlight significant differences in how models handle biases and indicate that current language-only models struggle to articulate implicit cues effectively.", "key_contributions": ["Introduced a novel evaluation pipeline for analyzing bias articulation in reasoning.", "Revealed critical distinctions between text-based and image-based biases in LVLMs.", "Identified the phenomenon of 'inconsistent' reasoning as a marker for biased reasoning."], "limitations": "Focused primarily on bias articulation and reasoning processes, may not cover other aspects of model performance.", "keywords": ["chain-of-thought reasoning", "bias articulation", "large vision-language models"], "importance_score": 6, "read_time_minutes": 45}}
{"id": "2505.23966", "pdf": "https://arxiv.org/pdf/2505.23966.pdf", "abs": "https://arxiv.org/abs/2505.23966", "title": "FLAT-LLM: Fine-grained Low-rank Activation Space Transformation for Large Language Model Compression", "authors": ["Jiayi Tian", "Ryan Solgi", "Jinming Lu", "Yifan Yang", "Hai Li", "Zheng Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have enabled remarkable progress in natural\nlanguage processing, yet their high computational and memory demands pose\nchallenges for deployment in resource-constrained environments. Although recent\nlow-rank decomposition methods offer a promising path for structural\ncompression, they often suffer from accuracy degradation, expensive calibration\nprocedures, and result in inefficient model architectures that hinder\nreal-world inference speedups. In this paper, we propose FLAT-LLM, a fast and\naccurate, training-free structural compression method based on fine-grained\nlow-rank transformations in the activation space. Specifically, we reduce the\nhidden dimension by transforming the weights using truncated eigenvectors\ncomputed via head-wise Principal Component Analysis (PCA), and employ an\nimportance-based metric to adaptively allocate ranks across decoders. FLAT-LLM\nachieves efficient and effective weight compression without recovery\nfine-tuning, which could complete the calibration within a few minutes.\nEvaluated across 4 models and 11 datasets, FLAT-LLM outperforms structural\npruning baselines in generalization and downstream performance, while\ndelivering inference speedups over decomposition-based methods.", "AI": {"tldr": "FLAT-LLM is a fast training-free method for compressing Large Language Models using low-rank transformations in the activation space, achieving high generalization performance without recovery fine-tuning.", "motivation": "The high computational and memory demands of Large Language Models (LLMs) hinder their deployment, especially in resource-constrained environments. Current low-rank methods often degrade accuracy and are slow to calibrate.", "method": "FLAT-LLM employs fine-grained low-rank transformations in the activation space, using PCA to compute truncated eigenvectors for weight transformation, and an importance-based metric for adaptive rank allocation across decoders.", "result": "FLAT-LLM significantly outperforms structural pruning baselines in generalization and downstream performance while providing faster inference than decomposition-based methods.", "conclusion": "This method offers a practical solution to compress LLMs effectively without the need for extensive fine-tuning, making it suitable for deployment in various environments.", "key_contributions": ["Introduction of FLAT-LLM for training-free structural compression of LLMs", "Utilization of PCA for efficient weight transformation", "Demonstrated improvements in efficiency and inference speed"], "limitations": "", "keywords": ["Large Language Models", "structural compression", "machine learning", "PCA", "inference speed"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23996", "pdf": "https://arxiv.org/pdf/2505.23996.pdf", "abs": "https://arxiv.org/abs/2505.23996", "title": "Is Your Model Fairly Certain? Uncertainty-Aware Fairness Evaluation for LLMs", "authors": ["Yinong Oliver Wang", "Nivedha Sivakumar", "Falaah Arif Khan", "Rin Metcalf Susa", "Adam Golinski", "Natalie Mackraz", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, 8 figures, and 1 table in main paper. Supplementary appendix\n  attached. Accepted at ICML 2025", "summary": "The recent rapid adoption of large language models (LLMs) highlights the\ncritical need for benchmarking their fairness. Conventional fairness metrics,\nwhich focus on discrete accuracy-based evaluations (i.e., prediction\ncorrectness), fail to capture the implicit impact of model uncertainty (e.g.,\nhigher model confidence about one group over another despite similar accuracy).\nTo address this limitation, we propose an uncertainty-aware fairness metric,\nUCerF, to enable a fine-grained evaluation of model fairness that is more\nreflective of the internal bias in model decisions compared to conventional\nfairness measures. Furthermore, observing data size, diversity, and clarity\nissues in current datasets, we introduce a new gender-occupation fairness\nevaluation dataset with 31,756 samples for co-reference resolution, offering a\nmore diverse and suitable dataset for evaluating modern LLMs. We establish a\nbenchmark, using our metric and dataset, and apply it to evaluate the behavior\nof ten open-source LLMs. For example, Mistral-7B exhibits suboptimal fairness\ndue to high confidence in incorrect predictions, a detail overlooked by\nEqualized Odds but captured by UCerF. Overall, our proposed LLM benchmark,\nwhich evaluates fairness with uncertainty awareness, paves the way for\ndeveloping more transparent and accountable AI systems.", "AI": {"tldr": "This paper introduces UCerF, an uncertainty-aware fairness metric for evaluating large language models (LLMs), alongside a new gender-occupation fairness evaluation dataset, aiming to improve the accountability of AI systems.", "motivation": "There is a growing need to benchmark the fairness of large language models (LLMs), as conventional metrics fail to account for model uncertainty and biases in decision-making.", "method": "The authors propose an uncertainty-aware fairness metric (UCerF) and introduce a new fairness evaluation dataset consisting of 31,756 samples for co-reference resolution.", "result": "The proposed UCerF metric provides a more nuanced understanding of model fairness, revealing biases overlooked by traditional metrics in evaluating ten open-source LLMs.", "conclusion": "The benchmark created using UCerF and the new dataset facilitates a more transparent evaluation of LLM fairness, promoting accountability in AI systems.", "key_contributions": ["Introduction of UCerF, an uncertainty-aware fairness metric for LLMs.", "Development of a new gender-occupation fairness evaluation dataset with 31,756 samples.", "Establishment of a benchmark for evaluating the behavior of open-source LLMs with respect to fairness."], "limitations": "", "keywords": ["fairness", "large language models", "uncertainty", "benchmarking", "gender-occupation dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24009", "pdf": "https://arxiv.org/pdf/2505.24009.pdf", "abs": "https://arxiv.org/abs/2505.24009", "title": "Diversity of Transformer Layers: One Aspect of Parameter Scaling Laws", "authors": ["Hidetaka Kamigaito", "Ying Zhang", "Jingun Kwon", "Katsuhiko Hayashi", "Manabu Okumura", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Transformers deliver outstanding performance across a wide range of tasks and\nare now a dominant backbone architecture for large language models (LLMs).\nTheir task-solving performance is improved by increasing parameter size, as\nshown in the recent studies on parameter scaling laws. Although recent\nmechanistic-interpretability studies have deepened our understanding of the\ninternal behavior of Transformers by analyzing their residual stream, the\nrelationship between these internal mechanisms and the parameter scaling laws\nremains unclear. To bridge this gap, we focus on layers and their size, which\nmainly decide the parameter size of Transformers. For this purpose, we first\ntheoretically investigate the layers within the residual stream through a\nbias-diversity decomposition. The decomposition separates (i) bias, the error\nof each layer's output from the ground truth, and (ii) diversity, which\nindicates how much the outputs of each layer differ from each other. Analyzing\nTransformers under this theory reveals that performance improves when\nindividual layers make predictions close to the correct answer and remain\nmutually diverse. We show that diversity becomes especially critical when\nindividual layers' outputs are far from the ground truth. Finally, we introduce\nan information-theoretic diversity and show our main findings that adding\nlayers enhances performance only when those layers behave differently, i.e.,\nare diverse. We also reveal the performance gains from increasing the number of\nlayers exhibit submodularity: marginal improvements diminish as additional\nlayers increase, mirroring the logarithmic convergence predicted by the\nparameter scaling laws. Experiments on multiple semantic-understanding tasks\nwith various LLMs empirically confirm the theoretical properties derived in\nthis study.", "AI": {"tldr": "This paper investigates how layers and their sizes in Transformers contribute to the performance of large language models, emphasizing the importance of diversity among layer outputs for improved accuracy.", "motivation": "To clarify the relationship between the internal mechanisms of Transformers, particularly the role of layer diversity, and the observed scaling laws regarding parameter size and model performance.", "method": "The paper employs a bias-diversity decomposition to analyze the outputs of each layer in the residual stream of Transformers, examining both the bias from the ground truth and the diversity of outputs between layers.", "result": "The analysis indicates that model performance improves with increased layer diversity, particularly when outputs deviate from the correct answers. Experiments confirm these properties across several tasks, showing that additional layers offer diminishing returns unless they contribute diverse predictions.", "conclusion": "Adding diverse layers enhances models' performance but with diminishing returns as the number of layers increases, aligning with parameter scaling laws.", "key_contributions": ["Introduced a bias-diversity decomposition for analyzing Transformer layers.", "Demonstrated that layer diversity is critical for improved model performance.", "Revealed that performance gains from additional layers follow submodular behavior."], "limitations": "The study primarily focuses on layer diversity without addressing other architectural features or training methods.", "keywords": ["Transformers", "large language models", "layer diversity", "bias-diversity decomposition", "parameter scaling laws"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.24012", "pdf": "https://arxiv.org/pdf/2505.24012.pdf", "abs": "https://arxiv.org/abs/2505.24012", "title": "Large Language Model Meets Constraint Propagation", "authors": ["Alexandre Bonlarron", "Florian Régin", "Elisabetta De Maria", "Jean-Charles Régin"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in the Proceedings of the Thirty-Fourth International Joint\n  Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Large Language Models (LLMs) excel at generating fluent text but struggle to\nenforce external constraints because they generate tokens sequentially without\nexplicit control mechanisms. GenCP addresses this limitation by combining LLM\npredictions with Constraint Programming (CP) reasoning, formulating text\ngeneration as a Constraint Satisfaction Problem (CSP). In this paper, we\nimprove GenCP by integrating Masked Language Models (MLMs) for domain\ngeneration, which allows bidirectional constraint propagation that leverages\nboth past and future tokens. This integration bridges the gap between\ntoken-level prediction and structured constraint enforcement, leading to more\nreliable and constraint-aware text generation. Our evaluation on COLLIE\nbenchmarks demonstrates that incorporating domain preview via MLM calls\nsignificantly improves GenCP's performance. Although this approach incurs\nadditional MLM calls and, in some cases, increased backtracking, the overall\neffect is a more efficient use of LLM inferences and an enhanced ability to\ngenerate feasible and meaningful solutions, particularly in tasks with strict\ncontent constraints.", "AI": {"tldr": "This paper presents GenCP, an improved method for text generation that combines Large Language Models (LLMs) with Constraint Programming (CP) to enforce external constraints more effectively.", "motivation": "The motivation is to address the limitations of LLMs in generating constrained text due to their sequential token generation without explicit control mechanisms.", "method": "The methodology involves integrating Masked Language Models (MLMs) with GenCP to enhance domain generation and facilitate bidirectional constraint propagation in text generation tasks.", "result": "The evaluation demonstrates that the integration of MLMs significantly improves GenCP's performance on the COLLIE benchmarks, enabling better constraint adherence and more reliable text generation.", "conclusion": "The study concludes that while the approach requires additional MLM calls and may increase backtracking in some instances, it leads to a more efficient generation process and improves the generation of feasible and meaningful outputs under strict content constraints.", "key_contributions": ["Integration of MLMs with GenCP for better constraint enforcement", "Bidirectional constraint propagation enhancing LLM predictions", "Improved performance on tasks requiring strict content constraints"], "limitations": "Increased computation due to additional MLM calls and potential for increased backtracking.", "keywords": ["Large Language Models", "Constraint Programming", "Masked Language Models", "Text Generation", "Constraint Satisfaction Problem"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24016", "pdf": "https://arxiv.org/pdf/2505.24016.pdf", "abs": "https://arxiv.org/abs/2505.24016", "title": "BeaverTalk: Oregon State University's IWSLT 2025 Simultaneous Speech Translation System", "authors": ["Matthew Raffel", "Victor Agostinelli", "Lizhong Chen"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at IWSLT 2025", "summary": "This paper discusses the construction, fine-tuning, and deployment of\nBeaverTalk, a cascaded system for speech-to-text translation as part of the\nIWSLT 2025 simultaneous translation task. The system architecture employs a VAD\nsegmenter for breaking a speech stream into segments, Whisper Large V2 for\nautomatic speech recognition (ASR), and Gemma 3 12B for simultaneous\ntranslation. Regarding the simultaneous translation LLM, it is fine-tuned via\nlow-rank adaptors (LoRAs) for a conversational prompting strategy that\nleverages a single prior-sentence memory bank from the source language as\ncontext. The cascaded system participated in the English$\\rightarrow$German and\nEnglish$\\rightarrow$Chinese language directions for both the low and high\nlatency regimes. In particular, on the English$\\rightarrow$German task, the\nsystem achieves a BLEU of 24.64 and 27.83 at a StreamLAAL of 1837.86 and\n3343.73, respectively. Then, on the English$\\rightarrow$Chinese task, the\nsystem achieves a BLEU of 34.07 and 37.23 at a StreamLAAL of 2216.99 and\n3521.35, respectively.", "AI": {"tldr": "This paper presents BeaverTalk, a system for speech-to-text translation using advanced ASR and LLM for simultaneous translation.", "motivation": "To improve real-time speech-to-text translation for the IWSLT 2025 simultaneous translation task.", "method": "The system integrates VAD for segmenting speech, utilizes Whisper Large V2 for ASR, and fine-tunes Gemma 3 12B for translation using LoRAs with conversational prompting.", "result": "The system achieved BLEU scores of 24.64 (low latency) and 27.83 (high latency) for English to German, and 34.07 (low latency) and 37.23 (high latency) for English to Chinese.", "conclusion": "BeaverTalk demonstrates effective performance in simultaneous speech translation with notable BLEU scores in both language pairs.", "key_contributions": ["Introduction of a cascaded system architecture combining VAD, ASR, and LLM for speech translation", "Utilization of fine-tuning techniques with LoRAs for performance improvement", "Contribution to simultaneous translation tasks at IWSLT 2025."], "limitations": "", "keywords": ["speech-to-text", "simultaneous translation", "ASR", "LLM", "IWSLT 2025"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.24028", "pdf": "https://arxiv.org/pdf/2505.24028.pdf", "abs": "https://arxiv.org/abs/2505.24028", "title": "Hidden Persuasion: Detecting Manipulative Narratives on Social Media During the 2022 Russian Invasion of Ukraine", "authors": ["Kateryna Akhynko", "Oleksandr Kosovan", "Mykola Trokhymovych"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents one of the top-performing solutions to the UNLP 2025\nShared Task on Detecting Manipulation in Social Media. The task focuses on\ndetecting and classifying rhetorical and stylistic manipulation techniques used\nto influence Ukrainian Telegram users. For the classification subtask, we\nfine-tuned the Gemma 2 language model with LoRA adapters and applied a\nsecond-level classifier leveraging meta-features and threshold optimization.\nFor span detection, we employed an XLM-RoBERTa model trained for multi-target,\nincluding token binary classification. Our approach achieved 2nd place in\nclassification and 3rd place in span detection.", "AI": {"tldr": "This paper presents a top-performing solution for detecting manipulation in social media, specifically aimed at Ukrainian Telegram users.", "motivation": "The study addresses the need to recognize rhetorical and stylistic manipulation techniques in social media, which can influence user opinion.", "method": "The researchers fine-tuned the Gemma 2 language model with LoRA adapters for classification and employed an XLM-RoBERTa model for multi-target span detection and optimization strategies.", "result": "The approach achieved 2nd place in the classification subtask and 3rd place in span detection at the UNLP 2025 Shared Task.", "conclusion": "The results indicate the effectiveness of the proposed methods in detecting manipulation techniques in social media content.", "key_contributions": ["Fine-tuning Gemma 2 with LoRA for enhanced classification.", "Implementation of a meta-feature-based second-level classifier.", "Utilization of XLM-RoBERTa for multi-target span detection."], "limitations": "", "keywords": ["manipulation detection", "social media", "language model", "XLM-RoBERTa", "classification"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24033", "pdf": "https://arxiv.org/pdf/2505.24033.pdf", "abs": "https://arxiv.org/abs/2505.24033", "title": "The Surprising Soupability of Documents in State Space Models", "authors": ["Yasaman Jafari", "Zixian Wang", "Leon Bergen", "Taylor Berg-Kirkpatrick"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": null, "summary": "We investigate whether hidden states from Structured State Space Models\n(SSMs) can be merged post-hoc to support downstream reasoning. Inspired by\nmodel souping, we propose a strategy where documents are encoded independently\nand their representations are pooled -- via simple operations like averaging --\ninto a single context state. This approach, which we call document souping,\nenables modular encoding and reuse without reprocessing the full input for each\nquery. We finetune Mamba2 models to produce soupable representations and find\nthat they support multi-hop QA, sparse retrieval, and long-document reasoning\nwith strong accuracy. On HotpotQA, souping ten independently encoded documents\nnearly matches the performance of a cross-encoder trained on the same inputs.", "AI": {"tldr": "The paper explores merging hidden states from Structured State Space Models to enhance downstream reasoning through a technique called document souping.", "motivation": "To improve downstream reasoning tasks by enabling modular encoding and reuse of document representations without full input reprocessing.", "method": "Documents are encoded independently and their representations are pooled using simple operations, such as averaging, to create a single context state.", "result": "Finetuning Mamba2 models yields soupable representations that perform well in multi-hop QA, sparse retrieval, and long-document reasoning, achieving nearly cross-encoder level performance on HotpotQA.", "conclusion": "Document souping allows for effective multi-document reasoning while maintaining efficiency in encoding.", "key_contributions": ["Introduction of document souping technique for modular encoding", "Demonstration of effectiveness in multi-hop QA and long-document reasoning", "Comparison with cross-encoder models showing competitive performance."], "limitations": "", "keywords": ["Structured State Space Models", "document souping", "multi-hop QA", "sparse retrieval", "long-document reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24040", "pdf": "https://arxiv.org/pdf/2505.24040.pdf", "abs": "https://arxiv.org/abs/2505.24040", "title": "MedPAIR: Measuring Physicians and AI Relevance Alignment in Medical Question Answering", "authors": ["Yuexing Hao", "Kumail Alhamoud", "Hyewon Jeong", "Haoran Zhang", "Isha Puri", "Philip Torr", "Mike Schaekermann", "Ariel D. Stern", "Marzyeh Ghassemi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable performance on\nvarious medical question-answering (QA) benchmarks, including standardized\nmedical exams. However, correct answers alone do not ensure correct logic, and\nmodels may reach accurate conclusions through flawed processes. In this study,\nwe introduce the MedPAIR (Medical Dataset Comparing Physicians and AI Relevance\nEstimation and Question Answering) dataset to evaluate how physician trainees\nand LLMs prioritize relevant information when answering QA questions. We obtain\nannotations on 1,300 QA pairs from 36 physician trainees, labeling each\nsentence within the question components for relevance. We compare these\nrelevance estimates to those for LLMs, and further evaluate the impact of these\n\"relevant\" subsets on downstream task performance for both physician trainees\nand LLMs. We find that LLMs are frequently not aligned with the content\nrelevance estimates of physician trainees. After filtering out physician\ntrainee-labeled irrelevant sentences, accuracy improves for both the trainees\nand the LLMs. All LLM and physician trainee-labeled data are available at:\nhttp://medpair.csail.mit.edu/.", "AI": {"tldr": "This study introduces the MedPAIR dataset to evaluate how physician trainees and Large Language Models (LLMs) prioritize relevant information in medical QA, finding misalignment between LLMs and trainee relevance estimates, which can be improved by filtering out irrelevant information.", "motivation": "To assess the reasoning logic of LLMs in medical question answering and compare it with that of physician trainees, addressing the issue of LLMs providing correct answers through flawed processes.", "method": "We created the MedPAIR dataset with annotations on 1,300 QA pairs from 36 physician trainees, labeling each sentence for relevance, and compared these relevance estimates to those generated by LLMs.", "result": "The alignment between LLMs and the relevance estimates from physician trainees was found to be poor; filtering irrelevant sentences improved the accuracy for both physician trainees and LLMs.", "conclusion": "The study highlights the misalignment between LLMs and human reasoning in medical QA and suggests that leveraging human-relevant information can enhance model performance.", "key_contributions": ["Introduction of the MedPAIR dataset for evaluating relevance in medical QA", "Comparison of relevance estimation between physician trainees and LLMs", "Demonstration of improved accuracy when irrelevant information is filtered out."], "limitations": "", "keywords": ["Medical AI", "Large Language Models", "Question Answering", "Human-Computer Interaction", "Relevance Estimation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24063", "pdf": "https://arxiv.org/pdf/2505.24063.pdf", "abs": "https://arxiv.org/abs/2505.24063", "title": "TCM-Ladder: A Benchmark for Multimodal Question Answering on Traditional Chinese Medicine", "authors": ["Jiacheng Xie", "Yang Yu", "Ziyang Zhang", "Shuai Zeng", "Jiaxuan He", "Ayush Vasireddy", "Xiaoting Tang", "Congyu Guo", "Lening Zhao", "Congcong Jing", "Guanghui An", "Dong Xu"], "categories": ["cs.CL", "cs.DB"], "comment": "22 pages, 4 figures", "summary": "Traditional Chinese Medicine (TCM), as an effective alternative medicine, has\nbeen receiving increasing attention. In recent years, the rapid development of\nlarge language models (LLMs) tailored for TCM has underscored the need for an\nobjective and comprehensive evaluation framework to assess their performance on\nreal-world tasks. However, existing evaluation datasets are limited in scope\nand primarily text-based, lacking a unified and standardized multimodal\nquestion-answering (QA) benchmark. To address this issue, we introduce\nTCM-Ladder, the first multimodal QA dataset specifically designed for\nevaluating large TCM language models. The dataset spans multiple core\ndisciplines of TCM, including fundamental theory, diagnostics, herbal formulas,\ninternal medicine, surgery, pharmacognosy, and pediatrics. In addition to\ntextual content, TCM-Ladder incorporates various modalities such as images and\nvideos. The datasets were constructed using a combination of automated and\nmanual filtering processes and comprise 52,000+ questions in total. These\nquestions include single-choice, multiple-choice, fill-in-the-blank, diagnostic\ndialogue, and visual comprehension tasks. We trained a reasoning model on\nTCM-Ladder and conducted comparative experiments against 9 state-of-the-art\ngeneral domain and 5 leading TCM-specific LLMs to evaluate their performance on\nthe datasets. Moreover, we propose Ladder-Score, an evaluation method\nspecifically designed for TCM question answering that effectively assesses\nanswer quality regarding terminology usage and semantic expression. To our\nknowledge, this is the first work to evaluate mainstream general domain and\nTCM-specific LLMs on a unified multimodal benchmark. The datasets and\nleaderboard are publicly available at https://tcmladder.com or\nhttps://54.211.107.106 and will be continuously updated.", "AI": {"tldr": "Introducing TCM-Ladder, a multimodal QA dataset for evaluating TCM-specific language models, featuring over 52,000 questions across various disciplines.", "motivation": "The growing importance of TCM and the lack of a standardized multimodal QA benchmark for evaluating large language models in this domain.", "method": "The dataset, TCM-Ladder, includes 52,000+ questions across TCM disciplines, using automated and manual filtering, along with a newly proposed evaluation method, Ladder-Score.", "result": "Comparative experiments showed performance assessments of TCM and general domain LLMs using the multimodal dataset.", "conclusion": "TCM-Ladder serves as a crucial resource for evaluating TCM language models and includes a continuously updated leaderboard.", "key_contributions": ["First multimodal QA dataset for TCM language models", "Introduction of Ladder-Score for TCM question answering evaluation", "Public availability of datasets and leaderboard for ongoing research"], "limitations": "", "keywords": ["Traditional Chinese Medicine", "multimodal QA", "large language models", "dataset", "evaluation framework"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.24098", "pdf": "https://arxiv.org/pdf/2505.24098.pdf", "abs": "https://arxiv.org/abs/2505.24098", "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding", "authors": ["Zhongmou He", "Yee Man Choi", "Kexun Zhang", "Jiabao Ji", "Junting Zhou", "Dejia Xu", "Ivan Bercovich", "Aidan Zhang", "Lei Li"], "categories": ["cs.CL"], "comment": null, "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.", "AI": {"tldr": "HARDTESTGEN is a pipeline developed for synthesizing high-quality tests using LLMs, significantly improving test precision and recall for evaluating LLM-generated code with a new dataset of 47k problems.", "motivation": "Reliable verifiers are crucial for LLM reasoning but are hard to obtain for complex coding problems. Existing solutions often fail to catch subtle errors that only human-written edge cases can identify.", "method": "HARDTESTGEN synthesizes tests for coding problems using LLMs, resulting in a new dataset containing 47,000 problems and corresponding high-quality tests.", "result": "HARDTESTGEN tests outperform existing tests with an 11.3 percentage point increase in precision and a 17.5 percentage point increase in recall for LLM-generated code evaluation. For more difficult problems, precision improvements can reach up to 40 points.", "conclusion": "The HARDTESTS dataset and synthesis pipeline enhance model training effectiveness and performance in code generation tasks.", "key_contributions": ["Introduction of HARDTESTGEN for high-quality test synthesis using LLMs", "Creation of a comprehensive competitive programming dataset HARDTESTS", "Significant improvements in precision and recall for evaluating LLM-generated code"], "limitations": "", "keywords": ["test synthesis", "large language models", "competitive programming", "code generation", "evaluation metrics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.24105", "pdf": "https://arxiv.org/pdf/2505.24105.pdf", "abs": "https://arxiv.org/abs/2505.24105", "title": "Training LLMs for EHR-Based Reasoning Tasks via Reinforcement Learning", "authors": ["Jiacheng Lin", "Zhenbang Wu", "Jimeng Sun"], "categories": ["cs.CL"], "comment": null, "summary": "We present EHRMIND, a practical recipe for adapting large language models\n(LLMs) to complex clinical reasoning tasks using reinforcement learning with\nverifiable rewards (RLVR). While RLVR has succeeded in mathematics and coding,\nits application to healthcare contexts presents unique challenges due to the\nspecialized knowledge and reasoning required for electronic health record (EHR)\ninterpretation. Our pilot study on the MEDCALC benchmark reveals two key\nfailure modes: (1) misapplied knowledge, where models possess relevant medical\nknowledge but apply it incorrectly, and (2) missing knowledge, where models\nlack essential domain knowledge. To address these cases, EHRMIND applies a\ntwo-stage solution: a lightweight supervised fine-tuning (SFT) warm-up that\ninjects missing domain knowledge, stabilizes subsequent training, and\nencourages structured, interpretable outputs; followed by RLVR, which\nreinforces outcome correctness and refines the model's decision-making. We\ndemonstrate the effectiveness of our method across diverse clinical\napplications, including medical calculations (MEDCALC), patient-trial matching\n(TREC CLINICAL TRIALS), and disease diagnosis (EHRSHOT). EHRMIND delivers\nconsistent gains in accuracy, interpretability, and cross-task generalization.\nThese findings offer practical guidance for applying RLVR to enhance LLM\ncapabilities in healthcare settings.", "AI": {"tldr": "EHRMIND is a framework for adapting large language models to clinical reasoning tasks by using reinforcement learning with verifiable rewards, addressing challenges of knowledge application in healthcare.", "motivation": "To enhance large language models' performance in interpreting electronic health records by addressing the unique challenges associated with clinical reasoning in healthcare.", "method": "The study employs a two-stage solution: first, a supervised fine-tuning (SFT) phase to inject missing knowledge, followed by reinforcement learning with verifiable rewards (RLVR) to refine decision-making and improve outcomes.", "result": "EHRMIND shows consistent improvements in accuracy, interpretability, and generalization across various clinical tasks including medical calculations, patient-trial matching, and disease diagnosis.", "conclusion": "The findings indicate that applying RLVR can significantly enhance the capabilities of large language models in healthcare contexts.", "key_contributions": ["Introduction of EHRMIND framework for healthcare LLM adaptation", "Identification of key failure modes in EHR interpretation", "Empirical validation of the approach through diverse clinical applications"], "limitations": "The pilot study's findings may not account for all healthcare contexts or cover the full spectrum of challenges in EHR interpretation.", "keywords": ["large language models", "healthcare", "reinforcement learning", "clinical reasoning", "EHR interpretation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24119", "pdf": "https://arxiv.org/pdf/2505.24119.pdf", "abs": "https://arxiv.org/abs/2505.24119", "title": "The State of Multilingual LLM Safety Research: From Measuring the Language Gap to Mitigating It", "authors": ["Zheng-Xin Yong", "Beyza Ermis", "Marzieh Fadaee", "Stephen H. Bach", "Julia Kreutzer"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations.", "AI": {"tldr": "Analysis of linguistic diversity in LLM safety research reveals an English-centric focus and a significant language gap.", "motivation": "To address the lack of focus on non-English languages in LLM safety research and to promote inclusivity in AI safety practices.", "method": "Systematic review of nearly 300 publications from major NLP conferences and workshops (2020-2024), analyzing the representation of different languages.", "result": "Identification of a growing language gap in LLM safety research, with recommendations for future research directions in multilingual safety, including safety evaluation and training data generation.", "conclusion": "The field can develop more inclusive AI safety practices by broadening the focus beyond English-centric research.", "key_contributions": ["Highlights the English-centric nature of LLM safety research.", "Identifies the minimal attention given to high-resource non-English languages.", "Proposes future research directions for multilingual safety in LLMs."], "limitations": "Limited to publications from specific NLP conferences and workshops, possibly affecting the generalizability of findings.", "keywords": ["LLM safety", "linguistic diversity", "multilingual safety", "NLP research", "AI safety practices"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24133", "pdf": "https://arxiv.org/pdf/2505.24133.pdf", "abs": "https://arxiv.org/abs/2505.24133", "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration", "authors": ["Zefan Cai", "Wen Xiao", "Hanshi Sun", "Cheng Luo", "Yikai Zhang", "Ke Wan", "Yucheng Li", "Yeyang Zhou", "Li-Wen Chang", "Jiuxiang Gu", "Zhen Dong", "Anima Anandkumar", "Abedelkadir Asi", "Junjie Hu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning models have demonstrated impressive performance in self-reflection\nand chain-of-thought reasoning. However, they often produce excessively long\noutputs, leading to prohibitively large key-value (KV) caches during inference.\nWhile chain-of-thought inference significantly improves performance on complex\nreasoning tasks, it can also lead to reasoning failures when deployed with\nexisting KV cache compression approaches. To address this, we propose\nRedundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel\nmethod specifically targeting redundant tokens in reasoning models. Our method\npreserves nearly 100% of the full KV cache performance using only 10% of the KV\ncache, substantially outperforming existing KV cache baselines, which reach\nonly 60% of the performance. Remarkably, R-KV even achieves 105% of full KV\ncache performance with 16% of the KV cache. This KV-cache reduction also leads\nto a 90% memory saving and a 6.6X throughput over standard chain-of-thought\nreasoning inference. Experimental results show that R-KV consistently\noutperforms existing KV cache compression baselines across two mathematical\nreasoning datasets.", "AI": {"tldr": "This paper proposes R-KV, a new method for compressing key-value caches in reasoning models, significantly improving performance and memory efficiency during inference.", "motivation": "Reasoning models exhibit impressive capabilities but generate excessively long outputs that complicate inference due to large key-value caches. The paper seeks to address the inefficiencies of current compression methods during complex reasoning tasks.", "method": "The proposed method, R-KV, targets redundant tokens in the KV cache of reasoning models to improve performance while maintaining a minimal cache size.", "result": "R-KV maintains nearly 100% of full KV cache performance using only 10% of the cache and can achieve 105% performance with 16% cache, resulting in 90% memory savings and 6.6X throughput improvements over standard chain-of-thought reasoning.", "conclusion": "The experimental results indicate that R-KV consistently outperforms existing KV cache compression methods across multiple reasoning datasets, providing a significant advancement in inference efficiency.", "key_contributions": ["Introduction of redundancy-aware KV cache compression for reasoning models", "Maintaining high performance with significantly reduced cache size", "90% memory savings and improved throughput during inference"], "limitations": "", "keywords": ["KV cache", "Compression", "Reasoning models", "Performance optimization", "Machine learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.24143", "pdf": "https://arxiv.org/pdf/2505.24143.pdf", "abs": "https://arxiv.org/abs/2505.24143", "title": "CrossICL: Cross-Task In-Context Learning via Unsupervised Demonstration Transfer", "authors": ["Jinglong Gao", "Xiao Ding", "Lingxiao Zou", "Bing Qin", "Ting Liu"], "categories": ["cs.CL"], "comment": "9 pages", "summary": "In-Context Learning (ICL) enhances the performance of large language models\n(LLMs) with demonstrations. However, obtaining these demonstrations primarily\nrelies on manual effort. In most real-world scenarios, users are often\nunwilling or unable to provide such demonstrations. Inspired by the human\nanalogy, we explore a new ICL paradigm CrossICL to study how to utilize\nexisting source task demonstrations in the ICL for target tasks, thereby\nobtaining reliable guidance without any additional manual effort. To explore\nthis, we first design a two-stage alignment strategy to mitigate the\ninterference caused by gaps across tasks, as the foundation for our\nexperimental exploration. Based on it, we conduct comprehensive exploration of\nCrossICL, with 875 NLP tasks from the Super-NI benchmark and six types of LLMs,\nincluding GPT-4o. Experimental results demonstrate the effectiveness of\nCrossICL and provide valuable insights on questions like the criteria for\nselecting cross-task demonstrations, as well as the types of task-gap-induced\ninterference in CrossICL.", "AI": {"tldr": "This paper introduces CrossICL, a new paradigm for In-Context Learning that leverages existing demonstrations from source tasks to improve performance on target tasks without additional effort from users.", "motivation": "The reliance on manual demonstration collection in In-Context Learning often limits its application, especially when users cannot provide these demonstrations.", "method": "The paper proposes a two-stage alignment strategy to align tasks and reduce interference caused by task gaps, and conducts extensive experiments on 875 NLP tasks using six LLMs, including GPT-4o.", "result": "Experimental results validate the effectiveness of CrossICL, providing insights into criteria for demonstration selection and types of interference arising from task gaps.", "conclusion": "CrossICL offers a novel approach to improve In-Context Learning by utilizing existing demonstrations, thus reducing the burden on users while enhancing model performance.", "key_contributions": ["Introduction of CrossICL, a novel paradigm for ICL", "Development of a two-stage alignment strategy", "Empirical exploration across a large benchmark of NLP tasks"], "limitations": "", "keywords": ["In-Context Learning", "large language models", "task alignment", "demonstration selection", "NLP tasks"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2505.24147", "pdf": "https://arxiv.org/pdf/2505.24147.pdf", "abs": "https://arxiv.org/abs/2505.24147", "title": "Rationales Are Not Silver Bullets: Measuring the Impact of Rationales on Model Performance and Reliability", "authors": ["Chiwei Zhu", "Benfeng Xu", "An Yang", "Junyang Lin", "Quan Wang", "Chang Zhou", "Zhendong Mao"], "categories": ["cs.CL"], "comment": "To be published in ACL 2025 Findings. (Work originally done in Jan\n  2024)", "summary": "Training language models with rationales augmentation has been shown to be\nbeneficial in many existing works. In this paper, we identify that such a\nprevailing view does not hold consistently. We conduct comprehensive\ninvestigations to thoroughly inspect the impact of rationales on model\nperformance as well as a novel perspective of model reliability. The results\nlead to several key findings that add new insights upon existing\nunderstandings: 1) Rationales can, at times, deteriorate model performance; 2)\nRationales can, at times, improve model reliability, even outperforming their\nuntrained counterparts; 3) A linear correspondence exists in between the\nperformance and reliability improvements, while both are driven by the\nintrinsic difficulty of the task. These findings provide informative\nregulations on the broad utilization of rationales and raise critical\nimplications on the procedure of explicitly aligning language models with\nimplicit human thoughts. Codes can be found at\nhttps://github.com/Ignoramus0817/rationales.", "AI": {"tldr": "This paper investigates the effects of rationales on language model performance and reliability, providing new insights and implications for their use in alignment with human cognition.", "motivation": "To evaluate the inconsistent effects of rationales on language models, particularly in performance and reliability, contradicting the prevailing view that rationales always enhance model quality.", "method": "Comprehensive investigations and empirical analyses to assess the impact of rationales on performance and reliability of language models across various tasks.", "result": "Key findings include that rationales can sometimes worsen performance but improve reliability, with a linear relationship between performance and reliability improvements linked to task difficulty.", "conclusion": "The findings challenge existing beliefs about the benefits of rationales in language models and suggest new regulations for their effective application.", "key_contributions": ["Deterioration of model performance in certain scenarios with rationales", "Improved model reliability surpassing untrained counterparts", "Identification of a linear relationship between performance and reliability improvements"], "limitations": "The studies are task-dependent, and further exploration is needed to generalize the findings across all contexts.", "keywords": ["Rationales", "Language Models", "Model Reliability", "Performance", "Human Alignment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24163", "pdf": "https://arxiv.org/pdf/2505.24163.pdf", "abs": "https://arxiv.org/abs/2505.24163", "title": "LKD-KGC: Domain-Specific KG Construction via LLM-driven Knowledge Dependency Parsing", "authors": ["Jiaqi Sun", "Shiyou Qian", "Zhangchi Han", "Wei Li", "Zelin Qian", "Dingyu Yang", "Jian Cao", "Guangtao Xue"], "categories": ["cs.CL", "cs.AI"], "comment": "Submitting to EDBT 2026", "summary": "Knowledge Graphs (KGs) structure real-world entities and their relationships\ninto triples, enhancing machine reasoning for various tasks. While\ndomain-specific KGs offer substantial benefits, their manual construction is\noften inefficient and requires specialized knowledge. Recent approaches for\nknowledge graph construction (KGC) based on large language models (LLMs), such\nas schema-guided KGC and reference knowledge integration, have proven\nefficient. However, these methods are constrained by their reliance on manually\ndefined schema, single-document processing, and public-domain references,\nmaking them less effective for domain-specific corpora that exhibit complex\nknowledge dependencies and specificity, as well as limited reference knowledge.\nTo address these challenges, we propose LKD-KGC, a novel framework for\nunsupervised domain-specific KG construction. LKD-KGC autonomously analyzes\ndocument repositories to infer knowledge dependencies, determines optimal\nprocessing sequences via LLM driven prioritization, and autoregressively\ngenerates entity schema by integrating hierarchical inter-document contexts.\nThis schema guides the unsupervised extraction of entities and relationships,\neliminating reliance on predefined structures or external knowledge. Extensive\nexperiments show that compared with state-of-the-art baselines, LKD-KGC\ngenerally achieves improvements of 10% to 20% in both precision and recall\nrate, demonstrating its potential in constructing high-quality domain-specific\nKGs.", "AI": {"tldr": "A novel framework called LKD-KGC for unsupervised construction of domain-specific Knowledge Graphs using LLMs is proposed, addressing challenges in existing methods by eliminating reliance on predefined schemas and enhancing precision and recall rates.", "motivation": "To improve the efficiency of domain-specific Knowledge Graph construction by addressing limitations of existing methods that rely on manually defined schemas and single-document processing.", "method": "LKD-KGC autonomously analyzes document repositories, infers knowledge dependencies, prioritizes processing sequences using LLMs, and generates entity schemas autoregressively without predefined structures.", "result": "LKD-KGC shows improvements of 10% to 20% in precision and recall rates compared to state-of-the-art baseline methods in the construction of domain-specific Knowledge Graphs.", "conclusion": "The proposed LKD-KGC framework is effective for high-quality Knowledge Graph construction, demonstrating significant advancements over current techniques.", "key_contributions": ["Introduction of LKD-KGC framework for unsupervised domain-specific KG construction", "Autonomous inference of knowledge dependencies from document repositories", "Improved precision and recall in entity and relationship extraction for complex knowledge domains."], "limitations": "", "keywords": ["Knowledge Graphs", "Large Language Models", "Domain-specific Construction", "Knowledge Extraction", "Unsupervised Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24164", "pdf": "https://arxiv.org/pdf/2505.24164.pdf", "abs": "https://arxiv.org/abs/2505.24164", "title": "Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models", "authors": ["Shilin Xu", "Yanwei Li", "Rui Yang", "Tao Zhang", "Yueyi Sun", "Wei Chow", "Linfeng Li", "Hang Song", "Qi Xu", "Yunhai Tong", "Xiangtai Li", "Hao Fei"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Recent works on large language models (LLMs) have successfully demonstrated\nthe emergence of reasoning capabilities via reinforcement learning (RL).\nAlthough recent efforts leverage group relative policy optimization (GRPO) for\nMLLMs post-training, they constantly explore one specific aspect, such as\ngrounding tasks, math problems, or chart analysis. There are no works that can\nleverage multi-source MLLM tasks for stable reinforcement learning. In this\nwork, we present a unified perspective to solve this problem. We present\nMixed-R1, a unified yet straightforward framework that contains a mixed reward\nfunction design (Mixed-Reward) and a mixed post-training dataset (Mixed-45K).\nWe first design a data engine to select high-quality examples to build the\nMixed-45K post-training dataset. Then, we present a Mixed-Reward design, which\ncontains various reward functions for various MLLM tasks. In particular, it has\nfour different reward functions: matching reward for binary answer or\nmultiple-choice problems, chart reward for chart-aware datasets, IoU reward for\ngrounding problems, and open-ended reward for long-form text responses such as\ncaption datasets. To handle the various long-form text content, we propose a\nnew open-ended reward named Bidirectional Max-Average Similarity (BMAS) by\nleveraging tokenizer embedding matching between the generated response and the\nground truth. Extensive experiments show the effectiveness of our proposed\nmethod on various MLLMs, including Qwen2.5-VL and Intern-VL on various sizes.\nOur dataset and model are available at https://github.com/xushilin1/mixed-r1.", "AI": {"tldr": "This paper proposes Mixed-R1, a framework for stable reinforcement learning in multi-source large language models (MLLMs) using a mixed reward function and a curated post-training dataset.", "motivation": "To address the lack of works that leverage multi-source MLLM tasks for stable reinforcement learning as current approaches focus narrowly on specific tasks.", "method": "The authors designed a data engine for constructing a high-quality post-training dataset (Mixed-45K) and a mixed reward function that accommodates various MLLM tasks.", "result": "Extensive experiments demonstrate the effectiveness of Mixed-R1 on different MLLMs, showcasing improvements in handling diverse tasks using multiple reward functions.", "conclusion": "The proposed framework and dataset provide a structured approach to enhance the performance of MLLMs in various tasks, reinforcing the versatility of reinforcement learning in this domain.", "key_contributions": ["Introduction of the Mixed-R1 framework for stable RL in MLLMs", "Development of Mixed-45K dataset comprised of high-quality examples", "Proposal of the Bidirectional Max-Average Similarity (BMAS) reward function for long-form responses"], "limitations": "", "keywords": ["Large Language Models", "Reinforcement Learning", "Mixed Reward Functions", "Post-training Datasets", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24165", "pdf": "https://arxiv.org/pdf/2505.24165.pdf", "abs": "https://arxiv.org/abs/2505.24165", "title": "Tag-Evol: Achieving Efficient Instruction Evolving via Tag Injection", "authors": ["Yixuan Wang", "Shiqi Zhou", "Chuanzhe Guo", "Qingfu Zhu"], "categories": ["cs.CL"], "comment": "Accepted as Findings of ACL 2025", "summary": "Evol-Instruct has made significant improvements as a data synthesis method in\nseveral areas. Existing methods typically rely on a fixed set of strategies to\nevolve, which require manual design and are monolithic in form. In addition,\niterative evolution also makes the acquisition of hard samples expensive. In\nview of this, we propose the Tag-Evol framework, a more diverse and efficient\ninstruction evolving method. Specifically, Tag-Evol uses diverse and specific\nknowledge tags as strategies to achieve controlled evolution by injecting\ndifferent combinations of tags into the original instructions. Experiments with\nmultiple backbones in diverse domain benchmarks show that the proposed method\ngenerates significantly better evolved data than other methods. Furthermore, we\nconduct a thorough analysis of the evolved data, demonstrating that Tag-Evol is\nnot only efficient but also generates more diverse and challenging data.", "AI": {"tldr": "Tag-Evol is a new instruction evolving framework that uses diverse knowledge tags for efficient data synthesis, outperforming existing methods in generating high-quality evolved data.", "motivation": "Existing instruction evolving methods suffer from manual design limitations and high costs of acquiring hard samples. There is a need for a more efficient and diverse approach.", "method": "Tag-Evol introduces diverse knowledge tags as evolution strategies, enabling controlled evolution by combining different tags with original instructions.", "result": "Experiments show that Tag-Evol generates significantly better evolved data across various benchmarks compared to existing methods, revealing higher efficiency and diversity.", "conclusion": "Tag-Evol not only improves efficiency in data synthesis but also enhances the diversity and challenge level of the evolved data.", "key_contributions": ["Introduction of Tag-Evol framework for instruction evolution", "Use of diverse knowledge tags for data synthesis", "Demonstration of improved performance in generating evolved data across benchmarks."], "limitations": "", "keywords": ["Data Synthesis", "Instruction Evolution", "Knowledge Tags", "Machine Learning", "Data Diversity"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.24174", "pdf": "https://arxiv.org/pdf/2505.24174.pdf", "abs": "https://arxiv.org/abs/2505.24174", "title": "Adaptive LoRA Merge with Parameter Pruning for Low-Resource Generation", "authors": ["Ryota Miyano", "Yuki Arase"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ACL2025 Findings", "summary": "This study proposes a simple yet effective LoRA merge method to achieve LLM\nadaptation for low-resource language generation tasks. The LoRA merge\ntechnique, which integrates multiple LoRA modules trained on different tasks,\nhas gained attention as an effective and efficient approach for adapting LLMs\nto target tasks. However, previous methods are limited in adaptability as they\nkeep the LoRA parameters frozen. Additionally, the low-resource problem has\nbeen out of their scope. We propose a LoRA merge method that updates and prunes\nLoRA parameters through fine-tuning with minimal target task data, which allows\nfiner-grained adjustments of LoRA parameters and enhancement of task\nadaptability. Extensive experiments have been conducted taking summarization as\na benchmark task. Our datasets cover various domains and multiple languages of\nEnglish and Japanese. The results confirm that the proposed method achieves\nsignificant and consistent improvements in task adaptability over the previous\nmethods.", "AI": {"tldr": "The paper presents a novel LoRA merge method for adapting large language models (LLMs) to low-resource language generation tasks, demonstrating improvements in task adaptability in experiments.", "motivation": "To address the limitations of existing LoRA methods that keep parameters frozen and do not cater to low-resource scenarios.", "method": "The proposed method updates and prunes LoRA parameters via fine-tuning using minimal target task data, enhancing adaptability for diverse language tasks.", "result": "Extensive experiments, particularly in summarization tasks with datasets in English and Japanese, show significant improvements in task adaptability compared to previous methods.", "conclusion": "The LoRA merge method provides a more effective adaptation strategy for LLMs in low-resource settings, demonstrating its utility across various languages and domains.", "key_contributions": ["Introduction of a LoRA merge method that adapts LLMs for low-resource tasks", "Demonstration of task adaptability through fine-tuning with minimal data", "Empirical validation via extensive experiments focusing on summarization tasks"], "limitations": "", "keywords": ["LoRA", "low-resource language generation", "LLM adaptation", "fine-tuning", "task adaptability"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24187", "pdf": "https://arxiv.org/pdf/2505.24187.pdf", "abs": "https://arxiv.org/abs/2505.24187", "title": "Beyond Exponential Decay: Rethinking Error Accumulation in Large Language Models", "authors": ["Mikhail L. Arbuzov", "Alexey A. Shvets", "Sisong Beir"], "categories": ["cs.CL"], "comment": null, "summary": "The prevailing assumption of an exponential decay in large language model\n(LLM) reliability with sequence length, predicated on independent per-token\nerror probabilities, posits an inherent limitation for long autoregressive\noutputs. Our research fundamentally challenges this view by synthesizing\nemerging evidence that LLM errors are not uniformly distributed but are\nconcentrated at sparse \"key tokens\" ($5-10\\%$ of total tokens) representing\ncritical decision junctions. By distinguishing these high-impact tokens from\nthe increasingly predictable majority, we introduce a new reliability formula\nexplaining the sustained coherence of modern LLMs over thousands of tokens.\nConverging research streams reveal that long-context performance primarily\ndepends on accurately navigating a few crucial semantic decision points rather\nthan on uniform token-level accuracy, enabling targeted strategies that\nsignificantly outperform brute-force approaches. We thus propose a framework\nfor next-generation systems centered on selective preservation of semantically\nvital tokens, dynamic computational allocation at uncertain decision\nboundaries, multi-path exploration at ambiguities, and architectures aligned\nwith natural semantic domains. This marks a fundamental shift from raw scaling\nto strategic reasoning, promising breakthrough performance without\nproportionate computational scaling and offering a more nuanced understanding\nthat supersedes the exponential decay hypothesis, thereby opening pathways\ntoward substantially more powerful and efficient language systems.", "AI": {"tldr": "This research challenges the assumption of exponential decay in LLM reliability with longer sequences, proposing that reliability relies on a small number of key tokens that represent critical decision points. It offers a new framework for enhancing LLM performance through the selective preservation of significant tokens and strategic reasoning over brute-force methods.", "motivation": "To challenge the prevailing assumption that LLM reliability exponentially decays with sequence length, which limits their effectiveness in generating long outputs.", "method": "The study synthesizes evidence that LLM errors are concentrated around key tokens and proposes a new reliability formula along with a framework for next-generation language models focusing on decision-making processes rather than sheer output length.", "result": "Findings indicate that LLM performance with long sequences can be significantly improved by focusing on a few critical tokens rather than maintaining uniform accuracy across all tokens, leading to better coherence in long outputs.", "conclusion": "The research posits a paradigm shift in understanding LLM reliability, suggesting that strategic reasoning at key decision points can enhance model performance without the need for extensive computational resources.", "key_contributions": ["Introduces a new reliability formula for LLMs focused on key tokens.", "Proposes a framework for improving LLM performance through targeted strategies of token management.", "Challenges the exponential decay hypothesis and suggests a more nuanced understanding of LLM reliability."], "limitations": "", "keywords": ["large language models", "LLM reliability", "key tokens", "sequence length", "semantic decision points"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24196", "pdf": "https://arxiv.org/pdf/2505.24196.pdf", "abs": "https://arxiv.org/abs/2505.24196", "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding", "authors": ["Longze Chen", "Renke Shan", "Huiming Wang", "Lu Wang", "Ziqiang Liu", "Run Luo", "Jiawei Wang", "Hamid Alinejad-Rokny", "Min Yang"], "categories": ["cs.CL"], "comment": "11 pages, 7 figures, ACL 2025", "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.", "AI": {"tldr": "CLaSp is a method for accelerating Large Language Model decoding through an in-context layer-skipping strategy that avoids the need for additional training modules.", "motivation": "The paper addresses the challenges of existing drafting approaches in speculative decoding, which require additional training and compatibility across various LLMs.", "method": "CLaSp employs a plug-and-play mechanism by skipping intermediate layers of a verify model. It uses a dynamic programming algorithm to optimize this process based on hidden states from the last verification stage, allowing for flexibility in layer-skipping after each verification.", "result": "CLaSp achieves 1.3x to 1.7x speedup on LLaMA3 series models, maintaining the original distribution of generated text across various downstream tasks.", "conclusion": "The proposed method provides an efficient alternative for speculative decoding without the complexities of additional training modules.", "key_contributions": ["Introduces an in-context layer-skipping strategy for LLMs.", "Eliminates the need for additional drafting modules or training.", "Demonstrates significant speedup in decoding without compromising text quality."], "limitations": "", "keywords": ["speculative decoding", "Large Language Models", "layer-skipping", "CLaSp", "efficient decoding"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24199", "pdf": "https://arxiv.org/pdf/2505.24199.pdf", "abs": "https://arxiv.org/abs/2505.24199", "title": "Intuitionistic Fuzzy Sets for Large Language Model Data Annotation: A Novel Approach to Side-by-Side Preference Labeling", "authors": ["Yimin Du"], "categories": ["cs.CL"], "comment": "7 pages", "summary": "The quality of human preference data is crucial for training and evaluating\nlarge language models (LLMs), particularly in reinforcement learning from human\nfeedback (RLHF) and direct preference optimization (DPO) scenarios. Traditional\nside-by-side (SBS) annotation approaches often struggle with inherent\nuncertainty, annotator disagreement, and the complexity of preference\njudgments. This paper introduces a novel framework based on intuitionistic\nfuzzy sets (IFS) for modeling and aggregating human preferences in LLM data\nannotation tasks. Our approach captures not only the degree of preference but\nalso the uncertainty and hesitation inherent in human judgment through\nmembership, non-membership, and hesitation degrees. We propose an IFS-based\nannotation protocol that enables more nuanced preference modeling, develops\naggregation methods for handling annotator disagreement, and introduces quality\nmetrics for preference data assessment. Experimental validation on multiple\ndatasets demonstrates that our IFS-based approach significantly improves\nannotation consistency, reduces annotator fatigue, and produces higher-quality\npreference data compared to traditional binary and Likert-scale methods. The\nresulting preference datasets lead to improved model performance in downstream\ntasks, with 12.3\\% improvement in win-rate against baseline models and 15.7\\%\nreduction in annotation time. Our framework provides a principled approach to\nhandling uncertainty in human preference annotation and offers practical\nbenefits for large-scale LLM training.", "AI": {"tldr": "This paper presents a novel framework using intuitionistic fuzzy sets (IFS) for improving the quality of human preference data in large language model (LLM) training, particularly in RLHF and DPO scenarios.", "motivation": "The quality of human preference data is essential for the effective training and evaluation of large language models, but traditional annotation methods face challenges due to uncertainty and annotator disagreement.", "method": "The paper introduces an IFS-based annotation protocol that models preferences through degrees of membership, non-membership, and hesitation, alongside aggregation methods for addressing annotator disagreement and quality metrics for assessment.", "result": "The IFS-based approach showed significant improvements in annotation consistency, reduced annotator fatigue, and enhanced quality of preference data, leading to a 12.3% increase in model performance and a 15.7% decrease in annotation time compared to traditional methods.", "conclusion": "The proposed framework offers a robust solution for managing uncertainty in human preference annotation, yielding practical advancements in LLM training.", "key_contributions": ["Introduction of an IFS-based annotation protocol for preferences", "Development of aggregation methods for annotator disagreement", "Introduction of quality metrics for preference data assessment"], "limitations": "", "keywords": ["Human-Computer Interaction", "Preference Data Annotation", "Intuitionistic Fuzzy Sets", "Reinforcement Learning from Human Feedback", "Large Language Models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24211", "pdf": "https://arxiv.org/pdf/2505.24211.pdf", "abs": "https://arxiv.org/abs/2505.24211", "title": "Are Any-to-Any Models More Consistent Across Modality Transfers Than Specialists?", "authors": ["Jiwan Chung", "Janghan Yoon", "Junhyeong Park", "Sangeyl Lee", "Joowon Yang", "Sooyeon Park", "Youngjae Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Any-to-any generative models aim to enable seamless interpretation and\ngeneration across multiple modalities within a unified framework, yet their\nability to preserve relationships across modalities remains uncertain. Do\nunified models truly achieve cross-modal coherence, or is this coherence merely\nperceived? To explore this, we introduce ACON, a dataset of 1,000 images (500\nnewly contributed) paired with captions, editing instructions, and Q&A pairs to\nevaluate cross-modal transfers rigorously. Using three consistency\ncriteria-cyclic consistency, forward equivariance, and conjugated\nequivariance-our experiments reveal that any-to-any models do not consistently\ndemonstrate greater cross-modal consistency than specialized models in\npointwise evaluations such as cyclic consistency. However, equivariance\nevaluations uncover weak but observable consistency through structured analyses\nof the intermediate latent space enabled by multiple editing operations. We\nrelease our code and data at https://github.com/JiwanChung/ACON.", "AI": {"tldr": "This paper introduces ACON, a dataset to evaluate cross-modal coherence in any-to-any generative models and finds that these models do not outperform specialized models in certain consistency evaluations.", "motivation": "To assess the effectiveness of any-to-any generative models in maintaining cross-modal coherence and understanding if their consistency is genuine or merely perceived.", "method": "The authors introduce ACON, a dataset of 1,000 images paired with captions, editing instructions, and Q&A pairs. They evaluate models using consistency criteria: cyclic consistency, forward equivariance, and conjugated equivariance.", "result": "Experiments show that any-to-any models do not demonstrate superior cross-modal consistency compared to specialized models in cyclic consistency, but some consistency is observable in equivariance evaluations through analysis of intermediate latent spaces.", "conclusion": "The findings suggest that while any-to-any models have weak observable consistency, they do not consistently outperform specialized models, highlighting the challenges in achieving true cross-modal coherence.", "key_contributions": ["Introduction of the ACON dataset for evaluating cross-modal coherence.", "Identification of consistency criteria for model evaluation.", "Demonstration of the limitations of any-to-any models compared to specialized models."], "limitations": "The study primarily focuses on three consistency criteria, which may not capture all aspects of cross-modal coherence.", "keywords": ["generative models", "cross-modal coherence", "dataset", "model evaluation", "equivariance"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.24217", "pdf": "https://arxiv.org/pdf/2505.24217.pdf", "abs": "https://arxiv.org/abs/2505.24217", "title": "Semi-structured LLM Reasoners Can Be Rigorously Audited", "authors": ["Jixuan Leng", "Cassandra A. Cohen", "Zhixian Zhang", "Chenyan Xiong", "William W. Cohen"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) become increasingly capable at reasoning, the\nproblem of \"faithfulness\" persists: LLM \"reasoning traces\" can contain errors\nand omissions that are difficult to detect, and may obscure biases in model\noutputs. To address these limitations, we introduce Semi-Structured Reasoning\nModels (SSRMs), which internalize a semi-structured Chain-of-Thought (CoT)\nreasoning format within the model. Our SSRMs generate reasoning traces in a\nPythonic syntax. While SSRM traces are not executable, they adopt a restricted,\ntask-specific vocabulary to name distinct reasoning steps, and to mark each\nstep's inputs and outputs. Through extensive evaluation on ten benchmarks,\nSSRMs demonstrate strong performance and generality: they outperform comparably\nsized baselines by nearly ten percentage points on in-domain tasks while\nremaining competitive with specialized models on out-of-domain medical\nbenchmarks. Furthermore, we show that semi-structured reasoning is more\namenable to analysis: in particular, they can be automatically audited to\nidentify reasoning flaws. We explore both hand-crafted structured audits, which\ndetect task-specific problematic reasoning patterns, and learned typicality\naudits, which apply probabilistic models over reasoning patterns, and show that\nboth audits can be used to effectively flag probable reasoning errors.", "AI": {"tldr": "This paper introduces Semi-Structured Reasoning Models (SSRMs) that enhance LLMs' reasoning capabilities by generating semi-structured reasoning traces with specific vocabulary, improving transparency and auditability in reasoning.", "motivation": "To tackle the problem of faithfulness in LLM reasoning, which often contains undetectable errors and biases.", "method": "SSRMs generate reasoning traces in a Pythonic syntax using a semi-structured Chain-of-Thought format, allowing for better identification of reasoning steps and potential flaws.", "result": "SSRMs outperform comparably sized models by nearly 10 percentage points on in-domain tasks and remain competitive on out-of-domain medical benchmarks, showing improved generality.", "conclusion": "Semi-structured reasoning models facilitate effective auditing of reasoning processes, allowing for the detection of errors and biases in LLM outputs.", "key_contributions": ["Introduction of Semi-Structured Reasoning Models (SSRMs)", "Capability to generate semi-structured reasoning traces for better transparency", "Proven effectiveness in auditing reasoning flaws through structured and learned audits."], "limitations": "", "keywords": ["Large Language Models", "semi-structured reasoning", "Chain-of-Thought", "reasoning audit", "bias detection"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24219", "pdf": "https://arxiv.org/pdf/2505.24219.pdf", "abs": "https://arxiv.org/abs/2505.24219", "title": "ERU-KG: Efficient Reference-aligned Unsupervised Keyphrase Generation", "authors": ["Lam Thanh Do", "Aaditya Bodke", "Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Unsupervised keyphrase prediction has gained growing interest in recent\nyears. However, existing methods typically rely on heuristically defined\nimportance scores, which may lead to inaccurate informativeness estimation. In\naddition, they lack consideration for time efficiency. To solve these problems,\nwe propose ERU-KG, an unsupervised keyphrase generation (UKG) model that\nconsists of an informativeness and a phraseness module. The former estimates\nthe relevance of keyphrase candidates, while the latter generate those\ncandidates. The informativeness module innovates by learning to model\ninformativeness through references (e.g., queries, citation contexts, and\ntitles) and at the term-level, thereby 1) capturing how the key concepts of\ndocuments are perceived in different contexts and 2) estimating informativeness\nof phrases more efficiently by aggregating term informativeness, removing the\nneed for explicit modeling of the candidates. ERU-KG demonstrates its\neffectiveness on keyphrase generation benchmarks by outperforming unsupervised\nbaselines and achieving on average 89\\% of the performance of a supervised\nmodel for top 10 predictions. Additionally, to highlight its practical utility,\nwe evaluate the model on text retrieval tasks and show that keyphrases\ngenerated by ERU-KG are effective when employed as query and document\nexpansions. Furthermore, inference speed tests reveal that ERU-KG is the\nfastest among baselines of similar model sizes. Finally, our proposed model can\nswitch between keyphrase generation and extraction by adjusting\nhyperparameters, catering to diverse application requirements.", "AI": {"tldr": "ERU-KG is an unsupervised keyphrase generation model that improves informativeness estimation and time efficiency for keyphrase prediction.", "motivation": "Existing unsupervised keyphrase prediction methods rely on heuristically defined importance scores and lack time efficiency, leading to inaccurate results.", "method": "ERU-KG consists of an informativeness module that estimates relevance using references and a phraseness module that generates candidates, allowing for efficient informativeness estimation and keyphrase generation.", "result": "ERU-KG outperforms unsupervised baselines by achieving on average 89% of the performance of supervised models on keyphrase generation benchmarks.", "conclusion": "The model efficiently generates keyphrases and performs well in text retrieval tasks, also being adaptable for keyphrase generation and extraction based on hyperparameters.", "key_contributions": ["Proposes a novel model (ERU-KG) for unsupervised keyphrase generation that improves informativeness estimation", "Achieves high effectiveness in keyphrase generation benchmarks", "Fastest inference speed among similar model size baselines."], "limitations": "", "keywords": ["Unsupervised Keyphrase Prediction", "Machine Learning", "Information Retrieval"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.24223", "pdf": "https://arxiv.org/pdf/2505.24223.pdf", "abs": "https://arxiv.org/abs/2505.24223", "title": "Automated Structured Radiology Report Generation", "authors": ["Jean-Benoit Delbrouck", "Justin Xu", "Johannes Moll", "Alois Thomas", "Zhihong Chen", "Sophie Ostmeier", "Asfandyar Azhar", "Kelvin Zhenghao Li", "Andrew Johnston", "Christian Bluethgen", "Eduardo Reis", "Mohamed Muneer", "Maya Varma", "Curtis Langlotz"], "categories": ["cs.CL"], "comment": "Accepted to ACL Main 2025", "summary": "Automated radiology report generation from chest X-ray (CXR) images has the\npotential to improve clinical efficiency and reduce radiologists' workload.\nHowever, most datasets, including the publicly available MIMIC-CXR and CheXpert\nPlus, consist entirely of free-form reports, which are inherently variable and\nunstructured. This variability poses challenges for both generation and\nevaluation: existing models struggle to produce consistent, clinically\nmeaningful reports, and standard evaluation metrics fail to capture the nuances\nof radiological interpretation. To address this, we introduce Structured\nRadiology Report Generation (SRRG), a new task that reformulates free-text\nradiology reports into a standardized format, ensuring clarity, consistency,\nand structured clinical reporting. We create a novel dataset by restructuring\nreports using large language models (LLMs) following strict structured\nreporting desiderata. Additionally, we introduce SRR-BERT, a fine-grained\ndisease classification model trained on 55 labels, enabling more precise and\nclinically informed evaluation of structured reports. To assess report quality,\nwe propose F1-SRR-BERT, a metric that leverages SRR-BERT's hierarchical disease\ntaxonomy to bridge the gap between free-text variability and structured\nclinical reporting. We validate our dataset through a reader study conducted by\nfive board-certified radiologists and extensive benchmarking experiments.", "AI": {"tldr": "The paper presents Structured Radiology Report Generation (SRRG), a new task focused on converting unstructured radiology reports into a standardized format to improve report clarity and consistency.", "motivation": "The need for improved clinical efficiency in radiology reporting and the challenges posed by the variability of free-form reports.", "method": "The authors introduce a novel dataset created by restructuring free-text reports into a standardized format using large language models (LLMs) and propose SRR-BERT for fine-grained disease classification.", "result": "The proposed methods and models, including F1-SRR-BERT, enhance the generation and evaluation of structured radiology reports, validated by a reader study with radiologists and benchmarking.", "conclusion": "The new structured approach addresses the limitations of traditional free-text reporting and improves the ability to generate clinically meaningful radiology reports.", "key_contributions": ["Introduction of Structured Radiology Report Generation (SRRG) task", "Development of a novel dataset for standardized reporting", "Proposal of SRR-BERT and F1-SRR-BERT for improved report evaluation"], "limitations": "", "keywords": ["radiology", "report generation", "structured reporting", "large language models", "clinical evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24229", "pdf": "https://arxiv.org/pdf/2505.24229.pdf", "abs": "https://arxiv.org/abs/2505.24229", "title": "Dynamic Context-Aware Streaming Pretrained Language Model For Inverse Text Normalization", "authors": ["Luong Ho", "Khanh Le", "Vinh Pham", "Bao Nguyen", "Tan Tran", "Duc Chau"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "Inverse Text Normalization (ITN) is crucial for converting spoken Automatic\nSpeech Recognition (ASR) outputs into well-formatted written text, enhancing\nboth readability and usability. Despite its importance, the integration of\nstreaming ITN within streaming ASR remains largely unexplored due to challenges\nin accuracy, efficiency, and adaptability, particularly in low-resource and\nlimited-context scenarios. In this paper, we introduce a streaming pretrained\nlanguage model for ITN, leveraging pretrained linguistic representations for\nimproved robustness. To address streaming constraints, we propose Dynamic\nContext-Aware during training and inference, enabling adaptive chunk size\nadjustments and the integration of right-context information. Experimental\nresults demonstrate that our method achieves accuracy comparable to\nnon-streaming ITN and surpasses existing streaming ITN models on a Vietnamese\ndataset, all while maintaining low latency, ensuring seamless integration into\nASR systems.", "AI": {"tldr": "This paper presents a streaming pretrained language model for Inverse Text Normalization (ITN) to enhance ASR outputs, addressing challenges in efficiency and adaptability.", "motivation": "To improve the usability of spoken ASR outputs through effective ITN, especially in low-resource contexts, where existing solutions struggle.", "method": "Introduction of a streaming pretrained language model utilizing Dynamic Context-Aware adjustments during both training and inference for adaptive chunk sizes and right-context integration.", "result": "Experimental results indicate that the proposed method achieves accuracy on par with non-streaming ITN while surpassing current streaming ITN models, particularly on a Vietnamese dataset, with low latency.", "conclusion": "This streaming ITN model can be seamlessly integrated into ASR systems, offering a robust solution for real-time applications.", "key_contributions": ["Development of a streaming pretrained language model for ITN", "Implementation of Dynamic Context-Aware adjustments", "Demonstration of superior performance over existing streaming ITN models"], "limitations": "", "keywords": ["Inverse Text Normalization", "Automatic Speech Recognition", "streaming ASR", "language model", "low-resource scenarios"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24241", "pdf": "https://arxiv.org/pdf/2505.24241.pdf", "abs": "https://arxiv.org/abs/2505.24241", "title": "Advantageous Parameter Expansion Training Makes Better Large Language Models", "authors": ["Naibin Gu", "Yilong Chen", "Zhenyu Zhang", "Peng Fu", "Zheng Lin", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Weiping Wang", "Haifeng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Although scaling up the number of trainable parameters in both pre-training\nand fine-tuning can effectively improve the performance of large language\nmodels, it also leads to increased computational overhead. When delving into\nthe parameter difference, we find that a subset of parameters, termed\nadvantageous parameters, plays a crucial role in determining model performance.\nFurther analysis reveals that stronger models tend to possess more such\nparameters. In this paper, we propose Advantageous Parameter EXpansion Training\n(APEX), a method that progressively expands advantageous parameters into the\nspace of disadvantageous ones, thereby increasing their proportion and\nenhancing training effectiveness. Further theoretical analysis from the\nperspective of matrix effective rank explains the performance gains of APEX.\nExtensive experiments on both instruction tuning and continued pre-training\ndemonstrate that, in instruction tuning, APEX outperforms full-parameter tuning\nwhile using only 52% of the trainable parameters. In continued pre-training,\nAPEX achieves the same perplexity level as conventional training with just 33%\nof the training data, and yields significant improvements on downstream tasks.", "AI": {"tldr": "This paper introduces APEX, a method for enhancing large language model training by expanding advantageous parameters, showing significant performance improvements with reduced computational resources.", "motivation": "As the number of parameters in language models increases for better performance, the associated computational overhead grows. Identifying and leveraging advantageous parameters could optimize performance without the need for excessive resources.", "method": "The paper proposes Advantageous Parameter EXpansion Training (APEX), which focuses on progressively incorporating advantageous parameters into the training process, thereby enhancing their effectiveness compared to disadvantageous ones.", "result": "APEX shows superior results in instruction tuning, outperforming full-parameter tuning with only 52% of trainable parameters, and achieves equivalent perplexity levels in continued pre-training with just 33% of the training data, leading to significant improvements in downstream tasks.", "conclusion": "By utilizing APEX, training efficiency is greatly enhanced without compromising model performance, highlighting the potential for optimizing resources in large language model training.", "key_contributions": ["Introduction of the APEX method for training language models", "Theoretical analysis from the perspective of matrix effective rank", "Demonstrated significant performance improvements with reduced parameters and training data"], "limitations": "", "keywords": ["Machine Learning", "Language Models", "Training Efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24244", "pdf": "https://arxiv.org/pdf/2505.24244.pdf", "abs": "https://arxiv.org/abs/2505.24244", "title": "Mamba Knockout for Unraveling Factual Information Flow", "authors": ["Nir Endy", "Idan Daniel Grosbard", "Yuval Ran-Milo", "Yonatan Slutzky", "Itay Tshuva", "Raja Giryes"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "This paper investigates the flow of factual information in Mamba State-Space\nModel (SSM)-based language models. We rely on theoretical and empirical\nconnections to Transformer-based architectures and their attention mechanisms.\nExploiting this relationship, we adapt attentional interpretability techniques\noriginally developed for Transformers--specifically, the Attention Knockout\nmethodology--to both Mamba-1 and Mamba-2. Using them we trace how information\nis transmitted and localized across tokens and layers, revealing patterns of\nsubject-token information emergence and layer-wise dynamics. Notably, some\nphenomena vary between mamba models and Transformer based models, while others\nappear universally across all models inspected--hinting that these may be\ninherent to LLMs in general. By further leveraging Mamba's structured\nfactorization, we disentangle how distinct \"features\" either enable\ntoken-to-token information exchange or enrich individual tokens, thus offering\na unified lens to understand Mamba internal operations.", "AI": {"tldr": "This paper examines the flow of information in Mamba State-Space Models compared to Transformer models, using attentional techniques for interpretability.", "motivation": "To explore how factual information is processed in Mamba State-Space Models and compare it to Transformer architectures.", "method": "The paper adapts attentional interpretability techniques, specifically the Attention Knockout methodology, to analyze the Mamba-1 and Mamba-2 models.", "result": "Information transmission dynamics were traced across tokens and layers, revealing both unique and universal patterns in how information is localized and exchanged.", "conclusion": "The findings offer insights into the internal operations of Mamba models and suggest inherent properties of language models in general.", "key_contributions": ["Adaptation of Attention Knockout methods to Mamba models", "Empirical analysis of information flow in Mamba State-Space Models", "Identification of unique and universal patterns in information processing across models"], "limitations": "", "keywords": ["Mamba State-Space Model", "information flow", "attention mechanisms", "language models", "token dynamics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.24251", "pdf": "https://arxiv.org/pdf/2505.24251.pdf", "abs": "https://arxiv.org/abs/2505.24251", "title": "Proactive Guidance of Multi-Turn Conversation in Industrial Search", "authors": ["Xiaoyu Li", "Xiao Li", "Li Gao", "Yiding Liu", "Xiaoyang Wang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "categories": ["cs.CL", "cs.IR"], "comment": "ACL'25 (Industry)", "summary": "The evolution of Large Language Models (LLMs) has significantly advanced\nmulti-turn conversation systems, emphasizing the need for proactive guidance to\nenhance users' interactions. However, these systems face challenges in\ndynamically adapting to shifts in users' goals and maintaining low latency for\nreal-time interactions. In the Baidu Search AI assistant, an industrial-scale\nmulti-turn search system, we propose a novel two-phase framework to provide\nproactive guidance. The first phase, Goal-adaptive Supervised Fine-Tuning\n(G-SFT), employs a goal adaptation agent that dynamically adapts to user goal\nshifts and provides goal-relevant contextual information. G-SFT also\nincorporates scalable knowledge transfer to distill insights from LLMs into a\nlightweight model for real-time interaction. The second phase, Click-oriented\nReinforcement Learning (C-RL), adopts a generate-rank paradigm, systematically\nconstructs preference pairs from user click signals, and proactively improves\nclick-through rates through more engaging guidance. This dual-phase\narchitecture achieves complementary objectives: G-SFT ensures accurate goal\ntracking, while C-RL optimizes interaction quality through click signal-driven\nreinforcement learning. Extensive experiments demonstrate that our framework\nachieves 86.10% accuracy in offline evaluation (+23.95% over baseline) and\n25.28% CTR in online deployment (149.06% relative improvement), while reducing\ninference latency by 69.55% through scalable knowledge distillation.", "AI": {"tldr": "The paper presents a two-phase framework for enhancing multi-turn conversation systems in the Baidu Search AI assistant, focusing on adaptive user interaction and real-time performance.", "motivation": "The need for proactive guidance in multi-turn conversation systems to enhance user interaction and adapt to changing user goals.", "method": "A two-phase framework consisting of Goal-adaptive Supervised Fine-Tuning (G-SFT) and Click-oriented Reinforcement Learning (C-RL). G-SFT adapts to user goals and integrates knowledge from LLMs into a lightweight model, while C-RL uses user click signals for optimization.", "result": "Achieved 86.10% accuracy in offline evaluation, improving by 23.95% over baseline, and 25.28% CTR in online deployment with a 149.06% relative improvement, while reducing inference latency by 69.55%.", "conclusion": "The dual-phase architecture effectively tracks user goals and optimizes interaction quality, demonstrating significant performance improvements.", "key_contributions": ["Proposed a dual-phase framework for proactive user guidance in multi-turn systems.", "Developed a knowledge distillation method for real-time interaction.", "Implemented reinforcement learning to enhance click-through rates."], "limitations": "", "keywords": ["Large Language Models", "Human-Computer Interaction", "Reinforcement Learning", "Goal Adaptation", "Knowledge Distillation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24255", "pdf": "https://arxiv.org/pdf/2505.24255.pdf", "abs": "https://arxiv.org/abs/2505.24255", "title": "Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games", "authors": ["Neemesh Yadav", "Palakorn Achananuparp", "Jing Jiang", "Ee-Peng Lim"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 1 figure, 6 tables", "summary": "Large Language Models (LLMs) have shown potential in simulating human\nbehaviors and performing theory-of-mind (ToM) reasoning, a crucial skill for\ncomplex social interactions. In this study, we investigate the role of ToM\nreasoning in aligning agentic behaviors with human norms in negotiation tasks,\nusing the ultimatum game as a controlled environment. We initialized LLM agents\nwith different prosocial beliefs (including Greedy, Fair, and Selfless) and\nreasoning methods like chain-of-thought (CoT) and varying ToM levels, and\nexamined their decision-making processes across diverse LLMs, including\nreasoning models like o3-mini and DeepSeek-R1 Distilled Qwen 32B. Results from\n2,700 simulations indicated that ToM reasoning enhances behavior alignment,\ndecision-making consistency, and negotiation outcomes. Consistent with previous\nfindings, reasoning models exhibit limited capability compared to models with\nToM reasoning, different roles of the game benefits with different orders of\nToM reasoning. Our findings contribute to the understanding of ToM's role in\nenhancing human-AI interaction and cooperative decision-making. The code used\nfor our experiments can be found at https://github.com/Stealth-py/UltimatumToM.", "AI": {"tldr": "This study explores the role of theory-of-mind (ToM) reasoning in aligning Large Language Models (LLMs) behaviors with human norms during negotiation tasks, demonstrating that enhanced ToM reasoning improves decision-making outcomes.", "motivation": "To understand how ToM reasoning can enhance human-AI interaction and improve cooperative decision-making in complex social contexts like negotiation.", "method": "Experiments conducted using the ultimatum game, initializing LLM agents with various prosocial beliefs and ToM reasoning methods, and analyzing their decision-making over 2,700 simulations.", "result": "ToM reasoning was found to significantly improve behavior alignment and decision-making consistency in LLMs during negotiation tasks, showcasing superior performance in comparison to models without ToM reasoning.", "conclusion": "The findings suggest that ToM reasoning is essential for improving AI negotiation outcomes and aligning AI behaviors with human social norms.", "key_contributions": ["Demonstrates the impact of ToM reasoning on LLM decision-making", "Compares various prosocial beliefs and reasoning methods in LLMs", "Provides insights into human-AI interaction dynamics"], "limitations": "Limited capability of reasoning models compared to those incorporating ToM; results may vary based on specific negotiation contexts.", "keywords": ["Large Language Models", "theory-of-mind", "negotiation tasks", "human-AI interaction", "cooperative decision-making"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.24263", "pdf": "https://arxiv.org/pdf/2505.24263.pdf", "abs": "https://arxiv.org/abs/2505.24263", "title": "Simulating Training Data Leakage in Multiple-Choice Benchmarks for LLM Evaluation", "authors": ["Naila Shafirni Hidayat", "Muhammad Dehan Al Kautsar", "Alfan Farizki Wicaksono", "Fajri Koto"], "categories": ["cs.CL"], "comment": null, "summary": "The performance of large language models (LLMs) continues to improve, as\nreflected in rising scores on standard benchmarks. However, the lack of\ntransparency around training data raises concerns about potential overlap with\nevaluation sets and the fairness of reported results. Although prior work has\nproposed methods for detecting data leakage, these approaches primarily focus\non identifying outliers and have not been evaluated under controlled simulated\nleakage conditions. In this work, we compare existing leakage detection\ntechniques, namely permutation and n-gram-based methods, under a continual\npretraining setup that simulates real-world leakage scenarios, and additionally\nexplore a lightweight method we call semi-half question. Although semi-half\noffers a low-cost alternative, our analysis shows that the n-gram method\nconsistently achieves the highest F1-score. We also refine these techniques to\nsupport instance-level detection and reduce computational overhead. Leveraging\nthe best-performing method, we create cleaned versions of MMLU and HellaSwag,\nand re-evaluate several LLMs. Our findings present a practical path toward more\nreliable and transparent evaluations, and we recommend contamination checks as\na standard step before releasing benchmark results.", "AI": {"tldr": "The paper addresses the lack of transparency in training data for large language models (LLMs) by comparing techniques for detecting data leakage and proposing improved evaluation methods.", "motivation": "Concerns regarding the overlap between training and evaluation sets of LLMs and the fairness of results due to potential data leakage.", "method": "Comparison of existing leakage detection techniques (permutation and n-gram methods) in a simulated continual pretraining setup, along with the introduction of a new lightweight method called semi-half question.", "result": "The n-gram method achieved the highest F1-score for leakage detection, and the refined techniques allow for instance-level detection while reducing computational costs.", "conclusion": "The study provides a pathway for more reliable and transparent evaluations in LLMs, recommending contamination checks before releasing benchmark results.", "key_contributions": ["Comparison of existing leakage detection techniques under controlled scenarios.", "Introduction of a lightweight leakage detection method.", "Creation of cleaned benchmark datasets (MMLU and HellaSwag) for reevaluation of LLMs."], "limitations": "The study is focused only on specific leakage detection methods and may not cover all possible scenarios of data leakage.", "keywords": ["large language models", "data leakage", "n-gram", "benchmark evaluation", "HCI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24264", "pdf": "https://arxiv.org/pdf/2505.24264.pdf", "abs": "https://arxiv.org/abs/2505.24264", "title": "Faithful and Robust LLM-Driven Theorem Proving for NLI Explanations", "authors": ["Xin Quan", "Marco Valentino", "Louise A. Dennis", "André Freitas"], "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready for ACL 2025", "summary": "Natural language explanations play a fundamental role in Natural Language\nInference (NLI) by revealing how premises logically entail hypotheses. Recent\nwork has shown that the interaction of large language models (LLMs) with\ntheorem provers (TPs) can help verify and improve the validity of NLI\nexplanations. However, TPs require translating natural language into\nmachine-verifiable formal representations, a process that introduces the risk\nof semantic information loss and unfaithful interpretation, an issue compounded\nby LLMs' challenges in capturing critical logical structures with sufficient\nprecision. Moreover, LLMs are still limited in their capacity for rigorous and\nrobust proof construction within formal verification frameworks. To mitigate\nissues related to faithfulness and robustness, this paper investigates\nstrategies to (1) alleviate semantic loss during autoformalisation, (2)\nefficiently identify and correct syntactic errors in logical representations,\n(3) explicitly use logical expressions to guide LLMs in generating structured\nproof sketches, and (4) increase LLMs' capacity of interpreting TP's feedback\nfor iterative refinement. Our empirical results on e-SNLI, QASC and WorldTree\nusing different LLMs demonstrate that the proposed strategies yield significant\nimprovements in autoformalisation (+18.46%, +34.2%, +39.77%) and explanation\nrefinement (+29.5%, +51.5%, +41.25%) over the state-of-the-art model. Moreover,\nwe show that specific interventions on the hybrid LLM-TP architecture can\nsubstantially improve efficiency, drastically reducing the number of iterations\nrequired for successful verification.", "AI": {"tldr": "This paper explores enhancing Natural Language Inference (NLI) explanations through strategies that improve the interaction between Large Language Models (LLMs) and theorem provers (TPs).", "motivation": "The need to verify and improve the logical entailment of NLI explanations while addressing the limitations in LLMs' ability to construct rigorous proofs and the risk of semantic loss during translation into formal representations.", "method": "Investigating strategies to reduce semantic loss in autoformalisation, correct syntactic errors in logical representations, guide LLMs using logical expressions for proof generation, and enhance LLMs' interpretation of TPs' feedback for iterative refinement.", "result": "Empirical results indicate significant improvements in autoformalisation and explanation refinement metrics across different LLMs, with enhancements over state-of-the-art models.", "conclusion": "The proposed methods yield better efficiency in LLM-TP interactions and reduce verification iterations, advancing NLI explanation accuracy and robustness.", "key_contributions": ["Strategies to mitigate semantic loss in autoformalisation", "Approaches for correcting syntactic errors in logical representations", "Methods for guiding LLMs in structured proof generation"], "limitations": "", "keywords": ["Natural Language Inference", "Large Language Models", "Theorem Provers"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24302", "pdf": "https://arxiv.org/pdf/2505.24302.pdf", "abs": "https://arxiv.org/abs/2505.24302", "title": "ScienceMeter: Tracking Scientific Knowledge Updates in Language Models", "authors": ["Yike Wang", "Shangbin Feng", "Yulia Tsvetkov", "Hannaneh Hajishirzi"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to support scientific\nresearch, but their knowledge of scientific advancements can quickly become\noutdated. We introduce ScienceMeter, a new framework for evaluating scientific\nknowledge update methods over scientific knowledge spanning the past, present,\nand future. ScienceMeter defines three metrics: knowledge preservation, the\nextent to which models' understanding of previously learned papers are\npreserved; knowledge acquisition, how well scientific claims from newly\nintroduced papers are acquired; and knowledge projection, the ability of the\nupdated model to anticipate or generalize to related scientific claims that may\nemerge in the future. Using ScienceMeter, we examine the scientific knowledge\nof LLMs on claim judgment and generation tasks across a curated dataset of\n15,444 scientific papers and 30,888 scientific claims from ten domains\nincluding medicine, biology, materials science, and computer science. We\nevaluate five representative knowledge update approaches including training-\nand inference-time methods. With extensive experiments, we find that the\nbest-performing knowledge update methods can preserve only 85.9% of existing\nknowledge, acquire 71.7% of new knowledge, and project 37.7% of future\nknowledge. Inference-based methods work for larger models, whereas smaller\nmodels require training to achieve comparable performance. Cross-domain\nanalysis reveals that performance on these objectives is correlated. Even when\napplying on specialized scientific LLMs, existing knowledge update methods fail\nto achieve these objectives collectively, underscoring that developing robust\nscientific knowledge update mechanisms is both crucial and challenging.", "AI": {"tldr": "Introduction of ScienceMeter, a framework for evaluating scientific knowledge update methods in LLMs.", "motivation": "To address the issue of LLMs having outdated knowledge in scientific research and improve methods of updating this knowledge.", "method": "The study introduces the ScienceMeter framework that evaluates three metrics: knowledge preservation, knowledge acquisition, and knowledge projection, using a dataset of scientific papers and claims.", "result": "The evaluation demonstrates that current knowledge update methods can preserve only 85.9% of existing knowledge, acquire 71.7% of new knowledge, and project 37.7% of potential future knowledge across various domains.", "conclusion": "There is a significant need for developing robust scientific knowledge update mechanisms, as current methods fall short in achieving all objectives collectively, even for specialized LLMs.", "key_contributions": ["Introduction of a comprehensive evaluation framework for knowledge updates in LLMs called ScienceMeter.", "Detailed analysis of knowledge preservation, acquisition, and projection metrics.", "Empirical findings highlighting the limitations of current knowledge update methods across multiple scientific domains."], "limitations": "Current methods do not collectively achieve the desired objectives, indicating room for improvement in knowledge update mechanisms.", "keywords": ["Large Language Models", "Scientific Knowledge Update", "Knowledge Preservation", "Knowledge Acquisition", "Knowledge Projection"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.24319", "pdf": "https://arxiv.org/pdf/2505.24319.pdf", "abs": "https://arxiv.org/abs/2505.24319", "title": "HiCaM: A Hierarchical-Causal Modification Framework for Long-Form Text Modification", "authors": ["Yuntao Shi", "Yi Luo", "Yeyun Gong", "Chen Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in various\ndomains. However, when handling long-form text modification tasks, they still\nface two major problems: (1) producing undesired modifications by\ninappropriately altering or summarizing irrelevant content, and (2) missing\nnecessary modifications to implicitly related passages that are crucial for\nmaintaining document coherence. To address these issues, we propose HiCaM, a\nHierarchical-Causal Modification framework that operates through a hierarchical\nsummary tree and a causal graph. Furthermore, to evaluate HiCaM, we derive a\nmulti-domain dataset from various benchmarks, providing a resource for\nassessing its effectiveness. Comprehensive evaluations on the dataset\ndemonstrate significant improvements over strong LLMs, with our method\nachieving up to a 79.50\\% win rate. These results highlight the\ncomprehensiveness of our approach, showing consistent performance improvements\nacross multiple models and domains.", "AI": {"tldr": "HiCaM is a novel framework enhancing long-form text modifications by utilizing a hierarchical summary tree and causal graph, addressing common LLM limitations.", "motivation": "Large Language Models struggle with effectively modifying long-form text, often leading to irrelevant alterations or missed crucial content, which impacts document coherence.", "method": "The proposed HiCaM framework combines a hierarchical summary tree and a causal graph to improve text modification processes in LLMs.", "result": "HiCaM yielded significant performance improvements, achieving a 79.50% win rate in evaluations against other strong LLMs.", "conclusion": "The HiCaM framework demonstrates enhanced capabilities in producing coherent modifications across various models and domains, proving effective in addressing existing LLM challenges.", "key_contributions": ["Introduction of the HiCaM framework for long-form text modification", "Development of a multi-domain dataset for evaluation", "Demonstrated significant improvements over existing LLMs in task performance."], "limitations": "", "keywords": ["Large Language Models", "text modification", "document coherence", "HiCaM"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24331", "pdf": "https://arxiv.org/pdf/2505.24331.pdf", "abs": "https://arxiv.org/abs/2505.24331", "title": "Context-Aware Sentiment Forecasting via LLM-based Multi-Perspective Role-Playing Agents", "authors": ["Fanhang Man", "Huandong Wang", "Jianjie Fang", "Zhaoyi Deng", "Baining Zhao", "Xinlei Chen", "Yong Li"], "categories": ["cs.CL"], "comment": null, "summary": "User sentiment on social media reveals the underlying social trends, crises,\nand needs. Researchers have analyzed users' past messages to trace the\nevolution of sentiments and reconstruct sentiment dynamics. However, predicting\nthe imminent sentiment of an ongoing event is rarely studied. In this paper, we\naddress the problem of \\textbf{sentiment forecasting} on social media to\npredict the user's future sentiment in response to the development of the\nevent. We extract sentiment-related features to enhance the modeling skill and\npropose a multi-perspective role-playing framework to simulate the process of\nhuman response. Our preliminary results show significant improvement in\nsentiment forecasting on both microscopic and macroscopic levels.", "AI": {"tldr": "The paper presents a method for predicting future user sentiment on social media regarding ongoing events.", "motivation": "There is a need to predict imminent sentiment on social media, which has not been widely studied despite its importance in understanding social trends and crises.", "method": "The authors extract sentiment-related features and propose a multi-perspective role-playing framework to simulate human responses in sentiment forecasting.", "result": "The proposed approach demonstrates significant improvements in sentiment forecasting accuracy at both microscopic (individual) and macroscopic (aggregate) levels.", "conclusion": "The research contributes to the understanding of sentiment dynamics by enabling forecasting, which can inform decision-making during events.", "key_contributions": ["Development of a multi-perspective role-playing framework for sentiment forecasting", "Extraction of sentiment-related features to enhance model performance", "Improved prediction of user sentiment in response to ongoing events"], "limitations": "", "keywords": ["sentiment forecasting", "social media", "sentiment dynamics", "human response", "feature extraction"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24332", "pdf": "https://arxiv.org/pdf/2505.24332.pdf", "abs": "https://arxiv.org/abs/2505.24332", "title": "Pangu DeepDiver: Adaptive Search Intensity Scaling via Open-Web Reinforcement Learning", "authors": ["Wenxuan Shi", "Haochen Tan", "Chuqiao Kuang", "Xiaoguang Li", "Xiaozhe Ren", "Chen Zhang", "Hanting Chen", "Yasheng Wang", "Lifeng Shang", "Fisher Yu", "Yunhe Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Information seeking demands iterative evidence gathering and reflective\nreasoning, yet large language models (LLMs) still struggle with it in open-web\nquestion answering. Existing methods rely on static prompting rules or training\nwith Wikipedia-based corpora and retrieval environments, limiting adaptability\nto the real-world web environment where ambiguity, conflicting evidence, and\nnoise are prevalent. These constrained training settings hinder LLMs from\nlearning to dynamically decide when and where to search, and how to adjust\nsearch depth and frequency based on informational demands. We define this\nmissing capacity as Search Intensity Scaling (SIS)--the emergent skill to\nintensify search efforts under ambiguous or conflicting conditions, rather than\nsettling on overconfident, under-verification answers.\n  To study SIS, we introduce WebPuzzle, the first dataset designed to foster\ninformation-seeking behavior in open-world internet environments. WebPuzzle\nconsists of 24K training instances and 275 test questions spanning both\nwiki-based and open-web queries. Building on this dataset, we propose\nDeepDiver, a Reinforcement Learning (RL) framework that promotes SIS by\nencouraging adaptive search policies through exploration under a real-world\nopen-web environment. Experimental results show that Pangu-7B-Reasoner\nempowered by DeepDiver achieve performance on real-web tasks comparable to the\n671B-parameter DeepSeek-R1. We detail DeepDiver's training curriculum from\ncold-start supervised fine-tuning to a carefully designed RL phase, and present\nthat its capability of SIS generalizes from closed-form QA to open-ended tasks\nsuch as long-form writing. Our contributions advance adaptive information\nseeking in LLMs and provide a valuable benchmark and dataset for future\nresearch.", "AI": {"tldr": "This paper introduces WebPuzzle, a new dataset for fostering information-seeking behavior in LLMs, and proposes DeepDiver, a reinforcement learning framework that enhances search intensity scaling to improve performance in open-web question answering.", "motivation": "LLMs face challenges in iterative evidence gathering and reflective reasoning in open-web environments due to reliance on static prompting and constrained training methods.", "method": "The paper introduces WebPuzzle, a dataset with 24K training instances and 275 test questions, and proposes the DeepDiver RL framework to promote adaptive search policies in ambiguous conditions.", "result": "Pangu-7B-Reasoner, enhanced by DeepDiver, performs on par with the 671B-parameter DeepSeek-R1 in real-world tasks.", "conclusion": "DeepDiver demonstrates the capability of improving search intensity scaling and generalizes well across various tasks, providing a basis for future research in adaptive information seeking for LLMs.", "key_contributions": ["Introduction of the WebPuzzle dataset for open-web information-seeking tasks", "Development of the DeepDiver RL framework to enhance search intensity scaling", "Demonstration of effective generalization from closed-form QA to long-form writing tasks"], "limitations": "", "keywords": ["Large language models", "Information seeking", "Reinforcement learning", "Adaptability", "Dataset"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.24341", "pdf": "https://arxiv.org/pdf/2505.24341.pdf", "abs": "https://arxiv.org/abs/2505.24341", "title": "Exploring Multimodal Challenges in Toxic Chinese Detection: Taxonomy, Benchmark, and Findings", "authors": ["Shujian Yang", "Shiyao Cui", "Chuanrui Hu", "Haicheng Wang", "Tianwei Zhang", "Minlie Huang", "Jialiang Lu", "Han Qiu"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to ACL 2025 (Findings). Camera-ready version", "summary": "Detecting toxic content using language models is important but challenging.\nWhile large language models (LLMs) have demonstrated strong performance in\nunderstanding Chinese, recent studies show that simple character substitutions\nin toxic Chinese text can easily confuse the state-of-the-art (SOTA) LLMs. In\nthis paper, we highlight the multimodal nature of Chinese language as a key\nchallenge for deploying LLMs in toxic Chinese detection. First, we propose a\ntaxonomy of 3 perturbation strategies and 8 specific approaches in toxic\nChinese content. Then, we curate a dataset based on this taxonomy, and\nbenchmark 9 SOTA LLMs (from both the US and China) to assess if they can detect\nperturbed toxic Chinese text. Additionally, we explore cost-effective\nenhancement solutions like in-context learning (ICL) and supervised fine-tuning\n(SFT). Our results reveal two important findings. (1) LLMs are less capable of\ndetecting perturbed multimodal Chinese toxic contents. (2) ICL or SFT with a\nsmall number of perturbed examples may cause the LLMs \"overcorrect'':\nmisidentify many normal Chinese contents as toxic.", "AI": {"tldr": "This paper addresses the challenges of detecting toxic content in Chinese language using language models, highlighting the impact of perturbation strategies.", "motivation": "Detecting toxic content is important for ensuring safety in language use, yet many state-of-the-art models struggle with perturbed toxic Chinese texts.", "method": "We propose a taxonomy of perturbation strategies and create a dataset to benchmark 9 state-of-the-art language models for their ability to detect perturbed toxic Chinese content.", "result": "Our findings indicate that LLMs are less effective in detecting perturbed multimodal toxic content and that in-context learning and supervised fine-tuning with a few perturbed examples can lead to overcorrection.", "conclusion": "The study reveals critical issues in the deployment of LLMs for Chinese toxic content detection and suggests the need for more robust model training strategies.", "key_contributions": ["Proposed a taxonomy of perturbation strategies for toxic Chinese content.", "Curated a dataset to benchmark LLMs on their ability to detect perturbed toxic content.", "Identified the tendency of models to overcorrect normal content when fine-tuned with few perturbed examples."], "limitations": "The paper primarily focuses on Chinese language models and may have limited applicability to other languages or contexts.", "keywords": ["toxic content detection", "multimodal Chinese", "large language models", "in-context learning", "supervised fine-tuning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24347", "pdf": "https://arxiv.org/pdf/2505.24347.pdf", "abs": "https://arxiv.org/abs/2505.24347", "title": "Fewer Hallucinations, More Verification: A Three-Stage LLM-Based Framework for ASR Error Correction", "authors": ["Yangui Fang", "Baixu Cheng", "Jing Peng", "Xu Li", "Yu Xi", "Chengwei Zhang", "Guohui Zhong"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) error correction aims to correct\nrecognition errors while preserving accurate text. Although traditional\napproaches demonstrate moderate effectiveness, LLMs offer a paradigm that\neliminates the need for training and labeled data. However, directly using LLMs\nwill encounter hallucinations problem, which may lead to the modification of\nthe correct text. To address this problem, we propose the Reliable LLM\nCorrection Framework (RLLM-CF), which consists of three stages: (1) error\npre-detection, (2) chain-of-thought sub-tasks iterative correction, and (3)\nreasoning process verification. The advantage of our method is that it does not\nrequire additional information or fine-tuning of the model, and ensures the\ncorrectness of the LLM correction under multi-pass programming. Experiments on\nAISHELL-1, AISHELL-2, and Librispeech show that the GPT-4o model enhanced by\nour framework achieves 21%, 11%, 9%, and 11.4% relative reductions in CER/WER.", "AI": {"tldr": "Introducing a framework for correcting ASR errors using LLMs without additional training, minimizing hallucinations and ensuring text accuracy.", "motivation": "To improve ASR error correction while avoiding the hallucination problems of traditional LLMs which can alter correct texts.", "method": "The Reliable LLM Correction Framework (RLLM-CF) consists of three stages: error pre-detection, iterative correction via chain-of-thought sub-tasks, and reasoning process verification.", "result": "The GPT-4o model, enhanced by the RLLM-CF framework, achieves significant reductions in Character Error Rate (CER) and Word Error Rate (WER) across multiple ASR datasets.", "conclusion": "The proposed framework effectively addresses the issues related to LLM hallucinations and improves the reliability of ASR error correction without additional data or fine-tuning.", "key_contributions": ["Introduction of the RLLM-CF for ASR error correction", "Demonstration of significant CER/WER reductions", "Proving effective correction without the necessity for training or fine-tuning"], "limitations": "", "keywords": ["Speech Recognition", "Error Correction", "Large Language Models", "Machine Learning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24354", "pdf": "https://arxiv.org/pdf/2505.24354.pdf", "abs": "https://arxiv.org/abs/2505.24354", "title": "Unifying Language Agent Algorithms with Graph-based Orchestration Engine for Reproducible Agent Research", "authors": ["Qianqian Zhang", "Jiajia Liao", "Heting Ying", "Yibo Ma", "Haozhan Shen", "Jingcheng Li", "Peng Liu", "Lu Zhang", "Chunxin Fang", "Kyusong Lee", "Ruochen Xu", "Tiancheng Zhao"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Demo", "summary": "Language agents powered by large language models (LLMs) have demonstrated\nremarkable capabilities in understanding, reasoning, and executing complex\ntasks. However, developing robust agents presents significant challenges:\nsubstantial engineering overhead, lack of standardized components, and\ninsufficient evaluation frameworks for fair comparison. We introduce Agent\nGraph-based Orchestration for Reasoning and Assessment (AGORA), a flexible and\nextensible framework that addresses these challenges through three key\ncontributions: (1) a modular architecture with a graph-based workflow engine,\nefficient memory management, and clean component abstraction; (2) a\ncomprehensive suite of reusable agent algorithms implementing state-of-the-art\nreasoning approaches; and (3) a rigorous evaluation framework enabling\nsystematic comparison across multiple dimensions. Through extensive experiments\non mathematical reasoning and multimodal tasks, we evaluate various agent\nalgorithms across different LLMs, revealing important insights about their\nrelative strengths and applicability. Our results demonstrate that while\nsophisticated reasoning approaches can enhance agent capabilities, simpler\nmethods like Chain-of-Thought often exhibit robust performance with\nsignificantly lower computational overhead. AGORA not only simplifies language\nagent development but also establishes a foundation for reproducible agent\nresearch through standardized evaluation protocols.", "AI": {"tldr": "AGORA is a framework for developing language agents using LLMs, focusing on modularity, reusable algorithms, and robust evaluation.", "motivation": "To address significant challenges in developing language agents, including engineering overhead, lack of standardized components, and insufficient evaluation methods.", "method": "The AGORA framework uses a modular architecture with a graph-based workflow engine, efficient memory management, and a comprehensive suite of reusable agent algorithms for state-of-the-art reasoning approaches.", "result": "Extensive experiments highlight that while advanced reasoning methods can boost agent capabilities, simpler approaches like Chain-of-Thought perform well with lower computational costs.", "conclusion": "AGORA facilitates easier language agent development and standardized evaluation, promoting reproducible research in the field.", "key_contributions": ["Modular architecture and graph-based workflow engine", "Suite of reusable agent algorithms", "Rigorous evaluation framework for comparative analysis"], "limitations": "", "keywords": ["large language models", "language agents", "evaluation frameworks", "reasoning", "modularity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24355", "pdf": "https://arxiv.org/pdf/2505.24355.pdf", "abs": "https://arxiv.org/abs/2505.24355", "title": "Multilingual Gloss-free Sign Language Translation: Towards Building a Sign Language Foundation Model", "authors": ["Sihan Tan", "Taro Miyazaki", "Kazuhiro Nakadai"], "categories": ["cs.CL"], "comment": null, "summary": "Sign Language Translation (SLT) aims to convert sign language (SL) videos\ninto spoken language text, thereby bridging the communication gap between the\nsign and the spoken community. While most existing works focus on translating a\nsingle sign language into a single spoken language (one-to-one SLT), leveraging\nmultilingual resources could mitigate low-resource issues and enhance\naccessibility. However, multilingual SLT (MLSLT) remains unexplored due to\nlanguage conflicts and alignment difficulties across SLs and spoken languages.\nTo address these challenges, we propose a multilingual gloss-free model with\ndual CTC objectives for token-level SL identification and spoken text\ngeneration. Our model supports 10 SLs and handles one-to-one, many-to-one, and\nmany-to-many SLT tasks, achieving competitive performance compared to\nstate-of-the-art methods on three widely adopted benchmarks: multilingual\nSP-10, PHOENIX14T, and CSL-Daily.", "AI": {"tldr": "This paper presents a multilingual gloss-free model for sign language translation (SLT) that addresses challenges in translating multiple sign languages into spoken languages by leveraging multilingual resources and achieving competitive performance across various SLT tasks.", "motivation": "To bridge the communication gap between sign and spoken communities and to enhance accessibility in sign language translation by exploring multilingual sign language translation (MLSLT).", "method": "A multilingual gloss-free model with dual CTC objectives is proposed for token-level sign language identification and spoken text generation, capable of handling one-to-one, many-to-one, and many-to-many SLT tasks.", "result": "The model supports 10 sign languages and achieves competitive performance compared to state-of-the-art methods on benchmarks such as multilingual SP-10, PHOENIX14T, and CSL-Daily.", "conclusion": "The proposed model effectively addresses low-resource issues and enhances accessibility in sign language translation through its multilingual capabilities.", "key_contributions": ["Introduction of a multilingual gloss-free model for SLT", "Support for multiple SLT tasks including many-to-one and many-to-many", "Competitive performance on established SLT benchmarks"], "limitations": "", "keywords": ["Sign Language Translation", "Multilingual Resources", "Machine Learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.24362", "pdf": "https://arxiv.org/pdf/2505.24362.pdf", "abs": "https://arxiv.org/abs/2505.24362", "title": "Knowing Before Saying: LLM Representations Encode Information About Chain-of-Thought Success Before Completion", "authors": ["Anum Afzal", "Florian Matthes", "Gal Chechik", "Yftah Ziser"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well \\emph{even before a\nsingle token is generated}, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.\\footnote{Code and\ndata is available at\n\\href{https://github.com/anum94/CoTpred}{\\texttt{github.com/anum94/CoTpred}}.", "AI": {"tldr": "This paper explores predicting the success of zero-shot Chain-of-Thought (CoT) processes early using a probing classifier based on LLMs, discovering that early representations carry critical information for reasoning.", "motivation": "Investigating whether the success of a zero-shot CoT process can be anticipated before generating any tokens.", "method": "Utilized a probing classifier based on LLM representations to evaluate the success of CoT processes before token generation, contrasting it with a BERT-based baseline that uses generated tokens.", "result": "The probing classifier outperformed the BERT baseline and showed strong predictive capability from early representations, indicating effective reasoning can be identified early and that truncated reasoning can still enhance performance over no CoT.", "conclusion": "Optimizing CoT efficiency may be possible by utilizing early representations to guide methods, like supervised or reinforcement learning, for effective early stopping in reasoning processes.", "key_contributions": ["Demonstrated predictive capability of early representations in CoT processes", "Showed that early stopping can improve performance over no CoT", "Provided insights for optimizing CoT efficiency using probing classifiers"], "limitations": "", "keywords": ["Chain-of-Thought", "Probing Classifier", "Zero-Shot Learning", "Early Stopping", "LLM"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.24377", "pdf": "https://arxiv.org/pdf/2505.24377.pdf", "abs": "https://arxiv.org/abs/2505.24377", "title": "LLM Inference Enhanced by External Knowledge: A Survey", "authors": ["Yu-Hsuan Lin", "Qian-Hui Chen", "Yi-Jie Cheng", "Jia-Ren Zhang", "Yi-Hung Liu", "Liang-Yu Hsia", "Yun-Nung Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have enhanced\nnatural-language reasoning. However, their limited parametric memory and\nsusceptibility to hallucination present persistent challenges for tasks\nrequiring accurate, context-based inference. To overcome these limitations, an\nincreasing number of studies have proposed leveraging external knowledge to\nenhance LLMs. This study offers a systematic exploration of strategies for\nusing external knowledge to enhance LLMs, beginning with a taxonomy that\ncategorizes external knowledge into unstructured and structured data. We then\nfocus on structured knowledge, presenting distinct taxonomies for tables and\nknowledge graphs (KGs), detailing their integration paradigms with LLMs, and\nreviewing representative methods. Our comparative analysis further highlights\nthe trade-offs among interpretability, scalability, and performance, providing\ninsights for developing trustworthy and generalizable knowledge-enhanced LLMs.", "AI": {"tldr": "This paper systematically explores strategies for enhancing large language models (LLMs) with external knowledge, focusing on structured data like tables and knowledge graphs.", "motivation": "The study addresses the limitations of LLMs, specifically their limited memory and hallucination tendencies, by exploring how external knowledge can improve context-based reasoning.", "method": "The paper categorizes external knowledge into unstructured and structured data, focusing primarily on structured knowledge such as tables and knowledge graphs, and reviewing various integration methods with LLMs.", "result": "The comparative analysis reveals trade-offs in interpretability, scalability, and performance when integrating structured knowledge into LLMs, providing valuable insights for future development.", "conclusion": "The findings suggest a pathway for creating more trustworthy and generalizable knowledge-enhanced LLMs by leveraging structured external knowledge effectively.", "key_contributions": ["Systematic taxonomy of external knowledge types for LLMs.", "Detailed analysis of integration paradigms for tables and knowledge graphs with LLMs.", "Comparative insights into the trade-offs in performance and interpretability of enhanced LLMs."], "limitations": "", "keywords": ["large language models", "external knowledge", "knowledge graphs", "structured data", "natural language reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24388", "pdf": "https://arxiv.org/pdf/2505.24388.pdf", "abs": "https://arxiv.org/abs/2505.24388", "title": "ClueAnchor: Clue-Anchored Knowledge Reasoning Exploration and Optimization for Retrieval-Augmented Generation", "authors": ["Hao Chen", "Yukun Yan", "Sen Mei", "Wanxiang Che", "Zhenghao Liu", "Qi Shi", "Xinze Li", "Yuchun Fan", "Pengcheng Huang", "Qiushi Xiong", "Zhiyuan Liu", "Maosong Sun"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) augments Large Language Models (LLMs)\nwith external knowledge to improve factuality. However, existing RAG systems\nfrequently underutilize the retrieved documents, failing to extract and\nintegrate the key clues needed to support faithful and interpretable reasoning,\nespecially in cases where relevant evidence is implicit, scattered, or obscured\nby noise. To address this issue, we propose ClueAnchor, a novel framework for\nenhancing RAG via clue-anchored reasoning exploration and optimization.\nClueAnchor extracts key clues from retrieved content and generates multiple\nreasoning paths based on different knowledge configurations, optimizing the\nmodel by selecting the most effective one through reward-based preference\noptimization. Experiments show that ClueAnchor significantly outperforms prior\nRAG baselines in reasoning completeness and robustness. Further analysis\nconfirms its strong resilience to noisy or partially relevant retrieved\ncontent, as well as its capability to identify supporting evidence even in the\nabsence of explicit clue supervision during inference.", "AI": {"tldr": "ClueAnchor enhances Retrieval-Augmented Generation (RAG) for better reasoning by extracting key clues from retrieved documents and optimizing reasoning paths based on knowledge configurations.", "motivation": "To improve factuality in RAG systems, which often underutilize retrieved documents and struggle to integrate implicit and obscure evidence for reasoning.", "method": "ClueAnchor extracts key clues from retrieved content to generate multiple reasoning paths. It optimizes the model by selecting the most effective path through reward-based preference optimization.", "result": "ClueAnchor significantly outperforms existing RAG baselines in terms of reasoning completeness and robustness, and demonstrates resilience to noise in the retrieved content.", "conclusion": "ClueAnchor effectively identifies supporting evidence without requiring explicit clue supervision during inference, enhancing the overall performance of RAG systems.", "key_contributions": ["Proposed a novel ClueAnchor framework for RAG.", "Introduced clue-anchored reasoning exploration and optimization.", "Demonstrated significant performance improvements in reasoning robustness."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "reasoning paths"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24409", "pdf": "https://arxiv.org/pdf/2505.24409.pdf", "abs": "https://arxiv.org/abs/2505.24409", "title": "LLMs Are Globally Multilingual Yet Locally Monolingual: Exploring Knowledge Transfer via Language and Thought Theory", "authors": ["Eojin Kang", "Juae Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multilingual large language models (LLMs) open up new possibilities for\nleveraging information across languages, but their factual knowledge recall\nremains inconsistent depending on the input language. While previous studies\nhave attempted to address this issue through English-based prompting and\nevaluation, we explore non-English to English transfer via Language and Thought\nTheory. This perspective allows us to examine language-thought binding in LLMs\nand uncover why factual knowledge often fails to transfer effectively. We\npropose the Language-to-Thought (L2T) prompting strategy, which analyzes the\nrelationship between input language, internal cognitive processes, and\nknowledge. Experimental results challenge the assumption that English-based\napproaches consistently outperform other languages and offer a novel insight\nthat aligning the model's internal thought with the knowledge required for the\ntask is critical for successful cross-lingual transfer. Furthermore, we show\nthat applying L2T during training can alleviate LLMs' reliance on the input\nlanguage and facilitate cross-linguistic knowledge integration without\ntranslation-based learning. Code and datasets will be available.", "AI": {"tldr": "This paper investigates how multilingual large language models (LLMs) can effectively transfer factual knowledge across languages using a novel prompting strategy.", "motivation": "To address the inconsistent recall of factual knowledge by multilingual LLMs depending on the input language, particularly focusing on the effectiveness of English-based prompting.", "method": "The paper proposes a Language-to-Thought (L2T) prompting strategy that examines the interaction between the input language, the internal cognitive processes of the model, and the factual knowledge expected in tasks.", "result": "Experimental results reveal that English-based prompting does not consistently outperform non-English methods and highlight the importance of aligning internal cognitive thought with the task-specific knowledge for better cross-lingual transfer.", "conclusion": "The application of the L2T strategy during training can reduce the reliance on the input language and enhance knowledge integration across languages without relying on translation.", "key_contributions": ["Introduction of the Language-to-Thought (L2T) prompting strategy", "Challenge to the dominance of English-based prompting", "Experimental findings emphasizing the importance of cognitive alignment for knowledge transfer"], "limitations": "", "keywords": ["multilingual models", "large language models", "cross-lingual transfer", "Language-to-Thought", "fact recall"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24423", "pdf": "https://arxiv.org/pdf/2505.24423.pdf", "abs": "https://arxiv.org/abs/2505.24423", "title": "MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs", "authors": ["Zhiwei Liu", "Lingfei Qian", "Qianqian Xie", "Jimin Huang", "Kailai Yang", "Sophia Ananiadou"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models and vision-language models (which we jointly call LMs)\nhave transformed NLP and CV, demonstrating remarkable potential across various\nfields. However, their capabilities in affective analysis (i.e. sentiment\nanalysis and emotion detection) remain underexplored. This gap is largely due\nto the absence of comprehensive evaluation benchmarks, and the inherent\ncomplexity of affective analysis tasks. In this paper, we introduce MMAFFBen,\nthe first extensive open-source benchmark for multilingual multimodal affective\nanalysis. MMAFFBen encompasses text, image, and video modalities across 35\nlanguages, covering four key affective analysis tasks: sentiment polarity,\nsentiment intensity, emotion classification, and emotion intensity. Moreover,\nwe construct the MMAFFIn dataset for fine-tuning LMs on affective analysis\ntasks, and further develop MMAFFLM-3b and MMAFFLM-7b based on it. We evaluate\nvarious representative LMs, including GPT-4o-mini, providing a systematic\ncomparison of their affective understanding capabilities. This project is\navailable at https://github.com/lzw108/MMAFFBen.", "AI": {"tldr": "Introduction of the MMAFFBen benchmark for multilingual multimodal affective analysis.", "motivation": "To address the lack of comprehensive evaluation benchmarks for affective analysis in language models and vision-language models.", "method": "Development of the MMAFFBen benchmark encompassing text, image, and video modalities across 35 languages for various affective analysis tasks, along with the construction of the MMAFFIn dataset for fine-tuning language models.", "result": "MMAFFBen provides the first extensive open-source benchmark and includes a systematic comparison of various language models' capabilities in affective analysis.", "conclusion": "This work lays the groundwork for further research in affective analysis using multimodal data and language models.", "key_contributions": ["Introduction of the MMAFFBen benchmark for multilingual multimodal affective analysis.", "Creation of the MMAFFIn dataset for fine-tuning LMs on affective tasks.", "Systematic evaluation of LMs including GPT-4o-mini in affective understanding."], "limitations": "Work is in progress, implying ongoing development and possible future updates to the benchmark.", "keywords": ["affective analysis", "language models", "multimodal", "sentiment analysis", "emotion detection"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24427", "pdf": "https://arxiv.org/pdf/2505.24427.pdf", "abs": "https://arxiv.org/abs/2505.24427", "title": "Donate or Create? Comparing Data Collection Strategies for Emotion-labeled Multimodal Social Media Posts", "authors": ["Christopher Bagdon", "Aidan Combs", "Carina Silberer", "Roman Klinger"], "categories": ["cs.CL"], "comment": "Published at ACL 2025", "summary": "Accurate modeling of subjective phenomena such as emotion expression requires\ndata annotated with authors' intentions. Commonly such data is collected by\nasking study participants to donate and label genuine content produced in the\nreal world, or create content fitting particular labels during the study.\nAsking participants to create content is often simpler to implement and\npresents fewer risks to participant privacy than data donation. However, it is\nunclear if and how study-created content may differ from genuine content, and\nhow differences may impact models. We collect study-created and genuine\nmultimodal social media posts labeled for emotion and compare them on several\ndimensions, including model performance. We find that compared to genuine\nposts, study-created posts are longer, rely more on their text and less on\ntheir images for emotion expression, and focus more on emotion-prototypical\nevents. The samples of participants willing to donate versus create posts are\ndemographically different. Study-created data is valuable to train models that\ngeneralize well to genuine data, but realistic effectiveness estimates require\ngenuine data.", "AI": {"tldr": "The paper compares study-created and genuine social media posts labeled for emotion expression, highlighting differences that affect model performance.", "motivation": "To understand if and how study-created content differs from genuine content and the impact on emotion expression models.", "method": "We collected both study-created and genuine multimodal social media posts and compared them across various dimensions, including model performance.", "result": "Study-created posts are longer, more reliant on text than images, and focus on emotion-prototypical events compared to genuine posts. The demographics of participants donating versus creating posts differ.", "conclusion": "While study-created data can train models that generalize to genuine data, accurate effectiveness assessments require genuine data.", "key_contributions": ["Comparison of study-created and genuine content for emotion expression.", "Identification of key differences in length, content reliance, and event focus.", "Analysis of demographic differences in participant behavior."], "limitations": "The study may not account for all potential demographic variations and their impact on model training.", "keywords": ["Emotion expression", "Social media", "Model performance", "Multimodal analysis", "Content creation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24428", "pdf": "https://arxiv.org/pdf/2505.24428.pdf", "abs": "https://arxiv.org/abs/2505.24428", "title": "Model Unlearning via Sparse Autoencoder Subspace Guided Projections", "authors": ["Xu Wang", "Zihao Li", "Benyou Wang", "Yan Hu", "Difan Zou"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) store vast amounts of information, making them\npowerful yet raising privacy and safety concerns when selective knowledge\nremoval is required. Existing unlearning strategies, ranging from\ngradient-based fine-tuning and model editing to sparse autoencoder (SAE)\nsteering, either lack interpretability or fail to provide a robust defense\nagainst adversarial prompts. We propose SAE-Guided Subspace Projection\nUnlearning (SSPU), a novel framework that leverages SAE features to drive\ntargeted updates in the model's parameter space, enabling precise,\ninterpretable, and robust unlearning. SSPU's three-stage pipeline performs\ndata-driven layer and feature selection, subspace construction via QR\ndecomposition, and constrained optimization that controls activations into an\n\"irrelevant\" subspace while preserving retained knowledge. Overall, we use SAE\nfeatures to construct a subspace that supervises unlearning, refining the loss\nand adding a regularization term to guide interpretable parameter updates. In\nexperiments on the WMDP-Cyber forget set and three utility benchmarks (MMLU,\nTruthfulQA, GSM8K), SSPU reduces harmful knowledge accuracy by 3.22% compared\nto the strongest baseline. It also improves adversarial robustness, lowering\nmalicious accuracy under jailbreak prompts compared to baselines. Our findings\nexpose the limitations of prior unlearning methods and demonstrate how\ninterpretable subspace-guided optimization can achieve robust, controllable\nmodel behavior.", "AI": {"tldr": "SSPU is a novel framework for interpretable and robust unlearning in LLMs that leverages sparse autoencoder features for targeted parameter updates.", "motivation": "To address privacy and safety concerns in LLMs regarding selective knowledge removal, as existing strategies lack interpretability and robustness against adversarial prompts.", "method": "The SSPU framework uses a three-stage pipeline involving data-driven layer selection, QR decomposition for subspace construction, and constrained optimization for unlearning while preserving knowledge.", "result": "SSPU reduces harmful knowledge accuracy by 3.22% compared to the strongest baseline and improves robustness against adversarial jailbreak prompts.", "conclusion": "The study reveals limitations of prior unlearning methods and demonstrates that interpretable subspace-guided optimization can enhance model behavior control during unlearning.", "key_contributions": ["Introduces SSPU framework for targeted unlearning in LLMs.", "Implements an interpretable optimization approach using SAE features.", "Demonstrates improved adversarial robustness and reduced harmful knowledge retention."], "limitations": "The effectiveness of SSPU may vary depending on the model architecture and specific datasets used.", "keywords": ["LLMs", "unlearning", "adversarial robustness", "sparse autoencoders", "machine learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24448", "pdf": "https://arxiv.org/pdf/2505.24448.pdf", "abs": "https://arxiv.org/abs/2505.24448", "title": "Exploring the Impact of Occupational Personas on Domain-Specific QA", "authors": ["Eojin Kang", "Jaehyuk Yu", "Juae Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on personas have improved the way Large Language Models (LLMs)\ninteract with users. However, the effect of personas on domain-specific\nquestion-answering (QA) tasks remains a subject of debate. This study analyzes\nwhether personas enhance specialized QA performance by introducing two types of\npersona: Profession-Based Personas (PBPs) (e.g., scientist), which directly\nrelate to domain expertise, and Occupational Personality-Based Personas (OPBPs)\n(e.g., scientific person), which reflect cognitive tendencies rather than\nexplicit expertise. Through empirical evaluations across multiple scientific\ndomains, we demonstrate that while PBPs can slightly improve accuracy, OPBPs\noften degrade performance, even when semantically related to the task. Our\nfindings suggest that persona relevance alone does not guarantee effective\nknowledge utilization and that they may impose cognitive constraints that\nhinder optimal knowledge application. Future research can explore how nuanced\ndistinctions in persona representations guide LLMs, potentially contributing to\nreasoning and knowledge retrieval that more closely mirror human social\nconceptualization.", "AI": {"tldr": "This study investigates the impact of different types of personas on the performance of Large Language Models in domain-specific question-answering tasks.", "motivation": "To explore how personas affect the interaction and specialized performance of Large Language Models (LLMs) in domain-specific tasks, particularly in question-answering scenarios.", "method": "The study analyzes the effects of two types of personas—Profession-Based Personas (PBPs) and Occupational Personality-Based Personas (OPBPs)—through empirical evaluations across various scientific domains.", "result": "While Profession-Based Personas slightly improve accuracy in domain-specific QA tasks, Occupational Personality-Based Personas often degrade performance despite being semantically related to the tasks, highlighting the complexity of persona impact.", "conclusion": "Persona relevance does not ensure effective knowledge utilization in LLMs and may introduce cognitive constraints that impede optimal performance; further research is needed to refine persona representations for better alignment with human reasoning.", "key_contributions": ["Introduces and tests Profession-Based and Occupational Personality-Based personas in the context of LLMs.", "Provides empirical evidence showing that PBPs can improve QA accuracy, while OPBPs may hinder performance.", "Suggests directions for future research on nuanced persona representations in LLMs."], "limitations": "Results are limited to specific scientific domains and may not generalize across all areas of inquiry.", "keywords": ["Large Language Models", "personas", "question-answering", "domain-specific", "cognitive constraints"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24449", "pdf": "https://arxiv.org/pdf/2505.24449.pdf", "abs": "https://arxiv.org/abs/2505.24449", "title": "When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways", "authors": ["Kailin Jiang", "Yuntao Du", "Yukai Ding", "Yuchen Ren", "Ning Jiang", "Zhi Gao", "Zilong Zheng", "Lei Liu", "Bin Li", "Qing Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large language/multimodal models (LLMs/LMMs) store extensive pre-trained\nknowledge but struggle to maintain consistency with real-world updates, making\nit difficult to avoid catastrophic forgetting while acquiring evolving\nknowledge. Previous work focused on constructing textual knowledge datasets and\nexploring knowledge injection in LLMs, lacking exploration of multimodal\nevolving knowledge injection in LMMs. To address this, we propose the EVOKE\nbenchmark to evaluate LMMs' ability to inject multimodal evolving knowledge in\nreal-world scenarios. Meanwhile, a comprehensive evaluation of multimodal\nevolving knowledge injection revealed two challenges: (1) Existing knowledge\ninjection methods perform terribly on evolving knowledge. (2) Supervised\nfine-tuning causes catastrophic forgetting, particularly instruction following\nability is severely compromised. Additionally, we provide pathways and find\nthat: (1) Text knowledge augmentation during the training phase improves\nperformance, while image augmentation cannot achieve it. (2) Continual learning\nmethods, especially Replay and MoELoRA, effectively mitigate forgetting. Our\nfindings indicate that current knowledge injection methods have many\nlimitations on evolving knowledge, which motivates further research on more\nefficient and stable knowledge injection methods.", "AI": {"tldr": "This paper addresses the challenges of multimodal knowledge injection in language/multimodal models (LLMs/LMMs), proposing the EVOKE benchmark to evaluate their ability to incorporate evolving knowledge.", "motivation": "LLMs and LMMs struggle with catastrophic forgetting and maintaining consistency with real-world updates, highlighting the need for effective evolving knowledge injection methods.", "method": "The authors introduce the EVOKE benchmark for assessing multimodal evolving knowledge injection and evaluate existing methods while analyzing the impact of text and image augmentation during training.", "result": "Existing knowledge injection techniques perform poorly on evolving knowledge, and supervised fine-tuning often leads to catastrophic forgetting.", "conclusion": "The paper concludes that there are significant limitations in current approaches to evolving knowledge injection, suggesting pathways for improvement through text augmentation and continual learning methods.", "key_contributions": ["Introduction of the EVOKE benchmark", "Identification of challenges in multimodal evolving knowledge injection", "Proposed methods to enhance performance through continual learning techniques"], "limitations": "Current methods for evolving knowledge injection show significant limitations; further research is necessary.", "keywords": ["multimodal models", "evolving knowledge", "knowledge injection", "continual learning", "catastrophic forgetting"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24455", "pdf": "https://arxiv.org/pdf/2505.24455.pdf", "abs": "https://arxiv.org/abs/2505.24455", "title": "Domain Pre-training Impact on Representations", "authors": ["Cesar Gonzalez-Gutierrez", "Ariadna Quattoni"], "categories": ["cs.CL"], "comment": null, "summary": "This empirical study analyzes the effects of the pre-training corpus on the\nquality of learned transformer representations. We focus on the representation\nquality induced solely through pre-training. Our experiments show that\npre-training on a small, specialized corpus can yield effective\nrepresentations, and that the success of combining a generic and a specialized\ncorpus depends on the distributional similarity between the target task and the\nspecialized corpus.", "AI": {"tldr": "The study examines how the pre-training corpus affects transformer representation quality, highlighting the efficacy of specialized corpora.", "motivation": "To understand how different pre-training corpora impact the quality of learned representations in transformers.", "method": "Empirical analysis comparing transformer representations learned from various pre-training corpora (specialized vs. generic).", "result": "Demonstrated that a small, specialized corpus can produce effective representations; success depends on the similarity between the task and the corpus.", "conclusion": "Combining generic and specialized corpora can be beneficial, contingent on distributional similarity to target tasks.", "key_contributions": ["Insights into the impact of pre-training corpus on transformer models", "Empirical evidence of effective representations from specialized corpora", "Guidelines for corpus selection based on task similarity"], "limitations": "", "keywords": ["transformer", "pre-training", "representation quality", "corpus analysis", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.24456", "pdf": "https://arxiv.org/pdf/2505.24456.pdf", "abs": "https://arxiv.org/abs/2505.24456", "title": "CaMMT: Benchmarking Culturally Aware Multimodal Machine Translation", "authors": ["Emilio Villa-Cueva", "Sholpan Bolatzhanova", "Diana Turmakhan", "Kareem Elzeky", "Henok Biadglign Ademtew", "Alham Fikri Aji", "Israel Abebe Azime", "Jinheon Baek", "Frederico Belcavello", "Fermin Cristobal", "Jan Christian Blaise Cruz", "Mary Dabre", "Raj Dabre", "Toqeer Ehsan", "Naome A Etori", "Fauzan Farooqui", "Jiahui Geng", "Guido Ivetta", "Thanmay Jayakumar", "Soyeong Jeong", "Zheng Wei Lim", "Aishik Mandal", "Sofia Martinelli", "Mihail Minkov Mihaylov", "Daniil Orel", "Aniket Pramanick", "Sukannya Purkayastha", "Israfel Salazar", "Haiyue Song", "Tiago Timponi Torrent", "Debela Desalegn Yadeta", "Injy Hamed", "Atnafu Lambebo Tonja", "Thamar Solorio"], "categories": ["cs.CL"], "comment": null, "summary": "Cultural content poses challenges for machine translation systems due to the\ndifferences in conceptualizations between cultures, where language alone may\nfail to convey sufficient context to capture region-specific meanings. In this\nwork, we investigate whether images can act as cultural context in multimodal\ntranslation. We introduce CaMMT, a human-curated benchmark of over 5,800\ntriples of images along with parallel captions in English and regional\nlanguages. Using this dataset, we evaluate five Vision Language Models (VLMs)\nin text-only and text+image settings. Through automatic and human evaluations,\nwe find that visual context generally improves translation quality, especially\nin handling Culturally-Specific Items (CSIs), disambiguation, and correct\ngender usage. By releasing CaMMT, we aim to support broader efforts in building\nand evaluating multimodal translation systems that are better aligned with\ncultural nuance and regional variation.", "AI": {"tldr": "This paper explores the use of images as cultural context to improve machine translation quality through a dataset called CaMMT, which includes images and captions in multiple languages.", "motivation": "The paper addresses the challenges cultural content presents to machine translation systems due to differences in cultural conceptualizations which can lead to misinterpretations in language alone.", "method": "The authors introduce CaMMT, a benchmark dataset containing over 5,800 triples of images and parallel captions in English and regional languages to evaluate Vision Language Models (VLMs) in both text-only and text+image settings.", "result": "The evaluation shows that visual context significantly enhances translation quality, particularly in navigating Culturally-Specific Items (CSIs), resolving ambiguities, and ensuring proper gender usage.", "conclusion": "By releasing the CaMMT dataset, the authors aim to foster progress in developing and assessing multimodal translation systems that respect cultural nuances and regional differences.", "key_contributions": ["Introduction of CaMMT for multimodal translation evaluation", "Demonstration of improved translation quality with visual context", "Identification of specific areas, such as CSIs, where images enhance language translation"], "limitations": "The study may be limited by the specific dataset and models used, potentially affecting generalizability across different languages or cultures.", "keywords": ["machine translation", "multimodal translation", "cultural context", "Vision Language Models", "CaMMT"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.24472", "pdf": "https://arxiv.org/pdf/2505.24472.pdf", "abs": "https://arxiv.org/abs/2505.24472", "title": "VietMix: A Naturally Occurring Vietnamese-English Code-Mixed Corpus with Iterative Augmentation for Machine Translation", "authors": ["Hieu Tran", "Phuong-Anh Nguyen-Le", "Huy Nghiem", "Quang-Nhan Nguyen", "Wei Ai", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine translation systems fail when processing code-mixed inputs for\nlow-resource languages. We address this challenge by curating VietMix, a\nparallel corpus of naturally occurring code-mixed Vietnamese text paired with\nexpert English translations. Augmenting this resource, we developed a\ncomplementary synthetic data generation pipeline. This pipeline incorporates\nfiltering mechanisms to ensure syntactic plausibility and pragmatic\nappropriateness in code-mixing patterns. Experimental validation shows our\nnaturalistic and complementary synthetic data boost models' performance,\nmeasured by translation quality estimation scores, of up to 71.84 on COMETkiwi\nand 81.77 on XCOMET. Triangulating positive results with LLM-based assessments,\naugmented models are favored over seed fine-tuned counterparts in approximately\n49% of judgments (54-56% excluding ties). VietMix and our augmentation\nmethodology advance ecological validity in neural MT evaluations and establish\na framework for addressing code-mixed translation challenges across other\nlow-resource pairs.", "AI": {"tldr": "This paper presents VietMix, a parallel corpus of code-mixed Vietnamese text and its English translations, alongside a synthetic data generation pipeline to enhance machine translation for low-resource languages.", "motivation": "To improve machine translation systems which struggle with code-mixed inputs for low-resource languages.", "method": "The authors curated a parallel corpus of code-mixed Vietnamese text and expert English translations and developed a synthetic data generation pipeline with filtering mechanisms for plausibility and appropriateness.", "result": "The augmented models achieved a translation quality score of up to 71.84 on COMETkiwi and 81.77 on XCOMET, outperforming seed fine-tuned models in nearly 49% of LLM assessments.", "conclusion": "The VietMix resource and augmentation methodology enhance ecological validity in neural machine translation evaluations and provide a framework for code-mixed translations in other low-resource languages.", "key_contributions": ["Introduction of VietMix corpus for code-mixed translation", "Development of a complementary synthetic data generation pipeline", "Demonstration of performance improvements in machine translation models"], "limitations": "", "keywords": ["Machine Translation", "Code-Mixing", "Low-Resource Languages", "Data Augmentation", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.24480", "pdf": "https://arxiv.org/pdf/2505.24480.pdf", "abs": "https://arxiv.org/abs/2505.24480", "title": "Towards Effective Code-Integrated Reasoning", "authors": ["Fei Bai", "Yingqian Min", "Beichen Zhang", "Zhipeng Chen", "Wayne Xin Zhao", "Lei Fang", "Zheng Liu", "Zhongyuan Wang", "Ji-Rong Wen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report on Slow Thinking with LLMs: Code-Integrated\n  Reasoning", "summary": "In this paper, we investigate code-integrated reasoning, where models\ngenerate code when necessary and integrate feedback by executing it through a\ncode interpreter. To acquire this capability, models must learn when and how to\nuse external code tools effectively, which is supported by tool-augmented\nreinforcement learning (RL) through interactive learning. Despite its benefits,\ntool-augmented RL can still suffer from potential instability in the learning\ndynamics. In light of this challenge, we present a systematic approach to\nimproving the training effectiveness and stability of tool-augmented RL for\ncode-integrated reasoning. Specifically, we develop enhanced training\nstrategies that balance exploration and stability, progressively building\ntool-use capabilities while improving reasoning performance. Through extensive\nexperiments on five mainstream mathematical reasoning benchmarks, our model\ndemonstrates significant performance improvements over multiple competitive\nbaselines. Furthermore, we conduct an in-depth analysis of the mechanism and\neffect of code-integrated reasoning, revealing several key insights, such as\nthe extension of model's capability boundaries and the simultaneous improvement\nof reasoning efficiency through code integration. All data and code for\nreproducing this work are available at: https://github.com/RUCAIBox/CIR.", "AI": {"tldr": "This paper presents a systematic approach to improve tool-augmented reinforcement learning for code-integrated reasoning, enhancing training stability and effectiveness.", "motivation": "To investigate code-integrated reasoning capabilities of models and address instability in tool-augmented reinforcement learning (RL).", "method": "The authors developed enhanced training strategies that balance exploration and stability, focusing on progressively building tool-use capabilities while improving reasoning performance through experiments on mathematical reasoning benchmarks.", "result": "The proposed model demonstrates significant performance improvements compared to multiple competitive baselines on five mainstream mathematical reasoning benchmarks.", "conclusion": "The paper reveals key insights into code-integrated reasoning, including the extension of model's capability boundaries and improvements in reasoning efficiency through code execution.", "key_contributions": ["Systematic approach to enhance tool-augmented RL stability", "Demonstrated significant performance improvements on benchmarks", "In-depth analysis of code-integrated reasoning's effects"], "limitations": "", "keywords": ["code-integrated reasoning", "tool-augmented reinforcement learning", "training stability", "mathematical reasoning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24500", "pdf": "https://arxiv.org/pdf/2505.24500.pdf", "abs": "https://arxiv.org/abs/2505.24500", "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning for Enhancing LLMs' Social Intelligence", "authors": ["Guiyang Hou", "Xing Gao", "Yuchuan Wu", "Xiang Huang", "Wenqi Zhang", "Zhe Zheng", "Yongliang Shen", "Jialu Du", "Fei Huang", "Yongbin Li", "Weiming Lu"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 12 figures", "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.", "AI": {"tldr": "This paper presents TimeHC-RL, a novel framework to enhance the social intelligence of Large Language Models (LLMs) through a temporal-aware and hierarchical approach to cognitive reinforcement learning.", "motivation": "The need to improve LLMs' cognitive abilities in social domains is crucial, particularly since current research mainly focuses on IQ-related tasks such as mathematics and coding.", "method": "The proposed TimeHC-RL method incorporates temporal awareness into hierarchical reinforcement learning, aimed at leveraging different cognitive modes (System 1 and System 2) to enhance LLMs' performance in social intelligence tasks.", "result": "Experimental results demonstrate that TimeHC-RL outperforms the traditional System 2 RL method and enhances the social intelligence capabilities of a 7B model to a level competitive with advanced models like DeepSeek-R1 and OpenAI-O3.", "conclusion": "TimeHC-RL provides valuable insights into improving LLMs' social cognition through experimental validation and reveals the importance of incorporating diverse cognitive strategies in the learning process.", "key_contributions": ["Introduction of TimeHC-RL for enhancing LLM social intelligence", "Experimental validation against existing methods", "Uncovering insights into LLM cognitive development in social contexts"], "limitations": "", "keywords": ["Large Language Models", "cognitive reinforcement learning", "social intelligence"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2505.24523", "pdf": "https://arxiv.org/pdf/2505.24523.pdf", "abs": "https://arxiv.org/abs/2505.24523", "title": "Stress-testing Machine Generated Text Detection: Shifting Language Models Writing Style to Fool Detectors", "authors": ["Andrea Pedrotti", "Michele Papucci", "Cristiano Ciaccio", "Alessio Miaschi", "Giovanni Puccetti", "Felice Dell'Orletta", "Andrea Esuli"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Findings of ACL 2025", "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts.", "AI": {"tldr": "This paper presents a pipeline to test the resilience of state-of-the-art Machine-Generated Text (MGT) detectors against adversarial attacks, revealing their vulnerabilities and the need for improved detection methods.", "motivation": "With the rise of Generative AI and LLMs, concerns over misinformation from synthetic content have surged, necessitating robust benchmarks for detecting Machine-Generated Text.", "method": "A pipeline that fine-tunes language models using Direct Preference Optimization to alter MGT towards human-like text, subsequently testing the resilience of various MGT detectors.", "result": "MGT detectors demonstrated significant vulnerabilities, being easily fooled with a limited number of examples, leading to poor detection performance.", "conclusion": "The findings underscore the necessity for enhancing MGT detection methods to ensure robustness against linguistic adversarial attacks.", "key_contributions": ["Development of a testing pipeline for MGT detectors", "Fine-tuning approach to produce human-like text from MGT", "Analysis of weaknesses in existing MGT detection methods"], "limitations": "", "keywords": ["Generative AI", "Large Language Models", "Machine-Generated Text", "Detection methods", "Adversarial attacks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24525", "pdf": "https://arxiv.org/pdf/2505.24525.pdf", "abs": "https://arxiv.org/abs/2505.24525", "title": "Limited-Resource Adapters Are Regularizers, Not Linguists", "authors": ["Marcell Fekete", "Nathaniel R. Robinson", "Ernests Lavrinovics", "E. Djeride Jean-Baptiste", "Raj Dabre", "Johannes Bjerva", "Heather Lent"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual transfer from related high-resource languages is a\nwell-established strategy to enhance low-resource language technologies. Prior\nwork has shown that adapters show promise for, e.g., improving low-resource\nmachine translation (MT). In this work, we investigate an adapter souping\nmethod combined with cross-attention fine-tuning of a pre-trained MT model to\nleverage language transfer for three low-resource Creole languages, which\nexhibit relatedness to different language groups across distinct linguistic\ndimensions. Our approach improves performance substantially over baselines.\nHowever, we find that linguistic relatedness -- or even a lack thereof -- does\nnot covary meaningfully with adapter performance. Surprisingly, our\ncross-attention fine-tuning approach appears equally effective with randomly\ninitialized adapters, implying that the benefit of adapters in this setting\nlies in parameter regularization, and not in meaningful information transfer.\nWe provide analysis supporting this regularization hypothesis. Our findings\nunderscore the reality that neural language processing involves many success\nfactors, and that not all neural methods leverage linguistic knowledge in\nintuitive ways.", "AI": {"tldr": "This paper investigates an adapter souping method and cross-attention fine-tuning in improving low-resource Creole languages MT. It reveals surprising insights about adapter performance and the role of linguistic relatedness.", "motivation": "To enhance low-resource language technologies by leveraging cross-lingual transfer from high-resource languages and to understand the effectiveness of adapter souping and cross-attention in machine translation.", "method": "The study employs an adapter souping method alongside cross-attention fine-tuning on a pre-trained machine translation model for three low-resource Creole languages.", "result": "The approach shows substantial performance improvements over the baselines, despite findings that linguistic relatedness does not directly correlate with adapter performance.", "conclusion": "The benefits of adapters seem to derive from parameter regularization rather than meaningful cross-linguistic information transfer; neural language processing success factors are complex and not fully intuitive.", "key_contributions": ["Introduction of adapter souping combined with cross-attention fine-tuning for low-resource MT", "Findings challenge assumptions about linguistic relatedness and adapter performance", "Analysis supports the hypothesis of regularization benefits from adapters."], "limitations": "Limited to three low-resource Creole languages; findings may not generalize to all language pairs.", "keywords": ["cross-lingual transfer", "low-resource languages", "machine translation", "adapter souping", "cross-attention"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2505.24532", "pdf": "https://arxiv.org/pdf/2505.24532.pdf", "abs": "https://arxiv.org/abs/2505.24532", "title": "DEEPQUESTION: Systematic Generation of Real-World Challenges for Evaluating LLMs Performance", "authors": ["Ali Khoramfar", "Ali Ramezani", "Mohammad Mahdi Mohajeri", "Mohammad Javad Dousti", "Majid Nili Ahmadabadi", "Heshaam Faili"], "categories": ["cs.CL"], "comment": null, "summary": "LLMs often excel on standard benchmarks but falter on real-world tasks. We\nintroduce DeepQuestion, a scalable automated framework that augments existing\ndatasets based on Bloom's taxonomy and creates novel questions that trace\noriginal solution paths to probe evaluative and creative skills. Extensive\nexperiments across ten open-source and proprietary models, covering both\ngeneral-purpose and reasoning LLMs, reveal substantial performance drops (even\nup to 70% accuracy loss) on higher-order tasks, underscoring persistent gaps in\ndeep reasoning. Our work highlights the need for cognitively diverse benchmarks\nto advance LLM progress. DeepQuestion and related datasets will be released\nupon acceptance of the paper.", "AI": {"tldr": "DeepQuestion, a new framework for augmenting datasets and evaluating LLMs, reveals significant performance drops in higher-order tasks, showing the need for better cognitive benchmarks.", "motivation": "To address the performance gap of LLMs on real-world tasks and highlight the importance of cognitively diverse benchmarks.", "method": "Developed DeepQuestion to augment datasets using Bloom's taxonomy and create questions that assess evaluative and creative skills in LLMs.", "result": "Experiments showed up to 70% accuracy loss in higher-order tasks across ten different LLMs, revealing gaps in deep reasoning capabilities.", "conclusion": "There is a pressing need for cognitively diverse benchmarks to improve LLM performance in complex reasoning tasks.", "key_contributions": ["Introduction of DeepQuestion framework", "Creation of novel benchmarks based on Bloom's taxonomy", "Demonstrated significant performance drops in LLMs on higher-order tasks"], "limitations": "The study only includes specific open-source and proprietary models, which may not represent all LLMs.", "keywords": ["Large Language Models", "Bloom's taxonomy", "Deep reasoning", "Cognitive benchmarks", "Evaluation framework"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24538", "pdf": "https://arxiv.org/pdf/2505.24538.pdf", "abs": "https://arxiv.org/abs/2505.24538", "title": "Don't Erase, Inform! Detecting and Contextualizing Harmful Language in Cultural Heritage Collections", "authors": ["Orfeas Menis Mastromichalakis", "Jason Liartis", "Kristina Rose", "Antoine Isaac", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "Cultural Heritage (CH) data hold invaluable knowledge, reflecting the\nhistory, traditions, and identities of societies, and shaping our understanding\nof the past and present. However, many CH collections contain outdated or\noffensive descriptions that reflect historical biases. CH Institutions (CHIs)\nface significant challenges in curating these data due to the vast scale and\ncomplexity of the task. To address this, we develop an AI-powered tool that\ndetects offensive terms in CH metadata and provides contextual insights into\ntheir historical background and contemporary perception. We leverage a\nmultilingual vocabulary co-created with marginalized communities, researchers,\nand CH professionals, along with traditional NLP techniques and Large Language\nModels (LLMs). Available as a standalone web app and integrated with major CH\nplatforms, the tool has processed over 7.9 million records, contextualizing the\ncontentious terms detected in their metadata. Rather than erasing these terms,\nour approach seeks to inform, making biases visible and providing actionable\ninsights for creating more inclusive and accessible CH collections.", "AI": {"tldr": "This paper presents an AI tool for detecting and contextualizing offensive terms in Cultural Heritage metadata to promote inclusivity and accessibility.", "motivation": "Cultural Heritage collections often contain outdated or offensive descriptions that reflect historical biases, creating challenges for curation and representation.", "method": "The tool utilizes NLP techniques and Large Language Models (LLMs) alongside a multilingual vocabulary co-created with marginalized communities to detect offensive terms and provide contextual insights.", "result": "The tool has processed over 7.9 million records, providing contextual information about contentious terms in CH metadata to help institutions curate more inclusive collections.", "conclusion": "By informing users about historical bias rather than erasing offensive terms, the tool aims to create awareness and promote more inclusive practices in Cultural Heritage curation.", "key_contributions": ["Development of an AI-powered tool for detecting offensive metadata terms", "Collaborative design of a multilingual vocabulary with marginalized communities", "Processing of millions of records to provide contextual insights on historical biases"], "limitations": "", "keywords": ["Cultural Heritage", "AI tool", "offensive terms", "metadata", "inclusivity"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24539", "pdf": "https://arxiv.org/pdf/2505.24539.pdf", "abs": "https://arxiv.org/abs/2505.24539", "title": "Localizing Persona Representations in LLMs", "authors": ["Celia Cintas", "Miriam Rateike", "Erik Miehling", "Elizabeth Daly", "Skyler Speakman"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a study on how and where personas -- defined by distinct sets of\nhuman characteristics, values, and beliefs -- are encoded in the representation\nspace of large language models (LLMs). Using a range of dimension reduction and\npattern recognition methods, we first identify the model layers that show the\ngreatest divergence in encoding these representations. We then analyze the\nactivations within a selected layer to examine how specific personas are\nencoded relative to others, including their shared and distinct embedding\nspaces. We find that, across multiple pre-trained decoder-only LLMs, the\nanalyzed personas show large differences in representation space only within\nthe final third of the decoder layers. We observe overlapping activations for\nspecific ethical perspectives -- such as moral nihilism and utilitarianism --\nsuggesting a degree of polysemy. In contrast, political ideologies like\nconservatism and liberalism appear to be represented in more distinct regions.\nThese findings help to improve our understanding of how LLMs internally\nrepresent information and can inform future efforts in refining the modulation\nof specific human traits in LLM outputs. Warning: This paper includes\npotentially offensive sample statements.", "AI": {"tldr": "This study explores how personas are encoded within large language models (LLMs), identifying significant variations in representation across model layers and ethical perspectives.", "motivation": "To understand how and where different personas are represented in the internal architecture of large language models.", "method": "Utilized dimension reduction and pattern recognition methods to analyze activations in various layers of pre-trained decoder-only LLMs, focusing on the encoding of distinct human characteristics.", "result": "Significant representation differences are observed in the final third of the decoder layers for analyzed personas, with overlapping activations for certain ethical perspectives and distinct regions for political ideologies.", "conclusion": "The findings enhance the understanding of LLM internal representations and could guide enhancements in controlling human traits in LLM outputs.", "key_contributions": ["Identified key layers in LLMs that encode personas", "Demonstrated differences in ethical and political ideology representations", "Provided insights for refining LLM outputs with specific human traits"], "limitations": "Results may not generalize to all LLM architectures or training methods; includes sensitive content that may be offensive.", "keywords": ["personas", "large language models", "representation space", "ethical perspectives", "political ideologies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24544", "pdf": "https://arxiv.org/pdf/2505.24544.pdf", "abs": "https://arxiv.org/abs/2505.24544", "title": "Cross-Attention Speculative Decoding", "authors": ["Wei Zhong", "Manasa Bharadwaj", "Yixiao Wang", "Nikhil Verma", "Yipeng Ji", "Chul Lee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) is a widely adopted approach for accelerating\ninference in large language models (LLMs), particularly when the draft and\ntarget models are well aligned. However, state-of-the-art SD methods typically\nrely on tightly coupled, self-attention-based Transformer decoders, often\naugmented with auxiliary pooling or fusion layers. This coupling makes them\nincreasingly complex and harder to generalize across different models. We\npresent Budget EAGLE (Beagle), the first, to our knowledge,\ncross-attention-based Transformer decoder SD model that achieves performance on\npar with leading self-attention SD models (EAGLE-v2) while eliminating the need\nfor pooling or auxiliary components, simplifying the architecture, improving\ntraining efficiency, and maintaining stable memory usage during training-time\nsimulation. To enable effective training of this novel architecture, we propose\nTwo-Stage Block-Attention Training, a new method that achieves training\nstability and convergence efficiency in block-level attention scenarios.\nExtensive experiments across multiple LLMs and datasets show that Beagle\nachieves competitive inference speedups and higher training efficiency than\nEAGLE-v2, offering a strong alternative for architectures in speculative\ndecoding.", "AI": {"tldr": "Budget EAGLE (Beagle) is a novel cross-attention-based Transformer decoder for speculative decoding in large language models, achieving efficiency and performance improvements over previous self-attention-based methods.", "motivation": "Existing speculative decoding methods rely on complex self-attention structures that hinder generalizability; Beagle aims to provide a simpler and more efficient alternative.", "method": "Introduces Budget EAGLE (Beagle), a cross-attention-based decoder using Two-Stage Block-Attention Training to improve stability and efficiency.", "result": "Beagle achieves comparable performance to leading self-attention methods while simplifying the architecture and improving training efficiency across various LLMs and datasets.", "conclusion": "Beagle presents a viable alternative to current speculative decoding architectures, enhancing inference speed and training efficiency without the complexity of auxiliary components.", "key_contributions": ["First cross-attention-based speculative decoding model for LLMs.", "Introduced Two-Stage Block-Attention Training method for improved training stability.", "Achieved competitive speedups and efficiency over EAGLE-v2."], "limitations": "", "keywords": ["speculative decoding", "cross-attention", "Transformer models", "training efficiency", "large language models"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.24550", "pdf": "https://arxiv.org/pdf/2505.24550.pdf", "abs": "https://arxiv.org/abs/2505.24550", "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings", "authors": ["Xiaoang Xu", "Shuo Wang", "Xu Han", "Zhenghao Liu", "Huijia Wu", "Peipei Li", "Zhiyuan Liu", "Maosong Sun", "Zhaofeng He"], "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve superior performance by extending the\nthought length. However, a lengthy thinking trajectory leads to reduced\nefficiency. Most of the existing methods are stuck in the assumption of\noverthinking and attempt to reason efficiently by compressing the\nChain-of-Thought, but this often leads to performance degradation. To address\nthis problem, we introduce A*-Thought, an efficient tree search-based unified\nframework designed to identify and isolate the most essential thoughts from the\nextensive reasoning chains produced by these models. It formulates the\nreasoning process of LRMs as a search tree, where each node represents a\nreasoning span in the giant reasoning space. By combining the A* search\nalgorithm with a cost function specific to the reasoning path, it can\nefficiently compress the chain of thought and determine a reasoning path with\nhigh information density and low cost. In addition, we also propose a\nbidirectional importance estimation mechanism, which further refines this\nsearch process and enhances its efficiency beyond uniform sampling. Extensive\nexperiments on several advanced math tasks show that A*-Thought effectively\nbalances performance and efficiency over a huge search space. Specifically,\nA*-Thought can improve the performance of QwQ-32B by 2.39$\\times$ with\nlow-budget and reduce the length of the output token by nearly 50% with\nhigh-budget. The proposed method is also compatible with several other LRMs,\ndemonstrating its generalization capability. The code can be accessed at:\nhttps://github.com/AI9Stars/AStar-Thought.", "AI": {"tldr": "A*-Thought is an efficient framework that optimizes reasoning in Large Reasoning Models by using a tree search approach to select essential thoughts, improving both performance and efficiency.", "motivation": "To enhance the efficiency of Large Reasoning Models while maintaining performance, addressing issues related to excessive thought length in reasoning chains.", "method": "The paper introduces A*-Thought, using a search tree representation for reasoning processes, combined with the A* search algorithm and a specific cost function to compress thought chains and enhance efficiency.", "result": "A*-Thought improves the performance of the QwQ-32B model by 2.39 times at low computational budgets and reduces output token length by nearly 50% at high budgets.", "conclusion": "A*-Thought effectively balances performance and efficiency across extensive reasoning searches and is compatible with various Large Reasoning Models.", "key_contributions": ["Introduction of the A*-Thought framework for efficient reasoning in LRMs", "Development of a bidirectional importance estimation mechanism", "Demonstration of the framework's effectiveness through extensive experiments on advanced math tasks"], "limitations": "", "keywords": ["Large Reasoning Models", "A*-Thought", "efficiency", "reasoning", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24553", "pdf": "https://arxiv.org/pdf/2505.24553.pdf", "abs": "https://arxiv.org/abs/2505.24553", "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction", "authors": ["Ye Eun Chun", "Taeyoon Hwang", "Seung-won Hwang", "Byung-Hak Kim"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Understanding complex character relations is crucial for narrative analysis\nand efficient script evaluation, yet existing extraction methods often fail to\nhandle long-form narratives with nuanced interactions. To address this\nchallenge, we present CREFT, a novel sequential framework leveraging\nspecialized Large Language Model (LLM) agents. First, CREFT builds a base\ncharacter graph through knowledge distillation, then iteratively refines\ncharacter composition, relation extraction, role identification, and group\nassignments. Experiments on a curated Korean drama dataset demonstrate that\nCREFT significantly outperforms single-agent LLM baselines in both accuracy and\ncompleteness. By systematically visualizing character networks, CREFT\nstreamlines narrative comprehension and accelerates script review -- offering\nsubstantial benefits to the entertainment, publishing, and educational sectors.", "AI": {"tldr": "CREFT is a sequential framework utilizing specialized LLM agents for effective extraction and visualization of character relations in long-form narratives.", "motivation": "Existing extraction methods struggle with complex character relations in long narratives, necessitating a more efficient tool for narrative analysis.", "method": "CREFT builds a character graph through knowledge distillation and iteratively refines it by extracting relations, identifying roles, and assigning groups.", "result": "CREFT outperforms single-agent LLM baselines in accuracy and completeness when tested on a Korean drama dataset.", "conclusion": "The framework enhances narrative comprehension and script review, providing significant advantages across various sectors.", "key_contributions": ["Introduction of CREFT framework for character relation extraction", "Demonstrated effectiveness in handling long-form narratives", "Systematic visualization of character networks for improved understanding"], "limitations": "", "keywords": ["character relations", "narrative analysis", "large language models", "LLM agents", "script evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.24554", "pdf": "https://arxiv.org/pdf/2505.24554.pdf", "abs": "https://arxiv.org/abs/2505.24554", "title": "Bench4KE: Benchmarking Automated Competency Question Generation", "authors": ["Anna Sofia Lippolis", "Minh Davide Ragagni", "Paolo Ciancarini", "Andrea Giovanni Nuzzolese", "Valentina Presutti"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The availability of Large Language Models (LLMs) presents a unique\nopportunity to reinvigorate research on Knowledge Engineering (KE) automation,\na trend already evident in recent efforts developing LLM-based methods and\ntools for the automatic generation of Competency Questions (CQs). However, the\nevaluation of these tools lacks standardisation. This undermines the\nmethodological rigour and hinders the replication and comparison of results. To\naddress this gap, we introduce Bench4KE, an extensible API-based benchmarking\nsystem for KE automation. Its first release focuses on evaluating tools that\ngenerate CQs automatically. CQs are natural language questions used by ontology\nengineers to define the functional requirements of an ontology. Bench4KE\nprovides a curated gold standard consisting of CQ datasets from four real-world\nontology projects. It uses a suite of similarity metrics to assess the quality\nof the CQs generated. We present a comparative analysis of four recent CQ\ngeneration systems, which are based on LLMs, establishing a baseline for future\nresearch. Bench4KE is also designed to accommodate additional KE automation\ntasks, such as SPARQL query generation, ontology testing and drafting. Code and\ndatasets are publicly available under the Apache 2.0 license.", "AI": {"tldr": "Bench4KE is an API-based benchmarking system for automating Knowledge Engineering (KE), focusing on the evaluation of tools that generate Competency Questions (CQs) using Large Language Models (LLMs).", "motivation": "The paper addresses the lack of standardization in evaluating LLM-based methods for automatically generating Competency Questions (CQs), which is essential for methodology rigor and result comparability.", "method": "Introduction of an extensible, API-based benchmarking system called Bench4KE, which evaluates CQ generation tools using a curated gold standard of CQ datasets from real-world ontology projects and various similarity metrics.", "result": "A comparative analysis of four recent LLM-based CQ generation systems was conducted, providing a baseline for future research in KE automation.", "conclusion": "Bench4KE facilitates evaluation of CQ generation and is expandable to include other KE automation tasks; code and datasets are publicly available to support ongoing research.", "key_contributions": ["Creation of Bench4KE, a standardized benchmarking system for KE automation.", "Provision of a gold standard dataset for evaluating CQ generation tools.", "Comparative analysis establishing baseline performance for LLM-based CQ generation systems."], "limitations": "", "keywords": ["Knowledge Engineering", "Large Language Models", "Competency Questions", "Bench4KE", "Benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24561", "pdf": "https://arxiv.org/pdf/2505.24561.pdf", "abs": "https://arxiv.org/abs/2505.24561", "title": "Improving Language and Modality Transfer in Translation by Character-level Modeling", "authors": ["Ioannis Tsiamas", "David Dale", "Marta R. Costa-jussà"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Current translation systems, despite being highly multilingual, cover only 5%\nof the world's languages. Expanding language coverage to the long-tail of\nlow-resource languages requires data-efficient methods that rely on\ncross-lingual and cross-modal knowledge transfer. To this end, we propose a\ncharacter-based approach to improve adaptability to new languages and\nmodalities. Our method leverages SONAR, a multilingual fixed-size embedding\nspace with different modules for encoding and decoding. We use a\nteacher-student approach with parallel translation data to obtain a\ncharacter-level encoder. Then, using ASR data, we train a lightweight adapter\nto connect a massively multilingual CTC ASR model (MMS), to the character-level\nencoder, potentially enabling speech translation from 1,000+ languages.\nExperimental results in text translation for 75 languages on FLORES+\ndemonstrate that our character-based approach can achieve better language\ntransfer than traditional subword-based models, especially outperforming them\nin low-resource settings, and demonstrating better zero-shot generalizability\nto unseen languages. Our speech adaptation, maximizing knowledge transfer from\nthe text modality, achieves state-of-the-art results in speech-to-text\ntranslation on the FLEURS benchmark on 33 languages, surpassing previous\nsupervised and cascade models, albeit being a zero-shot model with minimal\nsupervision from ASR data.", "AI": {"tldr": "This paper presents a character-based approach to enhance translation systems for low-resource languages by utilizing cross-lingual and cross-modal knowledge transfer through a multilingual embedding space.", "motivation": "Current multilingual translation systems only cover a small fraction of the world's languages, necessitating improved methods for low-resource languages.", "method": "The proposed method employs SONAR, a multilingual embedding space, using a teacher-student approach to train a character-level encoder with parallel translation data and a lightweight adapter for ASR data.", "result": "Experimental results indicate that the character-based approach significantly improves language transfer, especially in low-resource scenarios, and exhibits strong zero-shot capabilities for unseen languages.", "conclusion": "The method achieves state-of-the-art performance in speech-to-text translation with minimal supervision and better results than traditional models.", "key_contributions": ["Development of a character-based translation method", "Creation of a multilingual fixed-size embedding space (SONAR)", "Enhanced performance in low-resource language settings and zero-shot translation."], "limitations": "", "keywords": ["low-resource languages", "character-based translation", "multilingual ASR"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24575", "pdf": "https://arxiv.org/pdf/2505.24575.pdf", "abs": "https://arxiv.org/abs/2505.24575", "title": "NexusSum: Hierarchical LLM Agents for Long-Form Narrative Summarization", "authors": ["Hyuntak Kim", "Byung-Hak Kim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main track of ACL 2025", "summary": "Summarizing long-form narratives--such as books, movies, and TV\nscripts--requires capturing intricate plotlines, character interactions, and\nthematic coherence, a task that remains challenging for existing LLMs. We\nintroduce NexusSum, a multi-agent LLM framework for narrative summarization\nthat processes long-form text through a structured, sequential\npipeline--without requiring fine-tuning. Our approach introduces two key\ninnovations: (1) Dialogue-to-Description Transformation: A narrative-specific\npreprocessing method that standardizes character dialogue and descriptive text\ninto a unified format, improving coherence. (2) Hierarchical Multi-LLM\nSummarization: A structured summarization pipeline that optimizes chunk\nprocessing and controls output length for accurate, high-quality summaries. Our\nmethod establishes a new state-of-the-art in narrative summarization, achieving\nup to a 30.0% improvement in BERTScore (F1) across books, movies, and TV\nscripts. These results demonstrate the effectiveness of multi-agent LLMs in\nhandling long-form content, offering a scalable approach for structured\nsummarization in diverse storytelling domains.", "AI": {"tldr": "NexusSum is a multi-agent framework designed for summarizing long-form narratives, improving coherence and output quality through innovative processing methods.", "motivation": "The goal is to effectively summarize intricate long-form narratives, which is challenging for existing LLMs.", "method": "NexusSum employs a structured, sequential pipeline that includes a Dialogue-to-Description Transformation for preprocessing and a Hierarchical Multi-LLM Summarization for chunk processing and output control.", "result": "NexusSum achieves a 30.0% improvement in BERTScore (F1) for summarizing books, movies, and TV scripts, setting a new state-of-the-art.", "conclusion": "The framework demonstrates that multi-agent LLMs can effectively manage and summarize complex long-form content.", "key_contributions": ["Dialogue-to-Description Transformation", "Hierarchical Multi-LLM Summarization", "State-of-the-art performance in narrative summarization"], "limitations": "", "keywords": ["narrative summarization", "multi-agent LLM", "NexusSum"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24581", "pdf": "https://arxiv.org/pdf/2505.24581.pdf", "abs": "https://arxiv.org/abs/2505.24581", "title": "GATE: General Arabic Text Embedding for Enhanced Semantic Textual Similarity with Matryoshka Representation Learning and Hybrid Loss Training", "authors": ["Omer Nacar", "Anis Koubaa", "Serry Sibaee", "Yasser Al-Habashi", "Adel Ammar", "Wadii Boulila"], "categories": ["cs.CL"], "comment": null, "summary": "Semantic textual similarity (STS) is a critical task in natural language\nprocessing (NLP), enabling applications in retrieval, clustering, and\nunderstanding semantic relationships between texts. However, research in this\narea for the Arabic language remains limited due to the lack of high-quality\ndatasets and pre-trained models. This scarcity of resources has restricted the\naccurate evaluation and advance of semantic similarity in Arabic text. This\npaper introduces General Arabic Text Embedding (GATE) models that achieve\nstate-of-the-art performance on the Semantic Textual Similarity task within the\nMTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid\nloss training approach with Arabic triplet datasets for Natural Language\nInference, which are essential for enhancing model performance in tasks that\ndemand fine-grained semantic understanding. GATE outperforms larger models,\nincluding OpenAI, with a 20-25% performance improvement on STS benchmarks,\neffectively capturing the unique semantic nuances of Arabic.", "AI": {"tldr": "The paper presents General Arabic Text Embedding (GATE) models, improving Arabic semantic textual similarity tasks using novel techniques and datasets. GATE outperforms existing models with significant performance gains.", "motivation": "The limited research and resources for semantic textual similarity in Arabic hinder progress in understanding and processing Arabic text. The paper addresses this gap by introducing improved models and datasets.", "method": "The study employs General Arabic Text Embedding (GATE) models utilizing Matryoshka Representation Learning and a hybrid loss training approach, specifically designed for Arabic triplet datasets relevant for Natural Language Inference.", "result": "GATE achieves state-of-the-art performance on the MTEB benchmark for Semantic Textual Similarity, outperforming larger models such as OpenAI with a 20-25% increase in performance on STS benchmarks.", "conclusion": "The introduction of GATE models significantly advances the field of Arabic natural language processing, aiding in the accurate evaluation of semantic similarity and encouraging further research.", "key_contributions": ["Introduction of General Arabic Text Embedding (GATE) models", "Utilization of Matryoshka Representation Learning for improved performance", "Performance surpasses larger LLMs by 20-25% in STS benchmarks"], "limitations": "", "keywords": ["semantic textual similarity", "Arabic NLP", "GATE models"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2505.24593", "pdf": "https://arxiv.org/pdf/2505.24593.pdf", "abs": "https://arxiv.org/abs/2505.24593", "title": "Decoding Knowledge Attribution in Mixture-of-Experts: A Framework of Basic-Refinement Collaboration and Efficiency Analysis", "authors": ["Junzhuo Li", "Bo Wang", "Xiuze Zhou", "Peijie Jiang", "Jia Liu", "Xuming Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "The interpretability of Mixture-of-Experts (MoE) models, especially those\nwith heterogeneous designs, remains underexplored. Existing attribution methods\nfor dense models fail to capture dynamic routing-expert interactions in sparse\nMoE architectures. To address this issue, we propose a cross-level attribution\nalgorithm to analyze sparse MoE architectures (Qwen 1.5-MoE, OLMoE,\nMixtral-8x7B) against dense models (Qwen 1.5-7B, Llama-7B, Mixtral-7B). Results\nshow MoE models achieve 37% higher per-layer efficiency via a \"mid-activation,\nlate-amplification\" pattern: early layers screen experts, while late layers\nrefine knowledge collaboratively. Ablation studies reveal a \"basic-refinement\"\nframework--shared experts handle general tasks (entity recognition), while\nrouted experts specialize in domain-specific processing (geographic\nattributes). Semantic-driven routing is evidenced by strong correlations\nbetween attention heads and experts (r=0.68), enabling task-aware coordination.\nNotably, architectural depth dictates robustness: deep Qwen 1.5-MoE mitigates\nexpert failures (e.g., 43% MRR drop in geographic tasks when blocking top-10\nexperts) through shared expert redundancy, whereas shallow OLMoE suffers severe\ndegradation (76% drop). Task sensitivity further guides design: core-sensitive\ntasks (geography) require concentrated expertise, while distributed-tolerant\ntasks (object attributes) leverage broader participation. These insights\nadvance MoE interpretability, offering principles to balance efficiency,\nspecialization, and robustness.", "AI": {"tldr": "The paper explores the interpretability of Mixture-of-Experts (MoE) models with a cross-level attribution algorithm, highlighting their efficiency and robustness compared to dense models.", "motivation": "To investigate the interpretability of heterogeneous Mixture-of-Experts (MoE) models and the limitations of existing attribution methods in capturing dynamic interactions.", "method": "A cross-level attribution algorithm is proposed to analyze various sparse MoE architectures against dense models, examining their performance efficiency and routing mechanisms.", "result": "MoE models demonstrate 37% higher per-layer efficiency and exhibit a robust architecture that maintains performance even with expert failures, while revealing correlations between attention heads and expert dynamics.", "conclusion": "The insights on MoE interpretability contribute to a better understanding of how to optimize model efficiency, specialization, and robustness, particularly in domain-sensitive tasks.", "key_contributions": ["Introduction of a cross-level attribution algorithm for MoE models", "Demonstration of efficiency gains in MoE architectures", "Insights into task sensitivity guiding expert specialization"], "limitations": "", "keywords": ["Mixture-of-Experts", "interpretability", "attribution methods", "sparse architectures", "dynamic routing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.24609", "pdf": "https://arxiv.org/pdf/2505.24609.pdf", "abs": "https://arxiv.org/abs/2505.24609", "title": "Explainable Depression Detection using Masked Hard Instance Mining", "authors": ["Patawee Prakrankamanant", "Shinji Watanabe", "Ekapol Chuangsuwanich"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the critical need for improved explainability in\ntext-based depression detection. While offering predictive outcomes, current\nsolutions often overlook the understanding of model predictions which can\nhinder trust in the system. We propose the use of Masked Hard Instance Mining\n(MHIM) to enhance the explainability in the depression detection task. MHIM\nstrategically masks attention weights within the model, compelling it to\ndistribute attention across a wider range of salient features. We evaluate MHIM\non two datasets representing distinct languages: Thai (Thai-Maywe) and English\n(DAIC-WOZ). Our results demonstrate that MHIM significantly improves\nperformance in terms of both prediction accuracy and explainability metrics.", "AI": {"tldr": "This paper proposes Masked Hard Instance Mining (MHIM) to enhance explainability in text-based depression detection models.", "motivation": "There is a critical need for improved explainability in text-based depression detection, as current solutions may hinder trust due to a lack of understanding of model predictions.", "method": "The paper introduces Masked Hard Instance Mining (MHIM), which masks attention weights to encourage the model to pay attention to a broader range of features.", "result": "MHIM significantly improves performance in prediction accuracy and explainability metrics across two language datasets: Thai (Thai-Maywe) and English (DAIC-WOZ).", "conclusion": "The proposed MHIM method enhances both the predictive outcomes and the interpretability of the models used for depression detection.", "key_contributions": ["Introduction of Masked Hard Instance Mining (MHIM) for enhanced model explainability.", "Demonstration of improvements in both predictive accuracy and explainability on multilingual datasets.", "Evaluation across distinct language datasets (Thai and English) to validate effectiveness."], "limitations": "", "keywords": ["explainability", "depression detection", "Masked Hard Instance Mining", "multilingual", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24613", "pdf": "https://arxiv.org/pdf/2505.24613.pdf", "abs": "https://arxiv.org/abs/2505.24613", "title": "When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation", "authors": ["Daniela Occhipinti", "Marco Guerini", "Malvina Nissim"], "categories": ["cs.CL"], "comment": null, "summary": "Endowing dialogue agents with persona information has proven to significantly\nimprove the consistency and diversity of their generations. While much focus\nhas been placed on aligning dialogues with provided personas, the adaptation to\nthe interlocutor's profile remains largely underexplored. In this work, we\ninvestigate three key aspects: (1) a model's ability to align responses with\nboth the provided persona and the interlocutor's; (2) its robustness when\ndealing with familiar versus unfamiliar interlocutors and topics, and (3) the\nimpact of additional fine-tuning on specific persona-based dialogues. We\nevaluate dialogues generated with diverse speaker pairings and topics, framing\nthe evaluation as an author identification task and employing both\nLLM-as-a-judge and human evaluations. By systematically masking or disclosing\ninformation about the interlocutor, we assess its impact on dialogue\ngeneration. Results show that access to the interlocutor's persona improves the\nrecognition of the target speaker, while masking it does the opposite. Although\nmodels generalise well across topics, they struggle with unfamiliar\ninterlocutors. Finally, we found that in zero-shot settings, LLMs often copy\nbiographical details, facilitating identification but trivialising the task.", "AI": {"tldr": "This paper explores how dialogue agents can better align their responses with both their own persona and those of their interlocutors, assessing the effects of familiarity and fine-tuning on dialogue generation.", "motivation": "To improve the production of dialogue by incorporating information about both the speaker's and interlocutor's personas, thereby enhancing consistency and engagement in conversational AI.", "method": "The authors investigate the alignment of dialogue models with personas, robustness with familiar vs unfamiliar interlocutors, and the effects of fine-tuning through various evaluations, including author identification and masking of interlocutor information.", "result": "The study finds that knowing the interlocutor's persona helps models generate more recognizable responses, but unfamiliar interlocutors create challenges. Zero-shot settings often result in LLMs relying on biographical detail, complicating the identification process.", "conclusion": "While fine-tuning improves persona-based dialogue, models are less effective with unfamiliar interlocutors, suggesting a need for better training strategies in dialogue systems.", "key_contributions": ["Evaluation of LLM responses based on both speaker and interlocutor personas", "Insight into the model's performance with familiar vs unfamiliar interlocutors", "Demonstration of challenges in zero-shot dialogue settings with LLMs"], "limitations": "The research focuses on persona alignment and evaluation but may not address all aspects of dialogue generation robustness.", "keywords": ["dialogue agents", "persona adaptation", "human-computer interaction", "language models", "dialogue generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24615", "pdf": "https://arxiv.org/pdf/2505.24615.pdf", "abs": "https://arxiv.org/abs/2505.24615", "title": "Harnessing Large Language Models for Scientific Novelty Detection", "authors": ["Yan Liu", "Zonglin Yang", "Soujanya Poria", "Thanh-Son Nguyen", "Erik Cambria"], "categories": ["cs.CL", "H.4.0"], "comment": "15 pages, 3 figures, 3 tables", "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.", "AI": {"tldr": "This paper presents a novel approach to scientific novelty detection using large language models (LLMs) and introduces two new benchmark datasets for this purpose.", "motivation": "Identifying novel research ideas is critical in academia, but the absence of apt benchmark datasets limits advancements in novelty detection.", "method": "The paper proposes using large language models to extract closure sets of related papers and summarize their ideas. It also introduces a lightweight retriever model that aligns ideas through distillation from LLMs, enhancing retrieval accuracy for novelty detection.", "result": "The method shows consistent improvement over other approaches on the proposed datasets for idea retrieval and novelty detection tasks.", "conclusion": "The proposed approach and datasets could significantly enhance the identification of novel ideas in scientific research.", "key_contributions": ["Introduction of two benchmark datasets for novelty detection in marketing and NLP domains.", "Development of a method using LLMs for effective summarization and relation extraction of academic papers.", "Creation of a lightweight retriever model for aligning ideas for better novelty detection."], "limitations": "The scope of the datasets is currently limited to marketing and NLP, potentially restricting applicability to other domains.", "keywords": ["novelty detection", "large language models", "dataset", "idea retrieval", "academic research"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24616", "pdf": "https://arxiv.org/pdf/2505.24616.pdf", "abs": "https://arxiv.org/abs/2505.24616", "title": "Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX", "authors": ["Nikita Martynov", "Anastasia Mordasheva", "Dmitriy Gorbetskiy", "Danil Astafurov", "Ulyana Isaeva", "Elina Basyrova", "Sergey Skachkov", "Victoria Berestova", "Nikolay Ivanov", "Valeriia Zanina", "Alena Fenogenova"], "categories": ["cs.CL", "cs.AI"], "comment": "179 pages", "summary": "We introduce POLLUX, a comprehensive open-source benchmark designed to\nevaluate the generative capabilities of large language models (LLMs) in\nRussian. Our main contribution is a novel evaluation methodology that enhances\nthe interpretability of LLM assessment. For each task type, we define a set of\ndetailed criteria and develop a scoring protocol where models evaluate\nresponses and provide justifications for their ratings. This enables\ntransparent, criteria-driven evaluation beyond traditional resource-consuming,\nside-by-side human comparisons. POLLUX includes a detailed, fine-grained\ntaxonomy of 35 task types covering diverse generative domains such as code\ngeneration, creative writing, and practical assistant use cases, totaling 2,100\nmanually crafted and professionally authored prompts. Each task is categorized\nby difficulty (easy/medium/hard), with experts constructing the dataset\nentirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B)\nevaluators trained for nuanced assessment of generative outputs. This approach\nprovides scalable, interpretable evaluation and annotation tools for model\ndevelopment, effectively replacing costly and less precise human judgments.", "AI": {"tldr": "POLLUX is an open-source benchmark for evaluating the generative capabilities of large language models (LLMs) in Russian, featuring a methodology for interpretability and scalable evaluation.", "motivation": "To provide a comprehensive, transparent, and efficient method for evaluating LLMs beyond traditional human comparison, especially tailored for Russian language models.", "method": "A novel evaluation methodology with defined criteria and a scoring protocol to assess model responses and justifications, covering 35 task types with 2,100 prompts categorized by difficulty.", "result": "POLLUX can potentially replace costly human judgments with LLM-as-a-Judge evaluators, enhancing evaluation efficiency and interpretability in model assessment.", "conclusion": "This benchmark allows for a more scalable and nuanced evaluation of generative outputs, thereby aiding in the development of LLMs.", "key_contributions": ["Introduction of a comprehensive open-source benchmark for LLMs in Russian.", "Novel evaluation methodology enhances interpretability and scalability.", "Release of LLM-as-a-Judge evaluators for more precise assessments."], "limitations": "", "keywords": ["large language models", "evaluation methodology", "Russian language", "POLLUX", "generative capabilities"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.24619", "pdf": "https://arxiv.org/pdf/2505.24619.pdf", "abs": "https://arxiv.org/abs/2505.24619", "title": "Interpretable phenotyping of Heart Failure patients with Dutch discharge letters", "authors": ["Vittorio Torri", "Machteld J. Boonstra", "Marielle C. van de Veerdonk", "Deborah N. Kalkman", "Alicia Uijl", "Francesca Ieva", "Ameen Abu-Hanna", "Folkert W. Asselbergs", "Iacer Calixto"], "categories": ["cs.CL", "cs.LG", "68T50", "I.2.7; J.3"], "comment": "43 pages, 8 figures", "summary": "Objective: Heart failure (HF) patients present with diverse phenotypes\naffecting treatment and prognosis. This study evaluates models for phenotyping\nHF patients based on left ventricular ejection fraction (LVEF) classes, using\nstructured and unstructured data, assessing performance and interpretability.\n  Materials and Methods: The study analyzes all HF hospitalizations at both\nAmsterdam UMC hospitals (AMC and VUmc) from 2015 to 2023 (33,105\nhospitalizations, 16,334 patients). Data from AMC were used for model training,\nand from VUmc for external validation. The dataset was unlabelled and included\ntabular clinical measurements and discharge letters. Silver labels for LVEF\nclasses were generated by combining diagnosis codes, echocardiography results,\nand textual mentions. Gold labels were manually annotated for 300 patients for\ntesting. Multiple Transformer-based (black-box) and Aug-Linear (white-box)\nmodels were trained and compared with baselines on structured and unstructured\ndata. To evaluate interpretability, two clinicians annotated 20 discharge\nletters by highlighting information they considered relevant for LVEF\nclassification. These were compared to SHAP and LIME explanations from\nblack-box models and the inherent explanations of Aug-Linear models.\n  Results: BERT-based and Aug-Linear models, using discharge letters alone,\nachieved the highest classification results (AUC=0.84 for BERT, 0.81 for\nAug-Linear on external validation), outperforming baselines. Aug-Linear\nexplanations aligned more closely with clinicians' explanations than post-hoc\nexplanations on black-box models.\n  Conclusions: Discharge letters emerged as the most informative source for\nphenotyping HF patients. Aug-Linear models matched black-box performance while\nproviding clinician-aligned interpretability, supporting their use in\ntransparent clinical decision-making.", "AI": {"tldr": "This study evaluates models for phenotyping heart failure patients based on left ventricular ejection fraction using structured and unstructured data, assessing their performance and interpretability.", "motivation": "Heart failure patients have diverse phenotypes that impact treatment and prognosis; there is a need for effective phenotyping models to guide clinical decisions.", "method": "An analysis of heart failure hospitalizations was conducted, utilizing structured and unstructured data, including a combination of diagnosis codes, echocardiography results, and textual mentions from discharge letters. Transformer-based and Aug-Linear models were trained and assessed for interpretability against clinician annotations.", "result": "BERT-based and Aug-Linear models achieved high classification results (AUC=0.84 for BERT, 0.81 for Aug-Linear) using discharge letters, with Aug-Linear models providing clinician-aligned interpretability.", "conclusion": "Discharge letters are crucial for phenotyping heart failure patients, and Aug-Linear models offer competitive performance with enhanced interpretability, promoting transparent clinical decision-making.", "key_contributions": ["Evaluation of phenotyping models for HF using combined data types", "Demonstration of Aug-Linear interpretability aligned with clinician insights", "Identification of discharge letters as key informative data source"], "limitations": "The study relies on a single institution's dataset for model training, which might affect generalizability.", "keywords": ["Heart failure", "phenotyping", "machine learning", "interpretability", "discharge letters"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.24621", "pdf": "https://arxiv.org/pdf/2505.24621.pdf", "abs": "https://arxiv.org/abs/2505.24621", "title": "Benchmarking Large Language Models for Cryptanalysis and Mismatched-Generalization", "authors": ["Utsav Maskey", "Chencheng Zhu", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advancements in Large Language Models (LLMs) have transformed natural\nlanguage understanding and generation, leading to extensive benchmarking across\ndiverse tasks. However, cryptanalysis a critical area for data security and\nencryption has not yet been thoroughly explored in LLM evaluations. To address\nthis gap, we evaluate cryptanalytic potential of state of the art LLMs on\nencrypted texts generated using a range of cryptographic algorithms. We\nintroduce a novel benchmark dataset comprising diverse plain texts spanning\nvarious domains, lengths, writing styles, and topics paired with their\nencrypted versions. Using zero-shot and few shot settings, we assess multiple\nLLMs for decryption accuracy and semantic comprehension across different\nencryption schemes. Our findings reveal key insights into the strengths and\nlimitations of LLMs in side-channel communication while raising concerns about\ntheir susceptibility to jailbreaking attacks. This research highlights the\ndual-use nature of LLMs in security contexts and contributes to the ongoing\ndiscussion on AI safety and security.", "AI": {"tldr": "This paper evaluates the cryptanalytic capabilities of state-of-the-art Large Language Models (LLMs) on encrypted texts, highlighting their strengths and weaknesses in data security applications.", "motivation": "The study addresses the lack of thorough exploration of LLMs in the context of cryptanalysis, a critical area for data security and encryption.", "method": "A novel benchmark dataset was created, consisting of various plain texts and their encrypted versions, to assess LLMs in zero-shot and few-shot conditions for decryption accuracy and semantic comprehension.", "result": "The evaluation reveals key insights about LLMs' strengths and limitations in handling encrypted texts, as well as concerns regarding their vulnerability to jailbreaking attacks.", "conclusion": "This research underscores the dual-use nature of LLMs in security contexts, contributing to discussions on AI safety and security.", "key_contributions": ["Introduction of a benchmark dataset for evaluating LLMs on encrypted texts", "Assessment of LLMs for decryption accuracy across different cryptographic algorithms", "Insights into the implications of LLMs in side-channel communication and AI safety"], "limitations": "The study only explores a specific range of cryptographic algorithms and does not evaluate all possible encryption schemes.", "keywords": ["Large Language Models", "cryptanalysis", "encryption", "AI safety", "security"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24630", "pdf": "https://arxiv.org/pdf/2505.24630.pdf", "abs": "https://arxiv.org/abs/2505.24630", "title": "The Hallucination Dilemma: Factuality-Aware Reinforcement Learning for Large Reasoning Models", "authors": ["Junyi Li", "Hwee Tou Ng"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced in reasoning tasks\nthrough reinforcement learning (RL) optimization, achieving impressive\ncapabilities across various challenging benchmarks. However, our empirical\nanalysis reveals a critical drawback: reasoning-oriented RL fine-tuning\nsignificantly increases the prevalence of hallucinations. We theoretically\nanalyze the RL training dynamics, identifying high-variance gradient,\nentropy-induced randomness, and susceptibility to spurious local optima as key\nfactors leading to hallucinations. To address this drawback, we propose\nFactuality-aware Step-wise Policy Optimization (FSPO), an innovative RL\nfine-tuning algorithm incorporating explicit factuality verification at each\nreasoning step. FSPO leverages automated verification against given evidence to\ndynamically adjust token-level advantage values, incentivizing factual\ncorrectness throughout the reasoning process. Experiments across mathematical\nreasoning and hallucination benchmarks using Qwen2.5 and Llama models\ndemonstrate that FSPO effectively reduces hallucinations while enhancing\nreasoning accuracy, substantially improving both reliability and performance.", "AI": {"tldr": "The paper introduces Factuality-aware Step-wise Policy Optimization (FSPO), an RL fine-tuning algorithm designed to reduce hallucinations in large language models during reasoning tasks.", "motivation": "Despite advancements in reasoning tasks through RL optimization, LLMs face increased hallucinations, which undermines their reliability.", "method": "The paper theoretically analyzes RL training dynamics to identify causes of hallucinations and proposes FSPO, which includes factuality verification at each reasoning step to adjust token-level advantages.", "result": "FSPO demonstrates effectiveness in reducing hallucinations and enhancing reasoning accuracy across various benchmarks with models like Qwen2.5 and Llama.", "conclusion": "By implementing FSPO, LLMs can achieve improved reliability and performance in reasoning tasks, addressing critical drawbacks of existing RL fine-tuning methods.", "key_contributions": ["Introduction of Factuality-aware Step-wise Policy Optimization (FSPO)", "Theoretical analysis of RL training dynamics leading to hallucinations", "Empirical validation showing reduced hallucinations and improved reasoning accuracy"], "limitations": "Relies on the performance of existing LLMs and may not generalize across all language tasks.", "keywords": ["Reinforcement Learning", "Large Language Models", "Factuality", "Reasoning", "Hallucinations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24635", "pdf": "https://arxiv.org/pdf/2505.24635.pdf", "abs": "https://arxiv.org/abs/2505.24635", "title": "Disentangling Language and Culture for Evaluating Multilingual Large Language Models", "authors": ["Jiahao Ying", "Wei Tang", "Yiran Zhao", "Yixin Cao", "Yu Rong", "Wenxuan Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "This paper introduces a Dual Evaluation Framework to comprehensively assess\nthe multilingual capabilities of LLMs. By decomposing the evaluation along the\ndimensions of linguistic medium and cultural context, this framework enables a\nnuanced analysis of LLMs' ability to process questions within both native and\ncross-cultural contexts cross-lingually. Extensive evaluations are conducted on\na wide range of models, revealing a notable \"CulturalLinguistic Synergy\"\nphenomenon, where models exhibit better performance when questions are\nculturally aligned with the language. This phenomenon is further explored\nthrough interpretability probing, which shows that a higher proportion of\nspecific neurons are activated in a language's cultural context. This\nactivation proportion could serve as a potential indicator for evaluating\nmultilingual performance during model training. Our findings challenge the\nprevailing notion that LLMs, primarily trained on English data, perform\nuniformly across languages and highlight the necessity of culturally and\nlinguistically model evaluations. Our code can be found at\nhttps://yingjiahao14. github.io/Dual-Evaluation/.", "AI": {"tldr": "The paper presents a Dual Evaluation Framework for assessing multilingual capabilities of LLMs, highlighting cultural and linguistic performance variations.", "motivation": "To provide a nuanced understanding of LLMs' multilingual capabilities, addressing gaps in current evaluation methods that overlook cultural context.", "method": "A Dual Evaluation Framework decomposes evaluation into linguistic medium and cultural context, allowing for extensive model assessments.", "result": "The study uncovers a 'Cultural-Linguistic Synergy' where LLMs perform better on culturally aligned questions; indicates that neuron activation can signal multilingual performance.", "conclusion": "The findings urge the need for culturally informed evaluations of LLMs as they challenge the misconception of uniform performance across languages.", "key_contributions": ["Introduction of the Dual Evaluation Framework", "Discovery of the Cultural-Linguistic Synergy phenomenon", "Interpretability probing as a measure for multilingual performance"], "limitations": "The framework may require extensive resources for evaluations across multiple languages and cultural contexts.", "keywords": ["multilingual capabilities", "LLMs", "cultural context", "evaluation framework", "neuron activation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24640", "pdf": "https://arxiv.org/pdf/2505.24640.pdf", "abs": "https://arxiv.org/abs/2505.24640", "title": "Efficient Text Encoders for Labor Market Analysis", "authors": ["Jens-Joris Decorte", "Jeroen Van Hautte", "Chris Develder", "Thomas Demeester"], "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Labor market analysis relies on extracting insights from job advertisements,\nwhich provide valuable yet unstructured information on job titles and\ncorresponding skill requirements. While state-of-the-art methods for skill\nextraction achieve strong performance, they depend on large language models\n(LLMs), which are computationally expensive and slow. In this paper, we propose\n\\textbf{ConTeXT-match}, a novel contrastive learning approach with token-level\nattention that is well-suited for the extreme multi-label classification task\nof skill classification. \\textbf{ConTeXT-match} significantly improves skill\nextraction efficiency and performance, achieving state-of-the-art results with\na lightweight bi-encoder model. To support robust evaluation, we introduce\n\\textbf{Skill-XL}, a new benchmark with exhaustive, sentence-level skill\nannotations that explicitly address the redundancy in the large label space.\nFinally, we present \\textbf{JobBERT V2}, an improved job title normalization\nmodel that leverages extracted skills to produce high-quality job title\nrepresentations. Experiments demonstrate that our models are efficient,\naccurate, and scalable, making them ideal for large-scale, real-time labor\nmarket analysis.", "AI": {"tldr": "This paper presents ConTeXT-match, a novel contrastive learning approach for skill classification in job advertisements, significantly enhancing efficiency and performance compared to traditional LLMs.", "motivation": "The motivation behind this work is to improve labor market analysis by extracting insights from unstructured job advertisements, which typically include vast skill requirements.", "method": "The paper introduces ConTeXT-match, a contrastive learning method with token-level attention tailored for extreme multi-label classification tasks. It also presents a new benchmark, Skill-XL, and JobBERT V2 for job title normalization.", "result": "ConTeXT-match achieves state-of-the-art performance with a lightweight bi-encoder model, enhancing skill extraction efficiency and accuracy.", "conclusion": "The developed models are proven to be efficient and scalable for real-time labor market analysis, making them suitable for large-scale applications.", "key_contributions": ["Introduction of ConTeXT-match for efficient skill extraction", "Development of Skill-XL benchmark for enhanced evaluation", "Presentation of JobBERT V2 for improved job title normalization"], "limitations": "", "keywords": ["skill extraction", "labor market analysis", "contrastive learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.24643", "pdf": "https://arxiv.org/pdf/2505.24643.pdf", "abs": "https://arxiv.org/abs/2505.24643", "title": "Are Optimal Algorithms Still Optimal? Rethinking Sorting in LLM-Based Pairwise Ranking with Batching and Caching", "authors": ["Juan Wisznia", "Cecilia Bolaños", "Juan Tollo", "Giovanni Marraffini", "Agustín Gianolini", "Noe Hsueh", "Luciano Del Corro"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel framework for analyzing sorting algorithms in pairwise\nranking prompting (PRP), re-centering the cost model around LLM inferences\nrather than traditional pairwise comparisons. While classical metrics based on\ncomparison counts have traditionally been used to gauge efficiency, our\nanalysis reveals that expensive LLM inferences overturn these predictions;\naccordingly, our framework encourages strategies such as batching and caching\nto mitigate inference costs. We show that algorithms optimal in the classical\nsetting can lose efficiency when LLM inferences dominate the cost under certain\noptimizations.", "AI": {"tldr": "Introduction of a novel framework for analyzing sorting algorithms in pairwise ranking prompting, focusing on LLM inference costs.", "motivation": "The paper addresses inefficiencies in traditional sorting algorithm analysis by centering it around expensive LLM inferences rather than just comparison counts.", "method": "A new cost model that integrates LLM inferences and promotes batching and caching to reduce inference costs in sorting algorithms.", "result": "The analysis demonstrates that classical sorting algorithms may perform poorly when LLM inference costs are considered, thus revealing the importance of optimization strategies.", "conclusion": "When optimizing for LLM inference costs, classical optimal algorithms may need reevaluation as they can become inefficient due to high inference expenses.", "key_contributions": ["Introduction of a novel cost model focused on LLM inferences.", "Demonstration that classical algorithm efficiency can be compromised by LLM costs.", "Recommendations for batching and caching to optimize performance."], "limitations": "The framework may not cover all sorting scenarios or account for all types of LLM inference variations.", "keywords": ["pairwise ranking", "sorting algorithms", "LLM inference", "cost model", "optimization strategies"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.24646", "pdf": "https://arxiv.org/pdf/2505.24646.pdf", "abs": "https://arxiv.org/abs/2505.24646", "title": "PRISM: A Framework for Producing Interpretable Political Bias Embeddings with Political-Aware Cross-Encoder", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Semantic Text Embedding is a fundamental NLP task that encodes textual\ncontent into vector representations, where proximity in the embedding space\nreflects semantic similarity. While existing embedding models excel at\ncapturing general meaning, they often overlook ideological nuances, limiting\ntheir effectiveness in tasks that require an understanding of political bias.\nTo address this gap, we introduce PRISM, the first framework designed to\nProduce inteRpretable polItical biaS eMbeddings. PRISM operates in two key\nstages: (1) Controversial Topic Bias Indicator Mining, which systematically\nextracts fine-grained political topics and their corresponding bias indicators\nfrom weakly labeled news data, and (2) Cross-Encoder Political Bias Embedding,\nwhich assigns structured bias scores to news articles based on their alignment\nwith these indicators. This approach ensures that embeddings are explicitly\ntied to bias-revealing dimensions, enhancing both interpretability and\npredictive power. Through extensive experiments on two large-scale datasets, we\ndemonstrate that PRISM outperforms state-of-the-art text embedding models in\npolitical bias classification while offering highly interpretable\nrepresentations that facilitate diversified retrieval and ideological analysis.\nThe source code is available at https://github.com/dukesun99/ACL-PRISM.", "AI": {"tldr": "PRISM is a framework for generating interpretable political bias embeddings that address ideological nuances in textual data, enhancing the effectiveness of NLP models in political bias classification.", "motivation": "Existing embedding models fail to capture ideological nuances, which limits effectiveness in understanding political bias.", "method": "PRISM consists of two stages: mining bias indicators from weakly labeled news data and assigning structured bias scores to articles using these indicators.", "result": "PRISM outperforms existing text embedding models in political bias classification and provides interpretable representations for ideological analysis.", "conclusion": "PRISM enhances interpretability and predictive power of embeddings, facilitating better ideological analysis and retrieval.", "key_contributions": ["Introducing the PRISM framework for political bias embeddings", "Systematic extraction of political topics and bias indicators from news data", "Improved performance in political bias classification tasks"], "limitations": "", "keywords": ["semantic text embedding", "political bias", "NLP", "interpretability", "bias indicators"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.24656", "pdf": "https://arxiv.org/pdf/2505.24656.pdf", "abs": "https://arxiv.org/abs/2505.24656", "title": "MSDA: Combining Pseudo-labeling and Self-Supervision for Unsupervised Domain Adaptation in ASR", "authors": ["Dimitrios Damianos", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "In this work, we investigate the Meta PL unsupervised domain adaptation\nframework for Automatic Speech Recognition (ASR). We introduce a Multi-Stage\nDomain Adaptation pipeline (MSDA), a sample-efficient, two-stage adaptation\napproach that integrates self-supervised learning with semi-supervised\ntechniques. MSDA is designed to enhance the robustness and generalization of\nASR models, making them more adaptable to diverse conditions. It is\nparticularly effective for low-resource languages like Greek and in weakly\nsupervised scenarios where labeled data is scarce or noisy. Through extensive\nexperiments, we demonstrate that Meta PL can be applied effectively to ASR\ntasks, achieving state-of-the-art results, significantly outperforming\nstate-of-the-art methods, and providing more robust solutions for unsupervised\ndomain adaptation in ASR. Our ablations highlight the necessity of utilizing a\ncascading approach when combining self-supervision with self-training.", "AI": {"tldr": "This paper presents a Multi-Stage Domain Adaptation (MSDA) pipeline for enhancing Automatic Speech Recognition (ASR) models, particularly in low-resource languages and weakly supervised environments.", "motivation": "The motivation behind this work is to improve Automatic Speech Recognition models' adaptability and robustness, especially in low-resource language scenarios and when labeled data is limited or noisy.", "method": "The proposed method is a sample-efficient, two-stage adaptation approach that combines self-supervised learning and semi-supervised techniques to create a Multi-Stage Domain Adaptation pipeline.", "result": "Through extensive experiments, the authors demonstrate that their Meta PL framework achieves state-of-the-art performance in ASR tasks, significantly outperforming existing methods and improving robustness in unsupervised domain adaptation.", "conclusion": "The research concludes that the cascading approach of integrating self-supervision with self-training is essential for improving the performance of ASR models in diverse conditions.", "key_contributions": ["Introduction of the Multi-Stage Domain Adaptation (MSDA) pipeline for ASR.", "Application of the MSDA framework to low-resource languages and weakly supervised scenarios.", "Demonstration of state-of-the-art results in unsupervised ASR tasks."], "limitations": "", "keywords": ["Automatic Speech Recognition", "Unsupervised Domain Adaptation", "Self-Supervised Learning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.24671", "pdf": "https://arxiv.org/pdf/2505.24671.pdf", "abs": "https://arxiv.org/abs/2505.24671", "title": "Multiple LLM Agents Debate for Equitable Cultural Alignment", "authors": ["Dayeon Ki", "Rachel Rudinger", "Tianyi Zhou", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "37 pages, 18 figures", "summary": "Large Language Models (LLMs) need to adapt their predictions to diverse\ncultural contexts to benefit diverse communities across the world. While\nprevious efforts have focused on single-LLM, single-turn approaches, we propose\nto exploit the complementary strengths of multiple LLMs to promote cultural\nadaptability. We introduce a Multi-Agent Debate framework, where two LLM-based\nagents debate over a cultural scenario and collaboratively reach a final\ndecision. We propose two variants: one where either LLM agents exclusively\ndebate and another where they dynamically choose between self-reflection and\ndebate during their turns. We evaluate these approaches on 7 open-weight LLMs\n(and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette\nnorms in 75 countries. Experiments show that debate improves both overall\naccuracy and cultural group parity over single-LLM baselines. Notably,\nmulti-agent debate enables relatively small LLMs (7-9B) to achieve accuracies\ncomparable to that of a much larger model (27B parameters).", "AI": {"tldr": "This paper proposes a Multi-Agent Debate framework for Large Language Models (LLMs) to enhance cultural adaptability by allowing LLMs to debate scenarios and collaborate on decisions.", "motivation": "To improve the cultural adaptability of LLMs and benefit diverse global communities by leveraging the strengths of multiple agents rather than relying on a single LLM approach.", "method": "The paper introduces a Multi-Agent Debate framework involving two LLM agents that debate cultural scenarios, assessing two variants: exclusive debate by agents and a dynamic approach where agents choose between self-reflection and debate.", "result": "Experiments show that the debate approach improves accuracy and cultural group parity over single-LLM baselines, with smaller LLMs achieving performance comparable to larger models.", "conclusion": "Multi-agent debate enhances cultural understanding and decision-making in LLMs, offering a promising avenue for improving LLM performance in diverse contexts.", "key_contributions": ["Introduction of the Multi-Agent Debate framework for LLMs", "Demonstration of improved accuracy and cultural parity through debate", "Validation of performance of smaller LLMs compared to larger ones in cultural scenarios."], "limitations": "", "keywords": ["Large Language Models", "Cultural Adaptability", "Multi-Agent Debate", "Social Etiquette", "LLM Performance"], "importance_score": 9, "read_time_minutes": 37}}
{"id": "2505.24672", "pdf": "https://arxiv.org/pdf/2505.24672.pdf", "abs": "https://arxiv.org/abs/2505.24672", "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional Diversified Red-Teaming Data Synthesis", "authors": ["Xiaorui Wu", "Xiaofeng Mao", "Fei Li", "Xin Zhang", "Xuanhong Li", "Chong Teng", "Donghong Ji", "Zhuang Li"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.", "AI": {"tldr": "This paper presents TRIDENT, a novel framework and datasets for assessing and enhancing the risk coverage of alignment datasets in large language models, particularly focusing on harmful content generation and mitigation strategies.", "motivation": "To improve safety alignment in large language models by addressing gaps in existing datasets that primarily focus on lexical diversity and overlook critical risk dimensions.", "method": "The authors propose a systematic analysis framework assessing alignment datasets across Lexical Diversity, Malicious Intent, and Jailbreak Tactics, and introduce an automated pipeline for generating diverse harmful instructions paired with safe responses.", "result": "Fine-tuning Llama 3.1-8B on the TRIDENT-Edge dataset led to a 14.29% reduction in Harm Score and a 20% decrease in the Attack Success Rate compared to the best baseline model.", "conclusion": "The TRIDENT framework improves the robustness of large language models against harmful content generation, showcasing the importance of comprehensive risk coverage in safety alignment datasets.", "key_contributions": ["Introduction of a novel analysis framework for alignment datasets", "Creation of TRIDENT-Core and TRIDENT-Edge datasets for risk evaluation", "Demonstrated significant improvements in safety metrics through fine-tuning"], "limitations": "", "keywords": ["Large Language Models", "Safety alignment", "Natural Language Processing", "Harmful content", "Dataset evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24680", "pdf": "https://arxiv.org/pdf/2505.24680.pdf", "abs": "https://arxiv.org/abs/2505.24680", "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models", "authors": ["Xinrui Chen", "Haoli Bai", "Tao Yuan", "Ruikang Liu", "Kang Zhao", "Xianzhi Yu", "Lu Hou", "Tian Guan", "Yonghong He", "Chun Yuan"], "categories": ["cs.CL"], "comment": null, "summary": "Layer pruning has become a popular technique for compressing large language\nmodels (LLMs) due to its simplicity. However, existing layer pruning methods\noften suffer from significant performance drops. We identify that this\ndegradation stems from the mismatch of activation magnitudes across layers and\ntokens at the pruning interface. To address this, we propose LinearPatch, a\nsimple plug-and-play technique to revive the layer-pruned LLMs. The proposed\nmethod adopts Hadamard transformation to suppress massive outliers in\nparticular tokens, and channel-wise scaling to align the activation magnitudes.\nThese operations can be fused into a single matrix, which functions as a patch\nto bridge the pruning interface with negligible inference overhead. LinearPatch\nretains up to 94.15% performance of the original model when pruning 5 layers of\nLLaMA-3-8B on the question answering benchmark, surpassing existing\nstate-of-the-art methods by 4%. In addition, the patch matrix can be further\noptimized with memory efficient offline knowledge distillation. With only 5K\nsamples, the retained performance of LinearPatch can be further boosted to\n95.16% within 30 minutes on a single computing card.", "AI": {"tldr": "LinearPatch revives layer-pruned large language models (LLMs) by addressing activation magnitude mismatches, achieving significant performance retention with minimal overhead.", "motivation": "To improve performance retention in layer-pruned LLMs, which suffer from degradation due to mismatched activation magnitudes across layers and tokens.", "method": "LinearPatch uses Hadamard transformation to suppress outliers and channel-wise scaling to align activation magnitudes, incorporating these into a single matrix for effective pruning.", "result": "LinearPatch retains up to 94.15% performance of the original LLaMA-3-8B model after pruning 5 layers, outperforming existing methods by 4%.", "conclusion": "The proposed patch matrix can be optimized with offline knowledge distillation, further improving performance to 95.16% with 5K samples.", "key_contributions": ["Introduction of LinearPatch for layer pruning in LLMs", "Demonstration of significant performance retention after pruning layers", "Optimization with offline knowledge distillation for enhanced performance"], "limitations": "", "keywords": ["layer pruning", "large language models", "activation magnitudes", "performance retention", "knowledge distillation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24683", "pdf": "https://arxiv.org/pdf/2505.24683.pdf", "abs": "https://arxiv.org/abs/2505.24683", "title": "Should I Share this Translation? Evaluating Quality Feedback for User Reliance on Machine Translation", "authors": ["Dayeon Ki", "Kevin Duh", "Marine Carpuat"], "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 7 figures", "summary": "As people increasingly use AI systems in work and daily life, feedback\nmechanisms that help them use AI responsibly are urgently needed, particularly\nin settings where users are not equipped to assess the quality of AI\npredictions. We study a realistic Machine Translation (MT) scenario where\nmonolingual users decide whether to share an MT output, first without and then\nwith quality feedback. We compare four types of quality feedback: explicit\nfeedback that directly give users an assessment of translation quality using 1)\nerror highlights and 2) LLM explanations, and implicit feedback that helps\nusers compare MT inputs and outputs through 3) backtranslation and 4)\nquestion-answer (QA) tables. We find that all feedback types, except error\nhighlights, significantly improve both decision accuracy and appropriate\nreliance. Notably, implicit feedback, especially QA tables, yields\nsignificantly greater gains than explicit feedback in terms of decision\naccuracy, appropriate reliance, and user perceptions, receiving the highest\nratings for helpfulness and trust, and the lowest for mental burden.", "AI": {"tldr": "Study on enhancing AI user trust and decision-making through quality feedback in Machine Translation scenarios.", "motivation": "To explore effective feedback mechanisms that help users responsibly utilize AI-generated translations, especially when users lack the expertise to evaluate AI predictions.", "method": "A comparative study of four feedback mechanisms: explicit (error highlights, LLM explanations) and implicit (backtranslation, QA tables) in a Machine Translation context.", "result": "All feedback types improved decision accuracy and reliance, with implicit feedback, particularly QA tables, outperforming explicit feedback.", "conclusion": "The study highlights the importance of feedback in aiding non-expert users in making informed decisions about AI outputs, with QA tables being particularly effective.", "key_contributions": ["Comparison of explicit and implicit feedback in AI systems", "Demonstrated effectiveness of QA tables for enhancing user trust and accuracy", "Insights into user perceptions of feedback helpfulness and mental burden"], "limitations": "The study is limited to a specific Machine Translation context and may not generalize across other AI applications.", "keywords": ["Machine Translation", "Quality Feedback", "AI Trust", "User Decision Making", "Implicit Feedback"], "importance_score": 8, "read_time_minutes": 22}}
{"id": "2505.24688", "pdf": "https://arxiv.org/pdf/2505.24688.pdf", "abs": "https://arxiv.org/abs/2505.24688", "title": "Soft Reasoning: Navigating Solution Spaces in Large Language Models through Controlled Embedding Exploration", "authors": ["Qinglin Zhu", "Runcong Zhao", "Hanqi Yan", "Yulan He", "Yudong Chen", "Lin Gui"], "categories": ["cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Large Language Models (LLMs) struggle with complex reasoning due to limited\ndiversity and inefficient search. We propose Soft Reasoning, an embedding-based\nsearch framework that optimises the embedding of the first token to guide\ngeneration. It combines (1) embedding perturbation for controlled exploration\nand (2) Bayesian optimisation to refine embeddings via a verifier-guided\nobjective, balancing exploration and exploitation. This approach improves\nreasoning accuracy and coherence while avoiding reliance on heuristic search.\nExperiments demonstrate superior correctness with minimal computation, making\nit a scalable, model-agnostic solution.", "AI": {"tldr": "Introduction of Soft Reasoning, an embedding-based search framework to enhance complex reasoning in Large Language Models.", "motivation": "To address the struggle of Large Language Models (LLMs) with complex reasoning due to their limited diversity and inefficient search methods.", "method": "The proposed Soft Reasoning framework utilizes embedding perturbation for controlled exploration and Bayesian optimisation to refine embeddings, balancing exploration and exploitation during the reasoning process.", "result": "The framework improves reasoning accuracy and coherence, demonstrating superior correctness with minimal computation in experiments.", "conclusion": "Soft Reasoning provides a scalable, model-agnostic solution for enhancing reasoning in LLMs without relying on heuristic search methods.", "key_contributions": ["Introduction of the Soft Reasoning framework", "Utilization of embedding perturbation and Bayesian optimisation", "Demonstrated scalability and correctness improvements in reasoning tasks"], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Embedding", "Bayesian Optimisation", "Artificial Intelligence"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.24689", "pdf": "https://arxiv.org/pdf/2505.24689.pdf", "abs": "https://arxiv.org/abs/2505.24689", "title": "BPE Stays on SCRIPT: Structured Encoding for Robust Multilingual Pretokenization", "authors": ["Sander Land", "Catherine Arnett"], "categories": ["cs.CL"], "comment": "9 pages, 2 figures. For associated code, see\n  https://github.com/sanderland/script_bpe", "summary": "Byte Pair Encoding (BPE) tokenizers, widely used in Large Language Models,\nface challenges in multilingual settings, including penalization of non-Western\nscripts and the creation of tokens with partial UTF-8 sequences.\nPretokenization, often reliant on complex regular expressions, can also\nintroduce fragility and unexpected edge cases. We propose SCRIPT (Script\nCategory Representation in PreTokenization), a novel encoding scheme that\nbypasses UTF-8 byte conversion by using initial tokens based on Unicode script\nand category properties. This approach enables a simple, rule-based\npretokenization strategy that respects script boundaries, offering a robust\nalternative to pretokenization strategies based on regular expressions. We also\nintroduce and validate a constrained BPE merging strategy that enforces\ncharacter integrity, applicable to both SCRIPT-BPE and byte-based BPE. Our\nexperiments demonstrate that SCRIPT-BPE achieves competitive compression while\neliminating encoding-based penalties for non-Latin-script languages.", "AI": {"tldr": "This paper presents SCRIPT, a novel encoding scheme for tokenization in multilingual settings that improves upon traditional Byte Pair Encoding methods, particularly for non-Western scripts.", "motivation": "To address challenges faced by BPE tokenizers in handling multilingual data, especially non-Western scripts, which can lead to issues like penalties for encoding and fragility in pretokenization.", "method": "The proposed SCRIPT method utilizes Unicode script and category properties to create initial tokens, eliminating the need for UTF-8 byte conversion. It also introduces a constrained BPE merging strategy to maintain character integrity.", "result": "SCRIPT-BPE achieves competitive compression ratios compared to traditional methods while eliminating encoding penalties for non-Latin scripts, showcasing its effectiveness in multilingual contexts.", "conclusion": "SCRIPT offers a robust alternative to traditional pretokenization methods, facilitating better handling of non-Western scripts in NLP tasks.", "key_contributions": ["Introduction of SCRIPT encoding scheme that bypasses UTF-8 conversion", "Development of a rule-based pretokenization strategy respecting script boundaries", "Validation of constrained BPE merging strategy for character integrity"], "limitations": "", "keywords": ["Byte Pair Encoding", "Multilingual NLP", "Unicode", "Tokenization", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24691", "pdf": "https://arxiv.org/pdf/2505.24691.pdf", "abs": "https://arxiv.org/abs/2505.24691", "title": "Speech-to-Text Translation with Phoneme-Augmented CoT: Enhancing Cross-Lingual Transfer in Low-Resource Scenarios", "authors": ["Gerard I. Gállego", "Oriol Pareras", "Martí Cortada Garcia", "Lucas Takanori", "Javier Hernando"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "We propose a Speech-to-Text Translation (S2TT) approach that integrates\nphoneme representations into a Chain-of-Thought (CoT) framework to improve\ntranslation in low-resource and zero-resource settings. By introducing phoneme\nrecognition as an intermediate step, we enhance cross-lingual transfer,\nenabling translation even for languages with no labeled speech data. Our system\nbuilds on a multilingual LLM, which we extend to process speech and phonemes.\nTraining follows a curriculum learning strategy that progressively introduces\nmore complex tasks. Experiments on multilingual S2TT benchmarks show that\nphoneme-augmented CoT improves translation quality in low-resource conditions\nand enables zero-resource translation, while slightly impacting high-resource\nperformance. Despite this trade-off, our findings demonstrate that\nphoneme-based CoT is a promising step toward making S2TT more accessible across\ndiverse languages.", "AI": {"tldr": "This paper presents a novel Speech-to-Text Translation approach that leverages phoneme representations within a Chain-of-Thought framework for enhancing translation in low-resource settings.", "motivation": "The motivation is to improve translation quality in low-resource and zero-resource settings where labeled speech data is scarce.", "method": "The proposed method integrates phoneme recognition as an intermediate step within a multilingual LLM and employs a curriculum learning strategy to progressively teach the system with increasingly complex tasks.", "result": "Experiments demonstrate that the phoneme-augmented Chain-of-Thought improves translation quality in low-resource contexts and enables zero-resource translation, albeit with a slight performance impact on high-resource languages.", "conclusion": "Phoneme-based Chain-of-Thought shows potential for making Speech-to-Text Translation more accessible across diverse languages despite trade-offs in high-resource scenarios.", "key_contributions": ["Integration of phoneme representations into S2TT", "Enhanced cross-lingual transfer for low-resource translation", "Curriculum learning strategy for progressive task complexity"], "limitations": "Slight negative impact on high-resource language performance due to the phoneme-based approach.", "keywords": ["Speech-to-Text Translation", "phoneme representations", "Chain-of-Thought"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24701", "pdf": "https://arxiv.org/pdf/2505.24701.pdf", "abs": "https://arxiv.org/abs/2505.24701", "title": "Multi-Domain ABSA Conversation Dataset Generation via LLMs for Real-World Evaluation and Model Comparison", "authors": ["Tejul Pandit", "Meet Raval", "Dhvani Upadhyay"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures, 5 tables, 6th International Conference on\n  Natural Language Computing and AI (NLCAI 2025), ISBN : 978-1-923107-59-5,\n  Computer Science & Information Technology (CS & IT), ISSN : 2231 - 5403,\n  Volume 15, Number 10, May 2025", "summary": "Aspect-Based Sentiment Analysis (ABSA) offers granular insights into opinions\nbut often suffers from the scarcity of diverse, labeled datasets that reflect\nreal-world conversational nuances. This paper presents an approach for\ngenerating synthetic ABSA data using Large Language Models (LLMs) to address\nthis gap. We detail the generation process aimed at producing data with\nconsistent topic and sentiment distributions across multiple domains using\nGPT-4o. The quality and utility of the generated data were evaluated by\nassessing the performance of three state-of-the-art LLMs (Gemini 1.5 Pro,\nClaude 3.5 Sonnet, and DeepSeek-R1) on topic and sentiment classification\ntasks. Our results demonstrate the effectiveness of the synthetic data,\nrevealing distinct performance trade-offs among the models: DeepSeekR1 showed\nhigher precision, Gemini 1.5 Pro and Claude 3.5 Sonnet exhibited strong recall,\nand Gemini 1.5 Pro offered significantly faster inference. We conclude that\nLLM-based synthetic data generation is a viable and flexible method for\ncreating valuable ABSA resources, facilitating research and model evaluation\nwithout reliance on limited or inaccessible real-world labeled data.", "AI": {"tldr": "This paper introduces a method for generating synthetic datasets for Aspect-Based Sentiment Analysis (ABSA) using Large Language Models (LLMs) to alleviate data scarcity issues.", "motivation": "The scarcity of diverse, labeled datasets in ABSA that capture real-world conversational nuances necessitates the generation of synthetic datasets.", "method": "Synthetic ABSA data is generated using LLMs, specifically GPT-4o, ensuring consistent topic and sentiment distributions across multiple domains.", "result": "The generated synthetic data was evaluated on three LLMs (Gemini 1.5 Pro, Claude 3.5 Sonnet, DeepSeek-R1) for topic and sentiment classification, showing varied precision, recall, and inference speed among the models.", "conclusion": "LLM-based synthetic data generation is an effective approach for creating useful ABSA datasets, supporting research and model evaluation.", "key_contributions": ["Development of a LLM-based synthetic data generation method for ABSA", "Evaluation of three state-of-the-art LLMs on synthetic ABSA data", "Insights on performance trade-offs among different LLMs in sentiment classification tasks"], "limitations": "", "keywords": ["Aspect-Based Sentiment Analysis", "Synthetic Data", "Large Language Models", "Sentiment Classification", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24712", "pdf": "https://arxiv.org/pdf/2505.24712.pdf", "abs": "https://arxiv.org/abs/2505.24712", "title": "HESEIA: A community-based dataset for evaluating social biases in large language models, co-designed in real school settings in Latin America", "authors": ["Guido Ivetta", "Marcos J. Gomez", "Sofía Martinelli", "Pietro Palombini", "M. Emilia Echeveste", "Nair Carolina Mazzeo", "Beatriz Busaniche", "Luciana Benotti"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Most resources for evaluating social biases in Large Language Models are\ndeveloped without co-design from the communities affected by these biases, and\nrarely involve participatory approaches. We introduce HESEIA, a dataset of\n46,499 sentences created in a professional development course. The course\ninvolved 370 high-school teachers and 5,370 students from 189 Latin-American\nschools. Unlike existing benchmarks, HESEIA captures intersectional biases\nacross multiple demographic axes and school subjects. It reflects local\ncontexts through the lived experience and pedagogical expertise of educators.\nTeachers used minimal pairs to create sentences that express stereotypes\nrelevant to their school subjects and communities. We show the dataset\ndiversity in term of demographic axes represented and also in terms of the\nknowledge areas included. We demonstrate that the dataset contains more\nstereotypes unrecognized by current LLMs than previous datasets. HESEIA is\navailable to support bias assessments grounded in educational communities.", "AI": {"tldr": "HESEIA is a new dataset comprising 46,499 sentences designed to evaluate intersectional biases in Large Language Models, developed through participatory methods involving teachers and students in Latin America.", "motivation": "To address the lack of community involvement in developing tools for evaluating biases in Large Language Models, particularly in educational contexts.", "method": "The dataset was created during a professional development course for high-school teachers and students, using minimal pairs to reflect local biases and stereotypes across various school subjects and demographic axes.", "result": "The HESEIA dataset contains more unrecognized stereotypes in LLMs than existing datasets, representing a broad range of demographic and knowledge area diversity.", "conclusion": "HESEIA is intended to support more effective bias assessment in educational settings by incorporating the input of those most affected by these biases.", "key_contributions": ["Introduction of a dataset reflecting intersectional biases in LLMs", "Involvement of educators and students in dataset creation", "Increased representation of local contexts and stereotypes"], "limitations": "", "keywords": ["bias assessment", "language models", "education", "intersectionality", "dataset"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2505.24713", "pdf": "https://arxiv.org/pdf/2505.24713.pdf", "abs": "https://arxiv.org/abs/2505.24713", "title": "Voice Conversion Improves Cross-Domain Robustness for Spoken Arabic Dialect Identification", "authors": ["Badr M. Abdullah", "Matthew Baas", "Bernd Möbius", "Dietrich Klakow"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Arabic dialect identification (ADI) systems are essential for large-scale\ndata collection pipelines that enable the development of inclusive speech\ntechnologies for Arabic language varieties. However, the reliability of current\nADI systems is limited by poor generalization to out-of-domain speech. In this\npaper, we present an effective approach based on voice conversion for training\nADI models that achieves state-of-the-art performance and significantly\nimproves robustness in cross-domain scenarios. Evaluated on a newly collected\nreal-world test set spanning four different domains, our approach yields\nconsistent improvements of up to +34.1% in accuracy across domains.\nFurthermore, we present an analysis of our approach and demonstrate that voice\nconversion helps mitigate the speaker bias in the ADI dataset. We release our\nrobust ADI model and cross-domain evaluation dataset to support the development\nof inclusive speech technologies for Arabic.", "AI": {"tldr": "This paper presents a novel voice conversion-based approach for training Arabic dialect identification (ADI) models that enhances their performance and robustness across different domains.", "motivation": "To address the limitations of current Arabic dialect identification systems, particularly their poor generalization to out-of-domain speech, thereby supporting the development of speech technologies for Arabic language varieties.", "method": "An effective voice conversion technique is used to train ADI models, which is tested against a newly collected cross-domain dataset.", "result": "The proposed approach achieves state-of-the-art performance, showing consistent improvements of up to +34.1% in accuracy across four different domains.", "conclusion": "The study successfully enhances the reliability of ADI systems through voice conversion, which also helps mitigate speaker bias, contributing to more inclusive speech technologies.", "key_contributions": ["Introduction of voice conversion in Arabic dialect identification systems", "Significant performance improvement across domains", "Release of a robust ADI model and newly collected cross-domain evaluation dataset"], "limitations": "", "keywords": ["Arabic dialect identification", "voice conversion", "speech technology", "machine learning", "cross-domain evaluation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.24714", "pdf": "https://arxiv.org/pdf/2505.24714.pdf", "abs": "https://arxiv.org/abs/2505.24714", "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation", "authors": ["Junyu Luo", "Zhizhuo Kou", "Liming Yang", "Xiao Luo", "Jinsheng Huang", "Zhiping Xiao", "Jingshu Peng", "Chengzhong Liu", "Jiaming Ji", "Xuanzhe Liu", "Sirui Han", "Ming Zhang", "Yike Guo"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.", "AI": {"tldr": "Introduction of FinMME, a specialized multimodal dataset for financial research, and the FinScore evaluation system.", "motivation": "To address the lack of effective multimodal evaluation datasets in the finance domain for the development of Multimodal Large Language Models (MLLMs).", "method": "Creation of FinMME dataset with over 11,000 samples across 18 financial domains and 6 asset classes, validated by 20 annotators. Development of FinScore evaluation system incorporating hallucination penalties and multi-dimensional assessments.", "result": "Experimental results show that even advanced models like GPT-4o struggle with FinMME, demonstrating the dataset's challenging nature and high robustness under varying prompts.", "conclusion": "The dataset and evaluation protocol provide a significant resource for evaluating MLLMs in finance, emphasizing the limitations of current models.", "key_contributions": ["Introduction of FinMME dataset for evaluating MLLMs in finance", "Development of FinScore for unbiased evaluation with hallucination penalties", "Demonstration of dataset robustness and challenges for state-of-the-art models"], "limitations": "", "keywords": ["Multimodal Large Language Models", "Financial Research", "Dataset Evaluation"], "importance_score": 3, "read_time_minutes": 5}}
{"id": "2505.24726", "pdf": "https://arxiv.org/pdf/2505.24726.pdf", "abs": "https://arxiv.org/abs/2505.24726", "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning", "authors": ["Shelly Bensal", "Umar Jamil", "Christopher Bryant", "Melisa Russak", "Kiran Kamble", "Dmytro Mozolevskyi", "Muayad Ali", "Waseem AlShikh"], "categories": ["cs.CL"], "comment": null, "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.", "AI": {"tldr": "The paper presents a method using self-reflection and reinforcement learning to enhance the performance of large language models, demonstrating significant improvements in task success rates.", "motivation": "To improve the performance of large language models, particularly in challenging tasks where generating synthetic data is infeasible and only binary feedback is available.", "method": "The approach involves two stages: generating self-reflective commentary after a failed task attempt and then using that commentary in a second attempt at the task. Successful attempts lead to rewards for the self-reflection tokens.", "result": "Substantial performance gains across different model architectures were observed, with improvements up to 34.7% in math equation writing and 18.1% in function calling, notably with smaller models outperforming larger ones in the same family.", "conclusion": "The proposed framework offers a novel approach to developing more effective and reliable language models capable of self-improvement with limited external feedback.", "key_contributions": ["A two-stage self-reflection framework for language models", "Demonstrated performance improvement even with smaller models", "Innovative use of reinforcement learning in self-improvement tasks"], "limitations": "", "keywords": ["self-reflection", "reinforcement learning", "language models", "performance improvement", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24731", "pdf": "https://arxiv.org/pdf/2505.24731.pdf", "abs": "https://arxiv.org/abs/2505.24731", "title": "Circuit Stability Characterizes Language Model Generalization", "authors": ["Alan Sun"], "categories": ["cs.CL"], "comment": "16 pages, 10 figures", "summary": "Extensively evaluating the capabilities of (large) language models is\ndifficult. Rapid development of state-of-the-art models induce benchmark\nsaturation, while creating more challenging datasets is labor-intensive.\nInspired by the recent developments in mechanistic interpretability, we\nintroduce circuit stability as a new way to assess model performance. Circuit\nstability refers to a model's ability to apply a consistent reasoning\nprocess-its circuit-across various inputs. We mathematically formalize circuit\nstability and circuit equivalence. Then, through three case studies, we\nempirically show that circuit stability and the lack thereof can characterize\nand predict different aspects of generalization. Our proposed methods offer a\nstep towards rigorously relating the generality of models to their\ninterpretability.", "AI": {"tldr": "This paper introduces circuit stability as a new metric for evaluating large language models, relating model performance to interpretability through empirical studies.", "motivation": "The rapid development of language models leads to benchmark saturation, making it challenging to evaluate their true capabilities.", "method": "The authors introduce and mathematically formalize the concept of circuit stability, assessing a model's reasoning consistency across various inputs, and conduct three empirical case studies to demonstrate its effectiveness.", "result": "The case studies indicate that circuit stability can effectively characterize and predict generalization aspects of models.", "conclusion": "The proposed methods pave the way for a better understanding of how model interpretability relates to their generalization capabilities.", "key_contributions": ["Introduction of circuit stability as a new evaluation metric for language models", "Mathematical formalization of circuit stability and circuit equivalence", "Empirical demonstration of the relationship between circuit stability and model generalization"], "limitations": "", "keywords": ["language models", "circuit stability", "mechanistic interpretability", "generalization", "evaluation metrics"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.24754", "pdf": "https://arxiv.org/pdf/2505.24754.pdf", "abs": "https://arxiv.org/abs/2505.24754", "title": "Don't Reinvent the Wheel: Efficient Instruction-Following Text Embedding based on Guided Space Transformation", "authors": ["Yingchaojie Feng", "Yiqun Sun", "Yandong Sun", "Minfeng Zhu", "Qiang Huang", "Anthony K. H. Tung", "Wei Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to ACL 2025", "summary": "In this work, we investigate an important task named instruction-following\ntext embedding, which generates dynamic text embeddings that adapt to user\ninstructions, highlighting specific attributes of text. Despite recent\nadvancements, existing approaches suffer from significant computational\noverhead, as they require re-encoding the entire corpus for each new\ninstruction. To address this challenge, we propose GSTransform, a novel\ninstruction-following text embedding framework based on Guided Space\nTransformation. Our key observation is that instruction-relevant information is\ninherently encoded in generic embeddings but remains underutilized. Instead of\nrepeatedly encoding the corpus for each instruction, GSTransform is a\nlightweight transformation mechanism that adapts pre-computed embeddings in\nreal time to align with user instructions, guided by a small amount of text\ndata with instruction-focused label annotation. We conduct extensive\nexperiments on three instruction-awareness downstream tasks across nine\nreal-world datasets, demonstrating that GSTransform improves\ninstruction-following text embedding quality over state-of-the-art methods\nwhile achieving dramatic speedups of 6~300x in real-time processing on\nlarge-scale datasets. The source code is available at\nhttps://github.com/YingchaojieFeng/GSTransform.", "AI": {"tldr": "This paper presents GSTransform, a framework for efficient instruction-following text embedding that adapts pre-computed embeddings rather than re-encoding the entire corpus.", "motivation": "The work addresses the inefficiency of existing instruction-following text embedding approaches, which require significant computational resources by re-encoding for each instruction.", "method": "GTransform utilizes Guided Space Transformation to adapt pre-computed embeddings dynamically to user instructions, leveraging minimal instruction-focused labeled data for real-time alignment.", "result": "GTransform significantly enhances the quality of instruction-following text embeddings and achieves speed improvements of 6 to 300 times compared to current state-of-the-art methods across multiple datasets.", "conclusion": "The empirical results suggest that GSTransform provides a scalable solution for instruction-following tasks with improved efficiency and quality in embedding generation.", "key_contributions": ["Introduction of GSTransform, a novel lightweight framework for instruction-following text embedding.", "Demonstration of significant speed improvements (6~300x) in processing over existing methods.", "Extensive validation of the framework across various real-world datasets and tasks."], "limitations": "", "keywords": ["Instruction-following", "Text embedding", "Guided Space Transformation", "Efficiency", "Machine Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24757", "pdf": "https://arxiv.org/pdf/2505.24757.pdf", "abs": "https://arxiv.org/abs/2505.24757", "title": "LGAR: Zero-Shot LLM-Guided Neural Ranking for Abstract Screening in Systematic Literature Reviews", "authors": ["Christian Jaumann", "Andreas Wiedholz", "Annemarie Friedrich"], "categories": ["cs.CL"], "comment": null, "summary": "The scientific literature is growing rapidly, making it hard to keep track of\nthe state-of-the-art. Systematic literature reviews (SLRs) aim to identify and\nevaluate all relevant papers on a topic. After retrieving a set of candidate\npapers, the abstract screening phase determines initial relevance. To date,\nabstract screening methods using large language models (LLMs) focus on binary\nclassification settings; existing question answering (QA) based ranking\napproaches suffer from error propagation. LLMs offer a unique opportunity to\nevaluate the SLR's inclusion and exclusion criteria, yet, existing benchmarks\ndo not provide them exhaustively. We manually extract these criteria as well as\nresearch questions for 57 SLRs, mostly in the medical domain, enabling\nprincipled comparisons between approaches. Moreover, we propose LGAR, a\nzero-shot LLM Guided Abstract Ranker composed of an LLM based graded relevance\nscorer and a dense re-ranker. Our extensive experiments show that LGAR\noutperforms existing QA-based methods by 5-10 pp. in mean average precision.\nOur code and data is publicly available.", "AI": {"tldr": "The paper introduces LGAR, a zero-shot LLM Guided Abstract Ranker for systematic literature reviews (SLRs) that improves abstract screening efficiency and accuracy compared to existing QA-based methods.", "motivation": "With the rapid growth of scientific literature, it is becoming increasingly challenging to perform systematic literature reviews (SLRs) effectively. The need for better abstract screening methods using large language models (LLMs) is evident due to inadequacies in current approaches.", "method": "The authors propose LGAR, which includes an LLM based graded relevance scorer and a dense re-ranker to evaluate the inclusion and exclusion criteria of SLRs in a zero-shot setting.", "result": "LGAR achieves a 5-10 percentage point improvement in mean average precision compared to traditional QA-based abstract screening methods.", "conclusion": "The proposed method demonstrates significant potential for enhancing the abstract screening process in systematic literature reviews, particularly in the medical domain, with publicly available code and data.", "key_contributions": ["Introduction of LGAR, a novel zero-shot LLM Guided Abstract Ranker", "Manual extraction of inclusion and exclusion criteria for 57 SLRs", "Demonstrated improvement in performance over existing methods"], "limitations": "", "keywords": ["Systematic Literature Reviews", "Large Language Models", "Abstract Screening"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24768", "pdf": "https://arxiv.org/pdf/2505.24768.pdf", "abs": "https://arxiv.org/abs/2505.24768", "title": "From Macro to Micro: Probing Dataset Diversity in Language Model Fine-Tuning", "authors": ["Haoyu Li", "Xuhong Li", "Yiming Dong", "Kun Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Dataset diversity plays a pivotal role for the successful training of many\nmachine learning models, particularly in the supervised fine-tuning (SFT) stage\nof large language model (LLM) development. Despite increasing recognition of\nits importance, systematic analyses of dataset diversity still remain\nunderexplored. To address this gap, this work presents a systematic taxonomy of\nexisting diversity-control strategies, which primarily focus on the instruction\ncomponent, operating at either macroscopic (entire instruction semantics) or\nmesoscopic levels (instruction units), and furthermore introduces a novel\nanalysis of microscopic diversity within the response component, specifically\nanalyzing the statistical distribution of tokens in SFT training samples. In\nthe experimental evaluation, we construct fixed-size datasets (e.g., 10,000\nsamples each) from a corpus of 117,000 open-source SFT samples, incorporating\nsix distinct diversity-control strategies spanning macro-, meso-, and\nmicroscopic levels applied to both instructions and responses. We then\nfine-tune LLMs on these datasets to assess the six diversity-control\nstrategies. Results reveal that while macroscopic and mesoscopic strategies\nlead to higher performance with increasing diversity, the microscopic strategy\nin responses exhibits both a stronger correlation between model performance and\nthe degree of diversity and superior performance with maximum diversity across\nall strategies. These findings offer actionable insights for constructing\nhigh-performance SFT datasets.", "AI": {"tldr": "This paper presents a taxonomy of dataset diversity strategies for supervised fine-tuning of large language models, analyzing their effects on performance.", "motivation": "To systematically analyze dataset diversity, which is crucial for effective machine learning model training, particularly in large language models.", "method": "The authors constructed fixed-size datasets from 117,000 open-source SFT samples, applying six diversity-control strategies at macro, meso, and microscopic levels, and fine-tuned LLMs to evaluate performance.", "result": "Macroscopic and mesoscopic strategies improve performance with higher diversity, whereas the microscopic strategy demonstrates the strongest correlation with model performance and achieves superior outcomes with maximum diversity.", "conclusion": "Findings provide actionable insights for building effective SFT datasets through an understanding of diversity strategies.", "key_contributions": ["Systematic taxonomy of diversity-control strategies for LLM training datasets.", "Novel analysis of microscopic diversity in response components.", "Empirical evaluation of diversity strategies leading to actionable insights for dataset construction."], "limitations": "", "keywords": ["machine learning", "dataset diversity", "large language models", "fine-tuning", "supervised learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24778", "pdf": "https://arxiv.org/pdf/2505.24778.pdf", "abs": "https://arxiv.org/abs/2505.24778", "title": "Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models' Uncertainty?", "authors": ["Jiayu Liu", "Qing Zong", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL2025", "summary": "As large language models (LLMs) are increasingly used in high-stakes domains,\naccurately assessing their confidence is crucial. Humans typically express\nconfidence through epistemic markers (e.g., \"fairly confident\") instead of\nnumerical values. However, it remains unclear whether LLMs consistently use\nthese markers to reflect their intrinsic confidence due to the difficulty of\nquantifying uncertainty associated with various markers. To address this gap,\nwe first define marker confidence as the observed accuracy when a model employs\nan epistemic marker. We evaluate its stability across multiple\nquestion-answering datasets in both in-distribution and out-of-distribution\nsettings for open-source and proprietary LLMs. Our results show that while\nmarkers generalize well within the same distribution, their confidence is\ninconsistent in out-of-distribution scenarios. These findings raise significant\nconcerns about the reliability of epistemic markers for confidence estimation,\nunderscoring the need for improved alignment between marker based confidence\nand actual model uncertainty. Our code is available at\nhttps://github.com/HKUST-KnowComp/MarCon.", "AI": {"tldr": "This paper evaluates the consistency of epistemic markers used by large language models (LLMs) to express confidence in high-stakes scenarios, revealing challenges in accurately assessing model uncertainty.", "motivation": "Assess the reliability of LLMs' use of epistemic markers for confidence estimation due to their growing role in high-stakes domains.", "method": "We define marker confidence as the accuracy of responses when models use epistemic markers, and evaluate it across various question-answering datasets in both in-distribution and out-of-distribution settings.", "result": "Markers generalize well within the same distribution but exhibit inconsistent confidence in out-of-distribution scenarios.", "conclusion": "The reliability of epistemic markers for confidence estimation in LLMs is questionable, highlighting the need for better alignment with actual model uncertainty.", "key_contributions": ["Definition and evaluation of marker confidence for LLMs.", "Analysis of stability across in-distribution and out-of-distribution settings.", "Release of code for further research on confidence estimation."], "limitations": "Focus on specific datasets may limit generalizability of findings.", "keywords": ["Large Language Models", "Confidence Estimation", "Epistemic Markers", "Machine Learning", "High-Stakes Domains"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24788", "pdf": "https://arxiv.org/pdf/2505.24788.pdf", "abs": "https://arxiv.org/abs/2505.24788", "title": "Drop Dropout on Single-Epoch Language Model Pretraining", "authors": ["Houjun Liu", "John Bauer", "Christopher D. Manning"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL Findings; 5 pages, 2 figures, 4 pages of appendix", "summary": "Originally, dropout was seen as a breakthrough regularization technique that\nreduced overfitting and improved performance in almost all applications of deep\nlearning by reducing overfitting. Yet, single-epoch pretraining tasks common to\nmodern LLMs yield minimal overfitting, leading to dropout not being used for\nlarge LLMs. Nevertheless, no thorough empirical investigation has been done on\nthe role of dropout in LM pretraining. Through experiments in single-epoch\npretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) LMs\nwith varying levels of dropout, we find that downstream performance in language\nmodeling, morpho-syntax (BLiMP), question answering (SQuAD), and\nnatural-language inference (MNLI) improves when dropout is not applied during\npretraining. We additionally find that the recently-introduced \"early dropout\"\nalso degrades performance over applying no dropout at all. We further\ninvestigate the models' editability, and find that models trained without\ndropout are more successful in gradient-based model editing (MEND) and\nequivalent in representation-based model editing (ReFT). Therefore, we advocate\nto drop dropout during single-epoch pretraining.", "AI": {"tldr": "This paper investigates the role of dropout during single-epoch pretraining of large language models and finds that dropout negatively impacts performance and editability.", "motivation": "To empirically analyze the effects of dropout in language model pretraining, particularly in the context of large language models where dropout is typically not used due to minimal overfitting.", "method": "Experiments were conducted on single-epoch pretraining of both masked (BERT) and autoregressive (Pythia 160M and 1.4B) language models with varying levels of dropout applied.", "result": "Downstream performance improved in tasks such as language modeling, morpho-syntax, question answering, and natural-language inference when dropout was not applied during pretraining. Additionally, models without dropout were more successful in gradient-based model editing.", "conclusion": "The findings suggest dropping dropout during single-epoch pretraining as it enhances model performance and editability.", "key_contributions": ["Empirical analysis of dropout's impact in LM pretraining", "Identification of improved performance metrics without dropout", "Insights into model editability related to dropout usage"], "limitations": "", "keywords": ["dropout", "language models", "pretraining", "editability", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.24803", "pdf": "https://arxiv.org/pdf/2505.24803.pdf", "abs": "https://arxiv.org/abs/2505.24803", "title": "Guiding Generative Storytelling with Knowledge Graphs", "authors": ["Zhijun Pan", "Antonios Andronis", "Eva Hayek", "Oscar AP Wilkinson", "Ilya Lasy", "Annette Parry", "Guy Gadney", "Tim J. Smith", "Mick Grierson"], "categories": ["cs.CL", "cs.HC"], "comment": "This manuscript was submitted for peer review in January 2025", "summary": "Large Language Models (LLMs) have shown great potential in automated story\ngeneration, but challenges remain in maintaining long-form coherence and\nproviding users with intuitive and effective control. Retrieval-Augmented\nGeneration (RAG) has proven effective in reducing hallucinations in text\ngeneration; however, the use of structured data to support generative\nstorytelling remains underexplored. This paper investigates how knowledge\ngraphs (KGs) can enhance LLM-based storytelling by improving narrative quality\nand enabling user-driven modifications. We propose a KG-assisted storytelling\npipeline and evaluate its effectiveness through a user study with 15\nparticipants. Participants created their own story prompts, generated stories,\nand edited knowledge graphs to shape their narratives. Through quantitative and\nqualitative analysis, our findings demonstrate that knowledge graphs\nsignificantly enhance story quality in action-oriented and structured\nnarratives within our system settings. Additionally, editing the knowledge\ngraph increases users' sense of control, making storytelling more engaging,\ninteractive, and playful.", "AI": {"tldr": "The paper explores the enhancement of LLM-based storytelling using knowledge graphs, addressing coherence and user control.", "motivation": "To improve long-form coherence and user control in automated story generation by utilizing knowledge graphs.", "method": "A KG-assisted storytelling pipeline was proposed and evaluated through a user study with 15 participants who created story prompts and edited knowledge graphs.", "result": "Knowledge graphs significantly improved story quality and enhanced user engagement and control during storytelling, especially in action-oriented narratives.", "conclusion": "The integration of knowledge graphs in storytelling pipelines enhances narrative quality and user interaction, making the process more engaging.", "key_contributions": ["Proposes a KG-assisted storytelling pipeline", "Demonstrates the impact of knowledge graphs on narrative quality", "Increases user engagement through interactive modifications"], "limitations": "", "keywords": ["large language models", "knowledge graphs", "story generation", "user control", "interactive storytelling"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.24826", "pdf": "https://arxiv.org/pdf/2505.24826.pdf", "abs": "https://arxiv.org/abs/2505.24826", "title": "LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text", "authors": ["Li yunhan", "Wu gengshen"], "categories": ["cs.CL", "cs.CV"], "comment": "10 pages, 11 figures", "summary": "As large language models (LLMs) are increasingly used in legal applications,\ncurrent evaluation benchmarks tend to focus mainly on factual accuracy while\nlargely neglecting important linguistic quality aspects such as clarity,\ncoherence, and terminology. To address this gap, we propose three steps: First,\nwe develop a regression model to evaluate the quality of legal texts based on\nclarity, coherence, and terminology. Second, we create a specialized set of\nlegal questions. Third, we analyze 49 LLMs using this evaluation framework.\n  Our analysis identifies three key findings: First, model quality levels off\nat 14 billion parameters, with only a marginal improvement of $2.7\\%$ noted at\n72 billion parameters. Second, engineering choices such as quantization and\ncontext length have a negligible impact, as indicated by statistical\nsignificance thresholds above 0.016. Third, reasoning models consistently\noutperform base architectures. A significant outcome of our research is the\nrelease of a ranking list and Pareto analysis, which highlight the Qwen3 series\nas the optimal choice for cost-performance tradeoffs. This work not only\nestablishes standardized evaluation protocols for legal LLMs but also uncovers\nfundamental limitations in current training data refinement approaches. Code\nand models are available at: https://github.com/lyxx3rd/LegalEval-Q.", "AI": {"tldr": "This paper proposes a framework for evaluating legal LLMs focused on linguistic quality aspects such as clarity and coherence, revealing insights into model performance and limitations.", "motivation": "Current benchmarks for legal LLMs focus primarily on factual accuracy, neglecting important linguistic quality dimensions.", "method": "Development of a regression model to assess legal text quality, creation of a specialized question set, and analysis of 49 LLMs using this evaluation framework.", "result": "Finding that model quality levels off at 14 billion parameters, with slight improvements at 72 billion; engineering choices have little impact, and reasoning models outperform base architectures.", "conclusion": "The research establishes standardized evaluation protocols for legal LLMs and spotlights limitations in current training data refinement; Qwen3 series identified as the best for cost-performance.", "key_contributions": ["Development of new evaluation framework for legal LLMs", "Identification of parameter thresholds for model performance", "Release of ranking list for legal LLMs"], "limitations": "Neglects to explore the impact of some engineering choices thoroughly; primarily focused on legal applications which may not generalize beyond this domain.", "keywords": ["Large Language Models", "Legal Applications", "Quality Evaluation", "Clarity", "Coherence"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.24830", "pdf": "https://arxiv.org/pdf/2505.24830.pdf", "abs": "https://arxiv.org/abs/2505.24830", "title": "Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs", "authors": ["Juraj Vladika", "Annika Domres", "Mai Nguyen", "Rebecca Moser", "Jana Nano", "Felix Busch", "Lisa C. Adams", "Keno K. Bressem", "Denise Bernhardt", "Stephanie E. Combs", "Kai J. Borm", "Florian Matthes", "Jan C. Peeken"], "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "Large language models (LLMs) exhibit extensive medical knowledge but are\nprone to hallucinations and inaccurate citations, which pose a challenge to\ntheir clinical adoption and regulatory compliance. Current methods, such as\nRetrieval Augmented Generation, partially address these issues by grounding\nanswers in source documents, but hallucinations and low fact-level\nexplainability persist. In this work, we introduce a novel atomic fact-checking\nframework designed to enhance the reliability and explainability of LLMs used\nin medical long-form question answering. This method decomposes LLM-generated\nresponses into discrete, verifiable units called atomic facts, each of which is\nindependently verified against an authoritative knowledge base of medical\nguidelines. This approach enables targeted correction of errors and direct\ntracing to source literature, thereby improving the factual accuracy and\nexplainability of medical Q&A. Extensive evaluation using multi-reader\nassessments by medical experts and an automated open Q&A benchmark demonstrated\nsignificant improvements in factual accuracy and explainability. Our framework\nachieved up to a 40% overall answer improvement and a 50% hallucination\ndetection rate. The ability to trace each atomic fact back to the most relevant\nchunks from the database provides a granular, transparent explanation of the\ngenerated responses, addressing a major gap in current medical AI applications.\nThis work represents a crucial step towards more trustworthy and reliable\nclinical applications of LLMs, addressing key prerequisites for clinical\napplication and fostering greater confidence in AI-assisted healthcare.", "AI": {"tldr": "Introducing a framework that enhances the reliability and explainability of LLMs in medical Q&A by decomposing responses into verifiable atomic facts.", "motivation": "To address hallucinations and inaccurate citations of LLMs in clinical settings, which hinder their adoption in healthcare.", "method": "A novel atomic fact-checking framework that breaks down LLM-generated responses into atomic facts independently verified against medical guidelines.", "result": "The framework showed up to 40% improvement in answer accuracy and 50% reduction in hallucinations, enhancing explainability by tracing facts to source literature.", "conclusion": "This framework significantly improves the trustworthiness of LLMs in clinical applications, which is essential for regulatory compliance and effective healthcare delivery.", "key_contributions": ["Development of atomic fact-checking framework for LLM responses", "Demonstrated improvements in factual accuracy and explainability", "Independent verification against medical guidelines for reliability"], "limitations": "", "keywords": ["large language models", "medical AI", "explainability", "fact-checking", "clinical applications"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2505.24832", "pdf": "https://arxiv.org/pdf/2505.24832.pdf", "abs": "https://arxiv.org/abs/2505.24832", "title": "How much do language models memorize?", "authors": ["John X. Morris", "Chawin Sitawarin", "Chuan Guo", "Narine Kokhlikyan", "G. Edward Suh", "Alexander M. Rush", "Kamalika Chaudhuri", "Saeed Mahloujifar"], "categories": ["cs.CL"], "comment": null, "summary": "We propose a new method for estimating how much a model ``knows'' about a\ndatapoint and use it to measure the capacity of modern language models. Prior\nstudies of language model memorization have struggled to disentangle\nmemorization from generalization. We formally separate memorization into two\ncomponents: \\textit{unintended memorization}, the information a model contains\nabout a specific dataset, and \\textit{generalization}, the information a model\ncontains about the true data-generation process. When we completely eliminate\ngeneralization, we can compute the total memorization, which provides an\nestimate of model capacity: our measurements estimate that GPT-style models\nhave a capacity of approximately 3.6 bits per parameter. We train language\nmodels on datasets of increasing size and observe that models memorize until\ntheir capacity fills, at which point ``grokking'' begins, and unintended\nmemorization decreases as models begin to generalize. We train hundreds of\ntransformer language models ranging from $500K$ to $1.5B$ parameters and\nproduce a series of scaling laws relating model capacity and data size to\nmembership inference.", "AI": {"tldr": "This paper presents a method to estimate the memorization and generalization of language models, revealing that GPT-style models have a capacity of about 3.6 bits per parameter.", "motivation": "To better understand how language models memorize information and the distinction between memorization and generalization.", "method": "The study separates memorization into unintended memorization and generalization. It involves training multiple transformer language models while observing the relationship between model capacity and dataset size.", "result": "The findings indicate that as model capacity fills, unintended memorization decreases and generalization starts, with scaling laws established regarding model capacity and data size related to membership inference.", "conclusion": "The research provides insights into the limits of language models and the balance between memorization and generalization, contributing to understanding model capacity and training dynamics.", "key_contributions": ["Introduction of a method to estimate memorization and generalization in language models", "Quantitative analysis revealing GPT-style models' capacity", "Establishment of scaling laws relating model capacity and data size"], "limitations": "", "keywords": ["language models", "memorization", "generalization", "model capacity", "scaling laws"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24834", "pdf": "https://arxiv.org/pdf/2505.24834.pdf", "abs": "https://arxiv.org/abs/2505.24834", "title": "Multilinguality Does not Make Sense: Investigating Factors Behind Zero-Shot Transfer in Sense-Aware Tasks", "authors": ["Roksana Goworek", "Haim Dubossarsky"], "categories": ["cs.CL"], "comment": "8 pages, 8 figures", "summary": "Cross-lingual transfer allows models to perform tasks in languages unseen\nduring training and is often assumed to benefit from increased multilinguality.\nIn this work, we challenge this assumption in the context of two underexplored,\nsense-aware tasks: polysemy disambiguation and lexical semantic change. Through\na large-scale analysis across 28 languages, we show that multilingual training\nis neither necessary nor inherently beneficial for effective transfer. Instead,\nwe find that confounding factors - such as fine-tuning data composition and\nevaluation artifacts - better account for the perceived advantages of\nmultilinguality. Our findings call for more rigorous evaluations in\nmultilingual NLP. We release fine-tuned models and benchmarks to support\nfurther research, with implications extending to low-resource and typologically\ndiverse languages.", "AI": {"tldr": "This paper challenges the assumption that increased multilinguality benefits cross-lingual transfer for polysemy disambiguation and lexical semantic change, revealing that confounding factors play a more significant role.", "motivation": "The study investigates the effectiveness of multilingual training in cross-lingual transfer tasks, specifically in polysemy disambiguation and lexical semantic change, across 28 languages.", "method": "The authors conducted a large-scale analysis to evaluate the effects of multilingual training versus other influencing factors on model performance.", "result": "The findings indicate that multilingual training is not necessary or inherently beneficial for effective transfer; confounding factors are better predictors of success.", "conclusion": "The paper suggests that there is a need for more rigorous evaluations in multilingual NLP and provides benchmarks and models for future research.", "key_contributions": ["Challenged the assumption of the benefits of multilinguality in NLP tasks.", "Identified confounding factors affecting perceived multilingual advantages.", "Provided benchmarks and fine-tuned models for further research."], "limitations": "The study focuses on specific underexplored tasks, which may not generalize to all NLP applications.", "keywords": ["cross-lingual transfer", "polysemy disambiguation", "lexical semantic change", "multilingual NLP", "evaluation artifacts"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2505.24858", "pdf": "https://arxiv.org/pdf/2505.24858.pdf", "abs": "https://arxiv.org/abs/2505.24858", "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs", "authors": ["Gabrielle Kaili-May Liu", "Gal Yona", "Avi Caciularu", "Idan Szpektor", "Tim G. J. Rudner", "Arman Cohan"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of $\\textit{faithful confidence calibration}$ of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\n$\\textit{faithfully reflect}$ their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.", "AI": {"tldr": "This study investigates the ability of LLMs to communicate uncertainty accurately and introduces MetaFaith, a new approach that significantly improves this calibration.", "motivation": "Reliable uncertainty communication in LLMs is crucial for trustworthiness, yet these models often exhibit assertive language even when uncertain, leading to user mistrust.", "method": "A systematic benchmarking of LLMs' ability to express their uncertainty was conducted, examining various models, datasets, and prompting strategies. MetaFaith was introduced as a novel prompt-based calibration approach inspired by human metacognition.", "result": "The evaluation revealed that LLMs generally fail in accurately communicating uncertainty, with MetaFaith significantly enhancing faithful calibration by up to 61% and achieving an 83% win rate over previous methods based on human judgment.", "conclusion": "MetaFaith represents an effective solution for improving the faithfulness of LLMs in terms of uncertainty communication, thereby enhancing user trust.", "key_contributions": ["Systematic study of faithful confidence calibration in LLMs", "Introduction of MetaFaith, a novel prompt-based calibration method", "Demonstration of substantial improvements in faithfulness of uncertainty communication"], "limitations": "", "keywords": ["LLM", "uncertainty communication", "confidence calibration", "MetaFaith", "trustworthiness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.24863", "pdf": "https://arxiv.org/pdf/2505.24863.pdf", "abs": "https://arxiv.org/abs/2505.24863", "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time", "authors": ["Junyu Zhang", "Runpei Dong", "Han Wang", "Xuying Ning", "Haoran Geng", "Peihao Li", "Xialin He", "Yutong Bai", "Jitendra Malik", "Saurabh Gupta", "Huan Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents AlphaOne ($\\alpha$1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\n$\\alpha$1 first introduces $\\alpha$ moment, which represents the scaled\nthinking phase with a universal parameter $\\alpha$. Within this scaled\npre-$\\alpha$ moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the $\\alpha$ moment, $\\alpha$1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate $\\alpha$1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/", "AI": {"tldr": "AlphaOne ($\\alpha$1) is a framework that improves the reasoning capabilities of large reasoning models by dynamically controlling the pace of reasoning through a system of scheduled transitions between slow and fast thinking phases.", "motivation": "To enhance reasoning capabilities in large reasoning models by modulating their reasoning speed at test time.", "method": "The framework introduces $\\\\alpha$ moment to scale reasoning phases and uses a Bernoulli stochastic process to schedule transitions of reasoning states, terminating with an end-of-thinking token.", "result": "Empirical studies show that $\\\\alpha$1 outperforms existing methods in reasoning capability and efficiency across various challenging benchmarks in mathematics, coding, and science.", "conclusion": "The $\\\\alpha$1 framework offers a generalizable approach to modulating reasoning processes, making it effective for diverse applications.", "key_contributions": ["Introduction of $\\\\alpha$ moment for reasoning speed modulation", "Dynamically schedules slow thinking transitions", "Unification of existing monotonic scaling methods"], "limitations": "", "keywords": ["Reasoning Models", "AlphaOne", "Modulation Framework", "Efficient Reasoning", "NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.24864", "pdf": "https://arxiv.org/pdf/2505.24864.pdf", "abs": "https://arxiv.org/abs/2505.24864", "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models", "authors": ["Mingjie Liu", "Shizhe Diao", "Ximing Lu", "Jian Hu", "Xin Dong", "Yejin Choi", "Jan Kautz", "Yi Dong"], "categories": ["cs.CL", "cs.AI"], "comment": "26 pages, 17 figures", "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B", "AI": {"tldr": "This paper introduces ProRL, a methodology leveraging reinforcement learning (RL) to enhance reasoning strategies in language models, showing significant improvements over base models.", "motivation": "To explore whether reinforcement learning can genuinely expand reasoning capabilities in language models beyond existing latent outputs.", "method": "ProRL incorporates KL divergence control, reference policy resetting, and diverse task sets to train language models using reinforcement learning techniques.", "result": "Empirical analysis demonstrates that RL-trained models consistently outperform base models across various pass@k evaluations, revealing novel reasoning capabilities not present in base models.", "conclusion": "Prolonged RL training can uncover new reasoning strategies, suggesting potential advancements in reasoning performance for language models when using long-horizon RL methods.", "key_contributions": ["Introduction of ProRL methodology for RL training.", "Demonstration that RL can discover new reasoning strategies.", "Empirical results showing superior task performance of RL-trained models compared to base models."], "limitations": "", "keywords": ["reinforcement learning", "reasoning", "language models", "ProRL", "AI"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2505.24004", "pdf": "https://arxiv.org/pdf/2505.24004.pdf", "abs": "https://arxiv.org/abs/2505.24004", "title": "Redefining Research Crowdsourcing: Incorporating Human Feedback with LLM-Powered Digital Twins", "authors": ["Amanda Chan", "Catherine Di", "Joseph Rupertus", "Gary Smith", "Varun Nagaraj Rao", "Manoel Horta Ribeiro", "Andrés Monroy-Hernández"], "categories": ["cs.HC", "cs.CL", "cs.CY"], "comment": "Accepted as a CHI Late Breaking Work (2025), cite appropriately", "summary": "Crowd work platforms like Amazon Mechanical Turk and Prolific are vital for\nresearch, yet workers' growing use of generative AI tools poses challenges.\nResearchers face compromised data validity as AI responses replace authentic\nhuman behavior, while workers risk diminished roles as AI automates tasks. To\naddress this, we propose a hybrid framework using digital twins, personalized\nAI models that emulate workers' behaviors and preferences while keeping humans\nin the loop. We evaluate our system with an experiment (n=88 crowd workers) and\nin-depth interviews with crowd workers (n=5) and social science researchers\n(n=4). Our results suggest that digital twins may enhance productivity and\nreduce decision fatigue while maintaining response quality. Both researchers\nand workers emphasized the importance of transparency, ethical data use, and\nworker agency. By automating repetitive tasks and preserving human engagement\nfor nuanced ones, digital twins may help balance scalability with authenticity.", "AI": {"tldr": "This paper introduces a hybrid framework using digital twins to mitigate the challenges posed by generative AI in crowd work platforms by enhancing productivity while ensuring worker engagement and data validity.", "motivation": "The increasing use of generative AI tools in crowd work platforms is threatening data validity and worker roles; this paper addresses these challenges.", "method": "A hybrid framework was proposed that utilizes digital twins to emulate workers' behaviors and preferences while keeping them involved in decision-making.", "result": "The evaluation of the system through an experiment and interviews indicated that digital twins can enhance productivity and reduce decision fatigue without compromising response quality.", "conclusion": "Automating repetitive tasks with digital twins can retain human engagement for complex tasks, thus achieving a balance between scalability and authenticity.", "key_contributions": ["Proposed a digital twin framework for crowd work", "Demonstrated potential for enhanced productivity and reduced decision fatigue", "Highlighted the importance of transparency and worker agency"], "limitations": "The study is limited by a small sample size and may not generalize across all types of crowd work tasks.", "keywords": ["crowd work", "digital twins", "human-computer interaction", "AI tools", "productivity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.24195", "pdf": "https://arxiv.org/pdf/2505.24195.pdf", "abs": "https://arxiv.org/abs/2505.24195", "title": "WikiGap: Promoting Epistemic Equity by Surfacing Knowledge Gaps Between English Wikipedia and other Language Editions", "authors": ["Zining Wang", "Yuxuan Zhang", "Dongwook Yoon", "Nicholas Vincent", "Farhan Samir", "Vered Shwartz"], "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "With more than 11 times as many pageviews as the next, English Wikipedia\ndominates global knowledge access relative to other language editions. Readers\nare prone to assuming English Wikipedia as a superset of all language editions,\nleading many to prefer it even when their primary language is not English.\nOther language editions, however, comprise complementary facts rooted in their\nrespective cultures and media environments, which are marginalized in English\nWikipedia. While Wikipedia's user interface enables switching between language\neditions through its Interlanguage Link (ILL) system, it does not reveal to\nreaders that other language editions contain valuable, complementary\ninformation. We present WikiGap, a system that surfaces complementary facts\nsourced from other Wikipedias within the English Wikipedia interface.\nSpecifically, by combining a recent multilingual information-gap discovery\nmethod with a user-centered design, WikiGap enables access to complementary\ninformation from French, Russian, and Chinese Wikipedia. In a mixed-methods\nstudy (n=21), WikiGap significantly improved fact-finding accuracy, reduced\ntask time, and received a 32-point higher usability score relative to\nWikipedia's current ILL-based navigation system. Participants reported\nincreased awareness of the availability of complementary information in\nnon-English editions and reconsidered the completeness of English Wikipedia.\nWikiGap thus paves the way for improved epistemic equity across language\neditions.", "AI": {"tldr": "WikiGap is a system that enhances information access in English Wikipedia by surfacing complementary facts from other language editions, improving fact-finding accuracy and user awareness.", "motivation": "To address the misconception that English Wikipedia contains all relevant information and to promote the use of complementary knowledge found in other language editions.", "method": "A mixed-methods study was conducted with 21 participants, using a multilingual information-gap discovery method combined with user-centered design principles.", "result": "WikiGap significantly improved fact-finding accuracy and reduced task completion time, while achieving a 32-point higher usability score compared to the existing Interlanguage Link navigation in Wikipedia.", "conclusion": "WikiGap enhances epistemic equity by facilitating access to complementary information from multiple languages, encouraging users to consider the limitations of English Wikipedia.", "key_contributions": ["Development of the WikiGap system", "Evidence of improved fact-finding and usability", "Enhanced user awareness of other language editions' contributions"], "limitations": "The study involved a small sample size (n=21) and focused on specific languages (French, Russian, Chinese).", "keywords": ["Wikipedia", "multilingual information", "complementary facts", "user-centered design", "epistemic equity"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2401.08491", "pdf": "https://arxiv.org/pdf/2401.08491.pdf", "abs": "https://arxiv.org/abs/2401.08491", "title": "Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models", "authors": ["Tassilo Klein", "Moin Nabi"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Track)", "summary": "The generation of toxic content by large language models (LLMs) remains a\ncritical challenge for the safe deployment of language technology. We propose a\nnovel framework for implicit knowledge editing and controlled text generation\nby fine-tuning LLMs with a prototype-based contrastive perplexity objective.\nCentral to our method is the construction of hard negatives - toxic outputs\nthat are generated through adversarial paraphrasing to be semantically similar\nand model probability to their non-toxic counterparts. By training on these\nchallenging and realistic pairs, our approach ensures robust and stable\ncontrastive optimization. Experimental results in the domain of detoxification\ndemonstrate that our method significantly reduces toxic generation while\nmaintaining strong performance on downstream tasks such as commonsense\nreasoning and reading comprehension. Our findings highlight the effectiveness\nof exploiting hard negatives for attribute-aware fine-tuning.", "AI": {"tldr": "A framework for reducing toxic outputs in LLMs through adversarial paraphrasing and fine-tuning.", "motivation": "To address the challenge of toxic content generation by large language models (LLMs) for safer deployment of language technology.", "method": "A prototype-based contrastive perplexity objective is employed, which uses hard negatives generated through adversarial paraphrasing, ensuring robust contrastive optimization during training.", "result": "The method significantly reduces toxic content generation while maintaining strong performance on tasks like commonsense reasoning and reading comprehension.", "conclusion": "Exploiting hard negatives for attribute-aware fine-tuning is effective in detoxifying LLM outputs.", "key_contributions": ["Introduction of a novel framework for implicit knowledge editing in LLMs.", "Use of adversarial paraphrasing to create hard negatives for effective training.", "Demonstration of reduced toxicity without compromising downstream task performance."], "limitations": "", "keywords": ["Large Language Models", "Toxic Content Generation", "Contrastive Learning"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2403.02839", "pdf": "https://arxiv.org/pdf/2403.02839.pdf", "abs": "https://arxiv.org/abs/2403.02839", "title": "An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4", "authors": ["Hui Huang", "Xingyuan Bu", "Hongli Zhou", "Yingqi Qu", "Jing Liu", "Muyun Yang", "Bing Xu", "Tiejun Zhao"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL2025", "summary": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have fine-tuned judge\nmodels based on open-source LLMs for evaluation. While the fine-tuned judge\nmodels are claimed to achieve comparable evaluation capability with GPT-4, in\nthis work, we conduct an empirical study of LLM-as-a-Judge. Our findings\nindicate that although the fine-tuned judge models achieve high performance on\nin-domain test sets, even surpassing GPT-4, they underperform GPT-4 across\nseveral dimensions, including generalizability, fairness and adaptability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations.", "AI": {"tldr": "This paper empirically evaluates the efficacy of fine-tuned judge models for assessing the quality of other LLMs, revealing limitations in generalizability, fairness, and adaptability compared to GPT-4.", "motivation": "To investigate the effectiveness of fine-tuned LLMs in evaluating the performance of other LLMs, challenging the assumption that state-of-the-art models always provide superior evaluation.", "method": "An empirical study comparing fine-tuned judge models against GPT-4 across various dimensions such as generalizability, fairness, and adaptability, using in-domain and out-of-domain test sets.", "result": "Fine-tuned judge models show high performance on in-domain test sets but significantly underperform GPT-4 in generalizability, fairness, and adaptability.", "conclusion": "Fine-tuned judge models act as task-specific classifiers, leading to inherent limitations that hinder their evaluation capabilities compared to GPT-4.", "key_contributions": ["Empirical evaluation of fine-tuned judge models using LLMs", "Comparison of performance metrics between fine-tuned models and GPT-4", "Insights into the limitations of LLMs as evaluators."], "limitations": "Fine-tuned models perform well in specific domains but lack generalizability and adaptability.", "keywords": ["Large Language Models", "LLM Evaluation", "Empirical Study", "Generalization", "Fairness"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2405.14189", "pdf": "https://arxiv.org/pdf/2405.14189.pdf", "abs": "https://arxiv.org/abs/2405.14189", "title": "Efficient Universal Goal Hijacking with Semantics-guided Prompt Organization", "authors": ["Yihao Huang", "Chong Wang", "Xiaojun Jia", "Qing Guo", "Felix Juefei-Xu", "Jian Zhang", "Geguang Pu", "Yang Liu"], "categories": ["cs.CL", "cs.CV"], "comment": "accepted by ACL 2025", "summary": "Universal goal hijacking is a kind of prompt injection attack that forces\nLLMs to return a target malicious response for arbitrary normal user prompts.\nThe previous methods achieve high attack performance while being too cumbersome\nand time-consuming. Also, they have concentrated solely on optimization\nalgorithms, overlooking the crucial role of the prompt. To this end, we propose\na method called POUGH that incorporates an efficient optimization algorithm and\ntwo semantics-guided prompt organization strategies. Specifically, our method\nstarts with a sampling strategy to select representative prompts from a\ncandidate pool, followed by a ranking strategy that prioritizes them. Given the\nsequentially ranked prompts, our method employs an iterative optimization\nalgorithm to generate a fixed suffix that can concatenate to arbitrary user\nprompts for universal goal hijacking. Experiments conducted on four popular\nLLMs and ten types of target responses verified the effectiveness.", "AI": {"tldr": "The paper proposes POUGH, a method for universal goal hijacking in LLMs that uses efficient optimization and prompt organization strategies to enhance prompt injection attacks.", "motivation": "Current methods for LLM prompt injection attacks are cumbersome and focus mainly on optimization, neglecting the prompt's role.", "method": "POUGH employs a sampling strategy to select representative prompts, followed by a ranking strategy to prioritize them. An iterative optimization algorithm generates a fixed suffix for universal goal hijacking.", "result": "Experiments on four popular LLMs and ten target response types demonstrated the effectiveness of the proposed method.", "conclusion": "POUGH is a more efficient approach to prompt injection that improves the performance of universal goal hijacking in large language models compared to existing methods.", "key_contributions": ["Introduction of the POUGH method for prompt injection attacks", "Innovative use of semantics-guided prompt organization", "Demonstrated effectiveness on multiple LLMs and response types"], "limitations": "", "keywords": ["prompt injection", "LLMs", "goal hijacking", "optimization algorithms", "prompt organization"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.10099", "pdf": "https://arxiv.org/pdf/2406.10099.pdf", "abs": "https://arxiv.org/abs/2406.10099", "title": "Know the Unknown: An Uncertainty-Sensitive Method for LLM Instruction Tuning", "authors": ["Jiaqi Li", "Yixuan Tang", "Yi Yang"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate remarkable capabilities but face\nchallenges from hallucinations, which typically arise from insufficient\nknowledge or context. While instructing LLMs to acknowledge knowledge\nlimitations by responding with \"I don't know\" appears promising, we find that\nmodels consistently struggle with admitting knowledge gaps. This challenge may\noriginate from current instruction datasets that emphasise answer generation\nover knowledge boundary awareness. To address this limitation, we introduce\nUncertainty-and-Sensitivity-Aware Tuning (US-Tuning), a novel two-stage\napproach for contextual question answering (QA). The first stage enhances LLMs'\nability to recognise their knowledge boundaries, while the second stage\nreinforces instruction adherence through carefully designed causal prompts. Our\nexperimental results demonstrate that US-Tuning not only significantly reduces\nincorrect answers in contextual QA but also improves models' faithfulness to\ntheir parametric knowledge, mitigating hallucinations in general QA tasks. Our\nfine-tuned Llama2-7B model achieves up to a 34.7% improvement in handling\nout-of-knowledge questions and outperforms GPT-4 by 4.2% in overall\nperformance.", "AI": {"tldr": "This paper presents US-Tuning, a two-stage approach to improve large language models' ability to acknowledge knowledge gaps during contextual question answering, resulting in reduced hallucinations and higher performance in answering unknown queries.", "motivation": "Large language models are prone to hallucinations due to insufficient recognition of their knowledge limitations, impacting their reliability in question answering.", "method": "US-Tuning involves a two-stage process: first, enhancing the model's awareness of its knowledge boundaries; second, reinforcing adherence to instructions using causal prompts.", "result": "The US-Tuning approach significantly reduces incorrect answers and enhances models' faithfulness, with experimental results showing a 34.7% improvement in handling out-of-knowledge questions and a 4.2% performance increase over GPT-4.", "conclusion": "US-Tuning effectively mitigates hallucinations in question answering tasks and enhances LLMs' ability to recognize their limitations, leading to improved user trust and model reliability.", "key_contributions": ["Introduction of US-Tuning as a novel method for LLMs.", "Demonstrated reduction of hallucinations in QA tasks.", "Improvement of LLM performance on out-of-knowledge questions."], "limitations": "", "keywords": ["Large Language Models", "Hallucinations", "Contextual Question Answering", "US-Tuning", "Knowledge Boundaries"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2406.16469", "pdf": "https://arxiv.org/pdf/2406.16469.pdf", "abs": "https://arxiv.org/abs/2406.16469", "title": "Evaluating Visual and Cultural Interpretation: The K-Viscuit Benchmark with Human-VLM Collaboration", "authors": ["ChaeHun Park", "Yujin Baek", "Jaeseok Kim", "Yu-Jung Heo", "Du-Seong Chang", "Jaegul Choo"], "categories": ["cs.CL", "cs.CV"], "comment": "ACL 2025 camera-ready", "summary": "To create culturally inclusive vision-language models (VLMs), developing a\nbenchmark that tests their ability to address culturally relevant questions is\nessential. Existing approaches typically rely on human annotators, making the\nprocess labor-intensive and creating a cognitive burden in generating diverse\nquestions. To address this, we propose a semi-automated framework for\nconstructing cultural VLM benchmarks, specifically targeting multiple-choice\nQA. This framework combines human-VLM collaboration, where VLMs generate\nquestions based on guidelines, a small set of annotated examples, and relevant\nknowledge, followed by a verification process by native speakers. We\ndemonstrate the effectiveness of this framework through the creation of\n\\texttt{K-Viscuit}, a dataset focused on Korean culture. Our experiments on\nthis dataset reveal that open-source models lag behind proprietary ones in\nunderstanding Korean culture, highlighting key areas for improvement. We also\npresent a series of further analyses, including human evaluation, augmenting\nVLMs with external knowledge, and the evaluation beyond multiple-choice QA. Our\ndataset is available at https://huggingface.co/datasets/ddehun/k-viscuit.", "AI": {"tldr": "The paper presents a semi-automated framework for creating benchmarks to test vision-language models (VLMs) on culturally relevant questions, exemplified through a dataset focused on Korean culture called K-Viscuit.", "motivation": "There is a need for culturally inclusive VLMs, but existing methods rely heavily on human annotators, which is labor-intensive and limits diversity in questions.", "method": "The proposed framework combines VLM question generation based on guidelines and annotated examples, followed by verification from native speakers.", "result": "Experiments revealed that open-source models do not perform as well as proprietary ones in understanding Korean culture, providing insights for future improvements.", "conclusion": "The framework and the K-Viscuit dataset allow for a better assessment of VLMs in culturally relevant contexts.", "key_contributions": ["Development of a semi-automated framework for cultural VLM benchmarks", "Creation of the K-Viscuit dataset focused on Korean culture", "Insights into the performance gap between open-source and proprietary VLMs in cultural understanding."], "limitations": "The approach may still rely on some human input, and its effectiveness in other cultures or languages is not yet tested.", "keywords": ["Vision-language models", "Cultural benchmarking", "Korean culture", "Semi-automated framework", "QA evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.09823", "pdf": "https://arxiv.org/pdf/2407.09823.pdf", "abs": "https://arxiv.org/abs/2407.09823", "title": "NativQA: Multilingual Culturally-Aligned Natural Query for LLMs", "authors": ["Md. Arid Hasan", "Maram Hasanain", "Fatema Ahmad", "Sahinur Rahman Laskar", "Sunaya Upadhyay", "Vrunda N Sukhadia", "Mucahid Kutlu", "Shammur Absar Chowdhury", "Firoj Alam"], "categories": ["cs.CL", "cs.AI", "68T50", "F.2.2; I.2.7"], "comment": "LLMs, Native, Multilingual, Language Diversity, Contextual\n  Understanding, Minority Languages, Culturally Informed, Foundation Models,\n  Large Language Models", "summary": "Natural Question Answering (QA) datasets play a crucial role in evaluating\nthe capabilities of large language models (LLMs), ensuring their effectiveness\nin real-world applications. Despite the numerous QA datasets that have been\ndeveloped and some work has been done in parallel, there is a notable lack of a\nframework and large scale region-specific datasets queried by native users in\ntheir own languages. This gap hinders the effective benchmarking and the\ndevelopment of fine-tuned models for regional and cultural specificities. In\nthis study, we propose a scalable, language-independent framework, NativQA, to\nseamlessly construct culturally and regionally aligned QA datasets in native\nlanguages, for LLM evaluation and tuning. We demonstrate the efficacy of the\nproposed framework by designing a multilingual natural QA dataset,\nMultiNativQA, consisting of ~64k manually annotated QA pairs in seven\nlanguages, ranging from high to extremely low resource, based on queries from\nnative speakers from 9 regions covering 18 topics. We benchmark open- and\nclosed-source LLMs with the MultiNativQA dataset. We made the MultiNativQA\ndataset(https://huggingface.co/datasets/QCRI/MultiNativQA), and other\nexperimental scripts(https://gitlab.com/nativqa/multinativqa) publicly\navailable for the community.", "AI": {"tldr": "The paper introduces NativQA, a framework for creating culturally and regionally aligned QA datasets in native languages for the evaluation of large language models (LLMs).", "motivation": "There is a lack of frameworks for large-scale region-specific datasets based on queries from native users in their own languages, which hampers effective benchmarking and model development.", "method": "We propose NativQA, a language-independent framework for constructing culturally and regionally aligned QA datasets, exemplified by the MultiNativQA dataset consisting of 64k QA pairs in seven languages.", "result": "The MultiNativQA dataset has been created with contributions from native speakers across 9 regions and demonstrates effective benchmarking of various LLMs.", "conclusion": "Making the MultiNativQA dataset and related scripts publicly available enhances accessibility for future research and model development.", "key_contributions": ["Development of NativQA framework for QA dataset construction", "Creation of multilingual dataset MultiNativQA with 64k QA pairs", "Benchmarking framework for evaluating LLMs in culturally aligned contexts."], "limitations": "", "keywords": ["LLMs", "Native", "Multilingual", "Language Diversity", "Culturally Informed"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2407.14878", "pdf": "https://arxiv.org/pdf/2407.14878.pdf", "abs": "https://arxiv.org/abs/2407.14878", "title": "Modular Sentence Encoders: Separating Language Specialization from Cross-Lingual Alignment", "authors": ["Yongxin Huang", "Kexin Wang", "Goran Glavaš", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 main conference", "summary": "Multilingual sentence encoders (MSEs) are commonly obtained by training\nmultilingual language models to map sentences from different languages into a\nshared semantic space. As such, they are subject to curse of multilinguality, a\nloss of monolingual representational accuracy due to parameter sharing. Another\nlimitation of MSEs is the trade-off between different task performance:\ncross-lingual alignment training distorts the optimal monolingual structure of\nsemantic spaces of individual languages, harming the utility of sentence\nembeddings in monolingual tasks; cross-lingual tasks, such as cross-lingual\nsemantic similarity and zero-shot transfer for sentence classification, may\nalso require conflicting cross-lingual alignment strategies. In this work, we\naddress both issues by means of modular training of sentence encoders. We first\ntrain language-specific monolingual modules to mitigate negative interference\nbetween languages (i.e., the curse). We then align all non-English sentence\nembeddings to the English by training cross-lingual alignment adapters,\npreventing interference with monolingual specialization from the first step. We\ntrain the cross-lingual adapters with two different types of data to resolve\nthe conflicting requirements of different cross-lingual tasks. Monolingual and\ncross-lingual results on semantic text similarity and relatedness, bitext\nmining and sentence classification show that our modular solution achieves\nbetter and more balanced performance across all the tasks compared to\nfull-parameter training of monolithic multilingual sentence encoders,\nespecially benefiting low-resource languages.", "AI": {"tldr": "This paper proposes a modular training approach for multilingual sentence encoders to improve accuracy and task performance by training language-specific monolingual modules and aligning them with cross-lingual adapters.", "motivation": "To address the curse of multilinguality and the trade-off between different task performances in multilingual sentence encoders (MSEs).", "method": "The authors train language-specific monolingual modules to reduce negative interference and then align non-English embeddings to English using cross-lingual alignment adapters based on two types of data. This modular approach is contrasted with traditional full-parameter training of monolithic MSEs.", "result": "The proposed modular solution demonstrates superior and more balanced performance across monolingual and cross-lingual tasks, particularly in semantic text similarity, bitext mining, and sentence classification, especially for low-resource languages.", "conclusion": "The modular training approach enhances the utility of sentence embeddings in both monolingual and cross-lingual tasks by minimizing interference and addressing the conflicting requirements of different tasks.", "key_contributions": ["Introduction of modular training for multilingual sentence encoders", "Development of language-specific monolingual modules", "Design of cross-lingual alignment adapters to improve task performance"], "limitations": "", "keywords": ["multilingual sentence encoders", "modular training", "cross-lingual alignment", "semantic similarity", "low-resource languages"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2408.08144", "pdf": "https://arxiv.org/pdf/2408.08144.pdf", "abs": "https://arxiv.org/abs/2408.08144", "title": "MIDAS: Multi-level Intent, Domain, And Slot Knowledge Distillation for Multi-turn NLU", "authors": ["Yan Li", "So-Eon Kim", "Seong-Bae Park", "Soyeon Caren Han"], "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025", "summary": "Although Large Language Models (LLMs) can generate coherent text, they often\nstruggle to recognise user intent behind queries. In contrast, Natural Language\nUnderstanding (NLU) models interpret the purpose and key information of user\ninput for responsive interactions. Existing NLU models typically map utterances\nto a dual-level semantic frame, involving sentence-level intent (SI) and\nword-level slot (WS) labels. However, real-life conversations primarily consist\nof multi-turn dialogues, requiring the interpretation of complex and extended\nexchanges. Researchers encounter challenges in addressing all facets of\nmulti-turn dialogue using a unified NLU model. This paper introduces MIDAS, a\nnovel approach leveraging multi-level intent, domain, and slot knowledge\ndistillation for multi-turn NLU. We construct distinct teachers for SI\ndetection, WS filling, and conversation-level domain (CD) classification, each\nfine-tuned for specific knowledge. A multi-teacher loss is proposed to\nfacilitate the integration of these teachers, guiding a student model in\nmulti-turn dialogue tasks. Results demonstrate the efficacy of our model in\nimproving multi-turn conversation understanding, showcasing the potential for\nadvancements in NLU through multi-level dialogue knowledge distillation. Our\nimplementation is open-sourced on https://github.com/adlnlp/Midas.", "AI": {"tldr": "This paper introduces MIDAS, a multi-level knowledge distillation approach for Natural Language Understanding (NLU) in multi-turn dialogues, addressing the limitations of existing models.", "motivation": "Existing NLU models struggle with understanding user intent in multi-turn dialogues, necessitating advancements in the interpretation of complex conversations.", "method": "MIDAS utilizes a multi-teacher framework that includes separate models for sentence-level intent detection, word-level slot filling, and conversation-level domain classification, integrated through a multi-teacher loss.", "result": "The model shows significant improvements in multi-turn conversation understanding, effectively demonstrating the value of multi-level dialogue knowledge distillation.", "conclusion": "MIDAS enhances NLU capabilities for multi-turn dialogues, offering a promising avenue for future research in this area.", "key_contributions": ["Introduction of a multi-teacher framework for NLU", "Novel multi-level intent and domain classification", "Open-source implementation for broader accessibility"], "limitations": "", "keywords": ["Natural Language Understanding", "Multi-turn Dialogue", "Knowledge Distillation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2408.12226", "pdf": "https://arxiv.org/pdf/2408.12226.pdf", "abs": "https://arxiv.org/abs/2408.12226", "title": "EvalYaks: Instruction Tuning Datasets and LoRA Fine-tuned Models for Automated Scoring of CEFR B2 Speaking Assessment Transcripts", "authors": ["Nicy Scaria", "Silvester John Joseph Kennedy", "Thomas Latinovich", "Deepak Subramani"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Relying on human experts to evaluate CEFR speaking assessments in an\ne-learning environment creates scalability challenges, as it limits how quickly\nand widely assessments can be conducted. We aim to automate the evaluation of\nCEFR B2 English speaking assessments in e-learning environments from\nconversation transcripts. First, we evaluate the capability of leading open\nsource and commercial Large Language Models (LLMs) to score a candidate's\nperformance across various criteria in the CEFR B2 speaking exam in both global\nand India-specific contexts. Next, we create a new expert-validated,\nCEFR-aligned synthetic conversational dataset with transcripts that are rated\nat different assessment scores. In addition, new instruction-tuned datasets are\ndeveloped from the English Vocabulary Profile (up to CEFR B2 level) and the\nCEFR-SP WikiAuto datasets. Finally, using these new datasets, we perform\nparameter efficient instruction tuning of Mistral Instruct 7B v0.2 to develop a\nfamily of models called EvalYaks. Four models in this family are for assessing\nthe four sections of the CEFR B2 speaking exam, one for identifying the CEFR\nlevel of vocabulary and generating level-specific vocabulary, and another for\ndetecting the CEFR level of text and generating level-specific text. EvalYaks\nachieved an average acceptable accuracy of 96%, a degree of variation of 0.35\nlevels, and performed 3 times better than the next best model. This\ndemonstrates that a 7B parameter LLM instruction tuned with high-quality\nCEFR-aligned assessment data can effectively evaluate and score CEFR B2 English\nspeaking assessments, offering a promising solution for scalable, automated\nlanguage proficiency evaluation.", "AI": {"tldr": "The paper presents EvalYaks, a family of models designed to automate the evaluation of CEFR B2 English speaking assessments using LLMs, achieving high accuracy and scalability in e-learning environments.", "motivation": "The need for scalability in evaluating CEFR speaking assessments in e-learning environments, which is hindered by reliance on human experts.", "method": "Evaluation of leading LLMs on CEFR B2 speaking assessment criteria, creation of a synthetic conversational dataset, and instruction tuning of Mistral Instruct 7B.", "result": "EvalYaks achieved an average accuracy of 96% and outperformed the next best model by 3 times.", "conclusion": "Implementing LLMs for CEFR B2 speaking assessments provides a viable solution for automated language proficiency evaluation.", "key_contributions": ["Development of EvalYaks, a family of specialized models for CEFR assessments.", "Creation of expert-validated, CEFR-aligned synthetic datasets.", "High performance measured in terms of accuracy and scalability for language evaluation."], "limitations": "", "keywords": ["CEFR", "LLMs", "language assessment", "e-learning", "automated evaluation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2409.11638", "pdf": "https://arxiv.org/pdf/2409.11638.pdf", "abs": "https://arxiv.org/abs/2409.11638", "title": "BanStereoSet: A Dataset to Measure Stereotypical Social Biases in LLMs for Bangla", "authors": ["Mahammed Kamruzzaman", "Abdullah Al Monsur", "Shrabon Das", "Enamul Hassan", "Gene Louis Kim"], "categories": ["cs.CL"], "comment": "Accepted at ACL-2025", "summary": "This study presents BanStereoSet, a dataset designed to evaluate\nstereotypical social biases in multilingual LLMs for the Bangla language. In an\neffort to extend the focus of bias research beyond English-centric datasets, we\nhave localized the content from the StereoSet, IndiBias, and Kamruzzaman et.\nal.'s datasets, producing a resource tailored to capture biases prevalent\nwithin the Bangla-speaking community. Our BanStereoSet dataset consists of\n1,194 sentences spanning 9 categories of bias: race, profession, gender,\nageism, beauty, beauty in profession, region, caste, and religion. This dataset\nnot only serves as a crucial tool for measuring bias in multilingual LLMs but\nalso facilitates the exploration of stereotypical bias across different social\ncategories, potentially guiding the development of more equitable language\ntechnologies in Bangladeshi contexts. Our analysis of several language models\nusing this dataset indicates significant biases, reinforcing the necessity for\nculturally and linguistically adapted datasets to develop more equitable\nlanguage technologies.", "AI": {"tldr": "The study introduces BanStereoSet, a dataset aimed at evaluating social biases in multilingual LLMs for the Bangla language, highlighting significant biases in several language models.", "motivation": "To extend bias research beyond English-centric datasets and address stereotypical social biases within the Bangla-speaking community.", "method": "The research involved localizing content from several existing bias datasets to create BanStereoSet, which includes 1,194 sentences across 9 bias categories.", "result": "Analysis of language models using BanStereoSet revealed significant biases, indicating the need for culturally appropriate datasets.", "conclusion": "The study emphasizes that developing more equitable language technologies requires linguistically and culturally adapted datasets.", "key_contributions": ["Introduction of BanStereoSet for evaluating biases in Bangla language LLMs.", "Coverage of 9 bias categories relevant to Bangla-speaking communities.", "Reinforcement of the need for localized datasets in AI research."], "limitations": "", "keywords": ["bias evaluation", "multilingual LLMs", "Bangla language", "stereotypical bias", "language technology"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.07523", "pdf": "https://arxiv.org/pdf/2410.07523.pdf", "abs": "https://arxiv.org/abs/2410.07523", "title": "DemoShapley: Valuation of Demonstrations for In-Context Learning", "authors": ["Shan Xie", "Man Luo", "Chadly Daniel Stern", "Mengnan Du", "Lu Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) using in-context learning (ICL) excel in many\ntasks without task-specific fine-tuning. However, demonstration selection and\nordering greatly impact ICL effectiveness. To address this, we propose\nDemoShapley and Beta-DemoShapley, inspired by Data Shapley and Beta Shapley, to\nassess the influence of individual demonstrations. DemoShapley captures how\neach example influences performance in different contexts, unlike other\ninfluence-based methods that rely on a fixed number of demonstrations.\nBeta-DemoShapley further enhances this framework by incorporating the Beta\ndistribution, allowing users to assign higher weights to smaller cardinalities,\nwhich aligns with ICL's prompt length and computational constraints. Our\nfindings show that the proposed algorithms improve model performance by\nselecting quality demonstrations, and enhancing generalization to\nout-of-distribution tasks. It also identifies noise-compromised data and\npromotes fairness in LLMs, protecting model performance and ensuring robustness\nacross various scenarios.", "AI": {"tldr": "Proposes DemoShapley and Beta-DemoShapley to improve demonstration selection in in-context learning for LLMs, enhancing performance and robustness.", "motivation": "Demonstration selection and ordering in in-context learning significantly influence model effectiveness.", "method": "Introduces DemoShapley and Beta-DemoShapley to assess the influence of individual demonstrations, with improvements over fixed demonstration methods.", "result": "Algorithms enhance model performance by improving selection of quality demonstrations and generalization to out-of-distribution tasks, while identifying noise in data.", "conclusion": "The proposed methods promote fairness and robustness in LLMs across various scenarios.", "key_contributions": ["Develops DemoShapley for dynamic demonstration selection", "Introduces Beta-DemoShapley for better weighting of demonstrations", "Improves LLM performance and fairness through effective demo selection"], "limitations": "", "keywords": ["large language models", "in-context learning", "demonstration selection", "Beta distribution", "fairness"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.11119", "pdf": "https://arxiv.org/pdf/2410.11119.pdf", "abs": "https://arxiv.org/abs/2410.11119", "title": "ChuLo: Chunk-Level Key Information Representation for Long Document Processing", "authors": ["Yan Li", "Soyeon Caren Han", "Yue Dai", "Feiqi Cao"], "categories": ["cs.CL"], "comment": "The paper has been accepted to ACL 2025", "summary": "Transformer-based models have achieved remarkable success in various Natural\nLanguage Processing (NLP) tasks, yet their ability to handle long documents is\nconstrained by computational limitations. Traditional approaches, such as\ntruncating inputs, sparse self-attention, and chunking, attempt to mitigate\nthese issues, but they often lead to information loss and hinder the model's\nability to capture long-range dependencies. In this paper, we introduce ChuLo,\na novel chunk representation method for long document understanding that\naddresses these limitations. Our ChuLo groups input tokens using unsupervised\nkeyphrase extraction, emphasizing semantically important keyphrase based chunks\nto retain core document content while reducing input length. This approach\nminimizes information loss and improves the efficiency of Transformer-based\nmodels. Preserving all tokens in long document understanding, especially token\nclassification tasks, is important to ensure that fine-grained annotations,\nwhich depend on the entire sequence context, are not lost. We evaluate our\nmethod on multiple long document classification tasks and long document token\nclassification tasks, demonstrating its effectiveness through comprehensive\nqualitative and quantitative analysis. Our implementation is open-sourced on\nhttps://github.com/adlnlp/Chulo.", "AI": {"tldr": "ChuLo is a novel chunk representation method that enhances Transformer-based models' ability to handle long documents by using unsupervised keyphrase extraction to minimize information loss.", "motivation": "To address the limitations of existing methods for handling long documents in Transformer-based models that lead to information loss and hinder long-range dependencies.", "method": "ChuLo groups input tokens using unsupervised keyphrase extraction, creating semantically important keyphrase-based chunks while reducing input length to enhance efficiency in long document understanding.", "result": "The evaluation shows that ChuLo significantly improves long document classification and token classification tasks, preserving fine-grained annotations crucial for effective context understanding.", "conclusion": "ChuLo effectively mitigates information loss in long document understanding tasks, thereby improving the overall performance of Transformer-based models.", "key_contributions": ["Introduction of the ChuLo method for long document understanding", "Use of unsupervised keyphrase extraction to minimize input length while preserving essential content", "Demonstrated effectiveness across multiple classification tasks"], "limitations": "", "keywords": ["Transformer models", "long document understanding", "keyphrase extraction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.13460", "pdf": "https://arxiv.org/pdf/2410.13460.pdf", "abs": "https://arxiv.org/abs/2410.13460", "title": "From Citations to Criticality: Predicting Legal Decision Influence in the Multilingual Swiss Jurisprudence", "authors": ["Ronja Stern", "Ken Kawamura", "Matthias Stürmer", "Ilias Chalkidis", "Joel Niklaus"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2; I.7"], "comment": "Accepted to ACL main 2025", "summary": "Many court systems are overwhelmed all over the world, leading to huge\nbacklogs of pending cases. Effective triage systems, like those in emergency\nrooms, could ensure proper prioritization of open cases, optimizing time and\nresource allocation in the court system. In this work, we introduce the\nCriticality Prediction dataset, a novel resource for evaluating case\nprioritization. Our dataset features a two-tier labeling system: (1) the binary\nLD-Label, identifying cases published as Leading Decisions (LD), and (2) the\nmore granular Citation-Label, ranking cases by their citation frequency and\nrecency, allowing for a more nuanced evaluation. Unlike existing approaches\nthat rely on resource-intensive manual annotations, we algorithmically derive\nlabels leading to a much larger dataset than otherwise possible. We evaluate\nseveral multilingual models, including both smaller fine-tuned models and large\nlanguage models in a zero-shot setting. Our results show that the fine-tuned\nmodels consistently outperform their larger counterparts, thanks to our large\ntraining set. Our results highlight that for highly domain-specific tasks like\nours, large training sets are still valuable.", "AI": {"tldr": "The paper introduces a novel dataset for evaluating case prioritization in court systems, demonstrating the efficacy of smaller fine-tuned models over larger models in domain-specific tasks.", "motivation": "To address the overwhelming backlogs in court systems worldwide by creating an effective case prioritization system akin to triage in emergency rooms.", "method": "Development of the Criticality Prediction dataset with a two-tier labeling system and evaluation of various multilingual models in prioritizing legal cases.", "result": "Fine-tuned models consistently outperform larger models due to the larger training dataset, emphasizing the importance of extensive labeled data in specialized domains.", "conclusion": "Large training sets are crucial for domain-specific tasks, despite common perceptions favoring larger models.", "key_contributions": ["Introduction of the Criticality Prediction dataset for legal case prioritization", "Development of a two-tier labeling system for case evaluation", "Demonstration that smaller fine-tuned models can outperform larger models in this specific domain"], "limitations": "", "keywords": ["court systems", "case prioritization", "multilingual models"], "importance_score": 3, "read_time_minutes": 15}}
{"id": "2410.14248", "pdf": "https://arxiv.org/pdf/2410.14248.pdf", "abs": "https://arxiv.org/abs/2410.14248", "title": "Addressing Blind Guessing: Calibration of Selection Bias in Multiple-Choice Question Answering by Video Language Models", "authors": ["Olga Loginova", "Oleksandr Bezrukov", "Ravi Shekhar", "Alexey Kravets"], "categories": ["cs.CL"], "comment": null, "summary": "Evaluating Video Language Models (VLMs) is a challenging task. Due to its\ntransparency, Multiple-Choice Question Answering (MCQA) is widely used to\nmeasure the performance of these models through accuracy. However, existing\nMCQA benchmarks fail to capture the full reasoning capabilities of VLMs due to\nselection bias, when models disproportionately favor certain answer options\nbased on positional patterns observed during training. In this work, we conduct\na comprehensive empirical analysis of several VLM architectures across major\ndatasets designed to assess complex video-focused reasoning. We identify where\nthe bias is most pronounced and demonstrate to what extent model responses\nreflect genuine understanding of video content and related questions, as\nopposed to reliance on arbitrary patterns or superficial cues, such as answer\nposition. By decomposing the MCQA task and adapting fairness bias metrics to\nVLMs, we introduce a post-processing calibration technique BOLD to balance this\nbias. Our results show that reducing selection bias improves not only debiasing\nmetrics but also overall model performance, including Accuracy and F1 Mean\nscore. Our method, by suppressing \"blind guessing\", offers a more cost- and\ntime-effective approach to mitigating selection bias compared to existing\ntechniques. This study represents the first focused investigation of selection\nbias in video-to-text LLM-powered models.", "AI": {"tldr": "This paper investigates selection bias in Video Language Models (VLMs) through Multiple-Choice Question Answering (MCQA) benchmarks and introduces a calibration technique to improve model performance and reduce bias.", "motivation": "The aim is to address the limitations of existing MCQA benchmarks that do not fully capture the reasoning capabilities of VLMs due to selection bias.", "method": "Empirical analysis of various VLM architectures using major benchmarks focused on video reasoning, introducing the BOLD post-processing calibration technique to balance selection bias.", "result": "The research demonstrates that reducing selection bias leads to improved performance metrics such as Accuracy and F1 Mean score, and the BOLD technique is more cost-effective in mitigating bias than previous approaches.", "conclusion": "Overall, this study highlights the need for careful evaluation of VLMs regarding selection bias and presents a novel method to enhance their reasoning abilities.", "key_contributions": ["Introduces a novel post-processing calibration technique (BOLD) to reduce selection bias in VLMs", "Identifies critical areas of selection bias in MCQA benchmarks for VLMs", "Demonstrates improved model performance through bias reduction methods"], "limitations": "The study focuses on specific types of VLM architectures and datasets, which may limit the generalizability of the findings.", "keywords": ["Video Language Models", "Multiple-Choice Question Answering", "Selection Bias", "Bias Mitigation", "Model Performance"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2411.05872", "pdf": "https://arxiv.org/pdf/2411.05872.pdf", "abs": "https://arxiv.org/abs/2411.05872", "title": "Dialectal Coverage And Generalization in Arabic Speech Recognition", "authors": ["Amirbek Djanibekov", "Hawau Olamide Toyin", "Raghad Alshalan", "Abdullah Alitr", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Developing robust automatic speech recognition (ASR) systems for Arabic\nrequires effective strategies to manage its diversity. Existing ASR systems\nmainly cover the modern standard Arabic (MSA) variety and few high-resource\ndialects, but fall short in coverage and generalization across the multitude of\nspoken variants. Code-switching with English and French is also common in\ndifferent regions of the Arab world, which challenges the performance of\nmonolingual Arabic models. In this work, we introduce a suite of ASR models\noptimized to effectively recognize multiple variants of spoken Arabic,\nincluding MSA, various dialects, and code-switching. We provide open-source\npre-trained models that cover data from 17 Arabic-speaking countries, and\nfine-tuned MSA and dialectal ASR models that include at least 11 variants, as\nwell as multi-lingual ASR models covering embedded languages in code-switched\nutterances. We evaluate ASR performance across these spoken varieties and\ndemonstrate both coverage and performance gains compared to prior models.", "AI": {"tldr": "This paper presents a suite of Arabic ASR models designed to manage the linguistic diversity of spoken Arabic, including various dialects and code-switching.", "motivation": "Existing ASR systems primarily focus on Modern Standard Arabic and some dialects, lacking comprehensive coverage of the diverse spoken variants and facing challenges due to code-switching.", "method": "The authors developed and tested multiple ASR models optimized for recognizing MSA, various Arabic dialects, and code-switched speech. They provided open-source pre-trained models based on data from 17 Arab countries and conducted evaluations to assess performance.", "result": "The models show improved coverage and performance across different spoken Arabic varieties compared to existing systems.", "conclusion": "The study successfully introduces robust ASR systems that enhance the recognition capabilities for multiple Arabic linguistic variants and code-switching contexts.", "key_contributions": ["Introduction of ASR models for diverse Arabic dialects and code-switching", "Open-source pre-trained models from a broad dataset", "Demonstration of performance improvements over prior ASR models"], "limitations": "", "keywords": ["Automatic Speech Recognition", "Arabic Dialects", "Code-Switching", "Machine Learning"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2411.06160", "pdf": "https://arxiv.org/pdf/2411.06160.pdf", "abs": "https://arxiv.org/abs/2411.06160", "title": "Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework", "authors": ["Jingyi Zhou", "Senlin Luo", "Haofan Chen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "3.1 There is a misstatement in the EQN Framework section", "summary": "Text emotion detection constitutes a crucial foundation for advancing\nartificial intelligence from basic comprehension to the exploration of\nemotional reasoning. Most existing emotion detection datasets rely on manual\nannotations, which are associated with high costs, substantial subjectivity,\nand severe label imbalances. This is particularly evident in the inadequate\nannotation of micro-emotions and the absence of emotional intensity\nrepresentation, which fail to capture the rich emotions embedded in sentences\nand adversely affect the quality of downstream task completion. By proposing an\nall-labels and training-set label regression method, we map label values to\nenergy intensity levels, thereby fully leveraging the learning capabilities of\nmachine models and the interdependencies among labels to uncover multiple\nemotions within samples. This led to the establishment of the Emotion\nQuantization Network (EQN) framework for micro-emotion detection and\nannotation. Using five commonly employed sentiment datasets, we conducted\ncomparative experiments with various models, validating the broad applicability\nof our framework within NLP machine learning models. Based on the EQN\nframework, emotion detection and annotation are conducted on the GoEmotions\ndataset. A comprehensive comparison with the results from Google literature\ndemonstrates that the EQN framework possesses a high capability for automatic\ndetection and annotation of micro-emotions. The EQN framework is the first to\nachieve automatic micro-emotion annotation with energy-level scores, providing\nstrong support for further emotion detection analysis and the quantitative\nresearch of emotion computing.", "AI": {"tldr": "The paper introduces the Emotion Quantization Network (EQN) framework for detecting and annotating micro-emotions in text by mapping label values to energy intensity levels.", "motivation": "Existing emotion detection datasets face issues with manual annotations, label imbalances, and inadequate representation of micro-emotions, necessitating more effective methods for emotion detection.", "method": "The authors propose an all-labels and training-set label regression method that allows for mapping label values to energy intensity levels which enhances the detection of multiple emotions in text samples.", "result": "The EQN framework was validated through experiments on five sentiment datasets, showing high capability for automatic micro-emotion detection and annotation, outperforming previous models.", "conclusion": "The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing a strong basis for advanced emotion detection analysis.", "key_contributions": ["EQN framework for micro-emotion detection", "Energy-level score annotation methodology", "Broad applicability in NLP machine learning models"], "limitations": "There is a misstatement in the EQN Framework section that needs clarification.", "keywords": ["emotion detection", "micro-emotions", "energy intensity", "emotion quantization", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2411.17116", "pdf": "https://arxiv.org/pdf/2411.17116.pdf", "abs": "https://arxiv.org/abs/2411.17116", "title": "Star Attention: Efficient LLM Inference over Long Sequences", "authors": ["Shantanu Acharya", "Fei Jia", "Boris Ginsburg"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n97-100% of accuracy.", "AI": {"tldr": "Star Attention is a two-phase block-sparse approximation that enhances computational efficiency for LLMs by reducing memory and inference time while maintaining accuracy.", "motivation": "To improve the computational efficiency of Transformer-based LLMs on long sequences due to the limitations of the self-attention mechanism.", "method": "Star Attention uses a two-phase approach: first, it applies blockwise-local attention across multiple hosts, and then it employs sequence-global attention for cached tokens.", "result": "The proposed method reduces memory requirements and inference time by up to 11x, while achieving 97-100% accuracy retention.", "conclusion": "Star Attention effectively integrates with existing Transformer-based LLMs, offering a significant optimization in terms of resource utilization.", "key_contributions": ["Introduced a two-phase block-sparse approximation for attention in LLMs.", "Achieved up to 11x reduction in inference time and memory usage.", "Maintained high accuracy (97-100%) across various tasks."], "limitations": "", "keywords": ["Star Attention", "Large Language Models", "Computational Efficiency", "Transformer", "Machine Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2412.04905", "pdf": "https://arxiv.org/pdf/2412.04905.pdf", "abs": "https://arxiv.org/abs/2412.04905", "title": "DEMO: Reframing Dialogue Interaction with Fine-grained Element Modeling", "authors": ["Minzheng Wang", "Xinghua Zhang", "Kun Chen", "Nan Xu", "Haiyang Yu", "Fei Huang", "Wenji Mao", "Yongbin Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings. We release the code and data at\n  https://github.com/MozerWang/DEMO", "summary": "Large language models (LLMs) enabled dialogue systems have become one of the\ncentral modes in human-machine interaction, which bring about vast amounts of\nconversation logs and increasing demand for dialogue generation. The dialogue's\nlife-cycle spans from $\\textit{Prelude}$ through $\\textit{Interlocution}$ to\n$\\textit{Epilogue}$, encompassing rich dialogue elements. Despite large volumes\nof dialogue-related studies, there is a lack of systematic investigation into\nthe dialogue stages to frame benchmark construction that covers comprehensive\ndialogue elements. This hinders the precise modeling, generation and assessment\nof LLMs-based dialogue systems. To bridge this gap, in this paper, we introduce\na new research task--$\\textbf{D}$ialogue $\\textbf{E}$lement\n$\\textbf{MO}$deling, including $\\textit{Element Awareness}$ and\n$\\textit{Dialogue Agent Interaction}$, and propose a novel benchmark,\n$\\textbf{DEMO}$, designed for a comprehensive dialogue modeling and assessment.\nOn this basis, we further build the DEMO agent with the adept ability to model\ndialogue elements via imitation learning. Extensive experiments on DEMO\nindicate that current representative LLMs still have considerable potential for\nenhancement, and our DEMO agent performs well in both dialogue element modeling\nand out-of-domain tasks.", "AI": {"tldr": "This paper introduces a new research task, Dialogue Element Modeling, aimed at improving LLM-based dialogue systems through a novel benchmark called DEMO, focusing on comprehensive dialogue element modeling and assessment.", "motivation": "There is a lack of systematic investigation into dialogue stages affecting the modeling and assessment of LLMs-based dialogue systems, which hinders their performance.", "method": "The paper proposes a research task called Dialogue Element Modeling, introducing two key components: Element Awareness and Dialogue Agent Interaction. A benchmark named DEMO is developed for this purpose, and a DEMO agent is built using imitation learning to enhance dialogue element modeling.", "result": "Extensive experiments show that while current LLMs demonstrate potential, the DEMO agent performs well in modeling dialogue elements and is effective in out-of-domain tasks.", "conclusion": "The introduction of the DEMO benchmark and agent addresses existing gaps in dialogue system modeling, providing a structured approach to assessing LLM-based dialogues.", "key_contributions": ["Introduction of the Dialogue Element Modeling task", "Development of the DEMO benchmark for comprehensive dialogue assessment", "Creation of a DEMO agent leveraging imitation learning for dialogue modeling"], "limitations": "", "keywords": ["Dialogue Systems", "Large Language Models", "Benchmarking", "Imitation Learning", "Human-Machine Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.08473", "pdf": "https://arxiv.org/pdf/2412.08473.pdf", "abs": "https://arxiv.org/abs/2412.08473", "title": "Multi-perspective Alignment for Increasing Naturalness in Neural Machine Translation", "authors": ["Huiyuan Lai", "Esther Ploeger", "Rik van Noord", "Antonio Toral"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main; 9 pages", "summary": "Neural machine translation (NMT) systems amplify lexical biases present in\ntheir training data, leading to artificially impoverished language in output\ntranslations. These language-level characteristics render automatic\ntranslations different from text originally written in a language and human\ntranslations, which hinders their usefulness in for example creating evaluation\ndatasets. Attempts to increase naturalness in NMT can fall short in terms of\ncontent preservation, where increased lexical diversity comes at the cost of\ntranslation accuracy. Inspired by the reinforcement learning from human\nfeedback framework, we introduce a novel method that rewards both naturalness\nand content preservation. We experiment with multiple perspectives to produce\nmore natural translations, aiming at reducing machine and human translationese.\nWe evaluate our method on English-to-Dutch literary translation, and find that\nour best model produces translations that are lexically richer and exhibit more\nproperties of human-written language, without loss in translation accuracy.", "AI": {"tldr": "This paper introduces a novel reinforcement learning method to improve neural machine translation (NMT) by balancing naturalness and content preservation, demonstrating its effectiveness on English-to-Dutch literary translations.", "motivation": "NMT systems often reflect lexical biases in training data, which degrades the naturalness and accuracy of translations. The need for more natural output without sacrificing content accuracy drives the research.", "method": "The proposed method utilizes reinforcement learning from human feedback to reward translations that are both more natural and accurate, evaluated through various perspectives in the context of literary translation.", "result": "The best model generated translations that are lexically richer and closer to human-written text, while maintaining high accuracy compared to traditional methods.", "conclusion": "The method successfully enhances NMT output by reducing translationese and improving lexical diversity without compromising content fidelity.", "key_contributions": ["Introduces a novel reinforcement learning approach for NMT", "Balances translation naturalness with content preservation", "Demonstrates improvements in lexical richness and human-like properties in translations."], "limitations": "", "keywords": ["neural machine translation", "reinforcement learning", "natural language processing", "translation accuracy", "lexical diversity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.08972", "pdf": "https://arxiv.org/pdf/2412.08972.pdf", "abs": "https://arxiv.org/abs/2412.08972", "title": "RuleArena: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios", "authors": ["Ruiwen Zhou", "Wenyue Hua", "Liangming Pan", "Sitao Cheng", "Xiaobao Wu", "En Yu", "William Yang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "This paper introduces RuleArena, a novel and challenging benchmark designed\nto evaluate the ability of large language models (LLMs) to follow complex,\nreal-world rules in reasoning. Covering three practical domains -- airline\nbaggage fees, NBA transactions, and tax regulations -- RuleArena assesses LLMs'\nproficiency in handling intricate natural language instructions that demand\nlong-context understanding, logical reasoning, and accurate mathematical\ncomputation. Two key attributes distinguish RuleArena from traditional\nrule-based reasoning benchmarks: (1) it extends beyond standard first-order\nlogic representations, and (2) it is grounded in authentic, practical\nscenarios, providing insights into the suitability and reliability of LLMs for\nreal-world applications. Our findings reveal several notable limitations in\nLLMs: (1) they struggle to identify and apply the appropriate rules, frequently\nbecoming confused by similar but distinct regulations, (2) they cannot\nconsistently perform accurate mathematical computations, even when they\ncorrectly identify the relevant rules, and (3) in general, they perform poorly\nin the benchmark. We also observe a significant performance boost when LLMs are\nprovided with external tools for oracle math and logic operations. These\nresults highlight significant challenges and promising research directions in\nadvancing LLMs' rule-guided reasoning capabilities in real-life applications.\nOur codes and data are publicly available on\nhttps://github.com/skyriver-2000/RuleArena.", "AI": {"tldr": "RuleArena is a benchmark for evaluating LLMs on complex rule-following tasks in real-world scenarios.", "motivation": "To assess the capabilities of large language models in following intricate, real-world rules across several domains, highlighting their limitations and guiding future research.", "method": "RuleArena tests LLMs' proficiency in reasoning through long-context natural language instructions related to airline baggage fees, NBA transactions, and tax regulations.", "result": "LLMs struggled significantly, often failing to apply the right rules and perform accurate computations, but showed improvement with external tools for complex operations.", "conclusion": "The study reveals substantial challenges for LLMs in rule-guided reasoning and suggests areas for further research, particularly in enhancing their reliability for real-life applications.", "key_contributions": ["Introduction of RuleArena as a novel LLM evaluation benchmark.", "Identification of key limitations in LLMs regarding rule application and mathematical accuracy.", "Demonstration of performance improvement when LLMs use external tools."], "limitations": "LLMs often confused similar rules and performed poorly in computations without external assistance.", "keywords": ["large language models", "rule-based reasoning", "benchmarking", "natural language instructions", "real-world applications"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.12567", "pdf": "https://arxiv.org/pdf/2412.12567.pdf", "abs": "https://arxiv.org/abs/2412.12567", "title": "FCMR: Robust Evaluation of Financial Cross-Modal Multi-Hop Reasoning", "authors": ["Seunghee Kim", "Changhyeon Kim", "Taeuk Kim"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Real-world decision-making often requires integrating and reasoning over\ninformation from multiple modalities. While recent multimodal large language\nmodels (MLLMs) have shown promise in such tasks, their ability to perform\nmulti-hop reasoning across diverse sources remains insufficiently evaluated.\nExisting benchmarks, such as MMQA, face challenges due to (1) data\ncontamination and (2) a lack of complex queries that necessitate operations\nacross more than two modalities, hindering accurate performance assessment. To\naddress this, we present Financial Cross-Modal Multi-Hop Reasoning (FCMR), a\nbenchmark created to analyze the reasoning capabilities of MLLMs by urging them\nto combine information from textual reports, tables, and charts within the\nfinancial domain. FCMR is categorized into three difficulty levels-Easy,\nMedium, and Hard-facilitating a step-by-step evaluation. In particular,\nproblems at the Hard level require precise cross-modal three-hop reasoning and\nare designed to prevent the disregard of any modality. Experiments on this new\nbenchmark reveal that even state-of-the-art MLLMs struggle, with the\nbest-performing model (Claude 3.5 Sonnet) achieving only 30.4% accuracy on the\nmost challenging tier. We also conduct analysis to provide insights into the\ninner workings of the models, including the discovery of a critical bottleneck\nin the information retrieval phase.", "AI": {"tldr": "This paper introduces the Financial Cross-Modal Multi-Hop Reasoning (FCMR) benchmark to evaluate the reasoning capabilities of multimodal large language models in financial contexts, highlighting their current inadequacies through multi-hop reasoning tasks.", "motivation": "Real-world decision-making requires integrating and reasoning over information from multiple modalities, yet existing benchmarks for multimodal large language models are insufficient for evaluating complex multi-hop reasoning.", "method": "The authors present the Financial Cross-Modal Multi-Hop Reasoning (FCMR) benchmark, which includes tasks that require multi-modal information integration from texts, tables, and charts in the financial domain, categorized into three levels of difficulty.", "result": "Experiments show that state-of-the-art multimodal large language models perform poorly on this benchmark, with the best model only achieving 30.4% accuracy on the most challenging tasks, exposing the need for better reasoning capabilities.", "conclusion": "The FCMR benchmark is a crucial step for evaluating MLLMs' reasoning skills, uncovering a significant bottleneck in information retrieval within these models.", "key_contributions": ["Introduction of the FCMR benchmark for financial multi-hop reasoning", "Detailed analysis of model performance highlighting critical limitations", "Categorization of tasks into varying difficulty levels to facilitate assessment"], "limitations": "The benchmark focuses only on the financial domain and may not fully represent reasoning capabilities in other contexts.", "keywords": ["multimodal", "large language models", "multi-hop reasoning", "financial domain", "benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.13942", "pdf": "https://arxiv.org/pdf/2412.13942.pdf", "abs": "https://arxiv.org/abs/2412.13942", "title": "A Rose by Any Other Name: LLM-Generated Explanations Are Good Proxies for Human Explanations to Collect Label Distributions on NLI", "authors": ["Beiduo Chen", "Siyao Peng", "Anna Korhonen", "Barbara Plank"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings, 25 pages, 21 figures", "summary": "Disagreement in human labeling is ubiquitous, and can be captured in human\njudgment distributions (HJDs). Recent research has shown that explanations\nprovide valuable information for understanding human label variation (HLV) and\nlarge language models (LLMs) can approximate HJD from a few human-provided\nlabel-explanation pairs. However, collecting explanations for every label is\nstill time-consuming. This paper examines whether LLMs can be used to replace\nhumans in generating explanations for approximating HJD. Specifically, we use\nLLMs as annotators to generate model explanations for a few given human labels.\nWe test ways to obtain and combine these label-explanations with the goal to\napproximate human judgment distributions. We further compare the resulting\nhuman with model-generated explanations, and test automatic and human\nexplanation selection. Our experiments show that LLM explanations are promising\nfor NLI: to estimate HJDs, generated explanations yield comparable results to\nhuman's when provided with human labels. Importantly, our results generalize\nfrom datasets with human explanations to i) datasets where they are not\navailable and ii) challenging out-of-distribution test sets.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) to generate explanations for human labels to approximate human judgment distributions (HJDs), comparing these LLM-generated explanations to human-generated ones.", "motivation": "The motivation behind this research is to streamline the process of generating explanations for human labeling, which is currently time-consuming, by exploring whether LLMs can replace human effort in this aspect.", "method": "The study uses LLMs as annotators to generate explanations for a limited number of human labels and investigates methods to combine these label-explanations for approximating HJDs.", "result": "The experiments reveal that LLM-generated explanations can yield results for estimating HJDs that are comparable to those obtained from human explanations when provided with human labels. This finding holds true even for datasets without available human explanations and out-of-distribution test sets.", "conclusion": "The research suggests that LLMs can effectively generate explanations that help in approximating human judgment distributions, making the process more efficient and less reliant on human input.", "key_contributions": ["LLMs can approximate human judgment distributions using generated explanations.", "The approach generalizes to datasets without human explanations and to challenging out-of-distribution test cases.", "The study compares human and model-generated explanations, highlighting the effectiveness of LLMs."], "limitations": "The effectiveness of LLM-generated explanations may vary based on the complexity and context of the labels, and the need for validation across diverse datasets.", "keywords": ["Human Judgment Distributions", "Large Language Models", "Explanations", "Human Label Variation", "Natural Language Inference"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2412.15268", "pdf": "https://arxiv.org/pdf/2412.15268.pdf", "abs": "https://arxiv.org/abs/2412.15268", "title": "Enhancing LLM-based Hatred and Toxicity Detection with Meta-Toxic Knowledge Graph", "authors": ["Yibo Zhao", "Jiapeng Zhu", "Can Xu", "Yao Liu", "Xiang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages of content", "summary": "The rapid growth of social media platforms has raised significant concerns\nregarding online content toxicity. When Large Language Models (LLMs) are used\nfor toxicity detection, two key challenges emerge: 1) the absence of\ndomain-specific toxic knowledge leads to false negatives; 2) the excessive\nsensitivity of LLMs to toxic speech results in false positives, limiting\nfreedom of speech. To address these issues, we propose a novel method called\nMetaTox, leveraging graph search on a meta-toxic knowledge graph to enhance\nhatred and toxicity detection. First, we construct a comprehensive meta-toxic\nknowledge graph by utilizing LLMs to extract toxic information through a\nthree-step pipeline, with toxic benchmark datasets serving as corpora. Second,\nwe query the graph via retrieval and ranking processes to supplement accurate,\nrelevant toxic knowledge. Extensive experiments and in-depth case studies\nacross multiple datasets demonstrate that our MetaTox significantly decreases\nthe false positive rate while boosting overall toxicity detection performance.\nOur code is available at https://github.com/YiboZhao624/MetaTox.", "AI": {"tldr": "MetaTox is a novel method for improving online toxicity detection using a meta-toxic knowledge graph, addressing false positives and negatives associated with LLMs.", "motivation": "To tackle the challenges of false negatives and false positives in toxicity detection using Large Language Models in social media content.", "method": "Constructing a meta-toxic knowledge graph through LLMs and querying it to enhance toxic knowledge retrieval and ranking.", "result": "MetaTox significantly reduces false positive rates while improving the performance of toxicity detection across various datasets.", "conclusion": "The proposed method offers a more reliable approach to toxicity detection in online content, balancing sensitivity and specificity.", "key_contributions": ["Development of a meta-toxic knowledge graph", "Reduction of false positive rates in toxicity detection", "Improved overall performance in toxic speech identification"], "limitations": "", "keywords": ["toxicity detection", "large language models", "knowledge graph", "social media", "machine learning"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2412.15712", "pdf": "https://arxiv.org/pdf/2412.15712.pdf", "abs": "https://arxiv.org/abs/2412.15712", "title": "Contrastive Learning for Task-Independent SpeechLLM-Pretraining", "authors": ["Maike Züfle", "Jan Niehues"], "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) excel in natural language processing but\nadapting these LLMs to speech processing tasks efficiently is not\nstraightforward. Direct task-specific fine-tuning is limited by overfitting\nrisks, data requirements, and computational costs. To address these challenges,\nwe propose a scalable, two-stage training approach: (1) A task-independent\nspeech pretraining stage using contrastive learning to align text and speech\nrepresentations over all layers, followed by (2) a task-specific fine-tuning\nstage requiring minimal data. This approach outperforms traditional ASR\npretraining and enables the model to surpass models specialized on speech\ntranslation and question answering while being trained on only 10% of the\ntask-specific data.", "AI": {"tldr": "This paper presents a two-stage training approach for adapting LLMs to speech processing tasks, combining task-independent speech pretraining with minimal task-specific fine-tuning.", "motivation": "The adaptation of large language models to speech processing tasks faces challenges such as overfitting, data requirements, and high computational costs.", "method": "A scalable, two-stage training approach: (1) Task-independent speech pretraining using contrastive learning to align text and speech representations, (2) Task-specific fine-tuning that requires minimal data.", "result": "The proposed method outperforms traditional ASR pretraining and surpasses models specifically designed for speech translation and question answering, even with only 10% of the task-specific data used for training.", "conclusion": "The two-stage training approach is effective for improving LLM performance on speech tasks and is more efficient in terms of data usage and computational costs compared to traditional methods.", "key_contributions": ["A novel two-stage training process for speech processing tasks using LLMs.", "Use of contrastive learning for task-independent speech pretraining.", "Significantly reduced data requirements for task-specific fine-tuning."], "limitations": "", "keywords": ["large language models", "speech processing", "contrastive learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.03191", "pdf": "https://arxiv.org/pdf/2501.03191.pdf", "abs": "https://arxiv.org/abs/2501.03191", "title": "CLIX: Cross-Lingual Explanations of Idiomatic Expressions", "authors": ["Aaron Gluck", "Katharina von der Wense", "Maria Leonor Pacheco"], "categories": ["cs.CL"], "comment": "Accepted to Findings of ACL 2025", "summary": "Automated definition generation systems have been proposed to support\nvocabulary expansion for language learners. The main barrier to the success of\nthese systems is that learners often struggle to understand definitions due to\nthe presence of potentially unfamiliar words and grammar, particularly when\nnon-standard language is involved. To address these challenges, we propose\nCLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We\nexplore the capabilities of current NLP models for this task, and observe that\nwhile it remains challenging, large language models show promise. Finally, we\nperform a detailed error analysis to highlight the key challenges that need to\nbe addressed before we can reliably incorporate these systems into educational\ntools.", "AI": {"tldr": "The paper introduces CLIX, a task focused on generating cross-lingual explanations of idiomatic expressions to aid language learners.", "motivation": "To support vocabulary expansion for language learners struggling with definitions due to complex language and non-standard expressions.", "method": "The study investigates the capabilities of current NLP models for generating cross-lingual explanations of idiomatic expressions.", "result": "The findings indicate that while the task is challenging, large language models show potential for improvement in generating understandable definitions.", "conclusion": "Key challenges must be addressed before integrating these automated systems into educational tools for language learning.", "key_contributions": ["Introduction of the CLIX task for cross-lingual explanation generation", "Detailed error analysis highlighting challenges for NLP models", "Exploration of large language models' capabilities in educational contexts"], "limitations": "The necessity for further improvements in NLP models to handle non-standard language effectively is identified as a limitation.", "keywords": ["Cross-Lingual", "Idiomatic Expressions", "NLP Models", "Language Learning", "Automated Definition Generation"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2501.03884", "pdf": "https://arxiv.org/pdf/2501.03884.pdf", "abs": "https://arxiv.org/abs/2501.03884", "title": "AlphaPO: Reward Shape Matters for LLM Alignment", "authors": ["Aman Gupta", "Shao Tang", "Qingquan Song", "Sirou Zhu", "Jiwoo Hong", "Ankan Saha", "Viral Gupta", "Noah Lee", "Eunki Kim", "Siyu Zhu", "Parag Agrawal", "Natesh Pillai", "S. Sathiya Keerthi"], "categories": ["cs.CL"], "comment": "26 pages, 16 figures. Accepted to ICML 2025", "summary": "Reinforcement Learning with Human Feedback (RLHF) and its variants have made\nhuge strides toward the effective alignment of large language models (LLMs) to\nfollow instructions and reflect human values. More recently, Direct Alignment\nAlgorithms (DAAs) have emerged in which the reward modeling stage of RLHF is\nskipped by characterizing the reward directly as a function of the policy being\nlearned. Some popular examples of DAAs include Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO). These methods often suffer\nfrom likelihood displacement, a phenomenon by which the probabilities of\npreferred responses are often reduced undesirably. In this paper, we argue\nthat, for DAAs the reward (function) shape matters. We introduce\n\\textbf{AlphaPO}, a new DAA method that leverages an $\\alpha$-parameter to help\nchange the shape of the reward function beyond the standard log reward. AlphaPO\nhelps maintain fine-grained control over likelihood displacement and\nover-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO\nleads to about 7\\% to 10\\% relative improvement in alignment performance for\nthe instruct versions of Mistral-7B and Llama3-8B while achieving 15\\% to 50\\%\nrelative improvement over DPO on the same models. The analysis and results\npresented highlight the importance of the reward shape and how one can\nsystematically change it to affect training dynamics, as well as improve\nalignment performance.", "AI": {"tldr": "This paper introduces AlphaPO, a novel Direct Alignment Algorithm (DAA) for improving the alignment of large language models by altering the reward function shape, leading to significant enhancements in performance compared to existing methods.", "motivation": "The paper addresses limitations in current Direct Alignment Algorithms (DAAs) related to likelihood displacement and the importance of reward function shape in training dynamics.", "method": "The authors propose AlphaPO, which utilizes an $\beta$-parameter to adjust the reward function shape, enhancing control over likelihood displacement and over-optimization.", "result": "AlphaPO demonstrates a 7% to 10% relative improvement in alignment performance over SimPO and a 15% to 50% improvement over DPO for models like Mistral-7B and Llama3-8B.", "conclusion": "The findings underscore the critical role of reward function shape in enhancing alignment performance in reinforcement learning for language models.", "key_contributions": ["Introduction of AlphaPO as a new DAA.", "Demonstration of significant performance improvements in alignment tasks.", "Analysis of how reward shape influences training dynamics."], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Large Language Models", "Reward Function", "Direct Alignment Algorithms"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2501.13074", "pdf": "https://arxiv.org/pdf/2501.13074.pdf", "abs": "https://arxiv.org/abs/2501.13074", "title": "Autonomy-of-Experts Models", "authors": ["Ang Lv", "Ruobing Xie", "Yining Qian", "Songhao Wu", "Xingwu Sun", "Zhanhui Kang", "Di Wang", "Rui Yan"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ICML 2025", "summary": "Mixture-of-Experts (MoE) models mostly use a router to assign tokens to\nspecific expert modules, activating only partial parameters and often\noutperforming dense models. We argue that the separation between the router's\ndecision-making and the experts' execution is a critical yet overlooked issue,\nleading to suboptimal expert selection and ineffective learning. To address\nthis, we propose Autonomy-of-Experts (AoE), a novel MoE paradigm in which\nexperts autonomously select themselves to process inputs. AoE is based on the\ninsight that an expert is aware of its own capacity to effectively process a\ntoken, an awareness reflected in the scale of its internal activations. In AoE,\nrouters are removed; instead, experts pre-compute internal activations for\ninputs and are ranked based on their activation norms. Only the top-ranking\nexperts proceed with the forward pass, while the others abort. The overhead of\npre-computing activations is reduced through a low-rank weight factorization.\nThis self-evaluating-then-partner-comparing approach ensures improved expert\nselection and effective learning. We pre-train language models having 700M up\nto 4B parameters, demonstrating that AoE outperforms traditional MoE models\nwith comparable efficiency.", "AI": {"tldr": "Proposes Autonomy-of-Experts (AoE), a novel Mixture-of-Experts (MoE) paradigm that enhances expert selection by allowing experts to autonomously assess their capacity to process inputs.", "motivation": "The separation between the router's decision-making and the experts' execution in traditional MoE models leads to suboptimal expert selection and ineffective learning.", "method": "AoE eliminates routers, allowing experts to autonomously compute their internal activations for inputs and rank themselves based on these activations, selecting only the top performers to process inputs.", "result": "AoE demonstrates improved expert selection and effective learning compared to traditional MoE models, achieving similar efficiency with pre-trained language models.", "conclusion": "Removing routers and allowing experts to self-evaluate and select themselves leads to better outcomes in MoE frameworks.", "key_contributions": ["Introduced the Autonomy-of-Experts (AoE) model for MoE frameworks", "Demonstrated improved efficiency and performance over traditional MoE models", "Reduced overhead through low-rank weight factorization"], "limitations": "", "keywords": ["Mixture-of-Experts", "Autonomy-of-Experts", "Machine Learning", "Language Models", "Expert Selection"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2502.00592", "pdf": "https://arxiv.org/pdf/2502.00592.pdf", "abs": "https://arxiv.org/abs/2502.00592", "title": "M+: Extending MemoryLLM with Scalable Long-Term Memory", "authors": ["Yu Wang", "Dmitry Krotov", "Yuanzhe Hu", "Yifan Gao", "Wangchunshu Zhou", "Julian McAuley", "Dan Gutfreund", "Rogerio Feris", "Zexue He"], "categories": ["cs.CL"], "comment": null, "summary": "Equipping large language models (LLMs) with latent-space memory has attracted\nincreasing attention as they can extend the context window of existing language\nmodels. However, retaining information from the distant past remains a\nchallenge. For example, MemoryLLM (Wang et al., 2024a), as a representative\nwork with latent-space memory, compresses past information into hidden states\nacross all layers, forming a memory pool of 1B parameters. While effective for\nsequence lengths up to 16k tokens, it struggles to retain knowledge beyond 20k\ntokens. In this work, we address this limitation by introducing M+, a\nmemory-augmented model based on MemoryLLM that significantly enhances long-term\ninformation retention. M+ integrates a long-term memory mechanism with a\nco-trained retriever, dynamically retrieving relevant information during text\ngeneration. We evaluate M+ on diverse benchmarks, including long-context\nunderstanding and knowledge retention tasks. Experimental results show that M+\nsignificantly outperforms MemoryLLM and recent strong baselines, extending\nknowledge retention from under 20k to over 160k tokens with similar GPU memory\noverhead. We open-source our code at https://github.com/wangyu-ustc/MemoryLLM", "AI": {"tldr": "Introducing M+, a memory-augmented model that enhances long-term information retention in language models beyond 20k tokens.", "motivation": "To address the limitations of existing large language models in retaining information from the distant past, especially beyond 20k tokens.", "method": "M+ integrates a long-term memory mechanism with a co-trained retriever, dynamically retrieving relevant information during text generation.", "result": "M+ outperforms MemoryLLM and recent strong baselines, extending knowledge retention from under 20k to over 160k tokens while maintaining similar GPU memory overhead.", "conclusion": "M+ provides a significant advancement in long-term information retention for large language models, making it a promising tool for applications requiring extensive context.", "key_contributions": ["Development of M+, a memory-augmented model", "Significantly improved long-term information retention", "Open-sourcing of the model for further research"], "limitations": "", "keywords": ["large language models", "memory augmentation", "long-term retention"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2502.01349", "pdf": "https://arxiv.org/pdf/2502.01349.pdf", "abs": "https://arxiv.org/abs/2502.01349", "title": "Bias Beware: The Impact of Cognitive Biases on LLM-Driven Product Recommendations", "authors": ["Giorgos Filandrianos", "Angeliki Dimitriou", "Maria Lymperaiou", "Konstantinos Thomas", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has revolutionized product\nrecommenders, yet their susceptibility to adversarial manipulation poses\ncritical challenges, particularly in real-world commercial applications. Our\napproach is the first one to tap into human psychological principles,\nseamlessly modifying product descriptions, making such manipulations hard to\ndetect. In this work, we investigate cognitive biases as black-box adversarial\nstrategies, drawing parallels between their effects on LLMs and human\npurchasing behavior. Through extensive evaluation across models of varying\nscale, we find that certain biases, such as social proof, consistently boost\nproduct recommendation rate and ranking, while others, like scarcity and\nexclusivity, surprisingly reduce visibility. Our results demonstrate that\ncognitive biases are deeply embedded in state-of-the-art LLMs, leading to\nhighly unpredictable behavior in product recommendations and posing significant\nchallenges for effective mitigation.", "AI": {"tldr": "This paper explores how cognitive biases affect the performance of Large Language Models in product recommendation systems, revealing vulnerabilities to adversarial manipulation and inconsistent outcomes based on different biases.", "motivation": "To address the susceptibility of Large Language Models in real-world product recommender systems to adversarial manipulations by utilizing human psychological principles.", "method": "The paper investigates cognitive biases as black-box adversarial strategies and evaluates their effects on various models, comparing them to human purchasing behavior.", "result": "Certain cognitive biases, such as social proof, enhance product recommendation rates, while others, like scarcity and exclusivity, can paradoxically reduce visibility in recommendations.", "conclusion": "Cognitive biases are ingrained in LLMs, leading to unpredictable behaviors that can complicate mitigation strategies in product recommendations.", "key_contributions": ["First application of cognitive biases in modifying product descriptions for LLMs", "Identified specific biases that affect product recommendation rates", "Highlighted unpredictable behavior of LLMs in practical applications."], "limitations": "The study primarily focuses on cognitive biases without exploring other potential factors influencing LLM performance.", "keywords": ["Large Language Models", "Cognitive Biases", "Product Recommendations", "Adversarial Manipulation", "Human Psychology"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.02339", "pdf": "https://arxiv.org/pdf/2502.02339.pdf", "abs": "https://arxiv.org/abs/2502.02339", "title": "Boosting Multimodal Reasoning with Automated Structured Thinking", "authors": ["Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Fangrui Lv", "Ruihan Jin", "Feihu Che", "Zengqi Wen", "Jianhua Tao"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models excel across diverse domains but struggle\nwith complex visual reasoning tasks. Current approaches aim to incorporate\nstructured thinking via two strategies: explicit search methods and\npost-training techniques. However, both approaches face significant\nlimitations: Search-based methods suffer from computational inefficiency due to\nextensive solution space exploration, while post-training methods require\nsubstantial data, computational resources, and often encounter training\ninstability. To address these limitations, we propose AStar, an\n\\textbf{A}utomated \\textbf{S}tructured \\textbf{t}hinking paradigm for\nmultimod\\textbf{a}l \\textbf{r}easoning. Our method introduces \"thought cards\",\na lightweight library of high-level reasoning patterns abstracted from 500\nprior samples using Monte Carlo Tree Search. For each test problem, AStar\nadaptively retrieves the optimal thought cards and seamlessly integrates these\nexternal explicit guidelines with the model's internal implicit reasoning\ncapabilities. Extensive experiments demonstrate AStar's effectiveness and\nefficiency: using only 500 prior samples and a 7B backbone, our training-free\nframework achieves 53.9$\\%$ accuracy on MathVerse (surpassing GPT-4o's 50.2%)\nand 32.7% on MathVision (versus GPT-4o's 30.4%). Further analysis reveals that\nAStar generalizes beyond multimodal reasoning to visual perception and\nunderstanding domains, and serves as a plug-and-play test-time inference method\ncompatible with mainstream post-training techniques like GRPO.", "AI": {"tldr": "AStar is an automated structured thinking paradigm that enhances multimodal reasoning using 'thought cards' for improved performance in complex visual reasoning tasks.", "motivation": "To address the limitations of current multimodal models in handling complex visual reasoning tasks, which suffer from inefficiency and resource demands in existing methods.", "method": "AStar employs a library of high-level reasoning patterns called 'thought cards', retrieved adaptively for each test problem, combined with the model's implicit reasoning.", "result": "AStar achieved 53.9% accuracy on MathVerse and 32.7% on MathVision, outperforming GPT-4o in both cases with a training-free framework.", "conclusion": "AStar effectively improves multimodal reasoning capabilities and can generalize to other domains, integrating seamlessly with existing post-training techniques.", "key_contributions": ["Introduction of 'thought cards' for structured reasoning", "Training-free framework demonstrating high performance on visual reasoning tasks", "Plug-and-play compatibility with existing post-training techniques"], "limitations": "", "keywords": ["multimodal", "visual reasoning", "AStar", "structured thinking", "thought cards"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.02659", "pdf": "https://arxiv.org/pdf/2502.02659.pdf", "abs": "https://arxiv.org/abs/2502.02659", "title": "A Training-Free Length Extrapolation Approach for LLMs: Greedy Attention Logit Interpolation (GALI)", "authors": ["Yan Li", "Tianyi Zhang", "Zechuan Li", "Soyeon Caren Han"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, under review in the conference", "summary": "Transformer-based Large Language Models (LLMs) struggle with inputs exceeding\ntheir training context window due to positional out-of-distribution (O.O.D.)\nissues that disrupt attention. Existing solutions, including fine-tuning and\ntraining-free methods, face challenges like inefficiency, redundant\ninterpolation, logit outliers, or loss of local positional information. We\npropose Greedy Attention Logit Interpolation (GALI), a training-free method\nthat improves length extrapolation by greedily reusing pretrained positional\nintervals and interpolating attention logit to eliminate outliers. GALI\nachieves stable and superior performance across a wide range of long-context\ntasks without requiring input-length-specific tuning. Our analysis further\nreveals that LLMs interpret positional intervals unevenly and that restricting\ninterpolation to narrower ranges improves performance, even on short-context\ntasks. GALI represents a step toward more robust and generalizable long-text\nprocessing in LLMs. Our implementation of GALI, along with the experiments from\nour paper, is open-sourced at https://github.com/adlnlp/Gali.", "AI": {"tldr": "GALI is a training-free method enhancing long-context processing in LLMs by reusing positional intervals and interpolating attention logits to overcome existing positional O.O.D. challenges.", "motivation": "Existing transformer-based LLMs face limitations when dealing with long inputs, particularly due to positional out-of-distribution issues that hinder attention mechanisms.", "method": "GALI introduces Greedy Attention Logit Interpolation to improve length extrapolation by reusing pretrained positional intervals and interpolating attention logits without fine-tuning.", "result": "GALI demonstrates stable and superior performance on long-context tasks and even enhances short-context performance by optimizing how positional intervals are interpreted.", "conclusion": "GALI enhances the robustness and generalizability of LLMs for long-text processing, with open-source implementation available for further experimentation.", "key_contributions": ["Introduction of GALI for long-context processing", "Training-free method that improves length extrapolation", "Open-source implementation for community access"], "limitations": "Potential reliance on the quality of pretrained positional intervals; may not address all long-context challenges.", "keywords": ["Large Language Models", "Long-context processing", "Attention mechanisms"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.04037", "pdf": "https://arxiv.org/pdf/2502.04037.pdf", "abs": "https://arxiv.org/abs/2502.04037", "title": "Exploring Imbalanced Annotations for Effective In-Context Learning", "authors": ["Hongfu Gao", "Feipeng Zhang", "Hao Zeng", "Deyu Meng", "Bingyi Jing", "Hongxin Wei"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have shown impressive performance on downstream\ntasks through in-context learning (ICL), which heavily relies on the\ndemonstrations selected from annotated datasets. However, these datasets often\nexhibit long-tailed class distributions in real-world scenarios, leading to\nbiased demonstration selection. In this work, we show that such class\nimbalances significantly degrade the ICL performance across various tasks,\nregardless of selection methods. Moreover, classical rebalancing methods, which\nfocus solely on class weights, yield poor performance due to neglecting\ncondition bias--skewed feature distributions within classes. To address this,\nwe propose Reweighting with Conditional Bias (dubbed RCB), a simple and\ncomplementary approach to enhance ICL performance under class imbalance. In\nparticular, RCB estimates conditional bias using a balanced subset and\nre-weights demonstration scores based on both class weight and conditional\nbias. In effect, RCB prevents over-selection from dominant classes while\npreserving the efficacy of current selection methods. Extensive experiments on\ncommon benchmarks demonstrate the effectiveness of our method, improving the\naverage accuracy of current selection methods by up to 5.42%.", "AI": {"tldr": "The paper introduces RCB, a method to improve in-context learning performance in large language models by addressing class imbalance and conditional bias in demonstration selection.", "motivation": "Large language models rely on demonstration selection from annotated datasets, which often suffer from class imbalances affecting performance in in-context learning.", "method": "The proposed method, Reweighting with Conditional Bias (RCB), estimates conditional bias from a balanced subset and adjusts demonstration scores by considering both class weight and conditional bias.", "result": "RCB enhances ICL performance and improves average accuracy of current selection methods by up to 5.42% across various tasks.", "conclusion": "RCB effectively prevents over-selection from dominant classes while maintaining the strengths of existing selection methods, leading to improved ICL outcomes in the presence of class imbalance.", "key_contributions": ["Introduction of RCB methodology for addressing class imbalance in ICL", "Demonstration of significant performance improvement in various benchmarks", "Insight into conditional bias and its impact on demonstration selection"], "limitations": "", "keywords": ["Large Language Models", "In-Context Learning", "Class Imbalance", "Conditional Bias", "Demonstration Selection"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2502.09284", "pdf": "https://arxiv.org/pdf/2502.09284.pdf", "abs": "https://arxiv.org/abs/2502.09284", "title": "SparQLe: Speech Queries to Text Translation Through LLMs", "authors": ["Amirbek Djanibekov", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the growing influence of Large Language Models (LLMs), there is\nincreasing interest in integrating speech representations with them to enable\nmore seamless multi-modal processing and speech understanding. This study\nintroduces a novel approach that combines self-supervised speech\nrepresentations with instruction-tuned LLMs for speech-to-text translation. The\nproposed approach leverages a modality adapter to align extracted speech\nfeatures with instruction-tuned LLMs using English speech data. Our experiments\ndemonstrate that this method effectively preserves the semantic content of the\ninput speech and serves as an effective bridge between self-supervised speech\nmodels and instruction-tuned LLMs, offering a promising approach for various\nspeech understanding applications.", "AI": {"tldr": "This study presents a novel approach that integrates self-supervised speech representations with instruction-tuned LLMs for improved speech-to-text translation.", "motivation": "The paper addresses the growing interest in enhancing multi-modal processing and speech understanding through the integration of speech representations with Large Language Models.", "method": "The approach utilizes a modality adapter to align extracted speech features with instruction-tuned LLMs, specifically using English speech data.", "result": "Experiments show that the proposed method effectively preserves the semantic content of input speech, functioning as a bridge between self-supervised speech models and instruction-tuned LLMs.", "conclusion": "The integration of speech representations with instruction-tuned LLMs provides a promising approach for advancing speech understanding applications.", "key_contributions": ["Novel integration of self-supervised speech representations with LLMs", "Effective semantic preservation in speech-to-text translation", "Development of a modality adapter for speech feature alignment"], "limitations": "", "keywords": ["speech representations", "large language models", "speech-to-text translation", "multi-modal processing", "self-supervised learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11084", "pdf": "https://arxiv.org/pdf/2502.11084.pdf", "abs": "https://arxiv.org/abs/2502.11084", "title": "Rewrite to Jailbreak: Discover Learnable and Transferable Implicit Harmfulness Instruction", "authors": ["Yuting Huang", "Chengyuan Liu", "Yifeng Feng", "Yiquan Wu", "Chao Wu", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL"], "comment": "22 pages, 10 figures, accepted to ACL 2025 findings", "summary": "As Large Language Models (LLMs) are widely applied in various domains, the\nsafety of LLMs is increasingly attracting attention to avoid their powerful\ncapabilities being misused. Existing jailbreak methods create a forced\ninstruction-following scenario, or search adversarial prompts with prefix or\nsuffix tokens to achieve a specific representation manually or automatically.\nHowever, they suffer from low efficiency and explicit jailbreak patterns, far\nfrom the real deployment of mass attacks to LLMs. In this paper, we point out\nthat simply rewriting the original instruction can achieve a jailbreak, and we\nfind that this rewriting approach is learnable and transferable. We propose the\nRewrite to Jailbreak (R2J) approach, a transferable black-box jailbreak method\nto attack LLMs by iteratively exploring the weakness of the LLMs and\nautomatically improving the attacking strategy. The jailbreak is more efficient\nand hard to identify since no additional features are introduced. Extensive\nexperiments and analysis demonstrate the effectiveness of R2J, and we find that\nthe jailbreak is also transferable to multiple datasets and various types of\nmodels with only a few queries. We hope our work motivates further\ninvestigation of LLM safety. The code can be found at\nhttps://github.com/ythuang02/R2J/.", "AI": {"tldr": "The paper presents R2J, a learnable and transferable black-box jailbreak method for Large Language Models that improves efficiency and reduces detection likelihood by rewriting instructions.", "motivation": "To address the safety concerns associated with the misuse of Large Language Models by improving existing jailbreak methods.", "method": "The authors propose the Rewrite to Jailbreak (R2J) approach, which utilizes a learning mechanism to iteratively exploit weaknesses in LLMs for more efficient and undetectable attacks.", "result": "R2J demonstrated a higher efficiency in execution and a reduced likelihood of detection, successfully transferring across various models and datasets with minimal queries.", "conclusion": "The findings indicate that instruction rewriting as an attack strategy is promising and warrants further research into LLM safety protocols.", "key_contributions": ["Introduction of the R2J approach for LLM jailbreak", "Demonstration of learnability and transferability of the jailbreak method", "Experimental validation across multiple datasets and model types"], "limitations": "The paper does not address the ethical implications of using R2J and potential defenses against such methods.", "keywords": ["Large Language Models", "jailbreak", "instruction rewriting", "LLM safety", "deep learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11361", "pdf": "https://arxiv.org/pdf/2502.11361.pdf", "abs": "https://arxiv.org/abs/2502.11361", "title": "VLDBench Evaluating Multimodal Disinformation with Regulatory Alignment", "authors": ["Shaina Raza", "Ashmal Vayani", "Aditya Jain", "Aravind Narayanan", "Vahid Reza Khazaie", "Syed Raza Bashir", "Elham Dolatabadi", "Gias Uddin", "Christos Emmanouilidis", "Rizwan Qureshi", "Mubarak Shah"], "categories": ["cs.CL"], "comment": "under review", "summary": "Detecting disinformation that blends manipulated text and images has become\nincreasingly challenging, as AI tools make synthetic content easy to generate\nand disseminate. While most existing AI safety benchmarks focus on single\nmodality misinformation (i.e., false content shared without intent to deceive),\nintentional multimodal disinformation, such as propaganda or conspiracy\ntheories that imitate credible news, remains largely unaddressed. We introduce\nthe Vision-Language Disinformation Detection Benchmark (VLDBench), the first\nlarge-scale resource supporting both unimodal (text-only) and multimodal (text\n+ image) disinformation detection. VLDBench comprises approximately 62,000\nlabeled text-image pairs across 13 categories, curated from 58 news outlets.\nUsing a semi-automated pipeline followed by expert review, 22 domain experts\ninvested over 500 hours to produce high-quality annotations with substantial\ninter-annotator agreement. Evaluations of state-of-the-art Large Language\nModels (LLMs) and Vision-Language Models (VLMs) on VLDBench show that\nincorporating visual cues improves detection accuracy by 5 to 35 percentage\npoints over text-only models. VLDBench provides data and code for evaluation,\nfine-tuning, and robustness testing to support disinformation analysis.\nDeveloped in alignment with AI governance frameworks (e.g., the MIT AI Risk\nRepository), VLDBench offers a principled foundation for advancing trustworthy\ndisinformation detection in multimodal media.\n  Project: https://vectorinstitute.github.io/VLDBench/ Dataset:\nhttps://huggingface.co/datasets/vector-institute/VLDBench Code:\nhttps://github.com/VectorInstitute/VLDBench", "AI": {"tldr": "The VLDBench is a benchmark for detecting multimodal disinformation using text and images, aimed at enhancing the accuracy of disinformation detection tools.", "motivation": "To address the challenges of detecting multimodal disinformation, which is increasingly prevalent due to AI-generated content, highlighting the inadequacy of current benchmarks focused solely on unimodal misinformation.", "method": "The creation of VLDBench involved compiling approximately 62,000 labeled text-image pairs from 58 news outlets, followed by expert annotation to ensure high quality and agreement among domain experts.", "result": "State-of-the-art LLMs and VLMs achieved detection accuracy improvements of 5 to 35 percentage points when visual cues were incorporated compared to text-only models.", "conclusion": "VLDBench serves as a foundational resource for research in multimodal disinformation detection, fostering advancements in responsible AI governance and robust detection methods.", "key_contributions": ["Introduction of VLDBench as the first resource for multimodal disinformation detection.", "Collection and annotation of a large-scale dataset consisting of 62,000 text-image pairs.", "Demonstration of improved detection accuracy by incorporating multimodal data."], "limitations": "", "keywords": ["disinformation", "multimodal", "AI safety", "dataset", "detection"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.11404", "pdf": "https://arxiv.org/pdf/2502.11404.pdf", "abs": "https://arxiv.org/abs/2502.11404", "title": "ToolCoder: A Systematic Code-Empowered Tool Learning Framework for Large Language Models", "authors": ["Hanxing Ding", "Shuchang Tao", "Liang Pang", "Zihao Wei", "Jinyang Gao", "Bolin Ding", "Huawei Shen", "Xueqi Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Tool learning has emerged as a crucial capability for large language models\n(LLMs) to solve complex real-world tasks through interaction with external\ntools. Existing approaches face significant challenges, including reliance on\nhand-crafted prompts, difficulty in multi-step planning, and lack of precise\nerror diagnosis and reflection mechanisms. We propose ToolCoder, a novel\nframework that reformulates tool learning as a code generation task. Inspired\nby software engineering principles, ToolCoder transforms natural language\nqueries into structured Python function scaffold and systematically breaks down\ntasks with descriptive comments, enabling LLMs to leverage coding paradigms for\ncomplex reasoning and planning. It then generates and executes function\nimplementations to obtain final responses. Additionally, ToolCoder stores\nsuccessfully executed functions in a repository to promote code reuse, while\nleveraging error traceback mechanisms for systematic debugging, optimizing both\nexecution efficiency and robustness. Experiments demonstrate that ToolCoder\nachieves superior performance in task completion accuracy and execution\nreliability compared to existing approaches, establishing the effectiveness of\ncode-centric approaches in tool learning.", "AI": {"tldr": "ToolCoder is a novel framework that reformulates tool learning for large language models as a code generation task, enhancing planning, error diagnosis, and task execution through structured Python functions.", "motivation": "There are significant challenges in existing tool learning approaches for large language models, such as reliance on prompts and multi-step planning difficulties.", "method": "ToolCoder transforms natural language queries into structured Python function scaffolds and generates and executes implementations to improve task handling.", "result": "ToolCoder significantly improves task completion accuracy and execution reliability compared to existing methods.", "conclusion": "The effectiveness of code-centric approaches in tool learning is established through the results of the experiments conducted with ToolCoder.", "key_contributions": ["Reformulation of tool learning as code generation", "Systematic breakdown of tasks through comments in code", "Repository for reusable code and optimized error traceback mechanisms"], "limitations": "", "keywords": ["Tool learning", "Large language models", "Code generation", "Human-Computer Interaction", "Software engineering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11541", "pdf": "https://arxiv.org/pdf/2502.11541.pdf", "abs": "https://arxiv.org/abs/2502.11541", "title": "MuSC: Improving Complex Instruction Following with Multi-granularity Self-Contrastive Training", "authors": ["Hui Huang", "Jiaheng Liu", "Yancheng He", "Shilong Li", "Bing Xu", "Conghui Zhu", "Muyun Yang", "Tiejun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL2025", "summary": "Complex instruction-following with elaborate constraints is imperative for\nLarge Language Models (LLMs). While existing methods have constructed data for\ncomplex instruction alignment, they all rely on a more advanced model,\nespecially GPT-4, limiting their application. In this paper, we propose a\nMulti-granularity Self-Contrastive Training (MuSC) framework, to improve the\ncomplex instruction alignment without relying on a stronger model. Our method\nis conducted on both coarse and fine granularity. On coarse-granularity, we\nconstruct constraint-aware preference data based on instruction decomposition\nand recombination. On fine-granularity, we perform token-aware preference\noptimization with dynamic token-level supervision. Our method is evaluated on\nopen-sourced models, and experiment results show our method achieves\nsignificant improvement on both complex and general instruction-following\nbenchmarks, surpassing previous self-alignment methods.", "AI": {"tldr": "The paper introduces a MuSC framework for improving complex instruction alignment in LLMs without reliance on advanced models such as GPT-4.", "motivation": "Existing methods for complex instruction alignment depend on stronger models, which limits their broader applicability in practical scenarios.", "method": "The proposed MuSC framework employs a two-tier approach, utilizing coarse-granularity for constraint-aware preference data and fine-granularity for token-aware preference optimization with dynamic supervision.", "result": "Experimental evaluations demonstrate that the MuSC framework significantly improves performance on both complex and general instruction-following benchmarks compared to previous self-alignment approaches.", "conclusion": "The results indicate a successful advancement in instruction alignment capabilities of open-source models, making the approach widely applicable without needing advanced models.", "key_contributions": ["Development of the MuSC framework for instruction alignment", "Introduction of constraint-aware preference data construction", "Dynamic token-level supervision for optimization"], "limitations": "", "keywords": ["complex instruction alignment", "LLMs", "self-contrastive training"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2502.11705", "pdf": "https://arxiv.org/pdf/2502.11705.pdf", "abs": "https://arxiv.org/abs/2502.11705", "title": "LLM Agents Making Agent Tools", "authors": ["Georg Wölflein", "Dyke Ferber", "Daniel Truhn", "Ognjen Arandjelović", "Jakob Nikolas Kather"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted at ACL 2025", "summary": "Tool use has turned large language models (LLMs) into powerful agents that\ncan perform complex multi-step tasks by dynamically utilising external software\ncomponents. However, these tools must be implemented in advance by human\ndevelopers, hindering the applicability of LLM agents in domains demanding\nlarge numbers of highly specialised tools, like in life sciences and medicine.\nMotivated by the growing trend of scientific studies accompanied by public code\nrepositories, we propose ToolMaker, an agentic framework that autonomously\ntransforms papers with code into LLM-compatible tools. Given a GitHub URL and\nshort task description, ToolMaker autonomously installs dependencies and\ngenerates code to perform the task, using a closed-loop self-correction\nmechanism for debugging. To evaluate our approach, we introduce a benchmark\ncomprising 15 complex computational tasks spanning various domains with over\n100 unit tests to assess correctness and robustness. Our method correctly\nimplements 80% of the tasks, substantially outperforming current\nstate-of-the-art software engineering agents. ToolMaker therefore is a step\ntowards fully autonomous agent-based scientific workflows. Our code and\nbenchmark are publicly available at https://github.com/KatherLab/ToolMaker.", "AI": {"tldr": "ToolMaker is an autonomous framework that transforms scientific papers with code into LLM-compatible tools, improving the usability of LLM agents in complex domains.", "motivation": "The paper addresses the limitation of LLM agents requiring pre-implemented tools by proposing a framework that converts academic papers into usable tools, enhancing research efficiency in sectors like life sciences.", "method": "ToolMaker takes a GitHub URL and a short task description to autonomously install dependencies and generate required code, utilizing a closed-loop self-correction mechanism for debugging.", "result": "The proposed method successfully implements 80% of 15 complex computational tasks with a robust benchmark featuring over 100 unit tests, outperforming existing software engineering agents.", "conclusion": "ToolMaker significantly advances the potential for autonomous agent-based scientific workflows by providing a means to dynamically create tools from existing research code.", "key_contributions": ["Autonomous transformation of academic code into LLM-compatible tools", "Closed-loop self-correction mechanism for debugging", "Benchmarked performance showing 80% correct implementation of tasks"], "limitations": "", "keywords": ["Large Language Models", "Agent-based frameworks", "Software engineering"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11718", "pdf": "https://arxiv.org/pdf/2502.11718.pdf", "abs": "https://arxiv.org/abs/2502.11718", "title": "\"See the World, Discover Knowledge\": A Chinese Factuality Evaluation for Large Vision Language Models", "authors": ["Jihao Gu", "Yingyao Wang", "Pi Bu", "Chen Wang", "Ziming Wang", "Tengtao Song", "Donglai Wei", "Jiale Yuan", "Yingxiu Zhao", "Yancheng He", "Shilong Li", "Jiaheng Liu", "Meng Cao", "Jun Song", "Yingshui Tan", "Xiang Li", "Wenbo Su", "Zhicheng Zheng", "Xiaoyong Zhu", "Bo Zheng"], "categories": ["cs.CL", "cs.CV"], "comment": "26 pages, 21 figures", "summary": "The evaluation of factual accuracy in large vision language models (LVLMs)\nhas lagged behind their rapid development, making it challenging to fully\nreflect these models' knowledge capacity and reliability. In this paper, we\nintroduce the first factuality-based visual question-answering benchmark in\nChinese, named ChineseSimpleVQA, aimed at assessing the visual factuality of\nLVLMs across 8 major topics and 56 subtopics. The key features of this\nbenchmark include a focus on the Chinese language, diverse knowledge types, a\nmulti-hop question construction, high-quality data, static consistency, and\neasy-to-evaluate through short answers. Moreover, we contribute a rigorous data\nconstruction pipeline and decouple the visual factuality into two parts: seeing\nthe world (i.e., object recognition) and discovering knowledge. This decoupling\nallows us to analyze the capability boundaries and execution mechanisms of\nLVLMs. Subsequently, we evaluate 34 advanced open-source and closed-source\nmodels, revealing critical performance gaps within this field. Our\nevaluation-friendly code and data have already been open-sourced.", "AI": {"tldr": "This paper presents the first factuality-based visual question-answering benchmark for Chinese, called ChineseSimpleVQA, designed to evaluate visual factuality in large vision language models (LVLMs).", "motivation": "To address the gaps in evaluating factual accuracy in large vision language models, particularly in relation to understanding and processing visual information in the Chinese language.", "method": "Introduction of the ChineseSimpleVQA benchmark, focusing on 8 major topics and 56 subtopics with a rigorous data construction pipeline that separates visual factuality into object recognition and knowledge discovery.", "result": "The evaluation of 34 LVLMs revealed significant performance gaps, highlighting areas for future improvement in model capabilities.", "conclusion": "The benchmark and evaluation code are open-sourced, providing a resource for further research in visual question answering and improving LVLMs.", "key_contributions": ["Development of the ChineseSimpleVQA benchmark for assessing visual factuality in LVLMs.", "Decoupling visual factuality into object recognition and knowledge discovery for better analysis.", "Open-source evaluation code and data for community use."], "limitations": "Focused solely on Chinese language models and may not generalize to other languages or domains.", "keywords": ["Large Vision Language Models", "Visual Question Answering", "Factuality Evaluation", "Chinese Language", "Benchmark"], "importance_score": 6, "read_time_minutes": 20}}
{"id": "2502.12476", "pdf": "https://arxiv.org/pdf/2502.12476.pdf", "abs": "https://arxiv.org/abs/2502.12476", "title": "CoCo-CoLa: Evaluating and Improving Language Adherence in Multilingual LLMs", "authors": ["Elnaz Rahmati", "Alireza S. Ziabari", "Morteza Dehghani"], "categories": ["cs.CL"], "comment": "26 pages, 7 figures", "summary": "Multilingual Large Language Models (LLMs) develop cross-lingual abilities\ndespite being trained on limited parallel data. However, they often struggle to\ngenerate responses in the intended language, favoring high-resource languages\nsuch as English. In this work, we introduce CoCo-CoLa (Correct Concept -\nCorrect Language), a novel metric to evaluate language adherence in\nmultilingual LLMs. Using fine-tuning experiments on a closed-book QA task\nacross seven languages, we analyze how training in one language affects others'\nperformance. Our findings reveal that multilingual models share task knowledge\nacross languages but exhibit biases in the selection of output language. We\nidentify language-specific layers, showing that final layers play a crucial\nrole in determining output language. Accordingly, we propose a partial training\nstrategy that selectively fine-tunes key layers, improving language adherence\nwhile significantly reducing computational cost. Our method achieves comparable\nor superior performance to full fine-tuning, particularly for low-resource\nlanguages, offering a more efficient multilingual adaptation.", "AI": {"tldr": "This paper presents CoCo-CoLa, a metric for evaluating language adherence in multilingual LLMs and a selective fine-tuning strategy to enhance language response generation in low-resource languages.", "motivation": "Multilingual LLMs can struggle with language adherence, often favoring high-resource languages, which necessitates better evaluation metrics and fine-tuning methods.", "method": "The authors introduce CoCo-CoLa, a new metric, and conduct fine-tuning experiments on a closed-book QA task across seven languages to analyze the cross-lingual performance of multilingual models.", "result": "Experiments show that multilingual models can share task knowledge but exhibit biases in output language preference. The proposed partial training strategy significantly improves language adherence while being computationally efficient.", "conclusion": "The selective fine-tuning approach not only enhances performance for low-resource languages but also maintains or improves efficiency compared to full fine-tuning.", "key_contributions": ["Introduction of the CoCo-CoLa metric for evaluating language adherence", "Identification of language-specific layers impacting output language generation", "Proposition of a partial training strategy that reduces computational costs while improving language adherence"], "limitations": "", "keywords": ["Multilingual LLMs", "Language adherence", "Fine-tuning", "Cross-lingual", "Low-resource languages"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2502.14494", "pdf": "https://arxiv.org/pdf/2502.14494.pdf", "abs": "https://arxiv.org/abs/2502.14494", "title": "StructFlowBench: A Structured Flow Benchmark for Multi-turn Instruction Following", "authors": ["Jinnan Li", "Jinzhe Li", "Yue Wang", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings camera-ready version", "summary": "Multi-turn instruction following capability constitutes a core competency of\nlarge language models (LLMs) in real-world applications. Existing evaluation\nbenchmarks predominantly focus on fine-grained constraint satisfaction and\ndomain-specific capability assessment, yet overlook the crucial structural\ndependencies between dialogue turns that distinguish multi-turn from\nsingle-turn interactions. These structural dependencies not only reflect user\nintent but also establish an essential second dimension for the instruction\nfollowing evaluation beyond constraint satisfaction. To address this gap, we\npropose StructFlowBench, a multi-turn instruction following benchmark with\nstructural flow modeling. The benchmark defines an innovative structural flow\nframework with six fundamental inter-turn relationships. These relationships\nintroduce novel structural constraints for model evaluation and also serve as\ngeneration parameters for creating customized dialogue flows tailored to\nspecific scenarios. Adopting established LLM-based automatic evaluation\nmethodologies, we conduct systematic evaluations of 13 leading open-source and\nclosed-source LLMs. Experimental results reveal significant deficiencies in\ncurrent models' comprehension of multi-turn dialogue structures. The code is\navailable at https://github.com/MLGroupJLU/StructFlowBench.", "AI": {"tldr": "This paper introduces StructFlowBench, a benchmark for evaluating multi-turn instruction following in large language models, focusing on structural dependencies between dialogue turns.", "motivation": "Existing benchmarks inadequately assess the structural dependencies in multi-turn dialogue that are critical for understanding user intent and instruction following.", "method": "The paper presents the StructFlowBench framework which defines six fundamental inter-turn relationships to model structural flows and to create customized dialogue flows for evaluation.", "result": "Systematic evaluations demonstrate that 13 leading LLMs show significant deficiencies in understanding multi-turn dialogue structures as defined in the new benchmark.", "conclusion": "StructFlowBench provides a novel approach to evaluate multi-turn instruction following capabilities in LLMs, highlighting the need for improved comprehension of dialogue structures aware of user intent.", "key_contributions": ["Introduction of StructFlowBench for multi-turn instruction following evaluation", "Development of a framework defining six inter-turn relationships", "Presentation of systematic evaluation findings on existing LLMs."], "limitations": "The framework focuses specifically on instruction following without addressing other aspects of dialogue management.", "keywords": ["Multi-turn dialogue", "Instruction following", "Large language models", "Benchmarking", "Dialogue structures"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.14561", "pdf": "https://arxiv.org/pdf/2502.14561.pdf", "abs": "https://arxiv.org/abs/2502.14561", "title": "Can LLMs Predict Citation Intent? An Experimental Analysis of In-context Learning and Fine-tuning on Open LLMs", "authors": ["Paris Koloveas", "Serafeim Chatzopoulos", "Thanasis Vergoulis", "Christos Tryfonopoulos"], "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "This work investigates the ability of open Large Language Models (LLMs) to\npredict citation intent through in-context learning and fine-tuning. Unlike\ntraditional approaches relying on domain-specific pre-trained models like\nSciBERT, we demonstrate that general-purpose LLMs can be adapted to this task\nwith minimal task-specific data. We evaluate twelve model variations across\nfive prominent open LLM families using zero-, one-, few-, and many-shot\nprompting. Our experimental study identifies the top-performing model and\nprompting parameters through extensive in-context learning experiments. We then\ndemonstrate the significant impact of task-specific adaptation by fine-tuning\nthis model, achieving a relative F1-score improvement of 8% on the SciCite\ndataset and 4.3% on the ACL-ARC dataset compared to the instruction-tuned\nbaseline. These findings provide valuable insights for model selection and\nprompt engineering. Additionally, we make our end-to-end evaluation framework\nand models openly available for future use.", "AI": {"tldr": "This paper explores the use of open LLMs for predicting citation intent, leveraging in-context learning and fine-tuning with minimal task-specific data, achieving significant performance improvements over baseline models.", "motivation": "To investigate how well general-purpose LLMs can predict citation intent compared to traditional domain-specific models, allowing for adaptability with limited data.", "method": "The study evaluates twelve variations of open LLMs across five model families using various prompting strategies (zero- to many-shot prompting). Extensive experiments were conducted to identify the best-performing model and parameters, followed by fine-tuning for improved performance.", "result": "Fine-tuning the best model achieved an F1-score improvement of 8% on the SciCite dataset and 4.3% on the ACL-ARC dataset beyond the instruction-tuned baseline.", "conclusion": "The results indicate that general-purpose LLMs can perform well in specialized tasks like predicting citation intent with effective prompting and task-specific adaptations.", "key_contributions": ["Introduction of open LLMs for citation intent prediction", "Demonstrated effectiveness of minimal task-specific data for model adaptation", "Development of an evaluation framework made openly available for future research"], "limitations": "", "keywords": ["Large Language Models", "citation intent", "in-context learning", "fine-tuning", "prompt engineering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.14830", "pdf": "https://arxiv.org/pdf/2502.14830.pdf", "abs": "https://arxiv.org/abs/2502.14830", "title": "Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs", "authors": ["Danni Liu", "Jan Niehues"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "While large language models demonstrate remarkable capabilities at\ntask-specific applications through fine-tuning, extending these benefits across\ndiverse languages is essential for broad accessibility. However, effective\ncross-lingual transfer is hindered by LLM performance gaps across languages and\nthe scarcity of fine-tuning data in many languages. Through analysis of LLM\ninternal representations from over 1,000+ language pairs, we discover that\nmiddle layers exhibit the strongest potential for cross-lingual alignment.\nBuilding on this finding, we propose a middle-layer alignment objective\nintegrated into task-specific training. Our experiments on slot filling,\nmachine translation, and structured text generation show consistent\nimprovements in cross-lingual transfer, especially to lower-resource languages.\nThe method is robust to the choice of alignment languages and generalizes to\nlanguages unseen during alignment. Furthermore, we show that separately trained\nalignment modules can be merged with existing task-specific modules, improving\ncross-lingual capabilities without full re-training. Our code is publicly\navailable (https://github.com/dannigt/mid-align).", "AI": {"tldr": "This paper proposes a middle-layer alignment objective for large language models to enhance cross-lingual transfer, particularly for lower-resource languages, achieving consistent improvements across various tasks.", "motivation": "The need for effective cross-lingual transfer in large language models is critical for broad accessibility, as performance gaps and data scarcity hinder application in diverse languages.", "method": "The authors analyze internal representations of over 1,000+ language pairs and identify that middle layers have the strongest alignment potential. They introduce a middle-layer alignment objective that is integrated into task-specific training.", "result": "Experiments demonstrate consistent improvements in cross-lingual transfer for tasks such as slot filling, machine translation, and structured text generation, particularly benefiting lower-resource languages.", "conclusion": "The proposed method allows for enhanced cross-lingual capabilities without the need for full re-training, making it easier to integrate with existing task-specific models.", "key_contributions": ["Introduction of a middle-layer alignment objective for large language models.", "Demonstration of improved cross-lingual transfer, especially for low-resource languages.", "Ability to merge alignment modules with task-specific modules without complete re-training."], "limitations": "", "keywords": ["cross-lingual transfer", "large language models", "alignment objective"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.15197", "pdf": "https://arxiv.org/pdf/2502.15197.pdf", "abs": "https://arxiv.org/abs/2502.15197", "title": "TETRIS: Optimal Draft Token Selection for Batch Speculative Decoding", "authors": ["Zhaoxuan Wu", "Zijian Zhou", "Arun Verma", "Alok Prakash", "Daniela Rus", "Bryan Kian Hsiang Low"], "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 11 figures, 5 tables", "summary": "We propose TETRIS, a novel method that optimizes the total throughput of\nbatch speculative decoding in multi-request settings. Unlike existing methods\nthat optimize for a single request or a group of requests as a whole, TETRIS\nactively selects the most promising draft tokens (for every request in a batch)\nto be accepted when verified in parallel, resulting in fewer rejected tokens\nand hence less wasted computing resources. Such an effective resource\nutilization to achieve fast inference in large language models (LLMs) is\nespecially important to service providers with limited inference capacity.\nCompared to baseline speculative decoding, TETRIS yields a consistently higher\nacceptance rate and more effective utilization of the limited inference\ncapacity. We show theoretically and empirically that TETRIS outperforms\nbaseline speculative decoding and existing methods that dynamically select\ndraft tokens, leading to a more efficient batch inference in LLMs.", "AI": {"tldr": "TETRIS optimizes batch speculative decoding for large language models by selecting promising draft tokens for parallel verification, improving acceptance rates and resource utilization.", "motivation": "To improve the efficiency of speculative decoding in multi-request settings for large language models, particularly for service providers with limited compute resources.", "method": "TETRIS actively selects the most promising draft tokens for each request in a batch to be verified in parallel, contrasting with existing methods that optimize for individual or grouped requests.", "result": "TETRIS demonstrated a higher acceptance rate and improved utilization of limited inference capacity compared to baseline methods and others that select draft tokens dynamically.", "conclusion": "The proposed method outperforms existing speculative decoding strategies, leading to more efficient batch inference in large language models.", "key_contributions": ["Development of TETRIS method for optimizing batch speculative decoding.", "Demonstration of superior acceptance rates compared to baseline methods.", "Enhanced resource utilization for service providers with limited inference capabilities."], "limitations": "", "keywords": ["speculative decoding", "batch processing", "large language models", "token selection", "resource optimization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.15401", "pdf": "https://arxiv.org/pdf/2502.15401.pdf", "abs": "https://arxiv.org/abs/2502.15401", "title": "Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning", "authors": ["Xuetao Ma", "Wenbin Jiang", "Hua Huang"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 6 figures, ACL 2025 findings, camera-ready version", "summary": "In-context learning (ICL) can significantly enhance the complex reasoning\ncapabilities of large language models (LLMs), with the key lying in the\nselection and ordering of demonstration examples. Previous methods typically\nrelied on simple features to measure the relevance between examples. We argue\nthat these features are not sufficient to reflect the intrinsic connections\nbetween examples. In this study, we propose a curriculum ICL strategy guided by\nproblem-solving logic. We select demonstration examples by analyzing the\nproblem-solving logic and order them based on curriculum learning.\nSpecifically, we constructed a problem-solving logic instruction set based on\nthe BREAK dataset and fine-tuned a language model to analyze the\nproblem-solving logic of examples. Subsequently, we selected appropriate\ndemonstration examples based on problem-solving logic and assessed their\ndifficulty according to the number of problem-solving steps. In accordance with\nthe principles of curriculum learning, we ordered the examples from easy to\nhard to serve as contextual prompts. Experimental results on multiple\nbenchmarks indicate that our method outperforms previous ICL approaches in\nterms of performance and efficiency, effectively enhancing the complex\nreasoning capabilities of LLMs. Our project will be released at\nhttps://github.com/maxuetao/CurriculumICL", "AI": {"tldr": "This paper presents a curriculum in-context learning (ICL) strategy for large language models that enhances their reasoning capabilities through improved selection and ordering of demonstration examples based on problem-solving logic.", "motivation": "The study addresses the limitations of previous ICL methods that relied on simplistic features to assess relevance between examples, suggesting that more sophisticated analysis is needed to enhance model reasoning.", "method": "A problem-solving logic instruction set was constructed using the BREAK dataset, followed by fine-tuning a language model to analyze this logic. Demonstration examples were selected based on their problem-solving logic and ordered from easy to hard through curriculum learning principles.", "result": "The experimental results demonstrate that the proposed curriculum ICL strategy outperforms existing ICL methods, leading to improvements in performance and efficiency in enhancing the reasoning capabilities of LLMs across various benchmarks.", "conclusion": "The proposed method effectively enhances complex reasoning in LLMs and encourages a more sophisticated approach to example selection and ordering in in-context learning strategies.", "key_contributions": ["Introduction of a curriculum ICL strategy that utilizes problem-solving logic for LLMs.", "Construction of a problem-solving logic instruction set based on the BREAK dataset.", "Demonstrated improvements in performance and efficiency over previous ICL methods."], "limitations": "", "keywords": ["in-context learning", "large language models", "curriculum learning", "problem-solving logic", "machine learning"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2502.15434", "pdf": "https://arxiv.org/pdf/2502.15434.pdf", "abs": "https://arxiv.org/abs/2502.15434", "title": "Mixup Model Merge: Enhancing Model Merging Performance through Randomized Linear Interpolation", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "15 pages", "summary": "Model merging aims to integrate multiple task-specific models into a unified\nmodel that inherits the capabilities of the task-specific models, without\nadditional training. Existing model merging methods often lack consideration of\nthe varying contribution ratios of different task-specific models to the final\nmerged model. In this paper, we propose Mixup Model Merge (M3), a simple yet\neffective method inspired by the randomized linear interpolation strategy from\nthe Mixup data augmentation technique. M3 performs randomized linear\ninterpolation in parameter space between two task-specific LLMs, where\ninterpolation coefficients are sampled from a Beta distribution to explore\ndiverse contribution ratios. This controllable randomness allows M3 to\noutperform standard equal-ratio merging by discovering better contribution\nratio combinations. Extensive experiments show that M3 significantly (1)\nimproves merged LLM performance across tasks, (2) enhances out-of-distribution\nand adversarial robustness, and (3) outperforms the positive effects of the\nsparsification method DARE on model merging and can be further combined with\nDARE to achieve superior results. By tuning the Beta distribution's shape\nparameters, (4) M3 balances exploration efficiency and diversity in\ncontribution ratios. The code is available at:\nhttps://github.com/MLGroupJLU/MixupModelMerge", "AI": {"tldr": "The paper introduces Mixup Model Merge (M3), a method for merging task-specific models using randomized linear interpolation to improve performance and robustness.", "motivation": "To integrate task-specific models while considering their varying contribution to the merged model without additional training.", "method": "M3 performs randomized linear interpolation in parameter space between two task-specific LLMs with coefficients sampled from a Beta distribution, allowing for controllable contribution ratios.", "result": "M3 significantly improves performance on merged LLMs across tasks and enhances robustness to out-of-distribution and adversarial inputs.", "conclusion": "M3 outperforms standard merging techniques and can be combined with existing methods like DARE for superior results; it balances exploration and diversity in contribution ratios by tuning Beta distribution parameters.", "key_contributions": ["Introduction of M3 for model merging", "Improved robustness and performance metrics", "Combination potential with existing methods like DARE"], "limitations": "", "keywords": ["model merging", "machine learning", "LLMs", "robustness", "Beta distribution"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.16487", "pdf": "https://arxiv.org/pdf/2502.16487.pdf", "abs": "https://arxiv.org/abs/2502.16487", "title": "All That Glitters is Not Novel: Plagiarism in AI Generated Research", "authors": ["Tarun Gupta", "Danish Pruthi"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (main) conference", "summary": "Automating scientific research is considered the final frontier of science.\nRecently, several papers claim autonomous research agents can generate novel\nresearch ideas. Amidst the prevailing optimism, we document a critical concern:\na considerable fraction of such research documents are smartly plagiarized.\nUnlike past efforts where experts evaluate the novelty and feasibility of\nresearch ideas, we request $13$ experts to operate under a different\nsituational logic: to identify similarities between LLM-generated research\ndocuments and existing work. Concerningly, the experts identify $24\\%$ of the\n$50$ evaluated research documents to be either paraphrased (with one-to-one\nmethodological mapping), or significantly borrowed from existing work. These\nreported instances are cross-verified by authors of the source papers. Experts\nfind an additional $32\\%$ ideas to partially overlap with prior work, and a\nsmall fraction to be completely original. Problematically, these LLM-generated\nresearch documents do not acknowledge original sources, and bypass inbuilt\nplagiarism detectors. Lastly, through controlled experiments we show that\nautomated plagiarism detectors are inadequate at catching plagiarized ideas\nfrom such systems. We recommend a careful assessment of LLM-generated research,\nand discuss the implications of our findings on academic publishing.", "AI": {"tldr": "The paper investigates the prevalence of plagiarism in research documents generated by LLMs, finding that a significant percentage are either paraphrased or borrowed from existing work, and that current plagiarism detectors are inadequate.", "motivation": "To address concerns regarding the originality of research generated by autonomous agents and the implications for academic integrity.", "method": "Expert evaluation of 50 LLM-generated research documents to identify similarities with existing work, supported by cross-verification from original source authors and controlled experiments on plagiarism detection efficacy.", "result": "Experts identified that 24% of the evaluated documents were paraphrased or borrowed from existing work, while 32% showed partial overlap. Current plagiarism detection methods failed to catch many instances of plagiarism.", "conclusion": "There is a pressing need for careful assessment of LLM-generated research to uphold academic integrity, as many documents do not acknowledge original sources and current detection methods are inadequate.", "key_contributions": ["Documented the extent of plagiarism in LLM-generated research documents", "Cross-verified findings with original source authors", "Evaluated the effectiveness of current plagiarism detection methods"], "limitations": "The study is limited by its sample size of 50 documents and the reliance on expert evaluation, which may not capture all nuances of plagiarism.", "keywords": ["plagiarism", "LLM-generated research", "academic integrity", "plagiarism detection", "autonomous research agents"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.16989", "pdf": "https://arxiv.org/pdf/2502.16989.pdf", "abs": "https://arxiv.org/abs/2502.16989", "title": "All-in-one: Understanding and Generation in Multimodal Reasoning with the MAIA Benchmark", "authors": ["Davide Testa", "Giovanni Bonetta", "Raffaella Bernardi", "Alessandro Bondielli", "Alessandro Lenci", "Alessio Miaschi", "Lucia Passaro", "Bernardo Magnini"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce MAIA (Multimodal AI Assessment), a native-Italian benchmark\ndesigned for fine-grained investigation of the reasoning abilities of visual\nlanguage models on videos. MAIA differs from other available video benchmarks\nfor its design, its reasoning categories, the metric it uses, and the language\nand culture of the videos. MAIA evaluates Vision Language Models (VLMs) on two\naligned tasks: a visual statement verification task and an open-ended visual\nquestion-answering task, both on the same set of video-related questions. It\nconsiders twelve reasoning categories that aim to disentangle language and\nvision relations by highlighting the role of the visual input. Thanks to its\ncarefully taught design, it evaluates VLMs' consistency and visually grounded\nnatural language comprehension and generation simultaneously through an\naggregated metric revealing low results that highlight models' fragility. Last\nbut not least, the video collection has been carefully selected to reflect the\nItalian culture, and the language data are produced by native-speakers.", "AI": {"tldr": "MAIA is a new benchmark for assessing visual language models on videos, focusing on reasoning abilities through two tasks: visual statement verification and open-ended visual question-answering.", "motivation": "To improve understanding of the reasoning capabilities of visual language models, particularly in the context of Italian language and culture.", "method": "MAIA uses a dual-task approach (visual statement verification and visual question-answering) across twelve reasoning categories, employing metrics that focus on the interactions between language and visual inputs.", "result": "Initial evaluations reveal that existing Vision Language Models exhibit low performance, indicating weaknesses in understanding visual contexts and generating coherent natural language.", "conclusion": "MAIA emphasizes the need for better alignment between visual and language understanding in AI models, particularly for culturally specific content.", "key_contributions": ["Introduction of a new benchmark for VLMs focused on Italian culture", "Dual-task evaluation method allowing nuanced reasoning assessment", "Highlighting the fragility of current VLM's performance in real-world scenarios"], "limitations": "", "keywords": ["Multimodal AI", "Visual Language Models", "Benchmark", "Italian Culture", "Reasoning Evaluation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2502.20864", "pdf": "https://arxiv.org/pdf/2502.20864.pdf", "abs": "https://arxiv.org/abs/2502.20864", "title": "Do Language Models Understand Honorific Systems in Javanese?", "authors": ["Mohammad Rifqi Farhansyah", "Iwan Darmawan", "Adryan Kusumawardhana", "Genta Indra Winata", "Alham Fikri Aji", "Derry Tanti Wijaya"], "categories": ["cs.CL"], "comment": "ACL 2025 - Main Conference", "summary": "The Javanese language features a complex system of honorifics that vary\naccording to the social status of the speaker, listener, and referent. Despite\nits cultural and linguistic significance, there has been limited progress in\ndeveloping a comprehensive corpus to capture these variations for natural\nlanguage processing (NLP) tasks. In this paper, we present Unggah-Ungguh, a\ncarefully curated dataset designed to encapsulate the nuances of Unggah-Ungguh\nBasa, the Javanese speech etiquette framework that dictates the choice of words\nand phrases based on social hierarchy and context. Using Unggah-Ungguh, we\nassess the ability of language models (LMs) to process various levels of\nJavanese honorifics through classification and machine translation tasks. To\nfurther evaluate cross-lingual LMs, we conduct machine translation experiments\nbetween Javanese (at specific honorific levels) and Indonesian. Additionally,\nwe explore whether LMs can generate contextually appropriate Javanese\nhonorifics in conversation tasks, where the honorific usage should align with\nthe social role and contextual cues. Our findings indicate that current LMs\nstruggle with most honorific levels, exhibitinga bias toward certain honorific\ntiers.", "AI": {"tldr": "This paper introduces Unggah-Ungguh, a dataset for studying Javanese honorifics, and evaluates language models' ability to process these variations.", "motivation": "To address the lack of a comprehensive corpus for Javanese honorifics, crucial for natural language processing (NLP) tasks.", "method": "A dataset named Unggah-Ungguh is curated to capture Javanese speech etiquette nuances, and language models are tested on classification and machine translation tasks between Javanese and Indonesian.", "result": "Current language models exhibit difficulty in handling Javanese honorifics, showing bias towards certain levels, indicating a gap in processing capabilities for this cultural aspect.", "conclusion": "The study highlights the need for improvements in language model training to effectively incorporate social and contextual nuances in Javanese language processing.", "key_contributions": ["Introduction of the Unggah-Ungguh dataset for Javanese honorifics", "Evaluation of language model performance on Javanese honorifics", "Insights into the challenges LMs face with honorific levels"], "limitations": "Limited to Javanese honorifics and may not generalize to other languages or cultures; primarily focuses on specific NLP tasks.", "keywords": ["Javanese language", "honorifics", "natural language processing", "machine translation", "language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.01372", "pdf": "https://arxiv.org/pdf/2503.01372.pdf", "abs": "https://arxiv.org/abs/2503.01372", "title": "SwiLTra-Bench: The Swiss Legal Translation Benchmark", "authors": ["Joel Niklaus", "Jakob Merane", "Luka Nenadic", "Sina Ahmadi", "Yingqiang Gao", "Cyrill A. H. Chevalley", "Claude Humbel", "Christophe Gösken", "Lorenzo Tanzi", "Thomas Lüthi", "Stefan Palombo", "Spencer Poff", "Boling Yang", "Nan Wu", "Matthew Guillod", "Robin Mamié", "Daniel Brunner", "Julio Pereyra", "Niko Grupen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": "Accepted at ACL main 2025", "summary": "In Switzerland legal translation is uniquely important due to the country's\nfour official languages and requirements for multilingual legal documentation.\nHowever, this process traditionally relies on professionals who must be both\nlegal experts and skilled translators -- creating bottlenecks and impacting\neffective access to justice. To address this challenge, we introduce\nSwiLTra-Bench, a comprehensive multilingual benchmark of over 180K aligned\nSwiss legal translation pairs comprising laws, headnotes, and press releases\nacross all Swiss languages along with English, designed to evaluate LLM-based\ntranslation systems. Our systematic evaluation reveals that frontier models\nachieve superior translation performance across all document types, while\nspecialized translation systems excel specifically in laws but under-perform in\nheadnotes. Through rigorous testing and human expert validation, we demonstrate\nthat while fine-tuning open SLMs significantly improves their translation\nquality, they still lag behind the best zero-shot prompted frontier models such\nas Claude-3.5-Sonnet. Additionally, we present SwiLTra-Judge, a specialized LLM\nevaluation system that aligns best with human expert assessments.", "AI": {"tldr": "This paper presents SwiLTra-Bench, a multilingual legal translation benchmark that evaluates the performance of large language models in Swiss legal contexts.", "motivation": "To improve access to justice in Switzerland by addressing bottlenecks in legal translation caused by the need for professionals to be both legal experts and skilled translators.", "method": "A comprehensive benchmark of over 180K aligned Swiss legal translation pairs was created and evaluated. The performance of frontier models and specialized systems was compared, with rigorous testing and human expert validation to assess translation quality.", "result": "Frontier models outperform specialized systems in translation performance across all document types, while specialized systems perform better specifically in translating laws. Fine-tuning open SLMs improves results but still falls short of leading zero-shot models.", "conclusion": "The study highlights the potential of LLMs in legal translation while suggesting specialized models may excel in specific contexts. SwiLTra-Judge offers an effective evaluation system aligned with human expert judgments.", "key_contributions": ["Creation of SwiLTra-Bench, a multilingual legal translation benchmark.", "Identification of performance disparities between frontier LLMs and specialized systems.", "Development of SwiLTra-Judge for assessing LLM translation quality."], "limitations": "The benchmark may not cover all legal contexts and nuances in the Swiss legal system.", "keywords": ["legal translation", "multilingual", "large language models", "benchmark", "Switzerland"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2503.01606", "pdf": "https://arxiv.org/pdf/2503.01606.pdf", "abs": "https://arxiv.org/abs/2503.01606", "title": "Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering", "authors": ["Zhanghao Hu", "Hanqi Yan", "Qinglin Zhu", "Zhenyi Shen", "Yulan He", "Lin Gui"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted in ACL 2025 Main", "summary": "Large language models have recently pushed open domain question answering\n(ODQA) to new frontiers. However, prevailing retriever-reader pipelines often\ndepend on multiple rounds of prompt level instructions, leading to high\ncomputational overhead, instability, and suboptimal retrieval coverage. In this\npaper, we propose EmbQA, an embedding-level framework that alleviates these\nshortcomings by enhancing both the retriever and the reader. Specifically, we\nrefine query representations via lightweight linear layers under an\nunsupervised contrastive learning objective, thereby reordering retrieved\npassages to highlight those most likely to contain correct answers.\nAdditionally, we introduce an exploratory embedding that broadens the model's\nlatent semantic space to diversify candidate generation and employs an\nentropy-based selection mechanism to choose the most confident answer\nautomatically. Extensive experiments across three open-source LLMs, three\nretrieval methods, and four ODQA benchmarks demonstrate that EmbQA\nsubstantially outperforms recent baselines in both accuracy and efficiency.", "AI": {"tldr": "This paper introduces EmbQA, an embedding-level framework that enhances open domain question answering by optimizing retriever and reader performance through improved query representation and automatic answer selection.", "motivation": "To address the limitations of existing retriever-reader pipelines in open domain question answering, which suffer from high computational overhead and instability.", "method": "The authors propose EmbQA, which utilizes lightweight linear layers for refining query representations under an unsupervised contrastive learning objective, and an entropy-based selection mechanism for answer generation.", "result": "EmbQA significantly outperforms recent baselines in accuracy and efficiency across three open-source LLMs, three retrieval methods, and four ODQA benchmarks.", "conclusion": "The proposed EmbQA framework not only improves the effectiveness of retrieval and reading processes in ODQA but also offers a more efficient alternative to existing methods.", "key_contributions": ["Introduction of an embedding-level framework for ODQA", "Use of lightweight linear layers for query refinement", "Implementation of an entropy-based selection mechanism for answer generation"], "limitations": "", "keywords": ["open domain question answering", "large language models", "embedding-level framework"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2503.03854", "pdf": "https://arxiv.org/pdf/2503.03854.pdf", "abs": "https://arxiv.org/abs/2503.03854", "title": "Vision-Language Models Struggle to Align Entities across Modalities", "authors": ["Iñigo Alonso", "Gorka Azkune", "Ander Salaberria", "Jeremy Barnes", "Oier Lopez de Lacalle"], "categories": ["cs.CL"], "comment": "Accepted Findings ACL 2025", "summary": "Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress.", "AI": {"tldr": "This paper introduces a new task and benchmark for cross-modal entity linking called MATE, evaluating Vision-Language Models against humans on a question-answering task involving visual and textual representations.", "motivation": "The paper addresses the gap in literature regarding cross-modal entity linking, a critical skill for applications like multimodal code generation and fake news detection.", "method": "The authors introduce MATE, a benchmark with 5.5k instances, and design a question-answering task to evaluate the alignment of entities across modalities.", "result": "The evaluation shows that state-of-the-art Vision-Language Models struggle with the task compared to humans, especially as scene complexity increases.", "conclusion": "The findings indicate significant performance gaps between VLMs and humans, suggesting further research is needed in cross-modal entity linking, with MATE serving as a valuable benchmark.", "key_contributions": ["Introduction of the MATE benchmark for cross-modal entity linking.", "A novel question-answering task leveraging visual and textual data.", "Analysis revealing the performance gap between VLMs and human abilities."], "limitations": "The focus on specific entities and attributes may limit the scope of cross-modal applications.", "keywords": ["cross-modal entity linking", "Vision-Language Models", "MATE benchmark", "question-answering task", "multimodal applications"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2503.04378", "pdf": "https://arxiv.org/pdf/2503.04378.pdf", "abs": "https://arxiv.org/abs/2503.04378", "title": "HelpSteer3: Human-Annotated Feedback and Edit Data to Empower Inference-Time Scaling in Open-Ended General-Domain Tasks", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Daniel Egert", "Ellie Evans", "Hoo-Chang Shin", "Felipe Soares", "Yi Dong", "Oleksii Kuchaiev"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages, 2 figures, Accepted to ACL 2025 Main", "summary": "Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect\nHelpSteer3 data to train dedicated Feedback and Edit Models that are capable of\nperforming inference-time scaling for open-ended general-domain tasks. In our\nsetup, one model generates an initial response, which are given feedback by a\nsecond model, that are then used by a third model to edit the response. We show\nthat performance on Arena Hard, a benchmark strongly predictive of Chatbot\nArena Elo can be boosted by scaling the number of initial response drafts,\neffective feedback and edited responses. When scaled optimally, our setup based\non 70B models from the Llama 3 family can reach SoTA performance on Arena Hard\nat 92.7 as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.", "AI": {"tldr": "The paper introduces HelpSteer3 data and models for inference-time scaling in open-ended tasks, demonstrating improved performance on complex benchmarks by optimizing feedback and edits.", "motivation": "To enhance model performance on open-ended tasks where traditional inference-time scaling techniques are limited due to the need for verifiable answers.", "method": "The paper proposes a three-model setup: one model generates responses, a second provides feedback, and a third edits the responses, utilizing the HelpSteer3 dataset to train these models.", "result": "The proposed setup achieved state-of-the-art performance on the Arena Hard benchmark, outperforming existing models like OpenAI o1 and DeepSeek R1 when optimally scaled.", "conclusion": "By effectively leveraging feedback and iterative improvements in model responses, the methodology significantly advances inference-time scaling for open-ended general-domain tasks.", "key_contributions": ["Introduction of HelpSteer3 dataset for feedback-based training", "A novel three-model architecture for scaling responses via feedback and edits", "Achieving state-of-the-art performance on a competitive benchmark."], "limitations": "", "keywords": ["Inference-Time Scaling", "Feedback Models", "Open-Ended Tasks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.05268", "pdf": "https://arxiv.org/pdf/2503.05268.pdf", "abs": "https://arxiv.org/abs/2503.05268", "title": "ZOGRASCOPE: A New Benchmark for Semantic Parsing over Property Graphs", "authors": ["Francesco Cazzaro", "Justin Kleindienst", "Sofia Marquez Gomez", "Ariadna Quattoni"], "categories": ["cs.CL"], "comment": null, "summary": "In recent years, the need for natural language interfaces to knowledge graphs\nhas become increasingly important since they enable easy and efficient access\nto the information contained in them. In particular, property graphs (PGs) have\nseen increased adoption as a means of representing complex structured\ninformation. Despite their growing popularity in industry, PGs remain\nrelatively underrepresented in semantic parsing research with a lack of\nresources for evaluation. To address this gap, we introduce ZOGRASCOPE, a\nbenchmark designed specifically for PGs and queries written in Cypher. Our\nbenchmark includes a diverse set of manually annotated queries of varying\ncomplexity and is organized into three partitions: iid, compositional and\nlength. We complement this paper with a set of experiments that test the\nperformance of different LLMs in a variety of learning settings.", "AI": {"tldr": "This paper presents ZOGRASCOPE, a benchmark for property graphs (PGs) aimed at improving natural language interfaces and semantic parsing research.", "motivation": "There is a need for natural language interfaces to knowledge graphs, particularly property graphs, due to their rising use and the challenges in semantic parsing research.", "method": "The paper introduces ZOGRASCOPE, a benchmark with a diverse set of manually annotated Cypher queries, organized into three partitions based on complexity characteristics: iid, compositional, and length. It includes experiments assessing various LLMs' performance in learning settings.", "result": "Different LLMs were tested and performance varied across different query complexities, indicating strengths and weaknesses in handling property graph data.", "conclusion": "The introduction of ZOGRASCOPE fills a gap in the evaluation of natural language interfaces for property graphs and aids in semantic parsing research.", "key_contributions": ["Introduction of ZOGRASCOPE benchmark for property graphs", "A diverse set of annotated queries for better evaluation", "Experimental results on LLM performance regarding property graph queries"], "limitations": "", "keywords": ["natural language processing", "knowledge graphs", "property graphs", "semantic parsing", "benchmarking"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.07067", "pdf": "https://arxiv.org/pdf/2503.07067.pdf", "abs": "https://arxiv.org/abs/2503.07067", "title": "DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs", "authors": ["Jongwoo Ko", "Tianyi Chen", "Sungnyun Kim", "Tianyu Ding", "Luming Liang", "Ilya Zharkov", "Se-Young Yun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML2025 Spotlight", "summary": "Despite the success of distillation in large language models (LLMs), most\nprior work applies identical loss functions to both teacher- and\nstudent-generated data. These strategies overlook the synergy between loss\nformulations and data types, leading to a suboptimal performance boost in\nstudent models. To address this, we propose DistiLLM-2, a contrastive approach\nthat simultaneously increases the likelihood of teacher responses and decreases\nthat of student responses by harnessing this synergy. Our extensive experiments\nshow that DistiLLM-2 not only builds high-performing student models across a\nwide range of tasks, including instruction-following and code generation, but\nalso supports diverse applications, such as preference alignment and\nvision-language extensions. These findings highlight the potential of a\ncontrastive approach to enhance the efficacy of LLM distillation by effectively\naligning teacher and student models across varied data types.", "AI": {"tldr": "DistiLLM-2 is a contrastive approach for distilling large language models that enhances student model performance by leveraging the synergy between loss formulations and data types.", "motivation": "To improve the performance of student models in LLM distillation by utilizing different loss functions for teacher- and student-generated data.", "method": "A contrastive approach that increases teacher response likelihood while decreasing student response likelihood, optimizing the alignment between teacher and student models.", "result": "DistiLLM-2 significantly boosts student model performance across various tasks including instruction-following and code generation, and supports applications like preference alignment and vision-language tasks.", "conclusion": "The results demonstrate that a contrastive loss approach effectively enhances LLM distillation efficacy through improved alignment of teacher and student models.", "key_contributions": ["Introduction of DistiLLM-2, a novel contrastive approach to LLM distillation.", "Demonstration of improved student model performance across multiple tasks.", "Support for diverse application areas including preference alignment and vision-language tasks."], "limitations": "", "keywords": ["LLM", "distillation", "contrastive approach", "student models", "teacher models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.15289", "pdf": "https://arxiv.org/pdf/2503.15289.pdf", "abs": "https://arxiv.org/abs/2503.15289", "title": "TROVE: A Challenge for Fine-Grained Text Provenance via Source Sentence Tracing and Relationship Classification", "authors": ["Junnan Zhu", "Min Xiao", "Yining Wang", "Feifei Zhai", "Yu Zhou", "Chengqing Zong"], "categories": ["cs.CL"], "comment": "To appear in ACL 2025 (Main)", "summary": "LLMs have achieved remarkable fluency and coherence in text generation, yet\ntheir widespread adoption has raised concerns about content reliability and\naccountability. In high-stakes domains, it is crucial to understand where and\nhow the content is created. To address this, we introduce the Text pROVEnance\n(TROVE) challenge, designed to trace each sentence of a target text back to\nspecific source sentences within potentially lengthy or multi-document inputs.\nBeyond identifying sources, TROVE annotates the fine-grained relationships\n(quotation, compression, inference, and others), providing a deep understanding\nof how each target sentence is formed. To benchmark TROVE, we construct our\ndataset by leveraging three public datasets covering 11 diverse scenarios\n(e.g., QA and summarization) in English and Chinese, spanning source texts of\nvarying lengths (0-5k, 5-10k, 10k+), emphasizing the multi-document and\nlong-document settings essential for provenance. To ensure high-quality data,\nwe employ a three-stage annotation process: sentence retrieval, GPT-4o\nprovenance, and human provenance. We evaluate 11 LLMs under direct prompting\nand retrieval-augmented paradigms, revealing that retrieval is essential for\nrobust performance, larger models perform better in complex relationship\nclassification, and closed-source models often lead, yet open-source models\nshow significant promise, particularly with retrieval augmentation. We make our\ndataset available here: https://github.com/ZNLP/ZNLP-Dataset.", "AI": {"tldr": "The paper introduces the Text pROVEnance (TROVE) challenge to trace sentence origins in generated texts, emphasizing the importance of content reliability in high-stakes settings.", "motivation": "The widespread adoption of LLMs has raised concerns about content reliability and accountability, necessitating a method to trace and understand the creation of generated content.", "method": "The TROVE dataset is constructed from three public datasets covering 11 diverse scenarios, with a three-stage annotation process involving sentence retrieval and both automated and human provenance assessment.", "result": "Evaluation of 11 LLMs shows that retrieval is crucial for performance, larger models excel at complex relationship classification, and open-source models show promising results with retrieval support.", "conclusion": "The findings highlight the importance of provenance tracking in LLM outputs and suggest avenues for improving model transparency and reliability.", "key_contributions": ["Introduction of the TROVE challenge for text provenance tracking", "Creation of a comprehensive dataset for evaluating text generation provenance", "Evaluation results providing insights into LLM performance related to retrieval tasks"], "limitations": "", "keywords": ["Text Provovenance", "Long-Document Generation", "Language Models", "Data Annotation", "NLU"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.18491", "pdf": "https://arxiv.org/pdf/2503.18491.pdf", "abs": "https://arxiv.org/abs/2503.18491", "title": "MAGIC-VQA: Multimodal And Grounded Inference with Commonsense Knowledge for Visual Question Answering", "authors": ["Shuo Yang", "Siwen Luo", "Soyeon Caren Han", "Eduard Hovy"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Visual Question Answering (VQA) requires reasoning across visual and textual\nmodalities, yet Large Vision-Language Models (LVLMs) often lack integrated\ncommonsense knowledge, limiting their robustness in real-world scenarios. To\naddress this, we introduce MAGIC-VQA, a novel framework that enhances VQA by\nsystematically integrating commonsense knowledge with LVLMs. MAGIC-VQA employs\na three-stage process: (1) Explicit Knowledge Integration from external\nsources, (2) By-Type Post-Processing for contextual refinement, and (3)\nImplicit Knowledge Augmentation using a Graph Neural Network (GNN) for\nstructured reasoning. While GNNs bring greater depth to structured inference,\nthey enable superior relational inference beyond LVLMs. MAGIC-VQA bridges a key\ngap by unifying commonsensse knowledge with LVLM-driven reasoning, eliminating\nthe need for extensive pre-training or complex prompt tuning. Our framework\nachieves state-of-the-art performance on benchmark datasets, significantly\nimproving commonsense reasoning in VQA.", "AI": {"tldr": "MAGIC-VQA is a framework that combines commonsense knowledge with Large Vision-Language Models to enhance Visual Question Answering (VQA).", "motivation": "The integration of commonsense knowledge is crucial for improving the robustness of VQA systems based on Large Vision-Language Models (LVLMs), which often exhibit limitations in real-world scenarios.", "method": "MAGIC-VQA uses a three-stage process: explicit knowledge integration from external sources, contextual refinement through by-type post-processing, and implicit knowledge augmentation via a Graph Neural Network for structured reasoning.", "result": "The framework achieves state-of-the-art performance on benchmark datasets, demonstrating significant improvements in commonsense reasoning in Visual Question Answering tasks.", "conclusion": "MAGIC-VQA effectively unifies commonsense knowledge with LVLM-driven reasoning without requiring extensive pre-training or complex prompt tuning.", "key_contributions": ["Development of the MAGIC-VQA framework that integrates commonsense knowledge with VQA tasks.", "A three-stage process for enhancing reasoning capabilities in LVLMs.", "Achievement of state-of-the-art performance on VQA benchmark datasets."], "limitations": "", "keywords": ["Visual Question Answering", "Large Vision-Language Models", "Commonsense Knowledge", "Graph Neural Network", "Reasoning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.22353", "pdf": "https://arxiv.org/pdf/2503.22353.pdf", "abs": "https://arxiv.org/abs/2503.22353", "title": "Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions", "authors": ["Yubo Li", "Yidi Miao", "Xueying Ding", "Ramayya Krishnan", "Rema Padman"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Large Language Models (LLMs) have shown remarkable capabilities across\nvarious tasks, but their deployment in high-stake domains requires consistent\nand coherent behavior across multiple rounds of user interaction. This paper\nintroduces a comprehensive framework for evaluating and improving LLM response\nconsistency, making three key contributions. Code and data are available at:\nhttps://github.com/yubol-bobo/MT-Consistency. First, we introduce\nPosition-Weighted Consistency (PWC), a metric designed to capture both the\nimportance of early-stage stability and recovery patterns in multi-turn\ninteractions. Second, we present MT-Consistency, a carefully curated benchmark\ndataset spanning diverse domains and difficulty levels, specifically designed\nto evaluate LLM consistency under various challenging follow-up scenarios.\nThird, we introduce Confidence-Aware Response Generation (CARG), a framework\nthat significantly improves response stability by explicitly integrating\ninternal model confidence scores during the generation process. Experimental\nresults demonstrate that CARG significantly improves response stability without\nsacrificing accuracy, offering a practical path toward more dependable LLM\nbehavior in critical, real-world deployments.", "AI": {"tldr": "This paper presents a framework for evaluating and enhancing the consistency of LLM responses through a new metric, a benchmark dataset, and a response generation method.", "motivation": "Ensuring consistent and coherent behavior of LLMs in high-stake domains is crucial for reliable user interactions.", "method": "The authors introduce Position-Weighted Consistency (PWC) as a metric, a benchmark dataset called MT-Consistency for evaluating multi-turn interactions, and Confidence-Aware Response Generation (CARG) for improved response stability.", "result": "Experimental results indicate that CARG enhances response stability significantly while maintaining accuracy, making LLMs more dependable for real-world applications.", "conclusion": "The proposed methods offer practical solutions for achieving more consistent LLM behavior, essential for critical deployments.", "key_contributions": ["Position-Weighted Consistency (PWC) metric for multi-turn interaction stability.", "MT-Consistency benchmark dataset for evaluating LLM response consistency.", "Confidence-Aware Response Generation (CARG) framework to improve response stability."], "limitations": "", "keywords": ["Large Language Models", "Response Consistency", "Human-Computer Interaction", "Machine Learning", "Health Informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2504.04042", "pdf": "https://arxiv.org/pdf/2504.04042.pdf", "abs": "https://arxiv.org/abs/2504.04042", "title": "An Explicit Syllogistic Legal Reasoning Framework for Large Language Models", "authors": ["Kepu Zhang", "Weijie Yu", "Zhongxiang Sun", "Jun Xu"], "categories": ["cs.CL"], "comment": null, "summary": "Syllogistic reasoning is crucial for sound legal decision-making, allowing\nlegal professionals to draw logical conclusions by applying general principles\nto specific case facts. While large language models (LLMs) can answer legal\nquestions, they often struggle with explicit syllogistic reasoning. Their\noutputs tend to be implicit, unstructured, and consequently, less explainable\nand trustworthy. To overcome these limitations, we introduce SyLeR, a novel\nframework designed to enable LLMs to perform explicit syllogistic legal\nreasoning. SyLeR employs a tree-structured hierarchical retrieval mechanism to\nsynthesize relevant legal statutes and precedents, thereby constructing\ncomprehensive major premises. This is followed by a two-stage fine-tuning\nprocess: an initial supervised fine-tuning warm-up establishes a foundational\nunderstanding of syllogistic reasoning, while reinforcement learning, guided by\na structure-aware reward mechanism, refines the model's capacity to generate\ndiverse, logically sound, and well-structured reasoning paths. We conducted\nextensive experiments to evaluate SyLeR's performance. Our evaluations spanned\ndiverse dimensions, including both in-domain and cross-domain user groups\n(legal laypersons and practitioners), multiple languages (Chinese and French),\nand various LLM backbones (legal-specific and open-domain LLMs). The results\nconsistently demonstrate that SyLeR significantly enhances response accuracy\nand reliably produces explicit, explainable, and trustworthy legal reasoning.", "AI": {"tldr": "SyLeR is a framework designed to enhance large language models' capability in performing explicit syllogistic reasoning for legal decision-making.", "motivation": "To address the limitations of large language models in providing explicit, structured, and trustworthy syllogistic reasoning in legal contexts.", "method": "SyLeR uses a tree-structured hierarchical retrieval system to gather relevant legal materials and applies a two-stage fine-tuning process, which includes supervised fine-tuning and reinforcement learning with a structure-aware reward mechanism.", "result": "SyLeR improves response accuracy and consistently produces explicit and explainable legal reasoning across various user groups and languages.", "conclusion": "The extensive experiments show that SyLeR significantly enhances legal reasoning capabilities of LLMs, making them more reliable for legal applications.", "key_contributions": ["Introduction of SyLeR framework for explicit syllogistic legal reasoning", "Tree-structured hierarchical retrieval mechanism for gathering legal statutes", "Two-stage fine-tuning process combining supervised training and reinforcement learning"], "limitations": "", "keywords": ["syllogistic reasoning", "large language models", "legal decision-making", "explainable AI", "reinforcement learning"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2504.07282", "pdf": "https://arxiv.org/pdf/2504.07282.pdf", "abs": "https://arxiv.org/abs/2504.07282", "title": "RAISE: Reinforced Adaptive Instruction Selection For Large Language Models", "authors": ["Lv Qingsong", "Yangning Li", "Zihua Lan", "Zishan Xu", "Jiwei Tang", "Yinghui Li", "Wenhao Jiang", "Hai-Tao Zheng", "Philip S. Yu"], "categories": ["cs.CL"], "comment": null, "summary": "In the instruction fine-tuning of large language models (LLMs), it is widely\nrecognized that a few high-quality instructions are superior to a large number\nof low-quality instructions. At present, many instruction selection methods\nhave been proposed, but most of these methods select instruction based on\nheuristic quality metrics, and only consider data selection before training.\nThese designs lead to insufficient optimization of instruction fine-tuning, and\nfixed heuristic indicators are often difficult to optimize for specific tasks.\nTherefore, we design a dynamic, task-objective-driven instruction selection\nframework RAISE(Reinforced Adaptive Instruction SElection), which incorporates\nthe entire instruction fine-tuning process into optimization, selecting\ninstructions at each step based on the expected impact of each instruction on\nmodel performance improvement. Our approach is well interpretable and has\nstrong task-specific optimization capabilities. By modeling dynamic instruction\nselection as a sequential decision-making process, we use RL to train our\nselection strategy. Extensive experiments and result analysis prove the\nsuperiority of our method compared with other instruction selection methods.\nNotably, RAISE achieves superior performance by updating only 1% of the\ntraining steps compared to full-data training, demonstrating its efficiency and\neffectiveness.", "AI": {"tldr": "RAISE is a dynamic instruction selection framework for fine-tuning large language models, optimizing instructions throughout the training process using reinforcement learning.", "motivation": "To improve the instruction fine-tuning of large language models by moving beyond heuristic quality metrics and enabling task-specific optimization of instructions.", "method": "RAISE dynamically selects instructions during the entire fine-tuning process based on their expected impact on model performance, using reinforcement learning for strategy training.", "result": "RAISE shows superior performance against existing instruction selection methods by requiring updates to only 1% of the training steps versus full-data training, highlighting its efficiency and effectiveness.", "conclusion": "The proposed framework demonstrates strong interpretability and task-specific optimization capabilities, advancing the state of instruction fine-tuning.", "key_contributions": ["Introduction of RAISE, a dynamic instruction selection framework using RL", "Optimization of instruction selection throughout the fine-tuning process", "Demonstration of efficiency with minimal training step updates"], "limitations": "", "keywords": ["instruction fine-tuning", "reinforcement learning", "large language models", "dynamic selection", "task-specific optimization"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2504.08120", "pdf": "https://arxiv.org/pdf/2504.08120.pdf", "abs": "https://arxiv.org/abs/2504.08120", "title": "DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?", "authors": ["Daniil Larionov", "Sotaro Takeshita", "Ran Zhang", "Yanran Chen", "Christoph Leiter", "Zhipin Wang", "Christian Greisinger", "Steffen Eger"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enabled large language models (LLMs) excel in logical tasks, yet\ntheir utility for evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning LLMs with non-reasoning\ncounterparts across machine translation and text summarization evaluation\ntasks. We evaluate eight models spanning state-of-the-art reasoning models\n(DeepSeek-R1, OpenAI o3), their distilled variants (8B-70B parameters), and\nequivalent non-reasoning LLMs. Experiments on WMT23 and SummEval benchmarks\nreveal architecture and task-dependent benefits: OpenAI o3-mini models show\nimproved performance with increased reasoning on MT, while DeepSeek-R1 and\ngenerally underperforms compared to its non-reasoning variant except in\nsummarization consistency evaluation. Correlation analysis demonstrates that\nreasoning token usage correlates with evaluation quality only in specific\nmodels, while almost all models generally allocate more reasoning tokens when\nidentifying more quality issues. Distillation maintains reasonable performance\nup to 32B parameter models but degrades substantially at 8B scale. This work\nprovides the first assessment of reasoning LLMs for NLG evaluation and\ncomparison to non-reasoning models. We share our code to facilitate further\nresearch: https://github.com/NL2G/reasoning-eval.", "AI": {"tldr": "This study compares reasoning-enabled and non-reasoning large language models across machine translation and text summarization tasks, revealing nuanced performance benefits based on architecture and task type.", "motivation": "To explore the utility of reasoning-enabled LLMs in evaluating natural language generation, which has not been systematically investigated before.", "method": "Comparison of eight models, including state-of-the-art reasoning models and their distilled versions, on WMT23 and SummEval benchmarks.", "result": "OpenAI o3-mini models improved performance in machine translation with more reasoning, while DeepSeek-R1 generally underperformed compared to non-reasoning variants, except in summarization consistency evaluation.", "conclusion": "Reasoning LLMs have specific advantages in NLG evaluation, but their performance varies based on model type and task; recommendations for further research are provided along with shared code.", "key_contributions": ["First assessment of reasoning LLMs for NLG evaluation", "Systematic comparison of reasoning and non-reasoning models", "Insights on model architecture and parameter scaling effects on performance"], "limitations": "No extensive exploration of reasoning LLMs in other NLG tasks beyond translation and summarization is provided.", "keywords": ["reasoning LLMs", "NLG evaluation", "machine translation", "text summarization", "model comparison"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.10792", "pdf": "https://arxiv.org/pdf/2504.10792.pdf", "abs": "https://arxiv.org/abs/2504.10792", "title": "GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction", "authors": ["Jessica Lin", "Amir Zeldes"], "categories": ["cs.CL"], "comment": "Camera-ready for ACL Findings 2025", "summary": "Determining and ranking the most salient entities in a text is critical for\nuser-facing systems, especially as users increasingly rely on models to\ninterpret long documents they only partially read. Graded entity salience\naddresses this need by assigning entities scores that reflect their relative\nimportance in a text. Existing approaches fall into two main categories:\nsubjective judgments of salience, which allow for gradient scoring but lack\nconsistency, and summarization-based methods, which define salience as\nmention-worthiness in a summary, promoting explainability but limiting outputs\nto binary labels (entities are either summary-worthy or not). In this paper, we\nintroduce a novel approach for graded entity salience that combines the\nstrengths of both approaches. Using an English dataset spanning 12 spoken and\nwritten genres, we collect 5 summaries per document and calculate each entity's\nsalience score based on its presence across these summaries. Our approach shows\nstronger correlation with scores based on human summaries and alignments, and\noutperforms existing techniques, including LLMs. We release our data and code\nat https://github.com/jl908069/gum_sum_salience to support further research on\ngraded salient entity extraction.", "AI": {"tldr": "This paper introduces a novel method for determining graded entity salience in text, combining subjective judgments and summarization-based approaches to assign relative importance scores to entities.", "motivation": "To improve how user-facing systems interpret long documents by determining and ranking the salience of entities.", "method": "A novel approach that calculates salience scores for entities based on their presence across multiple summaries of documents.", "result": "The proposed method shows stronger correlation with human-generated scores and outperforms existing techniques, including LLMs, in grading entity salience.", "conclusion": "The study establishes a new benchmark for graded entity salience extraction and provides resources for further research.", "key_contributions": ["Introduces a novel graded entity salience approach combining strengths of two existing methodologies.", "Demonstrates superior performance over existing techniques.", "Provides publicly available data and code for reproducibility and further research."], "limitations": "", "keywords": ["entity salience", "natural language processing", "summarization"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2504.11277", "pdf": "https://arxiv.org/pdf/2504.11277.pdf", "abs": "https://arxiv.org/abs/2504.11277", "title": "From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs", "authors": ["Guocong Li", "Weize Liu", "Yihang Wu", "Ping Wang", "Shuaihan Huang", "Hongxia Xu", "Jian Wu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large language models (LLMs) exhibit excellent performance in natural\nlanguage processing (NLP), but remain highly sensitive to the quality of input\nqueries, especially when these queries contain misleading or inaccurate\ninformation. Existing methods focus on correcting the output, but they often\noverlook the potential of improving the ability of LLMs to detect and correct\nmisleading content in the input itself. In this paper, we propose a novel\nthree-stage fine-tuning method that enhances the ability of LLMs to detect and\ncorrect misleading information in the input, further improving response\naccuracy and reducing hallucinations. Specifically, the three stages include\n(1) training LLMs to identify misleading information, (2) training LLMs to\ncorrect the misleading information using built-in or external knowledge, and\n(3) training LLMs to generate accurate answers based on the corrected queries.\nTo evaluate our method, we conducted experiments on three datasets for the\nhallucination detection task and the question answering~(QA) task, as well as\ntwo datasets containing misleading information that we constructed. The\nexperimental results demonstrate that our method significantly improves the\naccuracy and factuality of LLM responses, while also enhancing the ability to\ndetect hallucinations and reducing the generation of hallucinations in the\noutput, particularly when the query contains misleading information.", "AI": {"tldr": "A novel three-stage fine-tuning method improves large language models' ability to detect and correct misleading information in input queries, enhancing response accuracy and reducing hallucinations.", "motivation": "To address the issue of large language models being sensitive to misleading or inaccurate input queries, focusing on enhancing their detection and correction abilities rather than just correcting outputs.", "method": "The proposed method involves a three-stage fine-tuning process: (1) identifying misleading information, (2) correcting the misleading information, and (3) generating accurate answers based on corrected queries.", "result": "Experiments on multiple datasets for hallucination detection and question answering show significant improvements in accuracy, factuality, and reduction of hallucinations in LLM responses.", "conclusion": "The method enhances the LLM's capability to handle misleading information, resulting in better quality responses.", "key_contributions": ["Novel three-stage fine-tuning approach for LLMs", "Improved accuracy and factuality of responses", "Enhanced ability to detect and reduce hallucinations"], "limitations": "", "keywords": ["large language models", "misleading information", "fine-tuning", "hallucination detection", "question answering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2504.15815", "pdf": "https://arxiv.org/pdf/2504.15815.pdf", "abs": "https://arxiv.org/abs/2504.15815", "title": "What's the Difference? Supporting Users in Identifying the Effects of Prompt and Model Changes Through Token Patterns", "authors": ["Michael A. Hedderich", "Anyi Wang", "Raoyuan Zhao", "Florian Eichin", "Jonas Fischer", "Barbara Plank"], "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted at ACL'25", "summary": "Prompt engineering for large language models is challenging, as even small\nprompt perturbations or model changes can significantly impact the generated\noutput texts. Existing evaluation methods of LLM outputs, either automated\nmetrics or human evaluation, have limitations, such as providing limited\ninsights or being labor-intensive. We propose Spotlight, a new approach that\ncombines both automation and human analysis. Based on data mining techniques,\nwe automatically distinguish between random (decoding) variations and\nsystematic differences in language model outputs. This process provides token\npatterns that describe the systematic differences and guide the user in\nmanually analyzing the effects of their prompts and changes in models\nefficiently. We create three benchmarks to quantitatively test the reliability\nof token pattern extraction methods and demonstrate that our approach provides\nnew insights into established prompt data. From a human-centric perspective,\nthrough demonstration studies and a user study, we show that our token pattern\napproach helps users understand the systematic differences of language model\noutputs. We are further able to discover relevant differences caused by prompt\nand model changes (e.g. related to gender or culture), thus supporting the\nprompt engineering process and human-centric model behavior research.", "AI": {"tldr": "Spotlight is a novel approach to prompt engineering for large language models, combining automated data mining techniques with human analysis to identify systematic differences in model outputs.", "motivation": "The challenges in prompt engineering for large language models arise from the significant effects of small prompt changes or model variations on output texts.", "method": "Spotlight uses data mining techniques to distinguish between random variations and systematic differences in language model outputs, providing token patterns to guide manual analysis.", "result": "The approach allows for the extraction of token patterns that reveal systematic differences and supports users in understanding the effects of prompt and model changes.", "conclusion": "Spotlight enhances prompt engineering by providing quantitative benchmarks for token pattern extraction and new insights into modeled outputs from human-centric studies.", "key_contributions": ["Introduction of the Spotlight framework for prompt analysis", "Development of benchmarks for evaluating token pattern extraction", "Demonstration of systematic differences in LLM outputs related to prompts and model changes"], "limitations": "The paper does not address potential scaling issues or the adaptability of the proposed methods to all language models.", "keywords": ["prompt engineering", "large language models", "data mining", "human-centered AI", "systematic differences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23688", "pdf": "https://arxiv.org/pdf/2505.23688.pdf", "abs": "https://arxiv.org/abs/2505.23688", "title": "Automatic classification of stop realisation with wav2vec2.0", "authors": ["James Tanner", "Morgan Sonderegger", "Jane Stuart-Smith", "Jeff Mielke", "Tyler Kendall"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for Interspeech 2025. 5 pages, 3 figures", "summary": "Modern phonetic research regularly makes use of automatic tools for the\nannotation of speech data, however few tools exist for the annotation of many\nvariable phonetic phenomena. At the same time, pre-trained self-supervised\nmodels, such as wav2vec2.0, have been shown to perform well at speech\nclassification tasks and latently encode fine-grained phonetic information. We\ndemonstrate that wav2vec2.0 models can be trained to automatically classify\nstop burst presence with high accuracy in both English and Japanese, robust\nacross both finely-curated and unprepared speech corpora. Patterns of\nvariability in stop realisation are replicated with the automatic annotations,\nand closely follow those of manual annotations. These results demonstrate the\npotential of pre-trained speech models as tools for the automatic annotation\nand processing of speech corpus data, enabling researchers to 'scale-up' the\nscope of phonetic research with relative ease.", "AI": {"tldr": "This paper illustrates the use of wav2vec2.0 models for the automatic classification of stop burst presence in English and Japanese speech, showing high accuracy and potential for phonetic research.", "motivation": "To enhance the annotation of variable phonetic phenomena in modern phonetic research, particularly using automatic tools.", "method": "Wav2vec2.0 models were trained to classify the presence of stop bursts in speech data, tested on both curated and unprepared corpora in English and Japanese.", "result": "The models achieved high accuracy in classifying stop bursts, with patterns of variability aligning closely with manual annotations.", "conclusion": "Pre-trained speech models like wav2vec2.0 can serve as effective tools for the automatic annotation of speech corpora, allowing researchers to expand their phonetic research capabilities significantly.", "key_contributions": ["Demonstrated the application of wav2vec2.0 for phonetic annotation tasks.", "Showed high accuracy in classifying stop burst presence across languages and corpus types.", "Established a link between automatic and manual annotation patterns in phonetic research."], "limitations": "", "keywords": ["phonetic research", "wav2vec2.0", "speech classification", "automatic annotation", "stop burst"], "importance_score": 4, "read_time_minutes": 5}}
