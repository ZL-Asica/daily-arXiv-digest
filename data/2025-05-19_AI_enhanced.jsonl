{"id": "2505.10648", "pdf": "https://arxiv.org/pdf/2505.10648.pdf", "abs": "https://arxiv.org/abs/2505.10648", "title": "Generative Muscle Stimulation: Physical Assistance by Constraining Multimodal-AI with Biomechanical Knowledge", "authors": ["Yun Ho", "Romain Nith", "Peili Jiang", "Shan-Yuan Teng", "Pedro Lopes"], "categories": ["cs.HC"], "comment": "13 pages, 19 figures", "summary": "Decades of interactive electrical-muscle-stimulation (EMS) revealed its\npromise as a wearable interface for physical assistance-EMS directly\ndemonstrates movements through the users' body (e.g., shaking a spray-can\nbefore painting). However, interactive EMS-systems are highly-specialized\nbecause their feedback is (1) fixed (e.g., one program executes spray-can\ninstructions, another executes piano instructions) and (2) non-contextual\n(e.g., using a spray-can while cooking likely involves cooking oil, not paint,\nand thus shaking is unnecessary). To address this, we explored a more flexible\napproach and engineered a system that generates muscle-stimulation-instructions\ngiven the user's context. Through our examples, we show that such a system is\nflexible: it enables unprecedented EMS-interactions (e.g., opening a\nchild-proof pill bottle cap) but also replicates existing systems (e.g., shake\na spray can)-all without requiring task-specific programming. To achieve this,\nour system takes in user's spoken-requests and images from their point of view.\nIt uses computer vision (e.g., detect objects/handedness) and\nlarge-language-models (e.g., reason about objects/situations) to generate\ntextual-instructions. Finally, these instructions are then constrained by\nbiomechanical-knowledge (e.g., joint limits, kinematic-chain, EMS capabilities)\nto produce suitable muscle-stimulation gestures. We believe our concept marks a\nshift toward more general-purpose EMS-interfaces, enabling more flexible and\ncontext-aware assistance.", "AI": {"tldr": "This paper presents a flexible interactive electrical-muscle-stimulation (EMS) system that generates muscle stimulation instructions based on user context, enabling task flexibility without task-specific programming.", "motivation": "Current EMS systems are highly specialized and non-contextual, limiting their applicability in dynamic situations. This work aims to create a more versatile EMS interface that can adapt to users' needs in real-time.", "method": "The developed system uses spoken requests and images from the user's perspective, employing computer vision for object detection and large language models for reasoning, combined with biomechanical knowledge to create suitable muscle stimulation instructions.", "result": "The system successfully generates context-aware muscle stimulation interactions, allowing for new capabilities (like opening child-proof bottles) while also replicating existing interactions without specific programming.", "conclusion": "This research indicates a significant advancement toward general-purpose EMS interfaces that accommodate contextual variations and enhance user assistance.", "key_contributions": ["Development of a flexible EMS system that adapts to user context", "Integration of computer vision and large language models for instruction generation", "Capacity to perform both novel and existing tasks without specific programming"], "limitations": "The system may face challenges in accurately interpreting complex user contexts and ensuring safe muscle stimulation at all times.", "keywords": ["HCI", "Electrical muscle stimulation", "Context-aware systems", "Machine learning", "Wearable technology"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.10661", "pdf": "https://arxiv.org/pdf/2505.10661.pdf", "abs": "https://arxiv.org/abs/2505.10661", "title": "It's only fair when I think it's fair: How Gender Bias Alignment Undermines Distributive Fairness in Human-AI Collaboration", "authors": ["Domenique Zipperling", "Luca Deck", "Julia Lanzl", "Niklas Kühl"], "categories": ["cs.HC"], "comment": "Accepted to ACM FAccT 2025", "summary": "Human-AI collaboration is increasingly relevant in consequential areas where\nAI recommendations support human discretion. However, human-AI teams'\neffectiveness, capability, and fairness highly depend on human perceptions of\nAI. Positive fairness perceptions have been shown to foster trust and\nacceptance of AI recommendations. Yet, work on confirmation bias highlights\nthat humans selectively adhere to AI recommendations that align with their\nexpectations and beliefs -- despite not being necessarily correct or fair. This\nraises the question whether confirmation bias also transfers to the alignment\nof gender bias between human and AI decisions. In our study, we examine how\ngender bias alignment influences fairness perceptions and reliance. The results\nof a 2x2 between-subject study highlight the connection between gender bias\nalignment, fairness perceptions, and reliance, demonstrating that merely\nconstructing a ``formally fair'' AI system is insufficient for optimal human-AI\ncollaboration; ultimately, AI recommendations will likely be overridden if\nbiases do not align.", "AI": {"tldr": "The paper explores the impact of gender bias alignment on human-AI collaboration, specifically how perceptions of fairness influence reliance on AI recommendations.", "motivation": "To understand how gender bias in AI impacts human trust and acceptance of AI recommendations in consequential decision-making.", "method": "A 2x2 between-subject study examining the interaction between gender bias alignment and perceptions of fairness in human-AI teams.", "result": "Findings show that gender bias alignment affects fairness perceptions and reliance on AI, indicating that just designing a fair AI is not enough for effective collaboration.", "conclusion": "Human reliance on AI recommendations decreases if biases do not align, emphasizing the need for both fair AI systems and aligned human perceptions.", "key_contributions": ["Investigates the role of gender bias in human-AI collaboration", "Demonstrates the influence of fairness perceptions on AI reliance", "Highlights that formal fairness is insufficient without bias alignment"], "limitations": "Focuses solely on gender bias; other biases may also influence perceptions and reliance.", "keywords": ["Human-AI collaboration", "Gender bias", "Fairness perceptions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10686", "pdf": "https://arxiv.org/pdf/2505.10686.pdf", "abs": "https://arxiv.org/abs/2505.10686", "title": "NeoLightning: A Modern Reimagination of Gesture-Based Sound Design", "authors": ["Yonghyun Kim", "Sangheon Park", "Marcus Parker", "Donghoon Seu", "Alexandria Smith"], "categories": ["cs.HC", "cs.MM", "cs.SD", "eess.AS"], "comment": "Accepted to the 50th International Computer Music Conference (ICMC),\n  2025", "summary": "This paper introduces NeoLightning, a modern reinterpretation of the Buchla\nLightning. NeoLightning preserves the innovative spirit of Don Buchla's \"Buchla\nLightning\" (introduced in the 1990s) while making its gesture-based interaction\naccessible to contemporary users. While the original Buchla Lightning and many\nother historical instruments were groundbreaking in their time, they are now\nlargely unsupported, limiting user interaction to indirect experiences. To\naddress this, NeoLightning leverages MediaPipe for deep learning-based gesture\nrecognition and employs Max/MSP and Processing for real-time multimedia\nprocessing. The redesigned system offers precise, low-latency gesture\nrecognition and immersive 3D interaction. By merging the creative spirit of the\noriginal Lightning with modern advancements, NeoLightning redefines\ngesture-based musical interaction, expanding possibilities for expressive\nperformance and interactive sound design.", "AI": {"tldr": "NeoLightning is a modern reinterpretation of the Buchla Lightning, enhancing gesture-based musical interaction with contemporary technology.", "motivation": "To make gesture-based interaction accessible to contemporary users while preserving the innovative spirit of the original Buchla Lightning.", "method": "Utilizes MediaPipe for deep learning-based gesture recognition, and employs Max/MSP and Processing for real-time multimedia processing.", "result": "Offers precise, low-latency gesture recognition and immersive 3D interaction for musical performance and sound design.", "conclusion": "NeoLightning expands possibilities for expressive performance and interactive sound design, redefining gesture-based musical interaction.", "key_contributions": ["Modern reinterpretation of Buchla Lightning", "Integration of deep learning for gesture recognition", "Real-time multimedia processing capabilities"], "limitations": "", "keywords": ["gesture recognition", "musical interaction", "multimedia processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831.pdf", "abs": "https://arxiv.org/abs/2505.10831", "title": "Creating General User Models from Computer Use", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture that user knowledge and preferences. GUMs can infer that a user is\npreparing for a wedding they're attending from messages with a friend. Or\nrecognize that a user is struggling with a collaborator's feedback on a draft\nby observing multiple stalled edits and a switch to reading related work. GUMs\nintroduce an architecture that infers new propositions about a user from\nmultimodal observations, retrieves related propositions for context, and\ncontinuously revises existing propositions. To illustrate the breadth of\napplications that GUMs enable, we demonstrate how they augment chat-based\nassistants with context, manage OS notifications to selectively surface\nimportant information, and enable interactive agents that adapt to preferences\nacross apps. We also instantiate proactive assistants (GUMBOs) that discover\nand execute useful suggestions on a user's behalf using their GUM. In our\nevaluations, we find that GUMs make calibrated and accurate inferences about\nusers, and that assistants built on GUMs proactively identify and perform\nactions that users wouldn't think to request explicitly. Altogether, GUMs\nintroduce methods that leverage multimodal models to understand unstructured\ncontext, enabling long-standing visions of HCI and entirely new interactive\nsystems that anticipate user needs.", "AI": {"tldr": "This paper presents a general user model (GUM) architecture that learns user preferences from multimodal observations to enhance human-computer interaction.", "motivation": "To address the limitations of current user models which are fragmented and unable to understand user preferences and habits cohesively.", "method": "The GUM architecture processes unstructured observations (like device screenshots) to create confidence-weighted propositions about user knowledge and preferences, revising these as more data is observed.", "result": "GUMs can accurately infer user intentions and struggles from interaction data, enabling proactive assistants that perform actions without explicit user requests.", "conclusion": "GUMs facilitate the development of interactive systems that anticipate user needs using multimodal context understanding, moving closer to the visions of seamless HCI.", "key_contributions": ["Introduction of the General User Model (GUM) architecture.", "Application of multimodal observations for user understanding.", "Development of proactive assistants (GUMBOs) that execute actions based on inferred user preferences."], "limitations": "", "keywords": ["Human-Computer Interaction", "General User Model", "Multimodal Observations", "Proactive Assistants", "Intelligent Systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.10643", "pdf": "https://arxiv.org/pdf/2505.10643.pdf", "abs": "https://arxiv.org/abs/2505.10643", "title": "Artificial Intelligence Bias on English Language Learners in Automatic Scoring", "authors": ["Shuchen Guo", "Yun Wang", "Jichao Yu", "Xuansheng Wu", "Bilgehan Ayik", "Field M. Watts", "Ehsan Latif", "Ninghao Liu", "Lei Liu", "Xiaoming Zhai"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study investigated potential scoring biases and disparities toward\nEnglish Language Learners (ELLs) when using automatic scoring systems for\nmiddle school students' written responses to science assessments. We\nspecifically focus on examining how unbalanced training data with ELLs\ncontributes to scoring bias and disparities. We fine-tuned BERT with four\ndatasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting\nthe real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced\nmixed dataset with equal representation of both groups. The study analyzed 21\nassessment items: 10 items with about 30,000 ELL responses, five items with\nabout 1,000 ELL responses, and six items with about 200 ELL responses. Scoring\naccuracy (Acc) was calculated and compared to identify bias using Friedman\ntests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and\nthen calculated the differences in MSGs generated through both the human and AI\nmodels to identify the scoring disparities. We found that no AI bias and\ndistorted disparities between ELLs and non-ELLs were found when the training\ndataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could\nexist if the sample size is limited (ELL = 200).", "AI": {"tldr": "The study examines scoring biases in automatic scoring systems for English Language Learners (ELLs) in science assessments, finding that large training datasets mitigate bias.", "motivation": "To investigate biases in automatic scoring systems towards English Language Learners (ELLs) and ensure fair assessment in educational contexts.", "method": "The study fine-tuned BERT using four datasets with varying proportions of ELL and non-ELL responses and analyzed scoring accuracy through Friedman tests.", "result": "The analysis showed no AI bias with larger training datasets, while smaller datasets raised concerns about disparities in scoring accuracy.", "conclusion": "Scoring systems can be unbiased and effective for ELLs if trained on sufficiently large datasets; caution is warranted for smaller datasets to avoid scoring disparities.", "key_contributions": ["Identification of scoring bias with ELLs in automatic systems", "Analysis impact of training data size on scoring fairness", "Quantitative measurements of Mean Score Gaps between ELLs and non-ELLs"], "limitations": "Concerns about scoring disparities when training datasets are small, specifically when ELL responses are limited in number.", "keywords": ["Automatic scoring", "English Language Learners", "BERT", "Scoring bias", "Education"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.10839", "pdf": "https://arxiv.org/pdf/2505.10839.pdf", "abs": "https://arxiv.org/abs/2505.10839", "title": "Alexandria: A Library of Pluralistic Values for Realtime Re-Ranking of Social Media Feeds", "authors": ["Akaash Kolluri", "Renn Su", "Farnaz Jahanbakhsh", "Dora Zhao", "Tiziano Piccardi", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.CY", "cs.SI"], "comment": null, "summary": "Social media feed ranking algorithms fail when they too narrowly focus on\nengagement as their objective. The literature has asserted a wide variety of\nvalues that these algorithms should account for as well -- ranging from\nwell-being to productive discourse -- far more than can be encapsulated by a\nsingle topic or theory. In response, we present a $\\textit{library of values}$\nfor social media algorithms: a pluralistic set of 78 values as articulated\nacross the literature, implemented into LLM-powered content classifiers that\ncan be installed individually or in combination for real-time re-ranking of\nsocial media feeds. We investigate this approach by developing a browser\nextension, $\\textit{Alexandria}$, that re-ranks the X/Twitter feed in real time\nbased on the user's desired values. Through two user studies, both qualitative\n(N=12) and quantitative (N=257), we found that diverse user needs require a\nlarge library of values, enabling more nuanced preferences and greater user\ncontrol. With this work, we argue that the values criticized as missing from\nsocial media ranking algorithms can be operationalized and deployed today\nthrough end-user tools.", "AI": {"tldr": "The paper presents a library of 78 values for social media algorithms, addressing the limitations of engagement-focused ranking by implementing LLM-powered content classifiers for personalized feed re-ranking via a browser extension called Alexandria.", "motivation": "To address the inadequacies of social media feed ranking algorithms that overly focus on engagement, the authors propose a comprehensive set of values that can enhance user experience and well-being.", "method": "Developed an extensive library of values and implemented it into LLM-powered classifiers which are integrated into a browser extension that re-ranks social media feeds based on user-selected values. The effectiveness was evaluated through qualitative and quantitative user studies.", "result": "User studies showed that a larger library of values allows for more nuanced user preferences and enhances control over content displayed in social media feeds.", "conclusion": "The study concludes that incorporating a diverse range of values into social media ranking algorithms is feasible and beneficial for users, enabling personalized content delivery aligned with their individual values.", "key_contributions": ["Creation of a library of 78 values for social media algorithms", "Development of the Alexandria browser extension for real-time feed re-ranking", "Demonstration of user engagement through qualitative and quantitative studies"], "limitations": "The study was limited to specific platforms and may not generalize across all social media contexts or user demographics.", "keywords": ["social media algorithms", "user values", "content re-ranking", "LLM", "browser extension"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10714", "pdf": "https://arxiv.org/pdf/2505.10714.pdf", "abs": "https://arxiv.org/abs/2505.10714", "title": "GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?", "authors": ["Bowen Jiang", "Yangxinyu Xie", "Xiaomeng Wang", "Jiashu He", "Joshua Bergerson", "John K Hutchison", "Jordan Branham", "Camillo J Taylor", "Tanwi Mallick"], "categories": ["cs.CL"], "comment": null, "summary": "We present GeoGrid-Bench, a benchmark designed to evaluate the ability of\nfoundation models to understand geo-spatial data in the grid structure.\nGeo-spatial datasets pose distinct challenges due to their dense numerical\nvalues, strong spatial and temporal dependencies, and unique multimodal\nrepresentations including tabular data, heatmaps, and geographic\nvisualizations. To assess how foundation models can support scientific research\nin this domain, GeoGrid-Bench features large-scale, real-world data covering 16\nclimate variables across 150 locations and extended time frames. The benchmark\nincludes approximately 3,200 question-answer pairs, systematically generated\nfrom 8 domain expert-curated templates to reflect practical tasks encountered\nby human scientists. These range from basic queries at a single location and\ntime to complex spatiotemporal comparisons across regions and periods. Our\nevaluation reveals that vision-language models perform best overall, and we\nprovide a fine-grained analysis of the strengths and limitations of different\nfoundation models in different geo-spatial tasks. This benchmark offers clearer\ninsights into how foundation models can be effectively applied to geo-spatial\ndata analysis and used to support scientific research.", "AI": {"tldr": "GeoGrid-Bench is a benchmark for evaluating foundation models on geo-spatial data, highlighting their performance and insights into spatiotemporal analysis.", "motivation": "To evaluate how foundation models can enhance understanding and analysis of geo-spatial data for scientific research.", "method": "GeoGrid-Bench comprises large-scale, real-world data including 16 climate variables across 150 locations, featuring 3,200 question-answer pairs generated from expert-curated templates.", "result": "Vision-language models outperform others in various geo-spatial tasks, demonstrating strengths and limitations in handling different aspects of geo-spatial data.", "conclusion": "The benchmark provides valuable insights into applying foundation models effectively for geo-spatial data analysis in scientific research.", "key_contributions": ["Introduction of GeoGrid-Bench for geo-spatial data benchmarking", "Evaluation of performance across various foundation models", "Insights into the applicability of models in scientific research contexts"], "limitations": "", "keywords": ["geo-spatial data", "benchmarking", "foundation models", "climate variables", "science research"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.10863", "pdf": "https://arxiv.org/pdf/2505.10863.pdf", "abs": "https://arxiv.org/abs/2505.10863", "title": "Conversations With The Stressed Body: Facilitating Stress Self-Disclosure Among Adolescent Girls Through An Embodied Approach", "authors": ["Xinglin Sun", "Caroline Claisse", "Runhua Zhang", "Xinyu Wu", "Jialin Yuan", "Qi Wang"], "categories": ["cs.HC"], "comment": null, "summary": "Adolescent girls face significant mental health challenges during their\ntransition to adulthood, often experiencing heightened stress from various\nsources. While various interactive technologies for self-disclosure had been\nexplored to support stress relief, little is known about how to encourage\nstress-related self-disclosure through an embodied approach. This study\npresents a co-design workshop centred on Embodied Probes, a series of artefacts\nand activities incorporating embodied methods and technologies. During the\nworkshop, nine participants aged 15 to 18 engaged with their bodies, expressed\nbodily sensations through tangible means, and designed embodied prototypes\ntailored to their personal needs for stress perception and relief. The workshop\nrevealed insights into somatic symptoms, sources, and coping strategies for\nstress among adolescent girls, as well as how embodied methods can support\ntheir stress self-disclosure. This paper contributes to the HCI community by\noffering design implications on leveraging embodied technologies to support\nself-disclosure for young women's mental well-being.", "AI": {"tldr": "This study explores how embodied methods and technologies can foster stress-related self-disclosure among adolescent girls, revealing coping strategies and insights into mental health challenges.", "motivation": "To address the significant mental health challenges faced by adolescent girls during their transition to adulthood, particularly focusing on stress relief through interactive technologies.", "method": "A co-design workshop was conducted where nine participants aged 15 to 18 interacted with Embodied Probes, engaging with their bodies to express sensations and design prototypes for stress management.", "result": "The workshop uncovered insights into somatic symptoms and coping strategies for stress, illustrating how embodied methods enhance stress self-disclosure.", "conclusion": "Embodied technologies can effectively support mental well-being by facilitating self-disclosure among young women, providing valuable design implications for the HCI community.", "key_contributions": ["Introduced Embodied Probes as a novel method for stress self-disclosure.", "Revealed tailored needs for stress perception and relief among adolescent girls.", "Provided design implications for using embodied methods in HCI research focused on mental well-being."], "limitations": "", "keywords": ["embodied methods", "self-disclosure", "stress relief", "adolescent girls", "HCI"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10717", "pdf": "https://arxiv.org/pdf/2505.10717.pdf", "abs": "https://arxiv.org/abs/2505.10717", "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment", "authors": ["Jean-Philippe Corbeil", "Amin Dada", "Jean-Michel Attendu", "Asma Ben Abacha", "Alessandro Sordoni", "Lucas Caccia", "François Beaulieu", "Thomas Lin", "Jens Kleesiek", "Paul Vozila"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average.", "AI": {"tldr": "The paper proposes the MediPhi framework for adapting small language models (SLMs) into high-performing clinical models, addressing challenges of computation costs and data sensitivity in healthcare settings.", "motivation": "High computation costs and latency of large language models limit their deployment in clinical settings; small language models need adaptation for effective use in healthcare.", "method": "The MediPhi framework utilizes pre-instruction tuning of experts on medical corpora, model merging, and alignment for clinical tasks. It introduces the CLUE+ benchmark and builds the MediFlow collection for training.", "result": "Expert models showed significant improvements over base models on the CLUE+ benchmark without task-specific fine-tuning, outperforming GPT-4-0125 in various clinical tasks.", "conclusion": "The MediPhi framework effectively adapts SLMs for clinical use, enhancing performance on medical NLP tasks with the introduction of MediFlow and rigorous benchmarking.", "key_contributions": ["Introduction of MediPhi for adapting small language models in clinical settings", "Development of the CLUE+ benchmark for comprehensive evaluation of clinical NLP", "Creation of MediFlow, a synthetic dataset for medical NLP tasks"], "limitations": "", "keywords": ["small language models", "clinical NLP", "MediPhi"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.10864", "pdf": "https://arxiv.org/pdf/2505.10864.pdf", "abs": "https://arxiv.org/abs/2505.10864", "title": "Anti-Sensing: Defense against Unauthorized Radar-based Human Vital Sign Sensing with Physically Realizable Wearable Oscillators", "authors": ["Md Farhan Tasnim Oshim", "Nigel Doering", "Bashima Islam", "Tsui-Wei Weng", "Tauhidur Rahman"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Recent advancements in Ultra-Wideband (UWB) radar technology have enabled\ncontactless, non-line-of-sight vital sign monitoring, making it a valuable tool\nfor healthcare. However, UWB radar's ability to capture sensitive physiological\ndata, even through walls, raises significant privacy concerns, particularly in\nhuman-robot interactions and autonomous systems that rely on radar for sensing\nhuman presence and physiological functions. In this paper, we present\nAnti-Sensing, a novel defense mechanism designed to prevent unauthorized\nradar-based sensing. Our approach introduces physically realizable\nperturbations, such as oscillatory motion from wearable devices, to disrupt\nradar sensing by mimicking natural cardiac motion, thereby misleading heart\nrate (HR) estimations. We develop a gradient-based algorithm to optimize the\nfrequency and spatial amplitude of these oscillations for maximal disruption\nwhile ensuring physiological plausibility. Through both simulations and\nreal-world experiments with radar data and neural network-based HR sensing\nmodels, we demonstrate the effectiveness of Anti-Sensing in significantly\ndegrading model accuracy, offering a practical solution for privacy\npreservation.", "AI": {"tldr": "This paper presents Anti-Sensing, a defense mechanism that uses oscillatory motion from wearable devices to disrupt UWB radar sensing of physiological data, enhancing privacy in healthcare applications.", "motivation": "To address privacy concerns arising from the use of UWB radar technology for non-line-of-sight vital sign monitoring in healthcare, particularly in human-robot interactions.", "method": "The approach introduces physically realizable perturbations that mimic natural cardiac motion to mislead heart rate estimations by UWB radar. It involves a gradient-based algorithm to optimize oscillation frequency and amplitude for maximum disruption while maintaining physiological plausibility.", "result": "Simulations and real-world experiments show that Anti-Sensing significantly degrades the accuracy of heart rate sensing models relying on UWB radar data.", "conclusion": "Anti-Sensing offers a practical solution for preserving privacy against unauthorized radar-based sensing, demonstrating that disruptive perturbations can effectively shield sensitive physiological information.", "key_contributions": ["Introduction of Anti-Sensing as a privacy protection mechanism", "Development of an optimization algorithm for perturbation parameters", "Empirical validation of the effectiveness of the proposed solution through simulations and experiments."], "limitations": "The effectiveness of the approach may vary based on specific radar technologies and environmental conditions that affect radar sensing.", "keywords": ["Ultra-Wideband", "privacy", "Human-Robot Interaction", "vital sign monitoring", "Anti-Sensing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10718", "pdf": "https://arxiv.org/pdf/2505.10718.pdf", "abs": "https://arxiv.org/abs/2505.10718", "title": "AI-enhanced semantic feature norms for 786 concepts", "authors": ["Siddharth Suresh", "Kushin Mukherjee", "Tyler Giallanza", "Xizheng Yu", "Mia Patil", "Jonathan D. Cohen", "Timothy T. Rogers"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures", "summary": "Semantic feature norms have been foundational in the study of human\nconceptual knowledge, yet traditional methods face trade-offs between\nconcept/feature coverage and verifiability of quality due to the\nlabor-intensive nature of norming studies. Here, we introduce a novel approach\nthat augments a dataset of human-generated feature norms with responses from\nlarge language models (LLMs) while verifying the quality of norms against\nreliable human judgments. We find that our AI-enhanced feature norm dataset,\nNOVA: Norms Optimized Via AI, shows much higher feature density and overlap\namong concepts while outperforming a comparable human-only norm dataset and\nword-embedding models in predicting people's semantic similarity judgments.\nTaken together, we demonstrate that human conceptual knowledge is richer than\ncaptured in previous norm datasets and show that, with proper validation, LLMs\ncan serve as powerful tools for cognitive science research.", "AI": {"tldr": "This paper introduces NOVA, a novel dataset of semantic feature norms that combines human-generated norms with responses from large language models (LLMs), showing improved feature density and predictive performance in semantic similarity judgments.", "motivation": "To address the trade-offs in traditional semantic feature norming methods while enhancing the depth of human conceptual knowledge representation.", "method": "A dataset of human-generated feature norms is augmented with responses from LLMs, and the quality of norms is validated against human judgments to ensure accuracy and reliability.", "result": "The NOVA dataset exhibits higher feature density and greater overlap among concepts compared to traditional human-only datasets and significantly improves predictions of semantic similarity judgments.", "conclusion": "The findings highlight the richness of human conceptual knowledge beyond previous datasets and the viability of using LLMs as tools for enhancing cognitive science research.", "key_contributions": ["Introduction of the NOVA dataset that integrates LLM responses with human norms", "Demonstration of superior performance in predicting semantic similarity", "Validation of LLM usage in cognitive science research"], "limitations": "The study relies on the quality of the LLM responses and human judgments for validation, which may introduce biases or inaccuracies.", "keywords": ["semantic feature norms", "large language models", "cognitive science", "semantic similarity", "NOVA"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.11056", "pdf": "https://arxiv.org/pdf/2505.11056.pdf", "abs": "https://arxiv.org/abs/2505.11056", "title": "Empowering the Teaching and Learning of Geometry in Basic Education by Combining Extended Reality and Machine Learning", "authors": ["Carlos R. Cunha", "André Moreira", "Sílvia Coelho", "Vítor Mendonça", "João Pedro Gomes"], "categories": ["cs.HC"], "comment": "C. R. Cunha, A. Moreira, S. Coelho, V. Mendon\\c{c}a, and J. P. Gomes,\n  'Empowering the Teaching and Learning of Geometry in Basic Education by\n  Combining Extended Reality and Machine Learning', in Good Practices and New\n  Perspectives in Information Systems and Technologies, 2024, pp. 98-109", "summary": "Technology has helped to innovate in the teaching-learning process. Today's\nstudents are more demanding actors when it comes to the environment, they have\nat their disposal to learn, experiment and develop critical thinking. The area\nof mathematics has successively suffered from students' learning difficulties,\nwhether due to lack of motivation, low abstraction ability, or lack of new\ntools for teachers to bring innovation into the classroom and outside it. While\nit is true that digitalization has entered schools, it often follows a process\nof digital replication of approaches and materials that were previously only\navailable on physical media. This work focuses on the use of Extended Realities\nfor teaching mathematics, and very particularly in the teaching of geometry,\nwith a proposition of a conceptual model that combines the use of Extended\nReality and Machine Learning. The proposed model was subject to prototyping,\nwhich is presented as a form of laboratory validation as a contribution to\ninnovate the way in which the geometry teaching-learning process is developed,\nas well as through the ability to obtain useful insights for teachers and\nstudents throughout the process.", "AI": {"tldr": "This paper proposes a model combining Extended Reality and Machine Learning to innovate geometry teaching by enhancing student engagement and providing insights for both teachers and students.", "motivation": "To address students' learning difficulties in mathematics, specifically geometry, due to a lack of motivation and innovative teaching tools.", "method": "The authors proposed a conceptual model integrating Extended Reality with Machine Learning, followed by prototyping for laboratory validation.", "result": "The model was validated through prototyping, demonstrating its potential to innovate the geometry teaching-learning process and providing useful insights for users.", "conclusion": "The integration of Extended Reality and Machine Learning can significantly enhance the methodology of teaching geometry, improving engagement and learning outcomes.", "key_contributions": ["Introduction of a novel model for geometry education", "Validation through prototyping", "Insights for teachers and students to enhance teaching-learning dynamics"], "limitations": "", "keywords": ["Extended Reality", "Machine Learning", "Geometry Education", "Teaching Innovation", "Student Engagement"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.10719", "pdf": "https://arxiv.org/pdf/2505.10719.pdf", "abs": "https://arxiv.org/abs/2505.10719", "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models", "authors": ["Tomás Vergara-Browne", "Álvaro Soto"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Motivated by the surge of large language models, there has been a push to\nformally characterize the symbolic abilities intrinsic to the transformer\narchitecture. A programming language, called RASP, has been proposed, which can\nbe directly compiled into transformer weights to implement these algorithms.\nHowever, the tasks that can be implemented in RASP are often uncommon to learn\nfrom natural unsupervised data, showing a mismatch between theoretical\ncapabilities of the transformer architecture, and the practical learnability of\nthese capabilities from unsupervised data. We propose tracr-injection, a method\nthat allows us to distill algorithms written in RASP directly into a\npre-trained language model. We showcase our method by injecting 3 different\nalgorithms into a language model. We show how our method creates an\ninterpretable subspace within the model's residual stream, which can be decoded\ninto the variables present in the code of the RASP algorithm. Additionally, we\nfound that the proposed method can improve out of distribution performance\ncompared to our baseline, indicating that indeed a more symbolic mechanism is\ntaking place in the inner workings of the model. We release the code used to\nrun our experiments.", "AI": {"tldr": "This paper introduces tracr-injection, a method for distilling algorithms written in a programming language called RASP into pre-trained language models, enhancing their symbolic capabilities and out-of-distribution performance.", "motivation": "To address the gap between the theoretical symbolic capabilities of transformer architectures and their practical learnability from unsupervised data, particularly in light of the advancements in large language models.", "method": "The paper proposes tracr-injection, which distills RASP-written algorithms into a pre-trained language model, resulting in an interpretable subspace within the model's residual stream.", "result": "The implementation of tracr-injection demonstrated improvements in out-of-distribution performance and allowed for decoding of variables from RASP algorithms into the model's framework.", "conclusion": "The findings support that a more symbolic mechanism is functioning in the model's inner workings, bridging the gap between RASP algorithms and practical applications in transformers.", "key_contributions": ["Introduction of tracr-injection method for distilling algorithms into language models", "Creation of an interpretable subspace in the model's residual stream", "Demonstration of improved out-of-distribution performance."], "limitations": "", "keywords": ["large language models", "transformers", "algorithm distillation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.11162", "pdf": "https://arxiv.org/pdf/2505.11162.pdf", "abs": "https://arxiv.org/abs/2505.11162", "title": "Sliding Speed Influences Electrovibration-Induced Finger Friction Dynamics on Touchscreens", "authors": ["Jagan K Balasubramanian", "Daan M Pool", "Yasemin Vardar"], "categories": ["cs.HC", "cs.SY", "eess.SY"], "comment": "19 pages, 13 figures, journal", "summary": "Electrovibration technology can render tactile textures on capacitive\ntouchscreens by modulating friction between the finger and the screen through\nelectrostatic attraction force generated by applying an alternating voltage\nsignal to the screen. This signal should be carefully calibrated for realistic\nand robust texture rendering. However, this process is challenging due to\nvariations in sliding speed, applied force, and individual skin mechanics,\nwhich affect friction in complex and unpredictable ways. Here, we investigate\nhow exploration conditions affect electrovibration-induced finger friction on\ntouchscreens and the role of skin mechanics in this process. Ten participants\nslid their index fingers across an electrovibration-enabled touchscreen at five\nsliding speeds ($20\\sim100$ mm/s) and applied force levels ($0.2\\sim0.6$ N)\nwhile we measured contact forces and skin accelerations. The touchscreen was\nexcited with amplitude-modulated voltage signals across frequencies relevant to\ntouch. We modeled the finger-touchscreen friction response as a first-order\nsystem and the skin mechanics as a mass-spring-damper system. Our results\nshowed that the sliding speed influenced the cutoff frequency of the friction\nresponse as well as the moving mass and stiffness of the finger for the tested\nexploration ranges. Specifically, for every 1 mm/s increase in speed, the\ncutoff frequency, the finger moving mass, and stiffness increased by $13.8$ Hz,\n$3.23\\times 10^{-5}$ kg, and $4.04$ N/m, respectively. Further correlation\nanalysis revealed that finger stiffness affected the cutoff frequency more than\nthe moving mass. Finally, we developed a practical model for\nelectrovibration-induced finger friction on touchscreens that accounts for\nsliding speed variations, paving the way for delivering consistent haptic\nfeedback through electrovibration.", "AI": {"tldr": "This paper investigates how variations in sliding speed and applied force impact electrovibration-induced finger friction on touchscreens. It reveals a model to predict friction response based on these exploration conditions, enhancing haptic feedback technology.", "motivation": "Electrovibration technology aims to improve user experience on touchscreen devices by providing tactile feedback, but variations in user interaction complicate the rendering of realistic textures.", "method": "Ten participants slid their fingers on an electrovibration-enabled touchscreen at varying speeds and forces, measuring contact forces and skin accelerations. A first-order system model was used for finger-touchscreen friction, and a mass-spring-damper model was used for skin mechanics.", "result": "The study found that sliding speed significantly influenced the friction response, modifying parameters like cutoff frequency, mass, and stiffness of the finger. A 1 mm/s increase in speed resulted in specific quantified increases in these parameters.", "conclusion": "A practical model for electrovibration-induced finger friction was developed, facilitating consistent haptic feedback and addressing challenges in texture rendering on touchscreens.", "key_contributions": ["Development of a model for finger friction on touchscreens considering sliding speed variations.", "Quantitative analysis of the effects of speed and force on finger mechanics during touchscreen interactions.", "Insights into the influence of skin mechanics on electrovibration technology."], "limitations": "", "keywords": ["Electrovibration", "Touchscreens", "Haptic feedback", "Finger friction", "Skin mechanics"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2505.10736", "pdf": "https://arxiv.org/pdf/2505.10736.pdf", "abs": "https://arxiv.org/abs/2505.10736", "title": "Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization", "authors": ["Ximing Dong", "Shaowei Wang", "Dayi Lin", "Ahmed E. Hassan"], "categories": ["cs.CL"], "comment": null, "summary": "Optimizing Large Language Model (LLM) performance requires well-crafted\nprompts, but manual prompt engineering is labor-intensive and often\nineffective. Automated prompt optimization techniques address this challenge\nbut the majority of them rely on randomly selected evaluation subsets, which\nfail to represent the full dataset, leading to unreliable evaluations and\nsuboptimal prompts. Existing coreset selection methods, designed for LLM\nbenchmarking, are unsuitable for prompt optimization due to challenges in\nclustering similar samples, high data collection costs, and the unavailability\nof performance data for new or private datasets. To overcome these issues, we\npropose IPOMP, an Iterative evaluation data selection for effective Prompt\nOptimization using real-time Model Performance. IPOMP is a two-stage approach\nthat selects representative and diverse samples using semantic clustering and\nboundary analysis, followed by iterative refinement with real-time model\nperformance data to replace redundant samples. Evaluations on the BIG-bench\ndataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by\nat least 57% compared with SOTA baselines, with minimal computational overhead\nbelow 1%. Furthermore, the results demonstrate that our real-time\nperformance-guided refinement approach can be universally applied to enhance\nexisting coreset selection methods.", "AI": {"tldr": "This paper introduces IPOMP, an automated method for optimizing prompts for Large Language Models (LLMs) by using real-time model performance data for more accurate evaluations and sample selection.", "motivation": "Manual prompt engineering for LLMs is labor-intensive and ineffective, necessitating better methods for automated prompt optimization.", "method": "IPOMP employs a two-stage approach involving semantic clustering and boundary analysis to select diverse samples, followed by iterative refinement using real-time model performance data.", "result": "Evaluations on the BIG-bench dataset show that IPOMP improves prompt effectiveness by 1.6% to 5.3% and stability by at least 57% compared to state-of-the-art baselines, with less than 1% computational overhead.", "conclusion": "The results indicate that real-time performance-guided refinement can enhance existing coreset selection methods across various datasets.", "key_contributions": ["Introduction of IPOMP for prompt optimization in LLMs", "Demonstrated improvements in effectiveness and stability of prompts", "Application of real-time performance data for sample selection and refinement"], "limitations": "", "keywords": ["prompt optimization", "large language models", "real-time performance", "automated techniques", "coreset selection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11406", "pdf": "https://arxiv.org/pdf/2505.11406.pdf", "abs": "https://arxiv.org/abs/2505.11406", "title": "Large Language Model Use Impact Locus of Control", "authors": ["Jenny Xiyu Fu", "Brennan Antone", "Kowe Kadoma", "Malte Jung"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity.", "AI": {"tldr": "The paper investigates how co-writing with AI influences individuals' locus of control, revealing that employment status significantly affects reliance on AI and personal agency.", "motivation": "To explore the psychological impact of co-writing with AI on self-perception and locus of control.", "method": "An empirical study was conducted with 462 participants to assess the relationship between employment status, reliance on AI, and locus of control.", "result": "Employed participants showed greater reliance on AI and a movement towards an internal locus of control, whereas unemployed participants experienced a decline in personal agency.", "conclusion": "The findings suggest that AI has a significant impact on personal agency and identity, particularly influenced by employment status.", "key_contributions": ["Identifies the psychological effects of AI co-writing on locus of control.", "Finds a correlation between employment status and reliance on AI.", "Highlights the implications of AI on personal agency and identity."], "limitations": "The study is limited to the sample population and may not generalize across different demographics or cultural contexts.", "keywords": ["AI", "locus of control", "employment status", "personal agency", "identity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10740", "pdf": "https://arxiv.org/pdf/2505.10740.pdf", "abs": "https://arxiv.org/abs/2505.10740", "title": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "authors": ["Qiwei Peng", "Robert Moro", "Michal Gregor", "Ivan Srba", "Simon Ostermann", "Marian Simko", "Juraj Podroužek", "Matúš Mesarčík", "Jaroslav Kopčan", "Anders Søgaard"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The rapid spread of online disinformation presents a global challenge, and\nmachine learning has been widely explored as a potential solution. However,\nmultilingual settings and low-resource languages are often neglected in this\nfield. To address this gap, we conducted a shared task on multilingual claim\nretrieval at SemEval 2025, aimed at identifying fact-checked claims that match\nnewly encountered claims expressed in social media posts across different\nlanguages. The task includes two subtracks: (1) a monolingual track, where\nsocial posts and claims are in the same language, and (2) a crosslingual track,\nwhere social posts and claims might be in different languages. A total of 179\nparticipants registered for the task contributing to 52 test submissions. 23\nout of 31 teams have submitted their system papers. In this paper, we report\nthe best-performing systems as well as the most common and the most effective\napproaches across both subtracks. This shared task, along with its dataset and\nparticipating systems, provides valuable insights into multilingual claim\nretrieval and automated fact-checking, supporting future research in this\nfield.", "AI": {"tldr": "This paper reports on a shared task for multilingual claim retrieval, exploring approaches to automated fact-checking in low-resource languages.", "motivation": "The paper addresses the challenge of online disinformation, especially in multilingual settings that often neglect low-resource languages.", "method": "The shared task at SemEval 2025 included a monolingual track (same language for posts and claims) and a crosslingual track (different languages).", "result": "179 participants registered, contributing to 52 submissions. The best-performing systems were identified, along with common effective approaches.", "conclusion": "The task and dataset provide valuable insights for future multilingual claim retrieval and automated fact-checking research.", "key_contributions": ["Introduction of a multilingual claim retrieval task", "Dataset for multilingual fact-checking", "Analysis of effective systems and approaches"], "limitations": "", "keywords": ["multilingual claim retrieval", "automated fact-checking", "social media"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.11417", "pdf": "https://arxiv.org/pdf/2505.11417.pdf", "abs": "https://arxiv.org/abs/2505.11417", "title": "EdgeWisePersona: A Dataset for On-Device User Profiling from Natural Language Interactions", "authors": ["Patryk Bartkowiak", "Michal Podstawski"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper introduces a novel dataset and evaluation benchmark designed to\nassess and improve small language models deployable on edge devices, with a\nfocus on user profiling from multi-session natural language interactions in\nsmart home environments. At the core of the dataset are structured user\nprofiles, each defined by a set of routines - context-triggered, repeatable\npatterns of behavior that govern how users interact with their home systems.\nUsing these profiles as input, a large language model (LLM) generates\ncorresponding interaction sessions that simulate realistic, diverse, and\ncontext-aware dialogues between users and their devices.\n  The primary task supported by this dataset is profile reconstruction:\ninferring user routines and preferences solely from interactions history. To\nassess how well current models can perform this task under realistic\nconditions, we benchmarked several state-of-the-art compact language models and\ncompared their performance against large foundation models. Our results show\nthat while small models demonstrate some capability in reconstructing profiles,\nthey still fall significantly short of large models in accurately capturing\nuser behavior. This performance gap poses a major challenge - particularly\nbecause on-device processing offers critical advantages, such as preserving\nuser privacy, minimizing latency, and enabling personalized experiences without\nreliance on the cloud. By providing a realistic, structured testbed for\ndeveloping and evaluating behavioral modeling under these constraints, our\ndataset represents a key step toward enabling intelligent, privacy-respecting\nAI systems that learn and adapt directly on user-owned devices.", "AI": {"tldr": "A dataset and benchmark for improving small language models in smart home environments by focusing on user profiling from multi-session interactions is introduced.", "motivation": "To enhance small language models for user profiling in smart home environments while ensuring user privacy and efficient on-device processing.", "method": "The paper introduces a dataset with structured user profiles based on routines, allowing LLMs to generate simulated dialogues for benchmarking profile reconstruction tasks.", "result": "Small language models can reconstruct user profiles but underperform compared to large foundation models, highlighting the challenge of balancing model size and performance with on-device processing benefits.", "conclusion": "The dataset offers a crucial step toward developing intelligent AI systems that respect user privacy and adapt on user-owned devices.", "key_contributions": ["Introduction of a novel dataset for user profiling from natural language interactions in smart homes", "Benchmarking of state-of-the-art compact language models against large foundation models", "Assessment of the performance gap in profile reconstruction tasks"], "limitations": "The small models, while effective, still lack the accuracy of large models in understanding user behavior.", "keywords": ["user profiling", "multi-session interactions", "small language models", "smart home environments", "privacy-respecting AI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.10772", "pdf": "https://arxiv.org/pdf/2505.10772.pdf", "abs": "https://arxiv.org/abs/2505.10772", "title": "Ranked Voting based Self-Consistency of Large Language Models", "authors": ["Weiqin Wang", "Yile Wang", "Hui Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Majority voting is considered an effective method to enhance chain-of-thought\nreasoning, as it selects the answer with the highest \"self-consistency\" among\ndifferent reasoning paths (Wang et al., 2023). However, previous\nchain-of-thought reasoning methods typically generate only a single answer in\neach trial, thereby ignoring the possibility of other potential answers. As a\nresult, these alternative answers are often overlooked in subsequent voting\nprocesses. In this work, we propose to generate ranked answers in each\nreasoning process and conduct ranked voting among multiple ranked answers from\ndifferent responses, thereby making the overall self-consistency more reliable.\nSpecifically, we use three ranked voting methods: Instant-runoff voting, Borda\ncount voting, and mean reciprocal rank voting. We validate our methods on six\ndatasets, including three multiple-choice and three open-ended\nquestion-answering tasks, using both advanced open-source and closed-source\nlarge language models. Extensive experimental results indicate that our\nproposed method outperforms the baselines, showcasing the potential of\nleveraging the information of ranked answers and using ranked voting to improve\nreasoning performance. The code is available at\nhttps://github.com/szu-tera/RankedVotingSC.", "AI": {"tldr": "This paper introduces a method for enhancing chain-of-thought reasoning through ranked voting among multiple ranked answers, improving self-consistency in reasoning processes.", "motivation": "To address the limitations of previous chain-of-thought reasoning methods that typically generate a single answer, which ignores other potential answers.", "method": "The authors propose generating ranked answers in each reasoning process and applying three ranked voting methods: Instant-runoff voting, Borda count voting, and mean reciprocal rank voting.", "result": "The proposed method outperforms baseline approaches across six datasets, including multiple-choice and open-ended tasks, demonstrating improved reasoning performance.", "conclusion": "Leveraging ranked answers and ranked voting significantly enhances the reliability of self-consistency in reasoning tasks.", "key_contributions": ["Introducing ranked voting methods for chain-of-thought reasoning.", "Validation across multiple datasets including both multiple-choice and open-ended tasks.", "Demonstrating improved performance over existing methods."], "limitations": "", "keywords": ["chain-of-thought reasoning", "ranked voting", "self-consistency", "large language models", "reasoning performance"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10472", "pdf": "https://arxiv.org/pdf/2505.10472.pdf", "abs": "https://arxiv.org/abs/2505.10472", "title": "Large Language Models for Cancer Communication: Evaluating Linguistic Quality, Safety, and Accessibility in Generative AI", "authors": ["Agnik Saha", "Victoria Churchill", "Anny D. Rodriguez", "Ugur Kursuncu", "Muhammed Y. Idris"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": null, "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.", "AI": {"tldr": "Evaluation of LLMs for generating cancer-related information reveals strengths in linguistic quality from general-purpose models and in accessibility from medical models, but highlights concerns around safety and bias.", "motivation": "To address persistent challenges in public understanding of cancer prevention, screening, and treatment due to ineffective communication.", "method": "Mixed-methods evaluation framework focusing on linguistic quality, safety, trustworthiness, and communication accessibility across five general-purpose and three medical LLMs, utilizing quantitative and qualitative ratings and statistical analysis.", "result": "General-purpose LLMs had higher linguistic quality and affectiveness, while medical LLMs showed better communication accessibility but exhibited higher potential harm and bias.", "conclusion": "There's a need for intentional design improvements in LLMs for health communication to ensure safety and effectiveness, particularly for cancer information.", "key_contributions": ["Evaluation of general-purpose vs medical LLMs for cancer communication", "Identification of duality between domain-specific knowledge and safety", "Recommendations for model design improvements in digital health tools."], "limitations": "Focus on breast and cervical cancer may not generalize to other health topics.", "keywords": ["Large Language Models", "cancer communication", "health informatics", "safety", "trustworthiness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.10775", "pdf": "https://arxiv.org/pdf/2505.10775.pdf", "abs": "https://arxiv.org/abs/2505.10775", "title": "A Systematic Analysis of Base Model Choice for Reward Modeling", "authors": ["Kian Ahrabian", "Pegah Jandaghi", "Negar Mokhberian", "Sai Praneeth Karimireddy", "Jay Pujara"], "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 13 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward\nmodeling have become a crucial part of training powerful large language models\n(LLMs). One commonly overlooked factor in training high-quality reward models\n(RMs) is the effect of the base model, which is becoming more challenging to\nchoose given the rapidly growing pool of LLMs. In this work, we present a\nsystematic analysis of the effect of base model selection on reward modeling\nperformance. Our results show that the performance can be improved by up to 14%\ncompared to the most common (i.e., default) choice. Moreover, we showcase the\nstrong statistical relation between some existing benchmarks and downstream\nperformances. We also demonstrate that the results from a small set of\nbenchmarks could be combined to boost the model selection ($+$18% on average in\nthe top 5-10). Lastly, we illustrate the impact of different post-training\nsteps on the final performance and explore using estimated data distributions\nto reduce performance prediction error.", "AI": {"tldr": "This paper investigates the impact of base model selection on reward modeling performance in reinforcement learning from human feedback, revealing significant potential improvements in model performance.", "motivation": "To analyze how the choice of base model affects the performance of reward models in training large language models via reinforcement learning from human feedback.", "method": "A systematic analysis comparing various base models and their influence on reward modeling performance, including the evaluation of benchmarks and post-training steps.", "result": "The study found that selecting the right base model can enhance performance by up to 14%, and combining benchmark results can improve model selection metrics by 18% on average.", "conclusion": "Choosing an appropriate base model and optimizing post-training steps is crucial for improving reward modeling performance in large language models.", "key_contributions": ["Systematic analysis of base model selection's effect on reward modeling performance.", "Demonstration of improved performance through better base model choice and benchmark combination.", "Insights on reducing performance prediction error using estimated data distributions."], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Reward Modeling", "Large Language Models", "Model Selection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.10792", "pdf": "https://arxiv.org/pdf/2505.10792.pdf", "abs": "https://arxiv.org/abs/2505.10792", "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "authors": ["Zhan Peng Lee", "Andre Lin", "Calvin Tan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\nimprove factuality in large language models (LLMs) by grounding their outputs\nin retrieved documents. However, ensuring perfect retrieval of relevant\ninformation remains challenging, and when irrelevant content is passed\ndownstream to an LLM, it can lead to hallucinations. In this work, we propose\nFinetune-RAG, a simple and effective fine-tuning approach that features the\nfirst-of-its-kind RAG training dataset constructed to mimic real-world\nimperfections. Experimental results show that Finetune-RAG improves factual\naccuracy by 21.2% over the base model. We also propose a Bench-RAG, an\nLLM-as-a-judge evaluation pipeline that stress tests models under realistic\nimperfect retrieval scenarios. Our codebase and dataset are fully open sourced\nfor community use.", "AI": {"tldr": "Finetune-RAG is a novel fine-tuning approach that improves factual accuracy in LLMs using a dataset designed to reflect real-world retrieval imperfections.", "motivation": "To enhance the factual accuracy of LLM outputs by addressing the challenges posed by imperfect retrieval in Retrieval-Augmented Generation (RAG).", "method": "A fine-tuning approach called Finetune-RAG, using a newly constructed RAG training dataset that simulates real-world imperfections, was implemented and tested.", "result": "Experimental results indicated a 21.2% improvement in factual accuracy over the base model utilizing Finetune-RAG.", "conclusion": "Finetune-RAG significantly enhances LLM performance in terms of factual correctness, offering a robust solution to the challenge of misinformation from irrelevant retrievals.", "key_contributions": ["Introduction of Finetune-RAG fine-tuning approach", "Development of a RAG training dataset for imperfect retrieval", "Creation of Bench-RAG, an evaluation pipeline for LLM performance under real-world scenarios"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Factual Accuracy"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.10798", "pdf": "https://arxiv.org/pdf/2505.10798.pdf", "abs": "https://arxiv.org/abs/2505.10798", "title": "Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets", "authors": ["Erica Cai", "Sean McQuade", "Kevin Young", "Brendan O'Connor"], "categories": ["cs.CL"], "comment": null, "summary": "When knowledge graphs (KGs) are automatically extracted from text, are they\naccurate enough for downstream analysis? Unfortunately, current annotated\ndatasets can not be used to evaluate this question, since their KGs are highly\ndisconnected, too small, or overly complex. To address this gap, we introduce\nAffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six\ndatasets that are the first to pair complete book scans with large, labeled\nknowledge graphs. Each dataset features affiliation graphs, which are simple\nKGs that capture Member relationships between Person and Organization entities\n-- useful in studies of migration, community interactions, and other social\nphenomena. In addition, three datasets include expanded KGs with a wider\nvariety of relation types. Our preliminary experiments demonstrate significant\nvariability in model performance across datasets, underscoring AffilKG's\nability to enable two critical advances: (1) benchmarking how extraction errors\npropagate to graph-level analyses (e.g., community structure), and (2)\nvalidating KG extraction methods for real-world social science research.", "AI": {"tldr": "AffilKG introduces six datasets linking complete book scans with labeled knowledge graphs to evaluate the accuracy of knowledge extraction from text for downstream analyses.", "motivation": "To address the lack of suitable annotated datasets for evaluating the accuracy of automatically extracted knowledge graphs, which often suffer from being disconnected, too small, or overly complex.", "method": "The study introduces AffilKG, a collection of six datasets that pair complete book scans with labeled knowledge graphs, focusing on affiliation graphs that capture relationships between Person and Organization entities.", "result": "Preliminary experiments reveal significant variability in model performance across the datasets, highlighting AffilKG's potential to benchmark error propagation in extraction and validate KG extraction methods for social science research.", "conclusion": "AffilKG enables critical advances in understanding the impact of extraction errors on graph-level analyses and validating knowledge graph extraction for real-world applications.", "key_contributions": ["Introduction of AffilKG, a unique dataset collection for knowledge graph evaluation", "Provides labeled knowledge graphs paired with complete book scans", "Facilitates benchmarking of extraction error impacts and validation of extraction methods."], "limitations": "The datasets may have limitations in representing all possible relationships in real-world scenarios.", "keywords": ["knowledge graphs", "data extraction", "social science", "affiliation graphs", "dataset evaluation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.10829", "pdf": "https://arxiv.org/pdf/2505.10829.pdf", "abs": "https://arxiv.org/abs/2505.10829", "title": "Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances", "authors": ["Chen-Chi Chang", "Chong-Fu Li", "Chu-Hsuan Lee", "Hung-Shin Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IntelliSys 2025", "summary": "This study investigates the challenges of translating low-resource languages\nby integrating Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG). Various model configurations were tested on Hakka translations, with\nBLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).\nThe best-performing model (Model 4) combined retrieval and advanced language\nmodeling, improving lexical coverage, particularly for specialized or\nculturally nuanced terms, and enhancing grammatical coherence. A two-stage\nmethod (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU\nscore of 26%, highlighting iterative correction's value and the challenges of\ndomain-specific expressions. Static dictionary-based approaches struggled with\ncontext-sensitive content, demonstrating the limitations of relying solely on\npredefined resources. These results emphasize the need for curated resources,\ndomain knowledge, and ethical collaboration with local communities, offering a\nframework that improves translation accuracy and fluency while supporting\ncultural preservation.", "AI": {"tldr": "This study explores the use of Large Language Models combined with Retrieval-Augmented Generation to improve translations of low-resource languages, specifically Hakka, showing improved accuracy and fluency over traditional methods.", "motivation": "The need to address the challenges faced in translating low-resource languages and improve the quality of translations.", "method": "Several model configurations were tested, particularly for Hakka translations, comparing dictionary-only approaches and various LLM and RAG integrations to assess their performance using BLEU scores.", "result": "The best-performing model (Model 4) with a configuration of retrieval and advanced language modeling achieved a BLEU score of 31%, demonstrating better lexical coverage and grammatical coherence, especially for nuanced terms.", "conclusion": "The study concludes that combining curated resources, domain knowledge, and ethical collaboration enhances translation accuracy and supports cultural preservation.", "key_contributions": ["Introduced a novel combination of LLMs and RAG for low-resource language translation.", "Demonstrated improved translation accuracy with specific model configurations.", "Highlighted the importance of cultural nuances in translation efforts."], "limitations": "Static dictionary-based approaches were ineffective for context-sensitive content, indicating challenges in solely relying on predefined resources.", "keywords": ["Low-resource languages", "Large Language Models", "Retrieval-Augmented Generation", "Translation", "Cultural preservation"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2505.10718", "pdf": "https://arxiv.org/pdf/2505.10718.pdf", "abs": "https://arxiv.org/abs/2505.10718", "title": "AI-enhanced semantic feature norms for 786 concepts", "authors": ["Siddharth Suresh", "Kushin Mukherjee", "Tyler Giallanza", "Xizheng Yu", "Mia Patil", "Jonathan D. Cohen", "Timothy T. Rogers"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures", "summary": "Semantic feature norms have been foundational in the study of human\nconceptual knowledge, yet traditional methods face trade-offs between\nconcept/feature coverage and verifiability of quality due to the\nlabor-intensive nature of norming studies. Here, we introduce a novel approach\nthat augments a dataset of human-generated feature norms with responses from\nlarge language models (LLMs) while verifying the quality of norms against\nreliable human judgments. We find that our AI-enhanced feature norm dataset,\nNOVA: Norms Optimized Via AI, shows much higher feature density and overlap\namong concepts while outperforming a comparable human-only norm dataset and\nword-embedding models in predicting people's semantic similarity judgments.\nTaken together, we demonstrate that human conceptual knowledge is richer than\ncaptured in previous norm datasets and show that, with proper validation, LLMs\ncan serve as powerful tools for cognitive science research.", "AI": {"tldr": "This paper presents NOVA, a dataset of semantic feature norms enhanced by large language models (LLMs), which improves feature density and semantic similarity predictions compared to traditional datasets.", "motivation": "Traditional methods for creating semantic feature norms face limitations in concept coverage and quality verification due to their labor-intensive nature.", "method": "The paper combines human-generated feature norms with responses from LLMs and validates the quality of the norms against reliable human judgments.", "result": "The NOVA dataset demonstrates significantly higher feature density and concept overlap, surpassing a comparable human-only norm dataset and enhancing predictions of semantic similarity judgments.", "conclusion": "With proper validation, LLMs can effectively enhance cognitive science research by providing richer conceptual knowledge through AI-assisted norm datasets.", "key_contributions": ["Introduction of NOVA, an AI-enhanced semantic feature norm dataset", "Demonstration of improved feature density and semantic similarity predictions", "Validation of LLMs as tools for cognitive science research"], "limitations": "Further studies needed to explore the limits of LLM contributions and the contexts in which they are most effective.", "keywords": ["semantic feature norms", "large language models", "cognitive science", "conceptual knowledge", "AI-enhanced datasets"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10832", "pdf": "https://arxiv.org/pdf/2505.10832.pdf", "abs": "https://arxiv.org/abs/2505.10832", "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "authors": ["Songjun Tu", "Jiahao Lin", "Qichao Zhang", "Xiangyu Tian", "Linjing Li", "Xiangyuan Lan", "Dongbin Zhao"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Project Page: https://github.com/TU2021/AutoThink", "summary": "Large reasoning models (LRMs) are proficient at generating explicit,\nstep-by-step reasoning sequences before producing final answers. However, such\ndetailed reasoning can introduce substantial computational overhead and\nlatency, particularly for simple problems. To address this over-thinking\nproblem, we explore how to equip LRMs with adaptive thinking capabilities:\nenabling them to dynamically decide whether or not to engage in explicit\nreasoning based on problem complexity. Building on R1-style distilled models,\nwe observe that inserting a simple ellipsis (\"...\") into the prompt can\nstochastically trigger either a thinking or no-thinking mode, revealing a\nlatent controllability in the reasoning behavior. Leveraging this property, we\npropose AutoThink, a multi-stage reinforcement learning (RL) framework that\nprogressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while\ndefaulting to succinct responses for simpler tasks. Experiments on five\nmainstream mathematical benchmarks demonstrate that AutoThink achieves\nfavorable accuracy-efficiency trade-offs compared to recent prompting and\nRL-based pruning methods. It can be seamlessly integrated into any R1-style\nmodel, including both distilled and further fine-tuned variants. Notably,\nAutoThink improves relative accuracy by 6.4 percent while reducing token usage\nby 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and\nadaptive reasoning paradigm for LRMs.", "AI": {"tldr": "AutoThink improves reasoning efficiency in large reasoning models by dynamically deciding when to use explicit reasoning based on task complexity, enhancing performance while reducing computational overhead.", "motivation": "To address the computational overhead and latency introduced by explicit reasoning in large reasoning models (LRMs) for simple problems.", "method": "A multi-stage reinforcement learning framework called AutoThink is developed to optimize reasoning policies, using prompt modifications to trigger thinking modes based on problem complexity.", "result": "AutoThink achieves a 6.4% improvement in accuracy and reduces token usage by 52% on certain benchmarks, outperforming other prompting and RL-based pruning methods.", "conclusion": "AutoThink establishes a scalable and adaptive reasoning paradigm for LRMs, making it easier to integrate with existing models for improved efficiency.", "key_contributions": ["Introduction of AutoThink framework for adaptive reasoning in LRMs", "Demonstration of enhanced accuracy-efficiency trade-offs", "Integration capabilities with existing R1-style models"], "limitations": "", "keywords": ["Large reasoning models", "Reinforcement learning", "Adaptive reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10836", "pdf": "https://arxiv.org/pdf/2505.10836.pdf", "abs": "https://arxiv.org/abs/2505.10836", "title": "Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs", "authors": ["Abhishek Dey", "Aabha Bothera", "Samhita Sarikonda", "Rishav Aryan", "Sanjay Kumar Podishetty", "Akshay Havalgi", "Gaurav Singh", "Saurabh Srivastava"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at NLDB 2025", "summary": "In this paper, we study the challenges of detecting events on social media,\nwhere traditional unimodal systems struggle due to the rapid and multimodal\nnature of data dissemination. We employ a range of models, including unimodal\nModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced\ngenerative models like GPT-4o, and LLaVA. Additionally, we also study the\neffect of providing multimodal generative models (such as GPT-4o) with a single\nmodality to assess their efficacy. Our results indicate that while multimodal\napproaches notably outperform unimodal counterparts, generative approaches\ndespite having a large number of parameters, lag behind supervised methods in\nprecision. Furthermore, we also found that they lag behind instruction-tuned\nmodels because of their inability to generate event classes correctly. During\nour error analysis, we discovered that common social media issues such as leet\nspeak, text elongation, etc. are effectively handled by generative approaches\nbut are hard to tackle using supervised approaches.", "AI": {"tldr": "This paper examines the challenges of detecting events on social media using various unimodal and multimodal approaches, highlighting the advantages and shortcomings of generative models compared to supervised methods.", "motivation": "To address the inadequacies of traditional unimodal systems in detecting events on social media due to the rapid and multimodal nature of data.", "method": "The authors use several models, including ModernBERT, ConvNeXt-V2, and advanced generative models like GPT-4o and LLaVA, assessing their performance in event detection on social media.", "result": "The study finds that multimodal approaches surpass unimodal ones in performance, but generative models like GPT-4o lag behind supervised methods in precision and event class generation.", "conclusion": "Multimodal methods are more effective than unimodal ones, while generative models struggle with precision and correct event classification compared to instruction-tuned models; however, they handle certain social media vernacular successfully.", "key_contributions": ["Evaluation of multimodal event detection in social media", "Comparison between generative and supervised models", "Insight into social media data characteristics affecting model performance"], "limitations": "Generative models have limitations in precision and event class generation compared to supervised methods.", "keywords": ["event detection", "social media", "multimodal", "generative models", "precision"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.10862", "pdf": "https://arxiv.org/pdf/2505.10862.pdf", "abs": "https://arxiv.org/abs/2505.10862", "title": "Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?", "authors": ["Tairan Fu", "Miguel González", "Javier Conde", "Elena Merino-Gómez", "Pedro Reviriego"], "categories": ["cs.CL", "I.2.7"], "comment": "6 pages, 5 figures, 2 tables", "summary": "Multimodal Large Language Models which can answer complex questions on an\nimage struggle to tell the time on analog clocks. This is probably due to the\nlack of images with clocks at different times in their training set. In this\nwork we explore this issue with one of the latest MLLMs: GPT-4.1 to understand\nwhy MLLMs fail to tell the time and whether fine-tuning can solve the problem.\nThe results show how models are making progress in reading the time on analog\nclocks. But have they really learned to do it, or have they only learned\npatterns in their training datasets? In this work we put the models to the test\nwith different clocks to illustrate the limitations of MLLMs to abstract and\ngeneralize.", "AI": {"tldr": "This paper investigates the ability of Multimodal Large Language Models (MLLMs), specifically GPT-4.1, to read and interpret analog clocks, highlighting their limitations and the impact of fine-tuning on performance.", "motivation": "The study aims to understand the challenges that MLLMs face when interpreting time from analog clocks, which may arise from inadequate training data.", "method": "The authors test GPT-4.1 on various analog clocks to assess its ability to read time and evaluate the effectiveness of fine-tuning the model.", "result": "The findings indicate that while MLLMs are improving in clock reading, their ability to accurately tell time may still be reliant on training patterns rather than genuine understanding.", "conclusion": "The work reveals that the progress made by MLLMs in reading analog clocks is limited and often superficial, raising questions about their true learning capabilities.", "key_contributions": ["Investigation of MLLMs' performance on analog clock reading", "Assessment of the impact of fine-tuning on model accuracy", "Analysis of training set limitations affecting model generalization"], "limitations": "The study primarily focuses on one model (GPT-4.1) and may not generalize across all MLLMs or other types of tasks.", "keywords": ["Multimodal Large Language Models", "GPT-4.1", "analog clocks", "fine-tuning", "machine learning"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.10870", "pdf": "https://arxiv.org/pdf/2505.10870.pdf", "abs": "https://arxiv.org/abs/2505.10870", "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate", "authors": ["Ziyang Huang", "Wangtao Sun", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "This paper systematically addresses the challenges of rule retrieval, a\ncrucial yet underexplored area. Vanilla retrieval methods using sparse or dense\nretrievers to directly search for relevant rules to support downstream\nreasoning, often suffer from low accuracy. This is primarily due to a\nsignificant semantic gap between the instantiated facts in the queries and the\nabstract representations of the rules. Such misalignment results in suboptimal\nretrieval quality, which in turn negatively impacts reasoning performance. To\novercome these challenges, we propose Self-Induction Augmented Retrieval\n(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce\npotential inferential rules that might offer benefits for reasoning by\nabstracting the underlying knowledge and logical structure in queries. These\ninduced rules are then used for query augmentation to improve retrieval\neffectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a\nmethod that re-estimates the relevance of retrieved rules by assessing whether\nthe abstract knowledge they contain can be instantiated to align with the facts\nin the queries and the helpfulness for reasoning. Extensive experiments across\nvarious settings demonstrate the effectiveness and versatility of our proposed\nmethods.", "AI": {"tldr": "This paper introduces Self-Induction Augmented Retrieval (SIAR) to enhance rule retrieval accuracy by leveraging Large Language Models (LLMs) for inducing inferential rules and improving query effectiveness.", "motivation": "To address the challenges of low accuracy in rule retrieval methods due to semantic gaps between queries and abstract rule representations, affecting reasoning performance.", "method": "The proposed SIAR approach uses LLMs to induce relevant inferential rules for query augmentation, while Rule Relevance ReEstimate (R$^3$) reassesses the relevance of retrieved rules based on their instantiation to align with query facts.", "result": "Experiments show that SIAR and R$^3$ significantly improve the retrieval effectiveness and reasoning performance across various settings.", "conclusion": "The methodologies proposed provide a systematic approach to improve rule retrieval, demonstrating their effectiveness and versatility in practical applications.", "key_contributions": ["Introduction of Self-Induction Augmented Retrieval (SIAR) for enhanced rule retrieval.", "Development of Rule Relevance ReEstimate (R$^3$) to reassess rule relevance to queries.", "Extensive experimental validation across multiple settings showcasing improved reasoning performance."], "limitations": "", "keywords": ["Rule Retrieval", "Large Language Models", "Query Augmentation", "Reasoning Performance", "Artificial Intelligence"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.10924", "pdf": "https://arxiv.org/pdf/2505.10924.pdf", "abs": "https://arxiv.org/abs/2505.10924", "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?", "authors": ["Ada Chen", "Yongjiang Wu", "Junyuan Zhang", "Shu Yang", "Jen-tse Huang", "Kun Wang", "Wenxuan Wang", "Shuai Wang"], "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.SE"], "comment": null, "summary": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents.", "AI": {"tldr": "The paper presents a systematic overview of safety and security threats associated with Computer-Using Agents (CUAs), along with a comprehensive literature review and guidance for future research and practical applications.", "motivation": "With the rise of LLM-based systems emulating human-like operations in GUIs, the safety and security risks presented by Computer-Using Agents (CUAs) have become critical to understand.", "method": "The authors conducted a comprehensive literature review to define CUAs, categorize safety threats, propose a taxonomy of defensive strategies, and summarize benchmarks and metrics for assessing safety and performance.", "result": "The study categorizes current safety threats among CUAs, proposes a structured taxonomy of existing defensive strategies, and summarizes various benchmarks and datasets that assess CUA performance.", "conclusion": "This work lays a structured foundation for future research into vulnerabilities in CUAs and provides actionable guidance for practitioners in designing secure systems.", "key_contributions": ["Systematization of safety and security threats of CUAs", "Proposed taxonomy of defensive strategies", "Overview of benchmarks and datasets for evaluating CUAs"], "limitations": "", "keywords": ["Computer-Using Agents", "safety and security", "large language models", "human-computer interaction", "defensive strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10936", "pdf": "https://arxiv.org/pdf/2505.10936.pdf", "abs": "https://arxiv.org/abs/2505.10936", "title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "categories": ["cs.CL"], "comment": "34 pages, 20 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nexecuting complex reasoning tasks. Chain-of-thought effectively enhances\nreasoning capabilities by unlocking the potential of large models, while\nmulti-agent systems provide more comprehensive solutions by integrating\ncollective intelligence of multiple agents. However, both approaches face\nsignificant limitations. Single-agent with chain-of-thought, due to the\ninherent complexity of designing cross-domain prompts, faces collaboration\nchallenges. Meanwhile, multi-agent systems consume substantial tokens and\ninevitably dilute the primary problem, which is particularly problematic in\nbusiness workflow tasks. To address these challenges, we propose Cochain, a\ncollaboration prompting framework that effectively solves business workflow\ncollaboration problem by combining knowledge and prompts at a reduced cost.\nSpecifically, we construct an integrated knowledge graph that incorporates\nknowledge from multiple stages. Furthermore, by maintaining and retrieving a\nprompts tree, we can obtain prompt information relevant to other stages of the\nbusiness workflow. We perform extensive evaluations of Cochain across multiple\ndatasets, demonstrating that Cochain outperforms all baselines in both prompt\nengineering and multi-agent LLMs. Additionally, expert evaluation results\nindicate that the use of a small model in combination with Cochain outperforms\nGPT-4.", "AI": {"tldr": "Cochain is a collaboration prompting framework that enhances business workflows by integrating knowledge and prompts, addressing limitations of single-agent and multi-agent LLM systems.", "motivation": "To overcome the collaboration challenges in single-agent systems and the token consumption issues in multi-agent systems for business workflow tasks.", "method": "Cochain combines knowledge from multiple stages using an integrated knowledge graph and retrieves relevant prompts through a maintained prompts tree.", "result": "Cochain outperforms all baseline approaches in prompt engineering and multi-agent LLMs across multiple datasets, and it shows that using a small model with Cochain surpasses GPT-4 performance.", "conclusion": "Cochain provides a cost-effective solution to improve collaboration in business workflows, maintaining efficiency by leveraging an integrated approach.", "key_contributions": ["Introduction of Cochain framework for business workflow collaboration", "Development of an integrated knowledge graph for enhanced prompt retrieval", "Demonstration of superior performance over existing models including GPT-4 with reduced costs."], "limitations": "", "keywords": ["Large Language Models", "Collaboration", "Knowledge Graph", "Business Workflows", "Multi-Agent Systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.10937", "pdf": "https://arxiv.org/pdf/2505.10937.pdf", "abs": "https://arxiv.org/abs/2505.10937", "title": "Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of large reasoning models (LRMs) has transformed Natural\nLanguage Processing by excelling in complex tasks such as mathematical\nproblem-solving and code generation. These models leverage chain-of-thought\n(CoT) processes, enabling them to emulate human-like reasoning strategies.\nHowever, the advancement of LRMs is hindered by the lack of comprehensive CoT\ndatasets. Current resources often fail to provide extensive reasoning problems\nwith coherent CoT processes distilled from multiple teacher models and do not\naccount for multifaceted properties describing the internal characteristics of\nCoTs. To address these challenges, we introduce OmniThought, a large-scale\ndataset featuring 2 million CoT processes generated and validated by two\npowerful LRMs as teacher models. Each CoT process in OmniThought is annotated\nwith novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which\ndescribe the appropriateness of CoT verbosity and cognitive difficulty level\nfor models to comprehend these reasoning processes. We further establish a\nself-reliant pipeline to curate this dataset. Extensive experiments using\nQwen2.5 models of various sizes demonstrate the positive impact of our proposed\nscores on LRM training effectiveness. Based on the proposed OmniThought\ndataset, we further train and release a series of high-performing LRMs,\nspecifically equipped with stronger reasoning abilities and optimal CoT output\nlength and difficulty level. Our contributions significantly enhance the\ndevelopment and training of LRMs for solving complex tasks.", "AI": {"tldr": "OmniThought is a dataset of 2 million chain-of-thought (CoT) processes aimed at enhancing the training of large reasoning models (LRMs) for complex tasks like problem-solving and code generation.", "motivation": "The creation of comprehensive datasets for chain-of-thought reasoning processes is essential for improving the capabilities of large reasoning models in handling complex tasks.", "method": "The dataset is curated using two powerful LRMs as teacher models, with annotations for Reasoning Verbosity and Cognitive Difficulty. Extensive experiments were conducted to validate the dataset's impact on training LRMs using Qwen2.5 models.", "result": "The implementation of the OmniThought dataset positively influences the training effectiveness of LRMs, yielding models with enhanced reasoning capabilities.", "conclusion": "The introduction of OmniThought and its unique annotations significantly contribute to advancing LRM development for complex reasoning tasks.", "key_contributions": ["Introduction of the OmniThought dataset with 2 million CoT processes", "Development of annotation scores for Reasoning Verbosity and Cognitive Difficulty", "Enhanced training efficiency for LRMs in complex task scenarios"], "limitations": "", "keywords": ["large reasoning models", "chain-of-thought", "cognitive difficulty", "reasoning verbosity", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.10938", "pdf": "https://arxiv.org/pdf/2505.10938.pdf", "abs": "https://arxiv.org/abs/2505.10938", "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing", "authors": ["Yi Su", "Yuechi Zhou", "Quantong Qiu", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.", "AI": {"tldr": "This paper presents a method to improve quantization accuracy in large language models (LLMs) by identifying and excluding unusual tokens from quantization, enhancing memory efficiency and throughput.", "motivation": "Large Language Models (LLMs) require substantial computational resources, and while KV Cache can reduce recomputation, it increases memory overhead. Effective quantization can help balance these demands.", "method": "The authors propose a method to identify outlier tokens during the decoding process that do not conform to the typical distribution of Keys and Values. These tokens are excluded from quantization.", "result": "The proposed method results in significant improvements in accuracy under 2-bit quantization, achieving a 6.4 times reduction in memory usage and a 2.3 times increase in throughput.", "conclusion": "Excluding outlier tokens from quantization provides a promising enhancement to the deployment efficiency of LLMs without compromising accuracy.", "key_contributions": ["Introduction of a method to identify outlier tokens during quantization", "Demonstration of significant accuracy improvements with a 2-bit quantization", "Achieved a balance of reduced memory usage and increased throughput in LLM deployment"], "limitations": "", "keywords": ["Large Language Models", "KV Cache", "quantization", "computational resources", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.10939", "pdf": "https://arxiv.org/pdf/2505.10939.pdf", "abs": "https://arxiv.org/abs/2505.10939", "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction", "authors": ["Mohammadtaha Bagherifard", "Sahar Rajabi", "Ali Edalat", "Yadollah Yaghoobzadeh"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages", "summary": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM.", "AI": {"tldr": "This paper presents a modular framework, GenKnowSub, that improves zero-shot generalization of large language models by disentangling general knowledge from task-specific adaptations via a library of LoRA modules.", "motivation": "The paper addresses the limitations of zero-shot generalization in large language models due to the entanglement of general knowledge and task-specific adaptations.", "method": "A modular framework is proposed that uses task-specific LoRA modules alongside a general-domain LoRA. General knowledge is subtracted to create residual modules focused on task-relevant information, and Arrow routing dynamically combines modules for new inputs.", "result": "The proposed approach shows consistent performance improvements in both monolingual and cross-lingual settings on various benchmarks compared to standard baselines.", "conclusion": "GenKnowSub demonstrates better generalization in weaker LLMs, indicating its effectiveness in enhancing task-specific performance while maintaining general knowledge.", "key_contributions": ["Introduction of general knowledge subtraction (GenKnowSub) for LLMs", "Development of a library of task-specific LoRA modules", "Demonstrated improved performance across multiple languages and benchmarks"], "limitations": "", "keywords": ["large language models", "zero-shot generalization", "LoRA modules"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.10945", "pdf": "https://arxiv.org/pdf/2505.10945.pdf", "abs": "https://arxiv.org/abs/2505.10945", "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer", "authors": ["Seungyoon Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures.", "AI": {"tldr": "The paper introduces Semantic Aware Linear Transfer (SALT), a method for effectively transferring multilingual capabilities from Pre-trained Language Models to Large Language Models, improving performance in cross-lingual understanding tasks.", "motivation": "As Large Language Models increasingly incorporate multilingual capabilities, there is a need for effective methods to transfer these capabilities into target language-specific models without losing expressiveness.", "method": "SALT recycles embeddings from target language Pre-trained Language Models and constructs unique regression lines based on vocabulary overlap to handle non-overlapping tokens in embedding space.", "result": "SALT significantly outperforms existing transfer methods, achieving lower loss and faster convergence during language adaptation, especially in cross-lingual understanding setups.", "conclusion": "The proposed SALT technique enhances the functionality of contemporary LLMs and demonstrates impressive performance, suggesting a strong path forward for multilingual model development.", "key_contributions": ["Introduction of SALT for cross-lingual transfer of embeddings", "Unique regression line construction based on vocabulary overlap", "Demonstrated significant improvements in performance and adaptability over existing methods"], "limitations": "", "keywords": ["Large Language Models", "Semantic Aware Linear Transfer", "cross-lingual transfer", "Pre-trained Language Models", "multilingual capabilities"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2311.16027", "pdf": "https://arxiv.org/pdf/2311.16027.pdf", "abs": "https://arxiv.org/abs/2311.16027", "title": "An HCAI Methodological Framework (HCAI-MF): Putting It Into Action to Enable Human-Centered AI", "authors": ["Wei Xu", "Zaifeng Gao", "Marvin Dainoff"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human-centered artificial intelligence (HCAI) is a design philosophy that\nprioritizes humans in the design, development, deployment, and use of AI\nsystems, aiming to maximize AI's benefits while mitigating its negative\nimpacts. Despite its growing prominence in literature, the lack of\nmethodological guidance for its implementation poses challenges to HCAI\npractice. To address this gap, this paper proposes a comprehensive HCAI\nmethodological framework (HCAI-MF) comprising five key components: HCAI\nrequirement hierarchy, approach and method taxonomy, process, interdisciplinary\ncollaboration approach, and multi-level design paradigms. A case study\ndemonstrates HCAI-MF's practical implications, while the paper also analyzes\nimplementation challenges. Actionable recommendations and a \"three-layer\" HCAI\nimplementation strategy are provided to address these challenges and guide\nfuture evolution of HCAI-MF. HCAI-MF is presented as a systematic and\nexecutable methodology capable of overcoming current gaps, enabling effective\ndesign, development, deployment, and use of AI systems, and advancing HCAI\npractice.", "AI": {"tldr": "This paper proposes a comprehensive framework for Human-Centered Artificial Intelligence (HCAI) to address challenges in its implementation by outlining a methodological approach.", "motivation": "To improve the implementation of Human-Centered AI by providing a structured methodological framework that addresses current gaps in practice.", "method": "The paper introduces the HCAI methodological framework (HCAI-MF) which includes components such as a requirement hierarchy, method taxonomy, interdisciplinary collaborations, and multi-level design paradigms. A case study is used to demonstrate its applicability.", "result": "The proposed HCAI-MF offers actionable recommendations and a three-layer implementation strategy, providing a systematic approach to enhance the effectiveness of AI system development with human-centered principles.", "conclusion": "The HCAI-MF framework is positioned as a vital tool for advancing the practice of Human-Centered AI and addressing the challenges faced in the field.", "key_contributions": ["Proposes a comprehensive methodological framework for HCAI.", "Demonstrates practical implications through a case study.", "Offers actionable recommendations for implementation challenges."], "limitations": "The paper's framework may need further empirical validation across diverse applications and contexts.", "keywords": ["Human-Centered AI", "Methodological Framework", "Design Principles", "Implementation Strategy", "AI Systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10948", "pdf": "https://arxiv.org/pdf/2505.10948.pdf", "abs": "https://arxiv.org/abs/2505.10948", "title": "The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs", "authors": ["Makoto Sato"], "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs), inspired by neuroscience, exhibit behaviors\nthat often evoke a sense of personality and intelligence-yet the mechanisms\nbehind these effects remain elusive. Here, we operationalize Conceptual\nBlending Theory (CBT) as an experimental framework, using prompt-based methods\nto reveal how LLMs blend and compress meaning. By systematically investigating\nPrompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we\nuncover structural parallels and divergences between artificial and biological\ncognition. Our approach bridges linguistics, neuroscience, and empirical AI\nresearch, demonstrating that human-AI collaboration can serve as a living\nprototype for the future of cognitive science. This work proposes prompt\nengineering not just as a technical tool, but as a scientific method for\nprobing the deep structure of meaning itself.", "AI": {"tldr": "This paper explores the operationalization of Conceptual Blending Theory in the context of large language models, revealing insights into their cognitive-like behaviors through experimental frameworks.", "motivation": "To understand the mechanisms of personality and intelligence-like behaviors in large language models.", "method": "Utilizing an experimental framework based on Conceptual Blending Theory, the authors employ prompt-based methods to investigate how LLMs blend and compress meaning, focusing on Prompt-Induced Transitions and Hallucinations.", "result": "The study uncovers structural parallels and divergences between artificial and biological cognition, showing how human-AI collaboration reflects principles of cognitive science.", "conclusion": "The research promotes prompt engineering as a vital method for exploring the layers of meaning in large language models.", "key_contributions": ["Operationalizes Conceptual Blending Theory in LLM research", "Investigates Prompt-Induced transitions and hallucinations", "Proposes prompt engineering as a scientific approach to understanding cognition"], "limitations": "", "keywords": ["large language models", "Conceptual Blending Theory", "prompt engineering", "cognition", "human-AI collaboration"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2404.13274", "pdf": "https://arxiv.org/pdf/2404.13274.pdf", "abs": "https://arxiv.org/abs/2404.13274", "title": "Augmented Object Intelligence with XR-Objects", "authors": ["Mustafa Doga Dogan", "Eric J. Gonzalez", "Karan Ahuja", "Ruofei Du", "Andrea Colaço", "Johnny Lee", "Mar Gonzalez-Franco", "David Kim"], "categories": ["cs.HC", "cs.AI", "H.5.0; H.5.1; H.5.2"], "comment": "15 pages, 15 figures, 2024 ACM Symposium on User Interface Software\n  and Technology (UIST)", "summary": "Seamless integration of physical objects as interactive digital entities\nremains a challenge for spatial computing. This paper explores Augmented Object\nIntelligence (AOI) in the context of XR, an interaction paradigm that aims to\nblur the lines between digital and physical by equipping real-world objects\nwith the ability to interact as if they were digital, where every object has\nthe potential to serve as a portal to digital functionalities. Our approach\nutilizes real-time object segmentation and classification, combined with the\npower of Multimodal Large Language Models (MLLMs), to facilitate these\ninteractions without the need for object pre-registration. We implement the AOI\nconcept in the form of XR-Objects, an open-source prototype system that\nprovides a platform for users to engage with their physical environment in\ncontextually relevant ways using object-based context menus. This system\nenables analog objects to not only convey information but also to initiate\ndigital actions, such as querying for details or executing tasks. Our\ncontributions are threefold: (1) we define the AOI concept and detail its\nadvantages over traditional AI assistants, (2) detail the XR-Objects system's\nopen-source design and implementation, and (3) show its versatility through\nvarious use cases and a user study.", "AI": {"tldr": "This paper presents Augmented Object Intelligence (AOI) in XR, enabling physical objects to act as interactive digital entities using MLLMs, through an open-source prototype called XR-Objects.", "motivation": "To address the challenge of integrating physical objects as interactive digital entities in spatial computing.", "method": "Utilizes real-time object segmentation and classification with Multimodal Large Language Models (MLLMs) to facilitate interaction without pre-registration of objects.", "result": "Demonstrates the AOI concept through a prototype system (XR-Objects) allowing physical objects to convey information and initiate digital actions, validated by user studies.", "conclusion": "AOI provides a significant advancement in user interaction with physical objects, offering advantages over traditional AI assistants and showcasing versatility through user engagement.", "key_contributions": ["Definition of the AOI concept and its advantages over traditional AI assistants", "Open-source design and implementation of the XR-Objects system", "Versatile use cases and validation through user studies"], "limitations": "", "keywords": ["Augmented Object Intelligence", "XR", "Multimodal Large Language Models", "spatial computing", "user interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.10975", "pdf": "https://arxiv.org/pdf/2505.10975.pdf", "abs": "https://arxiv.org/abs/2505.10975", "title": "Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio", "authors": ["Xinlu He", "Jacob Whitehill"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "13 pages. Submitted to IEEE/ACM Transaction on Audio Speech and\n  Language Processing (TASLP)", "summary": "Monaural multi-speaker automatic speech recognition (ASR) remains challenging\ndue to data scarcity and the intrinsic difficulty of recognizing and\nattributing words to individual speakers, particularly in overlapping speech.\nRecent advances have driven the shift from cascade systems to end-to-end (E2E)\narchitectures, which reduce error propagation and better exploit the synergy\nbetween speech content and speaker identity. Despite rapid progress in E2E\nmulti-speaker ASR, the field lacks a comprehensive review of recent\ndevelopments. This survey provides a systematic taxonomy of E2E neural\napproaches for multi-speaker ASR, highlighting recent advances and comparative\nanalysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)\nfor pre-segmented audio, analyzing their distinct characteristics and\ntrade-offs; (2) recent architectural and algorithmic improvements based on\nthese two paradigms; (3) extensions to long-form speech, including segmentation\nstrategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate\nand compare methods across standard benchmarks. We conclude with a discussion\nof open challenges and future research directions towards building robust and\nscalable multi-speaker ASR.", "AI": {"tldr": "A comprehensive survey on end-to-end neural approaches for multi-speaker automatic speech recognition (ASR), highlighting recent developments, architectural paradigms, and future directions.", "motivation": "Addresses challenges in monaural multi-speaker ASR due to data scarcity and overlapping speech recognition difficulties.", "method": "Systematic taxonomy and comparative analysis of recent end-to-end neural architectures for multi-speaker ASR, focusing on different paradigms and algorithmic improvements.", "result": "Provides a detailed evaluation of approaches and performance benchmarks for multi-speaker ASR, highlighting the trade-offs between different architectural models.", "conclusion": "Identifies open challenges and outlines future research directions for enhancing multi-speaker ASR systems.", "key_contributions": ["Comprehensive review of end-to-end neural ASR approaches for multi-speaker scenarios.", "Analysis of architectural paradigms (SIMO vs SISO) and their trade-offs.", "Evaluation against standard benchmarks to compare method effectiveness."], "limitations": "", "keywords": ["automatic speech recognition", "multi-speaker", "end-to-end architecture", "neural networks", "benchmark evaluation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2501.11803", "pdf": "https://arxiv.org/pdf/2501.11803.pdf", "abs": "https://arxiv.org/abs/2501.11803", "title": "Automating High Quality RT Planning at Scale", "authors": ["Riqiang Gao", "Mamadou Diallo", "Han Liu", "Anthony Magliari", "Jonathan Sackett", "Wilko Verbakel", "Sandra Meyers", "Rafe Mcbeth", "Masoud Zarepisheh", "Simon Arberet", "Martin Kraus", "Florin C. Ghesu", "Ali Kamen"], "categories": ["cs.HC", "cs.LG", "cs.RO"], "comment": "radiotherapy planning, data for AI training", "summary": "Radiotherapy (RT) planning is complex, subjective, and time-intensive.\nAdvances with artificial intelligence (AI) promise to improve its precision and\nefficiency, but progress is often limited by the scarcity of large,\nstandardized datasets. To address this, we introduce the Automated Iterative RT\nPlanning (AIRTP) system, a scalable solution for generating high-quality\ntreatment plans. This scalable solution is designed to generate substantial\nvolumes of consistently high-quality treatment plans, overcoming a key obstacle\nin the advancement of AI-driven RT planning. Our AIRTP pipeline adheres to\nclinical guidelines and automates essential steps, including organ-at-risk\n(OAR) contouring, helper structure creation, beam setup, optimization, and plan\nquality improvement, using AI integrated with RT planning software like Varian\nEclipse. Furthermore, a novel approach for determining optimization parameters\nto reproduce 3D dose distributions, i.e. a method to convert dose predictions\nto deliverable treatment plans constrained by machine limitations is proposed.\nA comparative analysis of plan quality reveals that our automated pipeline\nproduces treatment plans of quality comparable to those generated manually,\nwhich traditionally require several hours of labor per plan. Committed to\npublic research, the first data release of our AIRTP pipeline includes nine\ncohorts covering head-and-neck and lung cancer sites to support an AAPM 2025\nchallenge. To our best knowledge, this dataset features more than 10 times\nnumber of plans compared to the largest existing well-curated public dataset.\nRepo: https://github.com/RiqiangGao/GDP-HMM_AAPMChallenge.", "AI": {"tldr": "The AIRTP system automates radiotherapy planning, enhancing precision and efficiency by generating high-quality treatment plans using AI, overcoming data scarcity issues.", "motivation": "To enhance the precision and efficiency of radiotherapy planning, which is traditionally complex and time-intensive, by using AI-driven approaches.", "method": "The AIRTP system automates critical steps in radiotherapy planning, including organ-at-risk contouring, beam setup, and plan optimization, adhering to clinical guidelines and utilizing existing RT planning software.", "result": "The AIRTP pipeline produces treatment plans comparable in quality to manually generated plans, significantly reducing planning time from hours to a more efficient process.", "conclusion": "The AIRTP system not only improves the efficiency of radiotherapy planning but is also committed to public research with the release of a large dataset for further studies.", "key_contributions": ["Introduction of the AIRTP system for automated radiotherapy planning.", "Development of a novel approach for determining optimization parameters for 3D dose distributions.", "Release of a large dataset significantly larger than existing public datasets to support research."], "limitations": "", "keywords": ["radiotherapy", "AI", "automation", "treatment planning", "dataset"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2505.11004", "pdf": "https://arxiv.org/pdf/2505.11004.pdf", "abs": "https://arxiv.org/abs/2505.11004", "title": "Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning", "authors": ["Jingcheng Niu", "Subhabrata Dutta", "Ahmed Elshabrawy", "Harish Tayyar Madabushi", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale Transformer language models (LMs) trained solely on next-token\nprediction with web-scale data can solve a wide range of tasks after seeing\njust a few examples. The mechanism behind this capability, known as in-context\nlearning (ICL), remains both controversial and poorly understood. Some studies\nargue that it is merely the result of memorizing vast amounts of data, while\nothers contend that it reflects a fundamental, symbolic algorithmic development\nin LMs. In this work, we introduce a suite of investigative tasks and a novel\nmethod to systematically investigate ICL by leveraging the full Pythia scaling\nsuite, including interim checkpoints that capture progressively larger amount\nof training data. By carefully exploring ICL performance on downstream tasks\nand simultaneously conducting a mechanistic analysis of the residual stream's\nsubspace, we demonstrate that ICL extends beyond mere \"memorization\" of the\ntraining corpus, yet does not amount to the implementation of an independent\nsymbolic algorithm. Our results also clarify several aspects of ICL, including\nthe influence of training dynamics, model capabilities, and elements of\nmechanistic interpretability. Overall, our work advances the understanding of\nICL and its implications, offering model developers insights into potential\nimprovements and providing AI security practitioners with a basis for more\ninformed guidelines.", "AI": {"tldr": "This paper systematically investigates in-context learning (ICL) in large-scale Transformer language models, challenging the notion that it is simply data memorization and providing deeper insights into its mechanisms and implications.", "motivation": "Understanding in-context learning (ICL) is crucial for advancing language models and informing model developers and AI security practitioners on potential improvements and guidelines.", "method": "The study employs a novel method and a suite of tasks to investigate ICL using the Pythia scaling suite and interim checkpoints with varying amounts of training data.", "result": "The findings reveal that ICL goes beyond mere memorization, indicating significant implications for training dynamics and model capabilities, alongside insights into mechanistic interpretability.", "conclusion": "This work advances the understanding of ICL, highlighting its complexity and offering practical insights for model developers and AI security practitioners.", "key_contributions": ["Introduces a systematic investigation of ICL using the Pythia scaling suite.", "Demonstrates that ICL involves more than memorization, impacting training dynamics and model capabilities.", "Provides insights for model developers and guidelines for AI security practitioners."], "limitations": "", "keywords": ["in-context learning", "Transformer language models", "mechanistic interpretability", "model capabilities", "AI security"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.04114", "pdf": "https://arxiv.org/pdf/2503.04114.pdf", "abs": "https://arxiv.org/abs/2503.04114", "title": "Organize, Then Vote: Exploring Cognitive Load in Quadratic Survey Interfaces", "authors": ["Ti-Chung Cheng", "Yutong Zhang", "Yi-Hung Chou", "Vinay Koshy", "Tiffany Wenting Li", "Karrie Karahalios", "Hari Sundaram"], "categories": ["cs.HC", "H.5.2"], "comment": null, "summary": "Quadratic Surveys (QSs) elicit more accurate preferences than traditional\nmethods like Likert-scale surveys. However, the cognitive load associated with\nQSs has hindered their adoption in digital surveys for collective\ndecision-making. We introduce a two-phase \"organize-then-vote\" QS to reduce\ncognitive load. As interface design significantly impacts survey results and\naccuracy, our design scaffolds survey takers' decision-making while managing\nthe cognitive load imposed by QS. In a 2x2 between-subject in-lab study on\npublic resource allotment, we compared our interface with a traditional text\ninterface across a QS with 6 (short) and 24 (long) options. Two-phase interface\nparticipants spent more time per option and exhibited shorter voting edit\ndistances. We qualitatively observed shifts in cognitive effort from mechanical\noperations to constructing more comprehensive preferences. We conclude that\nthis interface promoted deeper engagement, potentially reducing satisficing\nbehaviors caused by cognitive overload in longer QSs. This research clarifies\nhow human-centered design improves preference elicitation tools for collective\ndecision-making.", "AI": {"tldr": "Introducing a two-phase 'organize-then-vote' Quadratic Survey interface that reduces cognitive load and improves decision-making accuracy compared to traditional methods.", "motivation": "Quadratic Surveys elicit more accurate preferences but face adoption challenges due to cognitive load; this research aims to mitigate those challenges through thoughtful interface design.", "method": "A 2x2 between-subject in-lab study compared a two-phase interface with a traditional text interface across Quadratic Surveys with varying option lengths (6 vs. 24).", "result": "Participants using the two-phase interface spent more time on each option and demonstrated shorter voting edit distances, indicating deeper engagement.", "conclusion": "The new interface enhances preference elicitation by reducing cognitive overload, promoting more comprehensive preference construction.", "key_contributions": ["Development of a two-phase 'organize-then-vote' Quadratic Survey interface", "Empirical validation showing reduced cognitive load and improved engagement in decision-making", "Insights into the impact of human-centered design on collective decision-making tools"], "limitations": "Focus on a specific context (public resource allotment) may limit generalizability; further testing in diverse scenarios needed.", "keywords": ["Quadratic Surveys", "Cognitive Load", "Human-Centered Design", "Preference Elicitation", "Collective Decision-Making"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11008", "pdf": "https://arxiv.org/pdf/2505.11008.pdf", "abs": "https://arxiv.org/abs/2505.11008", "title": "Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs", "authors": ["Ye Kyaw Thu", "Thazin Myint Oo"], "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "14 pages, 2 figures, 6 tables, 1 listing", "summary": "This paper explores syllable sequence prediction in Abugida languages using\nTransformer-based models, focusing on six languages: Bengali, Hindi, Khmer,\nLao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We\ninvestigate the reconstruction of complete syllable sequences from various\nincomplete input types, including consonant sequences, vowel sequences, partial\nsyllables (with random character deletions), and masked syllables (with fixed\nsyllable deletions). Our experiments reveal that consonant sequences play a\ncritical role in accurate syllable prediction, achieving high BLEU scores,\nwhile vowel sequences present a significantly greater challenge. The model\ndemonstrates robust performance across tasks, particularly in handling partial\nand masked syllable reconstruction, with strong results for tasks involving\nconsonant information and syllable masking. This study advances the\nunderstanding of sequence prediction for Abugida languages and provides\npractical insights for applications such as text prediction, spelling\ncorrection, and data augmentation in these scripts.", "AI": {"tldr": "Study on syllable sequence prediction in Abugida languages using Transformer models, highlighting the importance of consonant sequences in achieving high accuracy.", "motivation": "To advance the understanding of syllable sequence prediction in Abugida languages, focusing on practical applications like text prediction and spelling correction.", "method": "Transformer-based models were used to reconstruct complete syllable sequences from various input types, including consonant sequences, vowel sequences, and masked syllables.", "result": "The model achieved high BLEU scores, particularly excelling in tasks involving consonant sequences while struggling with vowel sequences.", "conclusion": "This paper provides insights into the robust performance of models in reconstructing syllable sequences, offering applications in text prediction and data augmentation.", "key_contributions": ["Investigation of syllable sequence prediction in Abugida languages", "High BLEU scores for consonant sequence predictions", "Insights for practical applications like text prediction and spelling correction"], "limitations": "Vowel sequences present a greater challenge than consonant sequences in prediction tasks.", "keywords": ["Syllable prediction", "Abugida languages", "Transformer models", "Data augmentation", "Natural language processing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2504.14406", "pdf": "https://arxiv.org/pdf/2504.14406.pdf", "abs": "https://arxiv.org/abs/2504.14406", "title": "ScholarMate: A Mixed-Initiative Tool for Qualitative Knowledge Work and Information Sensemaking", "authors": ["Runlong Ye", "Patrick Yung Kang Lee", "Matthew Varona", "Oliver Huang", "Carolina Nobre"], "categories": ["cs.HC", "cs.AI"], "comment": "accepted at CHIWORK '25", "summary": "Synthesizing knowledge from large document collections is a critical yet\nincreasingly complex aspect of qualitative research and knowledge work. While\nAI offers automation potential, effectively integrating it into human-centric\nsensemaking workflows remains challenging. We present ScholarMate, an\ninteractive system designed to augment qualitative analysis by unifying AI\nassistance with human oversight. ScholarMate enables researchers to dynamically\narrange and interact with text snippets on a non-linear canvas, leveraging AI\nfor theme suggestions, multi-level summarization, and evidence-based theme\nnaming, while ensuring transparency through traceability to source documents.\nInitial pilot studies indicated that users value this mixed-initiative\napproach, finding the balance between AI suggestions and direct manipulation\ncrucial for maintaining interpretability and trust. We further demonstrate the\nsystem's capability through a case study analyzing 24 papers. By balancing\nautomation with human control, ScholarMate enhances efficiency and supports\ninterpretability, offering a valuable approach for productive human-AI\ncollaboration in demanding sensemaking tasks common in knowledge work.", "AI": {"tldr": "ScholarMate is an interactive system designed to enhance qualitative research by integrating AI assistance with human oversight.", "motivation": "The need for effective integration of AI in human-centric sensemaking workflows in qualitative research is increasingly complex due to the volume of document collections.", "method": "ScholarMate allows researchers to arrange and interact with text snippets on a non-linear canvas, providing AI-assisted theme suggestions, multi-level summarization, and traceability to source documents.", "result": "Initial pilot studies showed users appreciate the balance between AI suggestions and direct manipulation in maintaining interpretability and trust.", "conclusion": "By combining automation with human control, ScholarMate improves efficiency and supports interpretability, facilitating productive human-AI collaboration in qualitative analysis tasks.", "key_contributions": ["Development of ScholarMate system for qualitative analysis", "Integration of AI assistance with human oversight", "Ensures transparency and interpretability in qualitative research"], "limitations": "", "keywords": ["qualitative research", "AI integration", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11010", "pdf": "https://arxiv.org/pdf/2505.11010.pdf", "abs": "https://arxiv.org/abs/2505.11010", "title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models", "authors": ["Jiangxu Wu", "Cong Wang", "TianHuang Su", "Jun Yang", "Haozhi Lin", "Chao Zhang", "Ming Peng", "Kai Shi", "SongPan Yang", "BinQing Pan", "ZiXian Li", "Ni Yang", "ZhenYu Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL2025 Accepted", "summary": "The effectiveness of large language models (LLMs) in conversational AI is\nhindered by their reliance on single-turn supervised fine-tuning (SFT) data,\nwhich limits contextual coherence in multi-turn dialogues. Existing methods for\ngenerating multi-turn dialogue data struggle to ensure both diversity and\nquality in instructions. To address this, we propose Review-Instruct, a novel\nframework that synthesizes multi-turn conversations through an iterative\n\"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple\nReviewers, and a Chairman. The framework iteratively refines instructions by\nincorporating Reviewer feedback, enhancing dialogue diversity and difficulty.\nWe construct a multi-turn dataset using the Alpaca dataset and fine-tune the\nLLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate\nsignificant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\%\non MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.\nAblation studies confirm the critical role of the Review stage and the use of\nmultiple Reviewers in boosting instruction diversity and difficulty. Our work\nhighlights the potential of review-driven, multi-agent frameworks for\ngenerating high-quality conversational data at scale.", "AI": {"tldr": "This paper presents Review-Instruct, a novel framework for generating multi-turn conversation data that enhances quality and diversity through an iterative review process.", "motivation": "Large language models (LLMs) face challenges in maintaining contextual coherence in multi-turn dialogues due to reliance on single-turn supervised fine-tuning (SFT) data.", "method": "The Review-Instruct framework synthesizes multi-turn conversations using an iterative 'Ask-Respond-Review' process with distinct agent roles: Candidate, multiple Reviewers, and a Chairman, aiming to refine dialogue instructions through feedback.", "result": "Significant improvements were observed, with absolute gains of 2.9% on MMLU-Pro and 2% on MT-Bench compared to prior models using LLaMA2-13B.", "conclusion": "The study demonstrates the efficacy of review-driven, multi-agent systems in producing high-quality conversational datasets.", "key_contributions": ["Introduction of the Review-Instruct framework for multi-turn dialogue synthesis", "Demonstrated improvements in conversational data quality", "Highlighting the benefit of multiple Reviewer roles in instruction refinement"], "limitations": "", "keywords": ["large language models", "multi-turn dialogue", "framework", "review-driven synthesis", "conversational AI"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.11026", "pdf": "https://arxiv.org/pdf/2505.11026.pdf", "abs": "https://arxiv.org/abs/2505.11026", "title": "StRuCom: A Novel Dataset of Structured Code Comments in Russian", "authors": ["Maria Dziuba", "Valentin Malykh"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Structured code comments in docstring format are essential for code\ncomprehension and maintenance, but existing machine learning models for their\ngeneration perform poorly for Russian compared to English. To bridge this gap,\nwe present StRuCom - the first large-scale dataset (153K examples) specifically\ndesigned for Russian code documentation. Unlike machine-translated English\ndatasets that distort terminology (e.g., technical loanwords vs. literal\ntranslations) and docstring structures, StRuCom combines human-written comments\nfrom Russian GitHub repositories with synthetically generated ones, ensuring\ncompliance with Python, Java, JavaScript, C#, and Go standards through\nautomated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom\nshows statistically significant improvements of chrf++ and BERTScore over\nbaseline models.", "AI": {"tldr": "StRuCom is a dataset aimed at improving machine learning models for generating structured code comments in Russian, achieving notable performance improvements in code documentation tasks.", "motivation": "Existing models for code comment generation struggle with Russian due to poor performance compared to English, necessitating a specialized dataset.", "method": "Creation of the StRuCom dataset consisting of 153K examples from Russian GitHub repositories, combined with synthetic data to conform to code documentation standards for multiple programming languages. Fine-tuned Qwen2.5-Coder models were evaluated on this dataset.", "result": "Fine-tuning Qwen2.5-Coder models on StRuCom resulted in statistically significant improvements in code comment generation metrics (chrf++ and BERTScore) compared to baseline models.", "conclusion": "StRuCom enhances the generation of structured code comments in Russian, paving the way for better comprehension and maintenance of codebases in non-English languages.", "key_contributions": ["First large-scale Russian code documentation dataset (StRuCom)", "Combination of human-written and synthetic data for model training", "Demonstrated significant performance improvements in NLP metrics for code comment generation"], "limitations": "", "keywords": ["code comments", "natural language processing", "machine learning", "dataset", "Russian language"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.09819", "pdf": "https://arxiv.org/pdf/2505.09819.pdf", "abs": "https://arxiv.org/abs/2505.09819", "title": "Visual Feedback of Pattern Separability Improves Myoelectric Decoding Performance of Upper Limb Prostheses", "authors": ["Ruichen Yang", "György M. Lévay", "Christopher L. Hunt", "Dániel Czeiner", "Megan C. Hodgson", "Damini Agarwal", "Rahul R. Kaliki", "Nitish V. Thakor"], "categories": ["cs.HC", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "State-of-the-art upper limb myoelectric prostheses often use pattern\nrecognition (PR) control systems that translate electromyography (EMG) signals\ninto desired movements. As prosthesis movement complexity increases, users\noften struggle to produce sufficiently distinct EMG patterns for reliable\nclassification. Existing training typically involves heuristic, trial-and-error\nuser adjustments to static decoder boundaries. Goal: We introduce the Reviewer,\na 3D visual interface projecting EMG signals directly into the decoder's\nclassification space, providing intuitive, real-time insight into PR algorithm\nbehavior. This structured feedback reduces cognitive load and fosters mutual,\ndata-driven adaptation between user-generated EMG patterns and decoder\nboundaries. Methods: A 10-session study with 12 able-bodied participants\ncompared PR performance after motor-based training and updating using the\nReviewer versus conventional virtual arm visualization. Performance was\nassessed using a Fitts law task that involved the aperture of the cursor and\nthe control of orientation. Results: Participants trained with the Reviewer\nachieved higher completion rates, reduced overshoot, and improved path\nefficiency and throughput compared to the standard visualization group.\nSignificance: The Reviewer introduces decoder-informed motor training,\nfacilitating immediate and consistent PR-based myoelectric control\nimprovements. By iteratively refining control through real-time feedback, this\napproach reduces reliance on trial-and-error recalibration, enabling a more\nadaptive, self-correcting training framework. Conclusion: The 3D visual\nfeedback significantly improves PR control in novice operators through\nstructured training, enabling feedback-driven adaptation and reducing reliance\non extensive heuristic adjustments.", "AI": {"tldr": "The paper presents the Reviewer, a 3D visual interface that improves myoelectric prosthesis control by providing real-time feedback on EMG signals, enhancing training effectiveness.", "motivation": "To address the challenge of producing distinct EMG patterns for reliable movement classification in myoelectric prostheses.", "method": "A 10-session study with 12 participants comparing performance outcomes after using the Reviewer versus conventional training visualization methods.", "result": "Participants using the Reviewer achieved higher completion rates, reduced overshoot, and improved efficiency in tasks compared to those using standard methods.", "conclusion": "The Reviewer enhances PR control in novice operators via structured feedback during training, reducing the need for extensive heuristic recalibrations.", "key_contributions": ["Introduction of the Reviewer as a 3D visual feedback tool for prosthesis control", "Demonstration of improved training outcomes in myoelectric control", "Reduction of reliance on heuristic adjustments through real-time insights into EMG patterns"], "limitations": "", "keywords": ["Myoelectric prosthesis", "Pattern recognition", "EMG signals", "3D visual interface", "Real-time feedback"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11031", "pdf": "https://arxiv.org/pdf/2505.11031.pdf", "abs": "https://arxiv.org/abs/2505.11031", "title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning", "authors": ["Xiao Zhang", "Huiyuan Lai", "Qianru Meng", "Johan Bos"], "categories": ["cs.CL"], "comment": "Paper submitted to NeruoIPS 2025 dataset and benchmark track", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing tasks, yet their ability to process\nstructured symbolic knowledge remains underexplored. To address this gap, we\npropose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the\nfirst comprehensive benchmark designed to systematically evaluate LLMs'\nproficiency in handling ontologies -- formal, symbolic representations of\ndomain knowledge through concepts, relationships, and instances. Based on the\nproposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n58,981 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing proficiency in understanding\nontological knowledge but substantial weaknesses in reasoning and learning\ntasks. These findings highlight fundamental limitations in LLMs' capability to\nprocess symbolic knowledge and establish OntoURL as a critical benchmark for\nadvancing the integration of LLMs with formal knowledge representations.", "AI": {"tldr": "This paper introduces OntoURL, a benchmark for evaluating large language models' ontological capabilities in processing structured symbolic knowledge.", "motivation": "The paper addresses the gap in understanding how large language models handle structured symbolic knowledge, specifically in terms of ontologies.", "method": "OntoURL was developed as a systematic benchmark assessing LLMs on three dimensions: understanding, reasoning, and learning, through 15 tasks featuring 58,981 questions derived from 40 ontologies across 8 domains.", "result": "Experiments with 20 open-source LLMs showed significant performance variability, with models excelling in understanding but struggling in reasoning and learning tasks.", "conclusion": "The findings indicate substantial limitations in LLMs' ability to process symbolic knowledge, establishing OntoURL as an essential tool for future research.", "key_contributions": ["Introduction of a taxonomy for LLMs' ontological capabilities", "Development of the OntoURL benchmark for systematic evaluation of ontological knowledge processing", "Revelation of LLMs' strengths and weaknesses in handling structured symbolic knowledge"], "limitations": "The paper does not explore potential improvements for LLMs in reasoning and learning tasks.", "keywords": ["Large language models", "Ontologies", "Benchmark", "Natural language processing", "Symbolic knowledge"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.09875", "pdf": "https://arxiv.org/pdf/2505.09875.pdf", "abs": "https://arxiv.org/abs/2505.09875", "title": "Characterizing Unintended Consequences in Human-GUI Agent Collaboration for Web Browsing", "authors": ["Shuning Zhang", "Jingruo Chen", "Zhiqi Gao", "Jiajing Gao", "Xin Yi", "Hewu Li"], "categories": ["cs.HC"], "comment": null, "summary": "The proliferation of Large Language Model (LLM)-based Graphical User\nInterface (GUI) agents in web browsing scenarios present complex unintended\nconsequences (UCs). This paper characterizes three UCs from three perspectives:\nphenomena, influence and mitigation, drawing on social media analysis (N=221\nposts) and semi-structured interviews (N=14). Key phenomenon for UCs include\nagents' deficiencies in comprehending instructions and planning tasks,\nchallenges in executing accurate GUI interactions and adapting to dynamic\ninterfaces, the generation of unreliable or misaligned outputs, and\nshortcomings in error handling and feedback processing. These phenomena\nmanifest as influences from unanticipated actions and user frustration, to\nprivacy violations and security vulnerabilities, and further to eroded trust\nand wider ethical concerns. Our analysis also identifies user-initiated\nmitigation, such as technical adjustments and manual oversight, and provides\nimplications for designing future LLM-based GUI agents that are robust,\nuser-centric, and transparent, fostering a crucial balance between automation\nand human oversight.", "AI": {"tldr": "This paper explores unintended consequences (UCs) of LLM-based GUI agents in web browsing, highlighting key phenomena, influences, and user-initiated mitigations.", "motivation": "To understand the unintended consequences of LLM-based GUI agents in web browsing scenarios and improve their design.", "method": "Social media analysis (221 posts) and semi-structured interviews (14 participants).", "result": "Identified deficiencies in agents' comprehension, task planning, reliability, and error handling, leading to user frustration, privacy risks, and undermined trust.", "conclusion": "Mitigation strategies include technical adjustments and manual oversight, stressing the importance of user-focused design in future LLM-based GUI agents.", "key_contributions": ["Characterization of three unintended consequences of LLM-based GUI agents.", "Insights from social media and interviews to inform design improvements.", "Recommendations for balancing automation with human oversight."], "limitations": "", "keywords": ["Large Language Models", "GUI Agents", "Unintended Consequences", "User-Centric Design", "Human Oversight"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11051", "pdf": "https://arxiv.org/pdf/2505.11051.pdf", "abs": "https://arxiv.org/abs/2505.11051", "title": "CAMEO: Collection of Multilingual Emotional Speech Corpora", "authors": ["Iwona Christop", "Maciej Czajka"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Under review at NeurIPS", "summary": "This paper presents CAMEO -- a curated collection of multilingual emotional\nspeech datasets designed to facilitate research in emotion recognition and\nother speech-related tasks. The main objectives were to ensure easy access to\nthe data, to allow reproducibility of the results, and to provide a\nstandardized benchmark for evaluating speech emotion recognition (SER) systems\nacross different emotional states and languages. The paper describes the\ndataset selection criteria, the curation and normalization process, and\nprovides performance results for several models. The collection, along with\nmetadata, and a leaderboard, is publicly available via the Hugging Face\nplatform.", "AI": {"tldr": "CAMEO is a multilingual emotional speech dataset aimed at enhancing emotion recognition research.", "motivation": "To provide easy access to data for emotion recognition and ensure reproducibility in research.", "method": "The paper outlines the dataset selection criteria, curation and normalization process, as well as performance results for several models tested on the dataset.", "result": "Performance results showcase various models' effectiveness on the dataset, highlighting its comprehensive nature.", "conclusion": "CAMEO offers a standardized benchmark and is publicly available for broader research in emotion recognition.", "key_contributions": ["Curated collection of multilingual emotional speech datasets", "Standardized benchmark for evaluating SER systems", "Public availability through Hugging Face platform"], "limitations": "", "keywords": ["emotional speech", "emotion recognition", "multilingual datasets"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.10412", "pdf": "https://arxiv.org/pdf/2505.10412.pdf", "abs": "https://arxiv.org/abs/2505.10412", "title": "Using Virtual Reality in Museums to Bridge the Gap Between Material Heritage and the Interpretation of Its Immaterial Context", "authors": ["Carlos R. Cunha", "Vítor Mendonça", "André Moreira", "João Pedro Gomes", "Aida Carvalho"], "categories": ["cs.HC"], "comment": "C. R. Cunha, V. Mendon\\c{c}a, A. Moreira, J. P. Gomes, and A.\n  Carvalho, 'Using Virtual Reality in Museums to Bridge the Gap Between\n  Material Heritage and the Interpretation of Its Immaterial Context', in\n  Advances in Tourism, Technology and Systems, 2022, pp. 397-408", "summary": "Material heritage typically has a whole set of associated immaterial\nheritage, which is essential to pass on to the visitor as a cultural mission of\nthe destinations and those who manage them. In this sense, the interpretation\nof material heritage is a complex process that is not a fully efficient process\nwith the mere observation of physical artifacts. In this context, it emerges as\nfundamental to provide visitors with a set of tools that allow them to\ncorrectly interpret the artifacts that come to fully understand the cultural\ndimension of the destinations and their heritage. Accordingly, the role of\nvirtual reality can leverage the creation of innovative and immersive solutions\nthat allow the visitor to understand and feel part of their own heritage and\nits ancestral component that defines the sociocultural roots of destinations\nand their civilizational traditions. This article, after dissecting and\nsubstantiating the role of virtual reality in the interpretation of heritage,\npresents a conceptual model, based on the use of virtual reality, which was, in\npart, prototyped in the scenario of the Portuguese Museum in the city of\nMiranda do Douro. This proposal is an ongoing contribution to the creation of\ninnovative and immersive tools for the interpretation of heritage.", "AI": {"tldr": "The paper discusses leveraging virtual reality (VR) to enhance the interpretation of material heritage in museums, providing tools for visitors to better understand cultural contexts.", "motivation": "To address the inefficiencies in interpreting material heritage solely through observation and to provide visitors with immersive tools for understanding cultural dimensions.", "method": "The article presents a conceptual model based on the use of virtual reality, specifically prototyped in the Portuguese Museum of Miranda do Douro.", "result": "The proposed VR solutions aim to bridge the gap between material heritage and its immaterial context, leading to a deeper visitor engagement with cultural heritage.", "conclusion": "The use of VR in museums is positioned as a significant advancement for the interpretation of heritage and the engagement of visitors with cultural traditions.", "key_contributions": ["Introduction of a conceptual model for virtual reality application in heritage interpretation", "Prototyping of VR solutions in a specific museum setting", "Identification of the cultural relevance of VR in understanding heritage."], "limitations": "", "keywords": ["Virtual Reality", "Cultural Heritage", "Museum Interpretation", "Immersive Experience", "Sociocultural Roots"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.11080", "pdf": "https://arxiv.org/pdf/2505.11080.pdf", "abs": "https://arxiv.org/abs/2505.11080", "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following", "authors": ["Yapei Chang", "Yekyung Kim", "Michael Krumdick", "Amir Zadeh", "Chuan Li", "Chris Tanner", "Mohit Iyyer"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 11 figures, 15 tables", "summary": "Reward models are central to aligning LLMs with human preferences, but they\nare costly to train, requiring large-scale human-labeled preference data and\npowerful pretrained LLM backbones. Meanwhile, the increasing availability of\nhigh-quality synthetic instruction-following datasets raises the question: can\nsimpler, reference-based metrics serve as viable alternatives to reward models\nduring RL-based alignment? In this paper, we show first that BLEU, a basic\nstring-matching metric, surprisingly matches strong reward models in agreement\nwith human preferences on general instruction-following datasets. Based on this\ninsight, we develop BLEUBERI, a method that first identifies challenging\ninstructions and then applies Group Relative Policy Optimization (GRPO) using\nBLEU directly as the reward function. We demonstrate that BLEUBERI-trained\nmodels are competitive with models trained via reward model-guided RL across\nfour challenging instruction-following benchmarks and three different base\nlanguage models. A human evaluation further supports that the quality of\nBLEUBERI model outputs is on par with those from reward model-aligned models.\nMoreover, BLEUBERI models generate outputs that are more factually grounded\nthan competing methods. Overall, we show that given access to high-quality\nreference outputs (easily obtained via existing instruction-following datasets\nor synthetic data generation), string matching-based metrics are cheap yet\neffective proxies for reward models during alignment. We release our code and\ndata at https://github.com/lilakk/BLEUBERI.", "AI": {"tldr": "This paper introduces BLEUBERI, a method that uses BLEU as a reward function for aligning LLMs with human preferences in instruction-following tasks, demonstrating its effectiveness compared to traditional reward models.", "motivation": "To explore simpler alternatives to costly reward models for aligning LLMs with human preferences, leveraging the availability of high-quality synthetic datasets.", "method": "The paper develops BLEUBERI, which uses BLEU as a reward function in a Group Relative Policy Optimization (GRPO) framework, targeting challenging instructions identified through the model's performance.", "result": "BLEUBERI-trained models perform competitively with those trained using reward models across several benchmarks, maintaining high alignment with human preferences and generating factually grounded outputs.", "conclusion": "BLEUBERI shows that string-matching metrics can serve as effective and inexpensive substitutes for reward models in aligning LLM outputs with user preferences, especially when quality reference outputs are available.", "key_contributions": ["Introduction of BLEUBERI, a new alignment method using BLEU as a reward metric", "Demonstration that BLEU can match reward models in terms of human preference alignment", "Release of code and data for further research on effective LLM alignment techniques."], "limitations": "The study primarily focuses on instruction-following tasks and may not generalize well to other LLM applications.", "keywords": ["Human-Computer Interaction", "Instruction-following", "Language Model Alignment", "BLEU", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2505.11095", "pdf": "https://arxiv.org/pdf/2505.11095.pdf", "abs": "https://arxiv.org/abs/2505.11095", "title": "Towards Better Evaluation for Generated Patent Claims", "authors": ["Lekang Jiang", "Pascal A Scherz", "Stephan Goetz"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. 14 pages, 8 tables", "summary": "Patent claims define the scope of protection and establish the legal\nboundaries of an invention. Drafting these claims is a complex and\ntime-consuming process that usually requires the expertise of skilled patent\nattorneys, which can form a large access barrier for many small enterprises. To\nsolve these challenges, researchers have investigated the use of large language\nmodels (LLMs) for automating patent claim generation. However, existing studies\nhighlight inconsistencies between automated evaluation metrics and human expert\nassessments. To bridge this gap, we introduce Patent-CE, the first\ncomprehensive benchmark for evaluating patent claims. Patent-CE includes\ncomparative claim evaluations annotated by patent experts, focusing on five key\ncriteria: feature completeness, conceptual clarity, terminology consistency,\nlogical linkage, and overall quality. Additionally, we propose PatClaimEval, a\nnovel multi-dimensional evaluation method specifically designed for patent\nclaims. Our experiments demonstrate that PatClaimEval achieves the highest\ncorrelation with human expert evaluations across all assessment criteria among\nall tested metrics. This research provides the groundwork for more accurate\nevaluations of automated patent claim generation systems.", "AI": {"tldr": "This paper presents Patent-CE, a benchmark for evaluating patent claims and PatClaimEval, a method that correlates well with human expert assessments.", "motivation": "The complexity of drafting patent claims creates access barriers for small enterprises, necessitating evaluation tools for automated systems.", "method": "Patent-CE benchmark is introduced with expert-annotated evaluations on features like completeness, clarity, consistency, linkage, and quality. PatClaimEval is designed to assess claims across these dimensions.", "result": "PatClaimEval shows the highest correlation with human evaluations among tested metrics.", "conclusion": "The study establishes a foundation for improving evaluations in automated patent claim generation.", "key_contributions": ["Introduction of Patent-CE benchmark for patent claims", "Development of PatClaimEval evaluation method", "Demonstration of high correlation with human expert evaluations"], "limitations": "", "keywords": ["patent claims", "evaluation", "large language models", "benchmarking", "human expert assessment"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2501.03266", "pdf": "https://arxiv.org/pdf/2501.03266.pdf", "abs": "https://arxiv.org/abs/2501.03266", "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena", "authors": ["Stefan Pasch"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": null, "summary": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. In particular,\nlittle is known about how users respond when models refuse to answer a\nprompt-one of the primary mechanisms used to enforce ethical boundaries in\nLLMs. We address this gap by analyzing nearly 50,000 model comparisons from\nChatbot Arena, a platform where users indicate their preferred LLM response in\npairwise matchups, providing a large-scale setting for studying real-world user\npreferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a\nhand-labeled dataset, we distinguish between refusals due to ethical concerns\nand technical limitations. Our results reveal a substantial refusal penalty:\nethical refusals yield significantly lower win rates than both technical\nrefusals and standard responses, indicating that users are especially\ndissatisfied when models decline a task for ethical reasons. However, this\npenalty is not uniform. Refusals receive more favorable evaluations when the\nunderlying prompt is highly sensitive (e.g., involving illegal content), and\nwhen the refusal is phrased in a detailed and contextually aligned manner.\nThese findings underscore a core tension in LLM design: safety-aligned\nbehaviors may conflict with user expectations, calling for more adaptive\nmoderation strategies that account for context and presentation.", "AI": {"tldr": "This paper investigates the impact of content moderation on user satisfaction in LLMs, particularly how users react to ethical refusals versus technical refusals.", "motivation": "The paper addresses the underexplored relationship between content moderation practices in LLMs and user satisfaction, especially in cases where models refuse to answer prompts due to ethical considerations.", "method": "The study analyzes nearly 50,000 model comparisons from Chatbot Arena, utilizing a fine-tuned RoBERTa-based refusal classifier to differentiate between ethical and technical refusals.", "result": "The analysis shows that ethical refusals lead to significantly lower win rates compared to technical refusals and standard responses, highlighting user dissatisfaction with ethical refusals, except when prompts are sensitive or refusals are contextually well-phrased.", "conclusion": "The findings emphasize the necessity for adaptive moderation strategies in LLM design that align safety behaviors with user expectations, especially in sensitive contexts.", "key_contributions": ["Introduces a novel refusal classifier for analyzing user preferences in LLM responses.", "Provides empirical evidence of the refusal penalty related to ethical versus technical refusals.", "Highlights the importance of contextual sensitivity in user evaluations of LLM refusals."], "limitations": "The study is limited to data from Chatbot Arena, which may not represent the broader user base and context. Additionally, it focuses primarily on refusal types without exploring other moderation strategies.", "keywords": ["content moderation", "user satisfaction", "ethical alignment", "LLM", "profanity moderation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11140", "pdf": "https://arxiv.org/pdf/2505.11140.pdf", "abs": "https://arxiv.org/abs/2505.11140", "title": "Scaling Reasoning can Improve Factuality in Large Language Models", "authors": ["Mike Zhang", "Johannes Bjerva", "Russa Biswas"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.", "AI": {"tldr": "This study investigates the impact of longer reasoning chains and additional computational resources on the factual accuracy of large language models (LLMs) in open-domain question-answering tasks.", "motivation": "To explore whether longer reasoning chains improve factual accuracy in LLMs beyond just mathematical reasoning.", "method": "The authors distill reasoning traces from advanced LLMs, fine-tune various models, and integrate factual information from knowledge graphs, conducting a comprehensive experimental evaluation over multiple datasets.", "result": "Smaller reasoning models show improved factual accuracy over original instruction-tuned models and using additional compute and token budgets leads to a consistent 2-8% increase in accuracy.", "conclusion": "The findings support the idea that more computational resources and longer reasoning processes can enhance the performance and accuracy of LLMs in open-domain QA scenarios.", "key_contributions": ["Introduction of reasoning traces from advanced LLMs", "Integration of knowledge graph information into reasoning processes", "Empirical evidence on the impact of compute and token budgets on accuracy"], "limitations": "", "keywords": ["large language models", "reasoning", "open-domain QA", "factual accuracy", "knowledge graphs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11166", "pdf": "https://arxiv.org/pdf/2505.11166.pdf", "abs": "https://arxiv.org/abs/2505.11166", "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization", "authors": ["Huashan Sun", "Shengyi Liao", "Yansen Han", "Yu Bai", "Yang Gao", "Cheng Fu", "Weizhou Shen", "Fanqi Wan", "Ming Yan", "Ji Zhang", "Fei Huang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.", "AI": {"tldr": "This paper proposes SoLoPO, a framework aimed at improving long-context utilization in LLMs by optimizing short-context preference and aligning rewards to enhance model performance in extended contexts.", "motivation": "To address the challenges LLMs face in utilizing long-context information due to issues like data quality and inefficiencies in training and optimization objectives.", "method": "The proposed SoLoPO framework decouples long-context preference optimization into short-context preference optimization and short-to-long reward alignment, leveraging theoretical and empirical evidence for its effectiveness.", "result": "SoLoPO shows enhancements in length and domain generalization abilities across various long-context benchmarks, alongside improvements in computational and memory efficiency compared to existing algorithms.", "conclusion": "The SoLoPO framework significantly enhances the ability of LLMs to utilize long-context information by improving preference optimization and reward alignment, yielding better performance without compromising efficiency.", "key_contributions": ["Introduction of SoLoPO framework for long-context optimization in LLMs.", "Decoupling of long-context preference optimization into two components: short-context preference optimization and SoLo-RA.", "Demonstration of improved generalization abilities and efficiency across long-context benchmarks."], "limitations": "", "keywords": ["long-context optimization", "large language models", "preference optimization", "reward alignment", "computational efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724.pdf", "abs": "https://arxiv.org/abs/2505.09724", "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "authors": ["Gino Carmona-Díaz", "William Jiménez-Leal", "María Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Bermúdez"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "This paper presents a tutorial on using LLMs to develop, test, and apply taxonomies for text analysis of unstructured data, emphasizing collaboration between researchers and LLMs.", "motivation": "The traditional process of analyzing texts like open-ended responses and social media posts is labor-intensive and biased. Utilizing LLMs can streamline this process without sacrificing quality.", "method": "The paper guides readers through an iterative and collaborative workflow with LLMs that includes developing prompts for dataset review, generating taxonomies, modifying and testing them, and achieving high reliability in categorization.", "result": "The approach allows for efficient taxonomy creation and application with demonstrated high intercoder reliability using personal goal examples.", "conclusion": "Using LLMs for text analysis presents both opportunities and limitations, while the proposed method enhances efficiency and reliability in the categorization of unstructured data.", "key_contributions": ["Step-by-step tutorial for LLM-assisted taxonomy development", "Emphasis on collaborative process between researchers and LLMs", "Demonstration of achieving high intercoder reliability in text categorization"], "limitations": "Potential bias in LLM outputs and dependency on data quality.", "keywords": ["LLM", "text analysis", "taxonomy", "HCI", "collaboration"], "importance_score": 8, "read_time_minutes": 31}}
{"id": "2505.11177", "pdf": "https://arxiv.org/pdf/2505.11177.pdf", "abs": "https://arxiv.org/abs/2505.11177", "title": "Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline", "authors": ["Hrishit Madhavi", "Jacob Cherian", "Yuvraj Khamkar", "Dhananjay Bhagat"], "categories": ["cs.CL", "cs.AI", "68T50 (Natural language processing), 68U10 (Image processing)"], "comment": "8 pages, 7 figures, direct arXiv submission", "summary": "This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments", "AI": {"tldr": "This paper describes a multilingual information extraction system that processes image-based documents using OCR and large language models.", "motivation": "To enhance access to information in image media across different linguistic environments.", "method": "The system employs Tesseract for OCR to extract text from images in multiple languages, followed by a pipeline utilizing Gemini for translation, summarization, and re-translation. It also includes modules for sentiment analysis, topic classification, and date extraction.", "result": "Demonstrates effective extraction and processing of multilingual information from image-based documents, improving document comprehension.", "conclusion": "The proposed end-to-end system effectively bridges the language gap in document access and comprehension.", "key_contributions": ["Development of a robust pipeline for multilingual document processing", "Integration of multiple AI tools (OCR, LLMs, classifiers) for improved information extraction", "Accessible Gradio interface for real-world application"], "limitations": "", "keywords": ["multilingual information extraction", "Optical Character Recognition", "large language models", "sentiment analysis", "document comprehension"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.11199", "pdf": "https://arxiv.org/pdf/2505.11199.pdf", "abs": "https://arxiv.org/abs/2505.11199", "title": "NoPE: The Counting Power of Transformers with No Positional Encodings", "authors": ["Chris Köcher", "Alexander Kozachinskiy", "Anthony Widjaja Lin", "Marco Sälzer", "Georg Zetzsche"], "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": null, "summary": "Positional Encodings (PEs) seem to be indispensable for ensuring\nexpressiveness of transformers; without them attention transformers reduce to a\nbag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard\nattention mechanisms were very recently shown to only be able to express\nregular languages, i.e., with limited counting ability. This paper shows that,\nwith average hard attention mechanisms, NoPE-transformers are still\nsurprisingly expressive: they can express counting languages corresponding to\nnonnegative integer solutions to multivariate polynomial equations (i.e.\nDiophantine equations), reasoning about which is well-known to be undecidable.\nIn fact, we provide a precise characterization of languages expressible by\nAverage Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond\nprecisely to what we call \\emph{semi-algebraic sets}, i.e., finite unions of\nsets of nonnegative integer solutions to systems of multivariate polynomial\ninequations. We obtain several interesting consequences of our\ncharacterization. Firstly, NoPE-transformers can express counting properties\nthat are far more complex than established models like simplified counter\nmachines and Petri nets, but cannot express a very simple counting property of\nPARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,\ne.g., whether a given NoPE transformer classifies all input strings in one\nclass. To complement our results, we exhibit a counting language that is not\nexpressible by average hard attention transformers even with arbitrary PEs but\nis expressible in the circuit complexity class TC$^0$, answering an open\nproblem.", "AI": {"tldr": "This paper explores the expressive capabilities of NoPE-transformers with average hard attention mechanisms, demonstrating they can express complex counting languages linked to Diophantine equations, while revealing their limitation in expressing certain properties like PARITY.", "motivation": "To investigate the expressiveness of NoPE-transformers without positional encodings and map their capabilities to algebraic concepts, challenging existing models.", "method": "The authors analyze Average Hard Attention NoPE-Transformers (NoPE-AHATs) and characterize the languages they can express in terms of semi-algebraic sets and counting properties.", "result": "NoPE-AHATs can express languages related to nonnegative integer solutions of multivariate polynomial inequations, surpassing simpler models but failing to express basic counting properties like PARITY.", "conclusion": "The study characterizes the limitations and capabilities of NoPE-transformers in terms of expressiveness, showing undecidability in analyzing their classification abilities and presenting a comparison with classical complexity classes.", "key_contributions": ["Characterization of languages expressible by NoPE-AHATs as semi-algebraic sets.", "Establishing the undecidability of analyzing NoPE-transformers.", "Identification of a counting language not expressible by average hard attention transformers."], "limitations": "The paper does not detail practical applications or implementations of NoPE-transformers in real-world scenarios.", "keywords": ["NoPE-transformers", "hard attention mechanisms", "expressiveness", "semi-algebraic sets", "undecidability"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2505.11225", "pdf": "https://arxiv.org/pdf/2505.11225.pdf", "abs": "https://arxiv.org/abs/2505.11225", "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization", "authors": ["Chengyu Huang", "Zhengxin Zhang", "Claire Cardie"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While scaling the length of responses at test-time has been shown to markedly\nimprove the reasoning abilities and performance of large language models\n(LLMs), it often results in verbose outputs and increases inference cost. Prior\napproaches for efficient test-time scaling, typically using universal budget\nconstraints or query-level length optimization, do not leverage historical\ninformation from previous encounters with the same problem during training. We\nhypothesize that this limits their ability to progressively make solutions more\nconcise over time. To address this, we present History-Aware Policy\nOptimization (HAPO), which keeps track of a history state (e.g., the minimum\nlength over previously generated correct responses) for each problem. HAPO\nemploys a novel length reward function based on this history state to\nincentivize the discovery of correct solutions that are more concise than those\npreviously found. Crucially, this reward structure avoids overly penalizing\nshorter incorrect responses with the goal of facilitating exploration towards\nmore efficient solutions. By combining this length reward with a correctness\nreward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to\ntrain DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and\nQwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span\nvarious difficulty levels. Experiment results demonstrate that HAPO effectively\ninduces LLMs' concise reasoning abilities, producing length reductions of\n33-59% with accuracy drops of only 2-5%.", "AI": {"tldr": "The paper presents a method called History-Aware Policy Optimization (HAPO) that aims to enhance the conciseness of responses from large language models (LLMs) by utilizing historical data from past encounters, optimizing both length and correctness during generation.", "motivation": "To improve the reasoning abilities of large language models while minimizing verbosity and reducing inference costs through better use of historical performance data.", "method": "HAPO keeps track of a history state for each problem, employing a novel length reward function based on this history to incentivize discovering more concise yet correct solutions. It combines length and correctness rewards to optimize the generation process.", "result": "HAPO was used to train several models and showed effectiveness in producing concise reasoning, achieving length reductions of 33-59% with minimal accuracy drops of 2-5% across various math benchmarks.", "conclusion": "HAPO successfully induces more concise reasoning in LLMs without significantly compromising accuracy, demonstrating a promising approach to test-time scaling.", "key_contributions": ["Introduction of History-Aware Policy Optimization (HAPO) for LLMs.", "Development of a length reward structure that incorporates historical performance.", "Demonstrated improvements in response conciseness and efficiency on math benchmarks."], "limitations": "", "keywords": ["large language models", "policy optimization", "conciseness", "historical data", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11271", "pdf": "https://arxiv.org/pdf/2505.11271.pdf", "abs": "https://arxiv.org/abs/2505.11271", "title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models", "authors": ["Camille Couturier", "Spyros Mastorakis", "Haiying Shen", "Saravan Rajmohan", "Victor Rühle"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.7"], "comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings", "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.", "AI": {"tldr": "Introduces a semantic caching method for efficient LLM-based question-answering workflows, reducing redundancy in processing lengthy contexts.", "motivation": "To address high computational overhead, memory usage, and network bandwidth from processing lengthy contexts in distributed systems for LLMs.", "method": "A novel semantic caching approach that stores and reuses intermediate contextual summaries for similar queries in QA workflows.", "result": "The method reduces redundant computations by 50-60% while maintaining answer accuracy comparable to full document processing, tested on various datasets.", "conclusion": "The approach achieves a balance between computational cost and response quality, which is essential for real-time AI assistants.", "key_contributions": ["Semantic caching for LLM workflows.", "Up to 60% reduction in redundant computations.", "Maintains answer accuracy similar to full document processing."], "limitations": "", "keywords": ["Large Language Models", "semantic caching", "real-time processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277.pdf", "abs": "https://arxiv.org/abs/2505.11277", "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "authors": ["Yaorui Shi", "Shihan Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.", "AI": {"tldr": "This paper presents AutoRefine, a reinforcement learning framework that enhances the reasoning capabilities of large language models through a refined search and evidence aggregation methodology.", "motivation": "Current retrieval-augmented reasoning methods often yield irrelevant information, which hampers the reasoning process of LLMs.", "method": "AutoRefine employs a search-and-refine-during-think paradigm, incorporating explicit knowledge refinement steps between search calls and utilizing tailored retrieval-specific rewards with group relative policy optimization.", "result": "Experiments show that AutoRefine significantly outperforms existing methods in both single-hop and multi-hop question answering benchmarks, especially in complex scenarios.", "conclusion": "The introduction of AutoRefine leads to improved search quality and effective evidence synthesis, thereby enhancing the overall reasoning capabilities of LLMs.", "key_contributions": ["Introduction of AutoRefine framework for LLMs", "Implementation of knowledge refinement steps in reasoning", "Development of tailored rewards for retrieval effectiveness"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "retrieval-augmented reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11280", "pdf": "https://arxiv.org/pdf/2505.11280.pdf", "abs": "https://arxiv.org/abs/2505.11280", "title": "Temporal fine-tuning for early risk detection", "authors": ["Horacio Thompson", "Esaú Villatoro-Tello", "Manuel Montes-y-Gómez", "Marcelo Errecalde"], "categories": ["cs.CL"], "comment": "In: Proceedings of the 53rd JAIIO / 50th CLEI - ASAID, 2024, p. 137.\n  ISSN: 2451-7496", "summary": "Early Risk Detection (ERD) on the Web aims to identify promptly users facing\nsocial and health issues. Users are analyzed post-by-post, and it is necessary\nto guarantee correct and quick answers, which is particularly challenging in\ncritical scenarios. ERD involves optimizing classification precision and\nminimizing detection delay. Standard classification metrics may not suffice,\nresorting to specific metrics such as ERDE(theta) that explicitly consider\nprecision and delay. The current research focuses on applying a multi-objective\napproach, prioritizing classification performance and establishing a separate\ncriterion for decision time. In this work, we propose a completely different\nstrategy, temporal fine-tuning, which allows tuning transformer-based models by\nexplicitly incorporating time within the learning process. Our method allows us\nto analyze complete user post histories, tune models considering different\ncontexts, and evaluate training performance using temporal metrics. We\nevaluated our proposal in the depression and eating disorders tasks for the\nSpanish language, achieving competitive results compared to the best models of\nMentalRiskES 2023. We found that temporal fine-tuning optimized decisions\nconsidering context and time progress. In this way, by properly taking\nadvantage of the power of transformers, it is possible to address ERD by\ncombining precision and speed as a single objective.", "AI": {"tldr": "This paper presents a novel approach for Early Risk Detection (ERD) on the Web, focusing on improving classification precision and minimizing detection delay through a temporal fine-tuning strategy for transformer models.", "motivation": "The need for timely and accurate identification of users facing social and health issues on the web, particularly in critical scenarios.", "method": "The authors propose a multi-objective approach that combines classification performance with decision time optimization, utilizing a novel temporal fine-tuning strategy for transformer-based models.", "result": "The evaluation demonstrates that temporal fine-tuning achieves competitive performance in diagnosing depression and eating disorders in Spanish, outperforming existing models in terms of precision and detection speed.", "conclusion": "The proposed method effectively addresses the challenges of ERD by integrating temporal considerations within the learning process, optimizing both precision and speed.", "key_contributions": ["Introduction of temporal fine-tuning for transformer-based models", "Application of a multi-objective approach combining precision and delay", "Competitive results in mental health detection tasks in Spanish context."], "limitations": "", "keywords": ["Early Risk Detection", "transformer models", "multi-objective optimization"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2505.11297", "pdf": "https://arxiv.org/pdf/2505.11297.pdf", "abs": "https://arxiv.org/abs/2505.11297", "title": "Probing Subphonemes in Morphology Models", "authors": ["Gal Astrach", "Yuval Pinter"], "categories": ["cs.CL"], "comment": null, "summary": "Transformers have achieved state-of-the-art performance in morphological\ninflection tasks, yet their ability to generalize across languages and\nmorphological rules remains limited. One possible explanation for this behavior\ncan be the degree to which these models are able to capture implicit phenomena\nat the phonological and subphonemic levels. We introduce a language-agnostic\nprobing method to investigate phonological feature encoding in transformers\ntrained directly on phonemes, and perform it across seven morphologically\ndiverse languages. We show that phonological features which are local, such as\nfinal-obstruent devoicing in Turkish, are captured well in phoneme embeddings,\nwhereas long-distance dependencies like vowel harmony are better represented in\nthe transformer's encoder. Finally, we discuss how these findings inform\nempirical strategies for training morphological models, particularly regarding\nthe role of subphonemic feature acquisition.", "AI": {"tldr": "This study introduces a language-agnostic probing method to explore phonological feature encoding in transformers, revealing varying effectiveness in capturing local and long-distance dependencies across seven languages.", "motivation": "To understand why transformers struggle to generalize across languages and morphological rules in inflection tasks, particularly in relation to phonological phenomena.", "method": "A language-agnostic probing method is introduced, focusing on phonological feature encoding in transformers trained on phonemes, assessed across seven morphologically diverse languages.", "result": "Phonological features that are local, such as devoicing, are well-represented in phoneme embeddings, while long-distance dependencies like vowel harmony are better captured by the transformer's encoder.", "conclusion": "The findings provide insights into empirical strategies for training morphological models, highlighting the importance of subphonemic feature acquisition.", "key_contributions": ["Introduction of a language-agnostic probing method for phonological features", "Analysis of local vs long-distance phonological dependencies in transformers", "Guidance for empirical strategies in training morphological models"], "limitations": "The study is limited to seven languages, which may not encompass the full range of morphological diversity.", "keywords": ["Transformers", "Morphological inflection", "Phonological features", "Language-agnostic probing", "Subphonemic features"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.11336", "pdf": "https://arxiv.org/pdf/2505.11336.pdf", "abs": "https://arxiv.org/abs/2505.11336", "title": "XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision", "authors": ["Nuo Chen", "Andre Lin HuiKai", "Jiaying Wu", "Junyi Hou", "Zining Zhang", "Qian Wang", "Xidong Wang", "Bingsheng He"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.", "AI": {"tldr": "The paper presents a human-AI collaboration framework called XtraGPT, aimed at enhancing academic writing via context-aware, instruction-guided LLMs.", "motivation": "The study addresses the limitations of existing large language models in supporting the complex demands of academic writing, which often requires more than just surface-level revisions.", "method": "The authors introduce a dataset of 7,040 annotated research papers with 140,000 instruction-response pairs, and develop XtraGPT, a suite of open-source LLMs designed for academic paper revision.", "result": "XtraGPT shows significant performance improvements over same-scale baselines and approaches the quality of proprietary systems through extensive experiments.", "conclusion": "The findings demonstrate that XtraGPT effectively aids in improving the quality of scientific drafts through iterative and context-aware revisions.", "key_contributions": ["Introduction of a comprehensive dataset for academic paper revisions.", "Development of XtraGPT, a suite of open-source LLMs for academic writing.", "Validation of enhanced performance through automated assessments and human evaluations."], "limitations": "", "keywords": ["human-AI collaboration", "large language models", "academic writing", "XtraGPT", "instruction-guided revision"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11341", "pdf": "https://arxiv.org/pdf/2505.11341.pdf", "abs": "https://arxiv.org/abs/2505.11341", "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models", "authors": ["Banca Calvo Figueras", "Rodrigo Agerri"], "categories": ["cs.CL"], "comment": null, "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose assumptions and\nchallenge the reasoning in arguments. Despite growing interest in this area,\nprogress has been hindered by the lack of suitable datasets and automatic\nevaluation standards. This work presents a comprehensive approach to support\nthe development and benchmarking of systems for this task. We construct the\nfirst large-scale manually-annotated dataset. We also investigate automatic\nevaluation methods and identify a reference-based technique using large\nlanguage models (LLMs) as the strategy that best correlates with human\njudgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline\nwhile showcasing the difficulty of the task. Data, code, and a public\nleaderboard are provided to encourage further research not only in terms of\nmodel performance, but also to explore the practical benefits of CQs-Gen for\nboth automated reasoning and human critical thinking.", "AI": {"tldr": "This paper presents a comprehensive approach to Critical Questions Generation by creating a large-scale dataset and evaluating LLMs for generating critical questions to enhance reasoning and critical thinking.", "motivation": "To promote critical thinking through the development of systems that generate questions challenging assumptions and reasoning in arguments, addressing the current limitations in datasets and evaluation standards.", "method": "Construction of the first large-scale manually-annotated dataset for Critical Questions Generation, along with the exploration of automatic evaluation methods, particularly utilizing LLMs.", "result": "The zero-shot evaluation of 11 LLMs establishes a strong baseline for the task and highlights its inherent difficulty, while a reference-based evaluation method correlates well with human judgments.", "conclusion": "Providing data, code, and a public leaderboard aims to facilitate further research into both models' performance and the practical applications of Critical Questions Generation for reasoning and human critical thinking.", "key_contributions": ["Creation of the first large-scale manually-annotated dataset for Critical Questions Generation.", "Identification of effective automatic evaluation methods correlating with human judgment using LLMs.", "Provision of data, code, and a public leaderboard to support ongoing research efforts."], "limitations": "", "keywords": ["Critical Questions Generation", "Dataset", "Large Language Models", "Automatic Evaluation", "Critical Thinking"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2505.11352", "pdf": "https://arxiv.org/pdf/2505.11352.pdf", "abs": "https://arxiv.org/abs/2505.11352", "title": "LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors", "authors": ["Rao Ma", "Tongzhou Chen", "Kartik Audhkhasi", "Bhuvana Ramabhadran"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recently, large-scale pre-trained speech encoders and Large Language Models\n(LLMs) have been released, which show state-of-the-art performance on a range\nof spoken language processing tasks including Automatic Speech Recognition\n(ASR). To effectively combine both models for better performance, continuous\nspeech prompts, and ASR error correction have been adopted. However, these\nmethods are prone to suboptimal performance or are inflexible. In this paper,\nwe propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using\nthe ASR posterior matrices. The speech encoder is trained to generate\nConnectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,\nwhich are used to reconstruct pseudo-audio embeddings by computing a weighted\nsum of the LLM input embeddings. These embeddings are concatenated with text\nembeddings in the LLM input space. Using the well-performing USM and Gemma\nmodels as an example, we demonstrate that our proposed LegoSLM method yields\ngood performance on both ASR and speech translation tasks. By connecting USM\nwith Gemma models, we can get an average of 49% WERR over the USM-CTC baseline\non 8 MLS testsets. The trained model also exhibits modularity in a range of\nsettings -- after fine-tuning the Gemma model weights, the speech encoder can\nbe switched and combined with the LLM in a zero-shot fashion. Additionally, we\npropose to control the decode-time influence of the USM and LLM using a softmax\ntemperature, which shows effectiveness in domain adaptation.", "AI": {"tldr": "The paper proposes LegoSLM, a method that combines speech encoders and LLMs using ASR posterior matrices for improved performance in ASR and speech translation tasks.", "motivation": "To overcome the limitations of existing methods that combine speech encoders and LLMs, such as inflexibility and suboptimal performance.", "method": "The proposed LegoSLM method uses the ASR posterior matrices to generate pseudo-audio embeddings from a speech encoder, which are then concatenated with text embeddings for LLM input.", "result": "The LegoSLM method achieves a 49% word error rate reduction (WERR) over the USM-CTC baseline on 8 multi-lingual speech test sets and demonstrates modularity by allowing easy swapping of the speech encoder.", "conclusion": "LegoSLM effectively bridges the gap between speech encoders and LLMs, enhancing ASR and speech translation tasks while offering flexibility in model configurations.", "key_contributions": ["Introduction of the LegoSLM paradigm for integrating speech encoders and LLMs", "Demonstration of effective performance improvements in ASR and speech translation tasks", "Modular architecture allows for zero-shot model switching and fine-tuning capabilities"], "limitations": "", "keywords": ["speech processing", "large language models", "automatic speech recognition", "model modularity", "domain adaptation"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2505.11368", "pdf": "https://arxiv.org/pdf/2505.11368.pdf", "abs": "https://arxiv.org/abs/2505.11368", "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.", "AI": {"tldr": "Introduction of GuideBench, a benchmark for evaluating large language models (LLMs) on domain-oriented guideline following capabilities.", "motivation": "The need for comprehensive benchmarks to assess LLMs' abilities in following domain-specific guidelines due to conflicts with their commonsense knowledge and frequent updates of those guidelines.", "method": "GuideBench evaluates LLMs on adherence to diverse rules, robustness to rule updates, and alignment with human preferences.", "result": "Experimental results show significant opportunities for improving LLMs’ performances in guideline adherence.", "conclusion": "GuideBench can facilitate better evaluation and development of LLMs as it addresses critical aspects of their performance in domain-oriented settings.", "key_contributions": ["Introduction of a new benchmark (GuideBench) for evaluating LLMs", "Focus on three critical evaluation aspects: adherence to rules, robustness to updates, and human alignment", "Experimental results highlighting areas for improvement in LLM guideline following capabilities."], "limitations": "", "keywords": ["large language models", "benchmark", "guideline following", "domain-oriented", "human preferences"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11379", "pdf": "https://arxiv.org/pdf/2505.11379.pdf", "abs": "https://arxiv.org/abs/2505.11379", "title": "A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography", "authors": ["Alicia González Martínez"], "categories": ["cs.CL"], "comment": null, "summary": "Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic\nnotation that can be traced back to the early stages of Islam, when the Quran\nwas mainly oral in nature and the first written renderings of it served as\nmemory aids for this oral tradition. The early systems of diacritical marks\ncreated on top of the Quranic Consonantal Text (QCT) motivated the creation and\nfurther development of a fine-grained system of phonetic notation that\nrepresented tajwid-the rules of recitation. We explored the systematicity of\nthe rules of tajwid, as they are encountered in the Cairo Quran, using a fully\nand accurately encoded digital edition of the Quranic text. For this purpose,\nwe developed a python module that can remove or add the orthographic layer of\ntajwid from a Quranic text in CQO. The interesting characteristic of these two\nsets of rules is that they address the complete Quranic text of the Cairo\nQuran, so they can be used as precise witnesses to study its phonetic and\nprosodic processes. From a computational point of view, the text of the Cairo\nQuran can be used as a linchpin to align and compare Quranic manuscripts, due\nto its richness and completeness. This will let us create a very powerful\nframework to work with the Arabic script, not just within an isolated text, but\nautomatically exploring a specific textual phenomenon in other connected\nmanuscripts. Having all the texts mapped among each other can serve as a\npowerful tool to study the nature of the notation systems of diacritics added\nto the consonantal skeleton.", "AI": {"tldr": "This paper presents a comprehensive system for analyzing the phonetic rules of tajwid in the Quran using computational methods.", "motivation": "To explore the systematic rules of tajwid in the Quran and enhance the understanding of its phonetic processes through digital tools.", "method": "Developed a Python module to manipulate the orthographic layer of tajwid from the Contemporary Quranic Orthography (CQO) while studying the Cairo Quran text.", "result": "The research allows for precise analysis of the Quranic text and facilitates alignment and comparison of Quranic manuscripts, enriching the study of Arabic script.", "conclusion": "The developed framework can serve as an effective tool for examining diacritical notation systems in Quranic manuscripts.", "key_contributions": ["Development of a Python module for tajwid analysis", "Creation of a framework for comparing Quranic manuscripts", "Insights into the phonetic and prosodic rules of tajwid"], "limitations": "", "keywords": ["Quranic Orthography", "tajwid", "phonetic notation", "Quran", "Arabic script"], "importance_score": 0, "read_time_minutes": 10}}
{"id": "2505.11413", "pdf": "https://arxiv.org/pdf/2505.11413.pdf", "abs": "https://arxiv.org/abs/2505.11413", "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs", "authors": ["Sijia Chen", "Xiaomin Li", "Mengxue Zhang", "Eric Hanchen Jiang", "Qingcheng Zeng", "Chen-Hsiang Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in medical contexts,\nraising critical concerns about safety, alignment, and susceptibility to\nadversarial manipulation. While prior benchmarks assess model refusal\ncapabilities for harmful prompts, they often lack clinical specificity, graded\nharmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES\n(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for\nevaluating LLM safety in healthcare. CARES includes over 18,000 prompts\nspanning eight medical safety principles, four harm levels, and four prompting\nstyles: direct, indirect, obfuscated, and role-play, to simulate both malicious\nand benign use cases. We propose a three-way response evaluation protocol\n(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess\nmodel behavior. Our analysis reveals that many state-of-the-art LLMs remain\nvulnerable to jailbreaks that subtly rephrase harmful prompts, while also\nover-refusing safe but atypically phrased queries. Finally, we propose a\nmitigation strategy using a lightweight classifier to detect jailbreak attempts\nand steer models toward safer behavior via reminder-based conditioning. CARES\nprovides a rigorous framework for testing and improving medical LLM safety\nunder adversarial and ambiguous conditions.", "AI": {"tldr": "CARES is a benchmark designed to evaluate the safety of large language models (LLMs) in medical contexts, addressing existing gaps in harmful prompt assessments.", "motivation": "With the increasing deployment of LLMs in healthcare, there is a need for specific benchmarks that examine their safety and robustness against adversarial prompts.", "method": "CARES includes over 18,000 prompts categorized by medical safety principles, harm levels, and prompting styles, employing a three-way response evaluation protocol to assess model responses.", "result": "Analysis shows many LLMs are still vulnerable to cleverly rephrased harmful prompts, with a high tendency to over-reject safe queries.", "conclusion": "CARES offers a framework for evaluating and improving the safety of LLMs in medical settings, incorporating strategies to mitigate vulnerabilities.", "key_contributions": ["Introduction of the CARES benchmark for medical LLM safety evaluation.", "Development of a fine-grained Safety Score metric and a three-way response evaluation protocol.", "Proposal of a mitigation strategy using a lightweight classifier for detecting jailbreak attempts."], "limitations": "The benchmark may require continuous updates as LLMs and adversarial techniques evolve.", "keywords": ["Large Language Models", "Medical Safety", "Adversarial Robustness", "Health Informatics", "Benchmarking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11421", "pdf": "https://arxiv.org/pdf/2505.11421.pdf", "abs": "https://arxiv.org/abs/2505.11421", "title": "Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model", "authors": ["Phan Tran Minh Dat", "Vo Hoang Nhat Khang", "Quan Thanh Tho"], "categories": ["cs.CL"], "comment": null, "summary": "This work explores the journey towards achieving Bahnaric-Vietnamese\ntranslation for the sake of culturally bridging the two ethnic groups in\nVietnam. However, translating from Bahnaric to Vietnamese also encounters some\ndifficulties. The most prominent challenge is the lack of available original\nBahnaric resources source language, including vocabulary, grammar, dialogue\npatterns and bilingual corpus, which hinders the data collection process for\ntraining. To address this, we leverage a transfer learning approach using\nsequence-to-sequence pre-training language model. First of all, we leverage a\npre-trained Vietnamese language model to capture the characteristics of this\nlanguage. Especially, to further serve the purpose of machine translation, we\naim for a sequence-to-sequence model, not encoder-only like BERT or\ndecoder-only like GPT. Taking advantage of significant similarity between the\ntwo languages, we continue training the model with the currently limited\nbilingual resources of Vietnamese-Bahnaric text to perform the transfer\nlearning from language model to machine translation. Thus, this approach can\nhelp to handle the problem of imbalanced resources between two languages, while\nalso optimizing the training and computational processes. Additionally, we also\nenhanced the datasets using data augmentation to generate additional resources\nand defined some heuristic methods to help the translation more precise. Our\napproach has been validated to be highly effective for the Bahnaric-Vietnamese\ntranslation model, contributing to the expansion and preservation of languages,\nand facilitating better mutual understanding between the two ethnic people.", "AI": {"tldr": "This work presents a transfer learning approach for Bahnaric-Vietnamese translation using a sequence-to-sequence pre-training language model to address resource scarcity and improve translation accuracy.", "motivation": "To bridge the cultural gap between the Bahnaric and Vietnamese ethnic groups through better translation capabilities.", "method": "A sequence-to-sequence model is trained using a pre-trained Vietnamese language model and enhanced with bilingual resources and data augmentation techniques.", "result": "The model effectively handles the translation task despite limited resources, improving accuracy and fostering better understanding between the ethnic groups.", "conclusion": "The approach successfully contributes to the preservation of languages and enhances cultural exchange by providing a viable translation solution.", "key_contributions": ["Development of a transfer learning model for Bahnaric-Vietnamese translation", "Utilization of a pre-trained Vietnamese language model", "Implementation of data augmentation techniques for resource improvement"], "limitations": "", "keywords": ["Bahnaric", "Vietnamese", "machine translation", "transfer learning", "data augmentation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.11423", "pdf": "https://arxiv.org/pdf/2505.11423.pdf", "abs": "https://arxiv.org/abs/2505.11423", "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "authors": ["Xiaomin Li", "Zhou Yu", "Zhiwei Zhang", "Xupeng Chen", "Ziji Zhang", "Yingying Zhuang", "Narayanan Sadagopan", "Anurag Beniwal"], "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies.", "AI": {"tldr": "This study reveals that explicit chain-of-thought reasoning can degrade instruction-following accuracy in large language models and proposes strategies for mitigation.", "motivation": "To investigate the adverse effects of reasoning-enhanced prompting methods like chain-of-thought on instruction-following accuracy in large language models.", "method": "The authors evaluate 15 models on two benchmarks, IFEval and ComplexBench, and conduct large-scale case studies alongside attention-based analysis to examine performance variations with chain-of-thought prompting.", "result": "The performance of models dropped when using chain-of-thought prompts due to a diversion of attention from instruction-relevant tokens, along with the introduction of unnecessary content. Four mitigation strategies were introduced, notably classifier-selective reasoning, which improved model performance.", "conclusion": "The study is the first to systematically identify reasoning-induced failures in instruction-following tasks and offers practical strategies to counteract these issues.", "key_contributions": ["Systematic identification of performance degradation due to chain-of-thought reasoning in large language models.", "Introduction of the metric 'constraint attention' to quantify model focus during language generation.", "Evaluation of novel strategies (in-context learning, self-reflection, self-selective reasoning, classifier-selective reasoning) to recover lost performance."], "limitations": "", "keywords": ["large language models", "chain-of-thought", "instruction-following", "reasoning strategies", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11436", "pdf": "https://arxiv.org/pdf/2505.11436.pdf", "abs": "https://arxiv.org/abs/2505.11436", "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "69 pages, 66 figures, accepted by ACL 2025", "summary": "Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.", "AI": {"tldr": "The paper introduces GODBench, a novel benchmark for evaluating the creative capabilities of Multimodal Large Language Models (MLLMs) in generating video comment art, and proposes a multi-step reasoning framework called Ripple of Thought (RoT) to enhance MLLM creativity.", "motivation": "To address the limitations of existing benchmarks in evaluating creativity in video-based Comment Art and to improve the creative generation abilities of MLLMs.", "method": "The authors introduce a new benchmark, GODBench, which integrates video and text modalities. They also propose Ripple of Thought (RoT), a multi-step reasoning framework to enhance the creativity of MLLMs.", "result": "Extensive experiments show that existing MLLMs and Chain-of-Thought methods struggle with creative tasks in video comment art generation, while RoT shows promise in improving creative outputs.", "conclusion": "The findings highlight the challenges faced by current MLLMs in creative tasks and suggest that RoT can lead to advancements in MLLM-based creative applications.", "key_contributions": ["Introduction of GODBench benchmark for evaluating creative video comment generation", "Development of the Ripple of Thought framework to enhance MLLM creativity", "Demonstration of the difficulties current MLLMs face in creative expression tasks"], "limitations": "", "keywords": ["Multimodal Large Language Models", "video comment art", "Ripple of Thought"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441.pdf", "abs": "https://arxiv.org/abs/2505.11441", "title": "Is Compression Really Linear with Code Intelligence?", "authors": ["Xianzhen Luo", "Shijie Xuyang", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain.", "AI": {"tldr": "This paper explores the relationship between data compression and the intelligence of Large Language Models (LLMs) in code intelligence, proposing a new evaluation methodology called Format Annealing.", "motivation": "The study aims to better understand how data compression correlates with the capabilities of LLMs in specialized domains like code intelligence, challenging previous linear assumptions.", "method": "The authors introduce Format Annealing, a lightweight training methodology for evaluating open-source Code LLMs across multi-language and multi-task benchmarks.", "result": "The study reveals a logarithmic relationship between measured code intelligence and bits-per-character (BPC), refining previous linearity hypotheses.", "conclusion": "This work enhances the understanding of compression in code intelligence development and establishes a robust evaluation framework for code-based LLMs.", "key_contributions": ["Introduction of Format Annealing for fair evaluation of Code LLMs", "Identification of a logarithmic relationship between code intelligence and compression", "Development of a novel evaluation framework using a large-scale code validation set from GitHub."], "limitations": "The work is still in progress and may require further validation and refinement.", "keywords": ["Large Language Models", "code intelligence", "data compression", "Format Annealing", "evaluation framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2505.11462", "pdf": "https://arxiv.org/pdf/2505.11462.pdf", "abs": "https://arxiv.org/abs/2505.11462", "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.", "AI": {"tldr": "This paper analyzes biomedical QA benchmarks to differentiate between reasoning and knowledge in medical reasoning capabilities of LLMs, proposing a new model, BioMed-R1, which shows improved reasoning performance.", "motivation": "To improve the evaluation of medical reasoning in LLMs by separating reasoning from factual recall in biomedical QA benchmarks.", "method": "The authors classify 11 biomedical QA benchmarks into reasoning and knowledge subsets using a PubMedBERT classifier and evaluate various biomedical and general-domain models on these subsets.", "result": "The analysis found that only 32.8% of questions require complex reasoning, with significant discrepancies between knowledge and reasoning performance in evaluated models, highlighting the need for targeted training.", "conclusion": "BioMed-R1, trained with fine-tuning and reinforcement learning on reasoning-heavy examples, shows the best performance among its peers, suggesting potential for further improvement through specific training methods.", "key_contributions": ["Introduced a classification of biomedical QA benchmarks into reasoning and knowledge subsets.", "Developed BioMed-R1, a model specifically tuned for better reasoning performance.", "Demonstrated the gaps in performance between biomedical and general-domain models in reasoning tasks."], "limitations": "The study may be limited by the specific models evaluated and the benchmarks used, along with the challenge of achieving robust reasoning under adversarial conditions.", "keywords": ["Medical reasoning", "Large language models", "Biomedical QA", "Reinforcement learning", "Clinical case reports"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.11470", "pdf": "https://arxiv.org/pdf/2505.11470.pdf", "abs": "https://arxiv.org/abs/2505.11470", "title": "No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce two reference-free metrics for quality evaluation of taxonomies.\nThe first metric evaluates robustness by calculating the correlation between\nsemantic and taxonomic similarity, covering a type of error not handled by\nexisting metrics. The second uses Natural Language Inference to assess logical\nadequacy. Both metrics are tested on five taxonomies and are shown to correlate\nwell with F1 against gold-standard taxonomies.", "AI": {"tldr": "Introduction of two metrics for evaluating taxonomy quality.", "motivation": "To assess the quality of taxonomies without relying on reference standards, addressing gaps in existing metrics.", "method": "Developed a robustness metric based on the correlation of semantic and taxonomic similarity, and a logical adequacy metric using Natural Language Inference.", "result": "Both metrics demonstrated a strong correlation with F1 scores compared to gold-standard taxonomies in tests across five different taxonomies.", "conclusion": "The proposed metrics effectively evaluate taxonomy quality, providing new tools for researchers and practitioners.", "key_contributions": ["First metric evaluates robustness through correlation with semantic similarity.", "Second metric uses Natural Language Inference for logical adequacy assessment.", "Both metrics show strong correlation with established gold-standard evaluations."], "limitations": "Metrics depend on the selection of taxonomies tested, and their applicability to diverse domains needs further exploration.", "keywords": ["taxonomy evaluation", "Natural Language Inference", "semantic similarity", "robustness", "logical adequacy"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2505.11475", "pdf": "https://arxiv.org/pdf/2505.11475.pdf", "abs": "https://arxiv.org/abs/2505.11475", "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "38 pages, 2 figures", "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference", "AI": {"tldr": "This paper introduces HelpSteer3-Preference, a high-quality dataset of over 40,000 human-annotated preference samples for training language models with RLHF, demonstrating significant performance improvements over existing models.", "motivation": "There is a constant need to improve the quality and diversity of preference datasets for training language models using Reinforcement Learning from Human Feedback (RLHF).", "method": "The authors introduce HelpSteer3-Preference, a dataset consisting of over 40,000 samples annotated by humans, covering various applications of large language models, and utilize it to train Reward Models (RMs).", "result": "HelpSteer3-Preference allows the training of Reward Models that achieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%), marking a significant improvement of ~10% over the best-reported existing RMs.", "conclusion": "HelpSteer3-Preference not only enhances the training of performance-improved Reward Models but also supports the training of Generative RMs and aligns policy models with RLHF.", "key_contributions": ["Introduction of HelpSteer3-Preference, a permissively licensed, large-scale human-annotated preference dataset.", "Demonstration of the dataset's effectiveness in training high-performing Reward Models that outperform previous benchmarks.", "Application of the dataset in training Generative RMs and alignment with RLHF."], "limitations": "", "keywords": ["Reinforcement Learning", "Language Models", "Preference Datasets", "HelpSteer3-Preference", "Human Feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11480", "pdf": "https://arxiv.org/pdf/2505.11480.pdf", "abs": "https://arxiv.org/abs/2505.11480", "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning", "authors": ["Anjiang Wei", "Tarun Suresh", "Huanmi Tan", "Yinglun Xu", "Gagandeep Singh", "Ke Wang", "Alex Aiken"], "categories": ["cs.CL", "cs.AI", "cs.PF", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.", "AI": {"tldr": "This paper explores the use of large language models (LLMs) for optimizing assembly code performance through a reinforcement learning framework, achieving significant improvements over existing compiler standards.", "motivation": "The potential of LLMs for code optimization in assembly language remains largely underexplored, particularly in achieving high performance optimizations that are hard to express in high-level programming languages.", "method": "A reinforcement learning framework utilizing Proximal Policy Optimization (PPO) was developed, with a focus on a reward function that considers functional correctness and execution performance, validated against a substantial benchmark of 8,072 real-world programs.", "result": "The Qwen2.5-Coder-7B-PPO model achieved a 96.0% test pass rate and an average speedup of 1.47x compared to the gcc -O3 compiler baseline, outperforming all 20 models tested, including Claude-3.7-sonnet.", "conclusion": "This study demonstrates that reinforcement learning can effectively enable LLMs to optimize assembly code performance, showcasing their potential beyond high-level programming tasks.", "key_contributions": ["Introduction of a reinforcement learning framework for LLM optimization of assembly code.", "Creation of a benchmark comprising 8,072 real-world programs for evaluation.", "Demonstration of significant performance improvements over standard compiler optimization."], "limitations": "", "keywords": ["large language models", "code optimization", "reinforcement learning", "assembly code", "performance benchmarks"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2505.11484", "pdf": "https://arxiv.org/pdf/2505.11484.pdf", "abs": "https://arxiv.org/abs/2505.11484", "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "categories": ["cs.CL"], "comment": "14 pages", "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.", "AI": {"tldr": "SoftCoT++ enhances Test-Time Scaling by enabling diverse exploration of reasoning paths through perturbation of latent thoughts and contrastive learning, outperforming existing methods.", "motivation": "To improve reasoning performance during inference by allowing diverse exploration of thinking paths in continuous latent space.", "method": "Introduced SoftCoT++ which extends SoftCoT by perturbing latent thoughts with multiple specialized initial tokens and applying contrastive learning.", "result": "SoftCoT++ significantly boosts SoftCoT's performance and outperforms it with self-consistency scaling on five reasoning benchmarks with strong compatibility with existing scaling techniques.", "conclusion": "SoftCoT++ provides a novel approach to continuous-space reasoning that enhances performance while allowing diverse explorations of thought pathways.", "key_contributions": ["Introduction of SoftCoT++", "Enhancement of reasoning performance through diverse exploration", "Strong compatibility with conventional scaling techniques"], "limitations": "", "keywords": ["Test-Time Scaling", "SoftCoT++", "latent representations"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2505.11485", "pdf": "https://arxiv.org/pdf/2505.11485.pdf", "abs": "https://arxiv.org/abs/2505.11485", "title": "Modeling cognitive processes of natural reading with transformer-based Language Models", "authors": ["Bruno Bianchi", "Fermín Travi", "Juan E. Kamienkowski"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers.", "AI": {"tldr": "This study evaluates transformer-based language models in relation to gaze duration during reading, finding they outperform earlier models but still do not fully capture human predictability.", "motivation": "To investigate the relationship between advanced NLP models and cognitive processes in language comprehension, specifically focusing on Gaze Duration during reading.", "method": "Evaluation of transformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) on gaze durations recorded from Rioplantense Spanish readers.", "result": "Transformer-based models outperform previous models, showing better predictive capacity for Gaze Durations, but still do not fully explain the variance captured by human predictability.", "conclusion": "State-of-the-art language models, despite their advancements, predict language processing differently from human readers.", "key_contributions": ["Demonstrated improved performance of transformer models over traditional models regarding gaze duration predictions.", "Highlighted the limitations of transformer models in fully capturing human cognitive predictability during reading."], "limitations": "Transformer models do not account for all the variance in gaze duration related to human predictability, indicating a gap between model predictions and actual human behavior.", "keywords": ["Natural Language Processing", "Cognitive Processes", "Gaze Duration"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831.pdf", "abs": "https://arxiv.org/abs/2505.10831", "title": "Creating General User Models from Computer Use", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture that user knowledge and preferences. GUMs can infer that a user is\npreparing for a wedding they're attending from messages with a friend. Or\nrecognize that a user is struggling with a collaborator's feedback on a draft\nby observing multiple stalled edits and a switch to reading related work. GUMs\nintroduce an architecture that infers new propositions about a user from\nmultimodal observations, retrieves related propositions for context, and\ncontinuously revises existing propositions. To illustrate the breadth of\napplications that GUMs enable, we demonstrate how they augment chat-based\nassistants with context, manage OS notifications to selectively surface\nimportant information, and enable interactive agents that adapt to preferences\nacross apps. We also instantiate proactive assistants (GUMBOs) that discover\nand execute useful suggestions on a user's behalf using their GUM. In our\nevaluations, we find that GUMs make calibrated and accurate inferences about\nusers, and that assistants built on GUMs proactively identify and perform\nactions that users wouldn't think to request explicitly. Altogether, GUMs\nintroduce methods that leverage multimodal models to understand unstructured\ncontext, enabling long-standing visions of HCI and entirely new interactive\nsystems that anticipate user needs.", "AI": {"tldr": "This paper introduces a General User Model (GUM) architecture that learns about users through their interactions, enabling various applications in human-computer interaction.", "motivation": "To develop technology that comprehensively understands user preferences and habits beyond isolated contexts, addressing the limitations of current user models.", "method": "The GUM architecture processes unstructured user observations, inferring confidence-weighted propositions about user knowledge and preferences through multimodal data.", "result": "GUMs accurately infer user intentions and preferences, leading to the development of proactive assistants that can perform actions based on inferred context without explicit user requests.", "conclusion": "GUMs offer a novel method for understanding unstructured context in user interactions, paving the way for advanced interactive systems that can anticipate user needs.", "key_contributions": ["Introduction of the General User Model (GUM) architecture", "Illustration of various applications across chat-based assistants and OS notifications", "Development of proactive assistants (GUMBOs) that execute user-oriented actions"], "limitations": "", "keywords": ["Human-Computer Interaction", "User Modeling", "Proactive Assistants"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.11406", "pdf": "https://arxiv.org/pdf/2505.11406.pdf", "abs": "https://arxiv.org/abs/2505.11406", "title": "Large Language Model Use Impact Locus of Control", "authors": ["Jenny Xiyu Fu", "Brennan Antone", "Kowe Kadoma", "Malte Jung"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity.", "AI": {"tldr": "This paper investigates how using AI in writing affects users' psychological state and sense of personal agency, highlighting different effects based on employment status.", "motivation": "To explore the psychological consequences of co-writing with AI and its influence on the sense of personal agency and identity.", "method": "An empirical study involving 462 participants was conducted to assess the impact of AI on users' locus of control, with a focus on employment status as a key variable.", "result": "The study found that employed individuals relied more on AI and experienced a shift towards internal control, while unemployed participants felt a diminished sense of personal agency.", "conclusion": "The findings suggest that AI tools may significantly influence users' perceptions of their own agency and identity, prompting further discussion on the societal implications of AI integration in writing.", "key_contributions": ["Investigates the psychological impact of AI on co-writing and personal agency", "Identifies employment status as a critical factor in users' reliance on AI", "Initiates dialogue on the broader implications of AI in shaping identity"], "limitations": "The study is limited to self-reported measures and may not capture all dimensions of personal agency.", "keywords": ["AI", "personal agency", "employment status", "locus of control", "psychological impact"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2305.15099", "pdf": "https://arxiv.org/pdf/2305.15099.pdf", "abs": "https://arxiv.org/abs/2305.15099", "title": "Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator", "authors": ["Ziwei He", "Meng Yang", "Minwei Feng", "Jingcheng Yin", "Xinbing Wang", "Jingwen Leng", "Zhouhan Lin"], "categories": ["cs.CL"], "comment": null, "summary": "The transformer model is known to be computationally demanding, and\nprohibitively costly for long sequences, as the self-attention module uses a\nquadratic time and space complexity with respect to sequence length. Many\nresearchers have focused on designing new forms of self-attention or\nintroducing new parameters to overcome this limitation, however a large portion\nof them prohibits the model to inherit weights from large pretrained models. In\nthis work, the transformer's inefficiency has been taken care of from another\nperspective. We propose Fourier Transformer, a simple yet effective approach by\nprogressively removing redundancies in hidden sequence using the ready-made\nFast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation\n(DCT). Fourier Transformer is able to significantly reduce computational costs\nwhile retain the ability to inherit from various large pretrained models.\nExperiments show that our model achieves state-of-the-art performances among\nall transformer-based models on the long-range modeling benchmark LRA with\nsignificant improvement in both speed and space. For generative seq-to-seq\ntasks including CNN/DailyMail and ELI5, by inheriting the BART weights our\nmodel outperforms the standard BART and other efficient models. Our code is\npublicly available at https://github.com/LUMIA-Group/FourierTransformer", "AI": {"tldr": "The Fourier Transformer reduces computational costs in transformer models using the Fast Fourier Transform, allowing weight inheritance from pretrained models, achieving state-of-the-art performances on benchmarks.", "motivation": "To address the inefficiencies of the transformer model in handling long sequences due to its quadratic complexity, while still enabling the model to inherit weights from large pretrained models.", "method": "The paper introduces the Fourier Transformer, which uses the Fast Fourier Transform (FFT) to perform Discrete Cosine Transformation (DCT) to remove redundancies in hidden sequences.", "result": "The Fourier Transformer significantly reduces computational costs and retains the ability to inherit weights, outperforming standard transformer models, including BART, on various benchmarks.", "conclusion": "This new approach demonstrates that it is possible to enhance transformer efficiency without sacrificing weight inheritance, achieving better performance on generative tasks.", "key_contributions": ["Introduction of the Fourier Transformer to improve efficiency in long-sequence tasks.", "Demonstration of state-of-the-art performance on the long-range modeling benchmark LRA.", "Ability to inherit from various large pretrained models while utilizing FFT for computational efficiency."], "limitations": "", "keywords": ["Fourier Transformer", "FFT", "DCT", "transformer models", "long-range modeling"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2311.07564", "pdf": "https://arxiv.org/pdf/2311.07564.pdf", "abs": "https://arxiv.org/abs/2311.07564", "title": "Can Authorship Attribution Models Distinguish Speakers in Speech Transcripts?", "authors": ["Cristina Aggazzotti", "Nicholas Andrews", "Elizabeth Allyn Smith"], "categories": ["cs.CL", "cs.LG"], "comment": "Published in Transactions of the Association for Computational\n  Linguistics; 1st revision includes additional experiments and evaluations;\n  2nd revision includes minor tweak to TFIDF table numbers", "summary": "Authorship verification is the task of determining if two distinct writing\nsamples share the same author and is typically concerned with the attribution\nof written text. In this paper, we explore the attribution of transcribed\nspeech, which poses novel challenges. The main challenge is that many stylistic\nfeatures, such as punctuation and capitalization, are not informative in this\nsetting. On the other hand, transcribed speech exhibits other patterns, such as\nfiller words and backchannels (e.g., 'um', 'uh-huh'), which may be\ncharacteristic of different speakers. We propose a new benchmark for speaker\nattribution focused on human-transcribed conversational speech transcripts. To\nlimit spurious associations of speakers with topic, we employ both conversation\nprompts and speakers participating in the same conversation to construct\nverification trials of varying difficulties. We establish the state of the art\non this new benchmark by comparing a suite of neural and non-neural baselines,\nfinding that although written text attribution models achieve surprisingly good\nperformance in certain settings, they perform markedly worse as conversational\ntopic is increasingly controlled. We present analyses of the impact of\ntranscription style on performance as well as the ability of fine-tuning on\nspeech transcripts to improve performance.", "AI": {"tldr": "This paper investigates authorship verification for transcribed speech, presenting a new benchmark and analyzing the effectiveness of various models.", "motivation": "To address the novel challenges of authorship verification in transcribed speech, where traditional stylistic features of written text are less informative.", "method": "The authors propose a new benchmark for speaker attribution specific to human-transcribed conversational speech transcripts, employing conversation prompts and controlled trials to mitigate speaker-topic associations.", "result": "The study establishes state-of-the-art performance on the new benchmark, revealing that while written text attribution models perform adequately, their effectiveness decreases as conversational topic control increases.", "conclusion": "Fine-tuning on speech transcripts can significantly enhance performance, and the analysis provides insights into how transcription styles impact model efficacy.", "key_contributions": ["Introduction of a new benchmark for author verification in conversational speech transcripts.", "Insights on the effectiveness of existing written text attribution models in new contexts.", "Analysis of the influence of transcription style on model performance."], "limitations": "", "keywords": ["authorship verification", "speaker attribution", "transcribed speech", "conversation transcripts", "neural models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2402.14889", "pdf": "https://arxiv.org/pdf/2402.14889.pdf", "abs": "https://arxiv.org/abs/2402.14889", "title": "COBIAS: Assessing the Contextual Reliability of Bias Benchmarks for Language Models", "authors": ["Priyanshul Govil", "Hemang Jain", "Vamshi Krishna Bonagiri", "Aman Chadha", "Ponnurangam Kumaraguru", "Manas Gaur", "Sanorita Dey"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often inherit biases from the web data they are\ntrained on, which contains stereotypes and prejudices. Current methods for\nevaluating and mitigating these biases rely on bias-benchmark datasets. These\nbenchmarks measure bias by observing an LLM's behavior on biased statements.\nHowever, these statements lack contextual considerations of the situations they\ntry to present. To address this, we introduce a contextual reliability\nframework, which evaluates model robustness to biased statements by considering\nthe various contexts in which they may appear. We develop the Context-Oriented\nBias Indicator and Assessment Score (COBIAS) to measure a biased statement's\nreliability in detecting bias, based on the variance in model behavior across\ndifferent contexts. To evaluate the metric, we augmented 2,291 stereotyped\nstatements from two existing benchmark datasets by adding contextual\ninformation. We show that COBIAS aligns with human judgment on the contextual\nreliability of biased statements (Spearman's $\\rho = 0.65, p = 3.4 * 10^{-60}$)\nand can be used to create reliable benchmarks, which would assist bias\nmitigation works.", "AI": {"tldr": "This paper introduces a new framework for evaluating bias in Large Language Models (LLMs) by considering contextual factors, leading to the development of the Context-Oriented Bias Indicator and Assessment Score (COBIAS).", "motivation": "Current bias evaluation methods for LLMs do not adequately consider the context in which biased statements are made, limiting their effectiveness in bias mitigation.", "method": "The authors developed the COBIAS metric, which evaluates the reliability of biased statements based on model behavior across various contexts, assessed using augmented stereotyped statements.", "result": "The study shows that COBIAS aligns well with human judgments, demonstrating a significant correlation, enabling the creation of more reliable benchmarks for bias assessment.", "conclusion": "COBIAS can enhance bias evaluation by integrating context into assessments and can aid in developing more effective bias mitigation strategies for LLMs.", "key_contributions": ["Introduction of a contextual reliability framework for LLM bias evaluation.", "Development of the COBIAS metric that incorporates context.", "Demonstration of COBIAS's alignment with human judgment on bias reliability."], "limitations": "The study relies on existing benchmark datasets for bias, which may have their own limitations and biases.", "keywords": ["Large Language Models", "bias", "contextual evaluation", "COBIAS", "human judgment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2404.10652", "pdf": "https://arxiv.org/pdf/2404.10652.pdf", "abs": "https://arxiv.org/abs/2404.10652", "title": "ViTextVQA: A Large-Scale Visual Question Answering Dataset for Evaluating Vietnamese Text Comprehension in Images", "authors": ["Quan Van Nguyen", "Dan Quang Tran", "Huy Quang Pham", "Thang Kien-Bao Nguyen", "Nghia Hieu Nguyen", "Kiet Van Nguyen", "Ngan Luu-Thuy Nguyen"], "categories": ["cs.CL"], "comment": null, "summary": "Visual Question Answerinng (VQA) is a complicated task that requires the\ncapability of simultaneously processing natural language and images. This task\nwas initially researched with a focus on developing methods to help machines\nunderstand objects and scene contexts in images. However, some scene text that\ncarries explicit information about the full content of the image is not\nmentioned. Along with the continuous development of the AI era, there have been\nmany studies on the reading comprehension ability of VQA models in the world.\nTherefore, we introduce the first large-scale dataset in Vietnamese\nspecializing in the ability to understand scene text, we call it ViTextVQA\n(\\textbf{Vi}etnamese \\textbf{Text}-based \\textbf{V}isual \\textbf{Q}uestion\n\\textbf{A}nswering dataset) which contains \\textbf{over 16,000} images and\n\\textbf{over 50,000} questions with answers. To tackle this task efficiently,\nwe propose ViTextBLIP-2, an novel multimodal feature fusion Method, which\noptimizes Vietnamese OCR-based VQA by integrating a frozen Vision Transformer,\nSwinTextSpotter OCR, and ViT5 LLM with a trainable Q-Former for multimodal\nfeature fusion. Through experiments with various state-of-the-art models, we\nuncover the significance of the order in which tokens in OCR text are processed\nand selected to formulate answers. This finding helped us significantly improve\nthe performance of the baseline models on the ViTextVQA dataset. Our dataset is\navailable (https://github.com/minhquan6203/ViTextVQA-Dataset) for research\npurposes.", "AI": {"tldr": "Introduction of the ViTextVQA dataset and the ViTextBLIP-2 method for improving visual question answering in Vietnamese.", "motivation": "Enhance the understanding of scene text in visual question answering models and address gaps in current datasets and methodologies.", "method": "Introduced a novel method called ViTextBLIP-2 which integrates a frozen Vision Transformer, SwinTextSpotter OCR, and ViT5 LLM, along with a trainable Q-Former for multimodal feature fusion.", "result": "Significant performance improvements on the ViTextVQA dataset compared to baseline models.", "conclusion": "The study highlights the importance of token processing order in OCR text for improving answer formulation in VQA tasks, emphasizing the effectiveness of the ViTextVQA dataset and proposed method.", "key_contributions": ["First large-scale Vietnamese dataset for VQA focusing on scene text comprehension", "Development of ViTextBLIP-2 for multimodal feature fusion in VQA", "Experimental findings on token processing order in OCR text influencing performance."], "limitations": "", "keywords": ["Visual Question Answering", "Vietnamese Dataset", "Multimodal Learning", "OCR", "Machine Learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2405.00715", "pdf": "https://arxiv.org/pdf/2405.00715.pdf", "abs": "https://arxiv.org/abs/2405.00715", "title": "Towards Adapting Open-Source Large Language Models for Expert-Level Clinical Note Generation", "authors": ["Hanyin Wang", "Chufan Gao", "Bolun Liu", "Qiping Xu", "Guleid Hussein", "Mohamad El Labban", "Kingsley Iheasirim", "Hariprasad Korsapati", "Chuck Outcalt", "Jimeng Sun"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Proprietary Large Language Models (LLMs) such as GPT-4 and Gemini have\ndemonstrated promising capabilities in clinical text summarization tasks.\nHowever, due to patient data privacy concerns and computational costs, many\nhealthcare providers prefer using small, locally-hosted models over external\ngeneric LLMs. This study presents a comprehensive domain- and task-specific\nadaptation process for the open-source LLaMA-2 13 billion parameter model,\nenabling it to generate high-quality clinical notes from outpatient\npatient-doctor dialogues. Our process incorporates continued pre-training,\nsupervised fine-tuning, and reinforcement learning from both AI and human\nfeedback. We introduced a new approach, DistillDirect, for performing on-policy\nreinforcement learning with Gemini 1.0 Pro as the teacher model. Our resulting\nmodel, LLaMA-Clinic, can generate clinical notes comparable in quality to those\nauthored by physicians. In a blinded physician reader study, the majority\n(90.4%) of individual evaluations rated the notes generated by LLaMA-Clinic as\n\"acceptable\" or higher across all three criteria: real-world readiness,\ncompleteness, and accuracy. In the more challenging \"Assessment and Plan\"\nsection, LLaMA-Clinic scored higher (4.2/5) in real-world readiness than\nphysician-authored notes (4.1/5). We highlight key considerations for future\nclinical note-generation tasks, emphasizing the importance of pre-defining a\nbest-practice note format, rather than relying on LLMs to determine this for\nclinical practice.", "AI": {"tldr": "A study adapting the open-source LLaMA-2 model for generating clinical notes, achieving quality comparable to physician-authored notes through a novel reinforcement learning approach.", "motivation": "To address patient data privacy concerns and computational costs associated with proprietary LLMs in healthcare, focusing on locally-hosted models for clinical text summarization.", "method": "The adaptation process involved continued pre-training, supervised fine-tuning, and reinforcement learning through a novel approach called DistillDirect, utilizing Gemini 1.0 Pro as the teacher model.", "result": "LLaMA-Clinic was developed to generate high-quality clinical notes, achieving a 90.4% acceptance rate in blinded evaluations by physician readers and scoring higher than physician notes in the Assessment and Plan section.", "conclusion": "The study underscores the importance of pre-defining best-practice note formats for clinical documentation instead of relying solely on LLM output.", "key_contributions": ["Development of LLaMA-Clinic for clinical note generation", "Novel reinforcement learning approach (DistillDirect) for on-policy training", "Empirical validation through physician evaluations displaying high acceptance rates"], "limitations": "", "keywords": ["Large Language Models", "Clinical Text Summarization", "Healthcare AI"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2406.06326", "pdf": "https://arxiv.org/pdf/2406.06326.pdf", "abs": "https://arxiv.org/abs/2406.06326", "title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching", "authors": ["Xiaoying Zhang", "Baolin Peng", "Ye Tian", "Jingyan Zhou", "Yipeng Zhang", "Haitao Mi", "Helen Meng"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from unseen raw documents through self-teaching. Specifically, we\ndevelop a Self-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection.\nAdditionally, we introduce three Wiki-Newpages-2023-QA datasets to facilitate\nan in-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nvarious models, e.g., Llama2-7B reveal that Self-Tuning consistently exhibits\nsuperior performance across all knowledge acquisition tasks and excels in\npreserving previous knowledge.", "AI": {"tldr": "This paper introduces Self-Tuning, a framework enhancing LLMs' ability to acquire new knowledge through self-teaching, leveraging a set of knowledge-intensive tasks.", "motivation": "To address LLMs' struggles with providing up-to-date information and their difficulties in extracting stored knowledge due to one-time training.", "method": "The Self-Tuning framework incorporates a Self-Teaching strategy that enhances documents with knowledge-intensive tasks focusing on memorization, comprehension, and self-reflection.", "result": "Experimental results show that Self-Tuning significantly improves LLM performance in knowledge acquisition tasks across various models, including Llama2-7B, while preserving previous knowledge.", "conclusion": "Self-Tuning represents a substantial advancement in enabling LLMs to continually learn from new documents and maintain their prior knowledge effectively.", "key_contributions": ["Introduction of the Self-Tuning framework for LLMs", "Development of a Self-Teaching strategy with novel tasks", "Three new datasets (Wiki-Newpages-2023-QA) for analyzing knowledge acquisition"], "limitations": "", "keywords": ["Large Language Models", "Self-Tuning", "Knowledge Acquisition", "Self-Teaching", "LLM Training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.06518", "pdf": "https://arxiv.org/pdf/2408.06518.pdf", "abs": "https://arxiv.org/abs/2408.06518", "title": "Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models", "authors": ["Hila Gonen", "Terra Blevins", "Alisa Liu", "Luke Zettlemoyer", "Noah A. Smith"], "categories": ["cs.CL"], "comment": null, "summary": "Despite their wide adoption, the biases and unintended behaviors of language\nmodels remain poorly understood. In this paper, we identify and characterize a\nphenomenon never discussed before, which we call semantic leakage, where models\nleak irrelevant information from the prompt into the generation in unexpected\nways. We propose an evaluation setting to detect semantic leakage both by\nhumans and automatically, curate a diverse test suite for diagnosing this\nbehavior, and measure significant semantic leakage in 13 flagship models. We\nalso show that models exhibit semantic leakage in languages besides English and\nacross different settings and generation scenarios. This discovery highlights\nyet another type of bias in language models that affects their generation\npatterns and behavior.", "AI": {"tldr": "This paper introduces the concept of semantic leakage in language models, where irrelevant information from prompts influences generated outputs, and provides a methodology for evaluation and measurement across various models and languages.", "motivation": "To understand the biases and unintended behaviors of language models, specifically focusing on the newly identified phenomenon of semantic leakage.", "method": "The authors propose an evaluation setting for detecting semantic leakage both manually and automatically, curate a test suite for diagnostics, and measure semantic leakage across 13 flagship language models in multiple languages.", "result": "Significant levels of semantic leakage were detected in multiple models across different settings, revealing that this issue is prevalent beyond English and affects generation patterns.", "conclusion": "Semantic leakage is a newly identified bias in language models that impacts their behavior and generation, necessitating further study and consideration in their evaluation.", "key_contributions": ["Introduction of the concept of semantic leakage in language models", "Development of evaluation settings for detecting semantic leakage", "Measurement of semantic leakage in 13 flagship models across various languages and scenarios."], "limitations": "The study focuses only on a limited set of language models and may not capture all aspects of semantic leakage in less prominent models.", "keywords": ["semantic leakage", "language models", "bias", "evaluation", "natural language generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2409.09636", "pdf": "https://arxiv.org/pdf/2409.09636.pdf", "abs": "https://arxiv.org/abs/2409.09636", "title": "Towards understanding evolution of science through language model series", "authors": ["Junjie Dong", "Zhuoqi Lyu", "Qing Ke"], "categories": ["cs.CL", "cs.CY", "cs.DL"], "comment": null, "summary": "We introduce AnnualBERT, a series of language models designed specifically to\ncapture the temporal evolution of scientific text. Deviating from the\nprevailing paradigms of subword tokenizations and \"one model to rule them all\",\nAnnualBERT adopts whole words as tokens and is composed of a base RoBERTa model\npretrained from scratch on the full-text of 1.7 million arXiv papers published\nuntil 2008 and a collection of progressively trained models on arXiv papers at\nan annual basis. We demonstrate the effectiveness of AnnualBERT models by\nshowing that they not only have comparable performances in standard tasks but\nalso achieve state-of-the-art performances on domain-specific NLP tasks as well\nas link prediction tasks in the arXiv citation network. We then utilize probing\ntasks to quantify the models' behavior in terms of representation learning and\nforgetting as time progresses. Our approach enables the pretrained models to\nnot only improve performances on scientific text processing tasks but also to\nprovide insights into the development of scientific discourse over time. The\nseries of the models is available at https://huggingface.co/jd445/AnnualBERTs.", "AI": {"tldr": "Introduction of AnnualBERT, a series of language models for capturing temporal evolution in scientific texts.", "motivation": "To address the limitations of existing language models in capturing the temporal aspects of scientific discourse and improving performance on domain-specific NLP tasks.", "method": "AnnualBERT uses whole words as tokens, consisting of a base RoBERTa model pretrained on arXiv papers and progressively trained models on annual data, enhancing representation learning and tracking changes over time.", "result": "AnnualBERT models show comparable performances in standard NLP tasks and achieve state-of-the-art results in domain-specific tasks and link prediction within the arXiv citation network.", "conclusion": "AnnualBERT enhances scientific text processing and provides valuable insights into how scientific discourse evolves, with the models available for public use.", "key_contributions": ["Introduction of whole-word tokenization in language models", "Demonstration of improved performance in specific NLP tasks", "Insights into the temporal evolution of scientific discourse"], "limitations": "", "keywords": ["AnnualBERT", "NLP", "temporal evolution", "scientific texts", "RoBERTa"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2409.20204", "pdf": "https://arxiv.org/pdf/2409.20204.pdf", "abs": "https://arxiv.org/abs/2409.20204", "title": "Divided by discipline? A systematic literature review on the quantification of online sexism and misogyny using a semi-automated approach", "authors": ["Aditi Dutta", "Susan Banducci", "Chico Q. Camargo"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Several computational tools have been developed to detect and identify\nsexism, misogyny, and gender-based hate speech, particularly on online\nplatforms. These tools draw on insights from both social science and computer\nscience. Given the increasing concern over gender-based discrimination in\ndigital spaces, the contested definitions and measurements of sexism, and the\nrise of interdisciplinary efforts to understand its online manifestations, a\nsystematic literature review is essential for capturing the current state and\ntrajectory of this evolving field. In this review, we make four key\ncontributions: (1) we synthesize the literature into five core themes:\ndefinitions of sexism and misogyny, disciplinary divergences, automated\ndetection methods, associated challenges, and design-based interventions; (2)\nwe adopt an interdisciplinary lens, bridging theoretical and methodological\ndivides across disciplines; (3) we highlight critical gaps, including the need\nfor intersectional approaches, the under-representation of non-Western\nlanguages and perspectives, and the limited focus on proactive design\nstrategies beyond text classification; and (4) we offer a methodological\ncontribution by applying a rigorous semi-automated systematic review process\nguided by PRISMA, establishing a replicable standard for future work in this\ndomain. Our findings reveal a clear disciplinary divide in how sexism and\nmisogyny are conceptualized and measured. Through an evidence-based synthesis,\nwe examine how existing studies have attempted to bridge this gap through\ninterdisciplinary collaboration. Drawing on both social science theories and\ncomputational modeling practices, we assess the strengths and limitations of\ncurrent methodologies. Finally, we outline key challenges and future directions\nfor advancing research on the detection and mitigation of online sexism and\nmisogyny.", "AI": {"tldr": "The paper conducts a systematic literature review on computational tools for detecting sexism and misogyny in online spaces, providing insights into the current state of research and key gaps.", "motivation": "To capture the current state and trajectory of the evolving field of sexism and misogyny detection, addressing the increasing concern over gender-based discrimination online.", "method": "A systematic literature review synthesizing existing research into five core themes, using a semi-automated review process guided by PRISMA.", "result": "Identified five key themes in the literature and highlighted significant gaps, such as the need for intersectional approaches and proactive design strategies.", "conclusion": "Current methodologies reveal a clear disciplinary divide regarding the conceptualization and measurement of sexism and misogyny, necessitating further interdisciplinary collaboration.", "key_contributions": ["Synthesis of literature into five themes: definitions, disciplinary divergences, detection methods, challenges, and interventions.", "Interdisciplinary approach to bridge methodological divides.", "Identification of critical gaps in the field, including the need for intersectional and non-Western perspectives."], "limitations": "Limited focus on proactive design strategies beyond text classification and under-representation of non-Western perspectives.", "keywords": ["sexism", "misogyny", "automated detection", "interdisciplinary research", "systematic review"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2410.16392", "pdf": "https://arxiv.org/pdf/2410.16392.pdf", "abs": "https://arxiv.org/abs/2410.16392", "title": "Training of Scaffolded Language Models with Language Supervision: A Survey", "authors": ["Matthieu Lin", "Jenny Sheng", "Andrew Zhao", "Shenzhi Wang", "Yang Yue", "Victor Shea Jay Huang", "Huan Liu", "Jun Liu", "Gao Huang", "Yong-Jin Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This survey organizes the intricate literature on the design and optimization\nof emerging structures around post-trained LMs. We refer to this overarching\nstructure as scaffolded LMs and focus on LMs that are integrated into\nmulti-step processes with tools. We view scaffolded LMs as semi-parametric\nmodels wherein we train non-parametric variables, including the prompt, tools,\nand scaffold's code. In particular, they interpret instructions, use tools, and\nreceive feedback all in language. Recent works use an LM as an optimizer to\ninterpret language supervision and update non-parametric variables according to\nintricate objectives. In this survey, we refer to this paradigm as training of\nscaffolded LMs with language supervision. A key feature of non-parametric\ntraining is the ability to learn from language. Parametric training excels in\nlearning from demonstration (supervised learning), exploration (reinforcement\nlearning), or observations (unsupervised learning), using well-defined loss\nfunctions. Language-based optimization enables rich, interpretable, and\nexpressive objectives, while mitigating issues like catastrophic forgetting and\nsupporting compatibility with closed-source models. Furthermore, agents are\nincreasingly deployed as co-workers in real-world applications such as Copilot\nin Office tools or software development. In these mixed-autonomy settings,\nwhere control and decision-making are shared between human and AI, users point\nout errors or suggest corrections. Accordingly, we discuss agents that\ncontinuously improve by learning from this real-time, language-based feedback\nand refer to this setting as streaming learning from language supervision.", "AI": {"tldr": "This survey focuses on the design and optimization of scaffolded language models (LMs) that integrate into multi-step processes, exploring their training with language supervision and the implications for real-world applications.", "motivation": "To organize the literature on scaffolded LMs which utilize language for non-parametric training and optimize processes in multi-step settings with tools.", "method": "The paper surveys existing research on scaffolded LMs and their training paradigms, particularly how they handle language feedback in real-time applications.", "result": "The survey reveals how scaffolded LMs optimize language-based supervision and continuously improve through real-time user feedback, mitigating issues like catastrophic forgetting.", "conclusion": "The findings emphasize the importance of language-based optimization in improving scaffolded LMs for practical applications, particularly in collaborative environments with mixed autonomy.", "key_contributions": ["Introduction of the concept of scaffolded LMs.", "Discussion on streaming learning from language supervision in mixed-autonomy settings.", "Insights into the optimization of LMs using language feedback."], "limitations": "", "keywords": ["scaffolded language models", "language supervision", "streaming learning", "optimization", "mixed-autonomy"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2410.19453", "pdf": "https://arxiv.org/pdf/2410.19453.pdf", "abs": "https://arxiv.org/abs/2410.19453", "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based Contrastive Framework", "authors": ["Hengyuan Zhang", "Chenming Shang", "Sizhe Wang", "Dongdong Zhang", "Feng Yao", "Renliang Sun", "Yiyao Yu", "Yujiu Yang", "Furu Wei"], "categories": ["cs.CL"], "comment": "23 pages, 11 figures", "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research", "AI": {"tldr": "Proposes ShifCon, a framework to enhance multilingual LLM performance by improving non-dominant language representations through a shift-based contrastive approach.", "motivation": "To address the performance gap in LLMs between dominant languages (like English) and non-dominant languages due to uneven training data.", "method": "Introduces ShifCon, which shifts non-dominant language representations into a dominant language subspace for enhancement, coupled with multilingual contrastive learning for better alignment.", "result": "Experiments show ShifCon significantly improves performance for non-dominant languages, especially low-resource ones.", "conclusion": "ShifCon enables enhanced representation access and insights that can drive further research in multilingual LLMs.", "key_contributions": ["Introduction of ShifCon framework for language representation enhancement", "Development of subspace distance metric for optimal representation shifting", "Application of multilingual contrastive learning to improve representation alignment"], "limitations": "", "keywords": ["Large Language Models", "Multilingual Learning", "Contrastive Learning", "Health Informatics", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2411.05527", "pdf": "https://arxiv.org/pdf/2411.05527.pdf", "abs": "https://arxiv.org/abs/2411.05527", "title": "How Good is Your Wikipedia? Auditing Data Quality for Low-resource and Multilingual NLP", "authors": ["Kushal Tatariya", "Artur Kulmizev", "Wessel Poelman", "Esther Ploeger", "Marcel Bollmann", "Johannes Bjerva", "Jiaming Luo", "Heather Lent", "Miryam de Lhoneux"], "categories": ["cs.CL"], "comment": null, "summary": "Wikipedia's perceived high quality and broad language coverage have\nestablished it as a fundamental resource in multilingual NLP. In the context of\nlow-resource languages, however, these quality assumptions are increasingly\nbeing scrutinised. This paper critically examines the data quality of Wikipedia\nin a non-English setting by subjecting it to various quality filtering\ntechniques, revealing widespread issues such as a high percentage of one-line\narticles and duplicate articles. We evaluate the downstream impact of quality\nfiltering on Wikipedia and find that data quality pruning is an effective means\nfor resource-efficient training without hurting performance, especially for\nlow-resource languages. Moreover, we advocate for a shift in perspective from\nseeking a general definition of data quality towards a more language- and\ntask-specific one. Ultimately, we aim for this study to serve as a guide to\nusing Wikipedia for pretraining in a multilingual setting.", "AI": {"tldr": "The paper analyzes the data quality of Wikipedia in low-resource languages, identifying issues like one-line and duplicate articles, and advocates for language-specific definitions of data quality to improve NLP outcomes.", "motivation": "To critique the quality of Wikipedia as a resource for NLP in non-English languages, especially in low-resource settings.", "method": "Various quality filtering techniques were applied to Wikipedia data to evaluate its quality in a non-English context.", "result": "The study reveals significant issues with data quality in Wikipedia, including many one-line and duplicate articles, and demonstrates that quality pruning can enhance resource-efficient training without harming performance.", "conclusion": "The research encourages a shift towards language- and task-specific definitions of data quality, providing guidelines for using Wikipedia in multilingual pretraining.", "key_contributions": ["Identified widespread data quality issues in Wikipedia for low-resource languages.", "Demonstrated the effectiveness of quality filtering for improving NLP training efficiency.", "Advocated for language-specific data quality definitions."], "limitations": "The study focuses primarily on low-resource languages and may not generalize to high-resource contexts.", "keywords": ["Wikipedia", "Data Quality", "Multilingual NLP", "Low-resource languages", "Quality Filtering"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2411.07019", "pdf": "https://arxiv.org/pdf/2411.07019.pdf", "abs": "https://arxiv.org/abs/2411.07019", "title": "UniHR: Hierarchical Representation Learning for Unified Knowledge Graph Link Prediction", "authors": ["Zhiqiang Liu", "Yin Hua", "Mingyang Chen", "Zhuo Chen", "Ziqi Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Beyond-triple fact representations including hyper-relational facts with\nauxiliary key-value pairs, temporal facts with additional timestamps, and\nnested facts implying relationships between facts, are gaining significant\nattention. However, constrained by complex fact representation forms, existing\nlink prediction models for beyond-triple facts have difficulty achieving\nhierarchical fact modeling and generalizing the modules for one specific facts\nto other fact types. To overcome this limitation, we propose a Unified\nHierarchical Representation learning framework (UniHR) for unified knowledge\ngraph link prediction. It consists of a unified Hierarchical Data\nRepresentation (HiDR) module and a unified Hierarchical Structure Learning\n(HiSL) module as graph encoder. The HiDR module unifies hyper-relational KGs,\ntemporal KGs, and nested factual KGs into triple-based representations. Then\nHiSL incorporates intra-fact and inter-fact message passing, focusing on\nenhancing the semantic information within individual facts and enriching the\nstructural information between facts. Empirical results demonstrate the\neffectiveness of UniHR and highlight the strong potential of unified\nrepresentations. Code and data are available at\nhttps://github.com/Lza12a/UniHR.", "AI": {"tldr": "The paper presents a Unified Hierarchical Representation learning framework (UniHR) to improve link prediction in complex knowledge graphs, integrating various fact representations.", "motivation": "There is a growing need for effective models that can generalize across different complex fact representations such as hyper-relational, temporal, and nested facts due to their increasing complexity.", "method": "The UniHR framework includes a Hierarchical Data Representation (HiDR) module that transforms various complex knowledge graph forms into triple-based representations, and a Hierarchical Structure Learning (HiSL) module for intra-fact and inter-fact message passing to enhance semantic and structural information.", "result": "Empirical results show that the UniHR framework effectively captures the complexities of various fact types in knowledge graphs, demonstrating improved link prediction performance compared to existing models.", "conclusion": "The study highlights the potential of unified representations in advancing the current state of link prediction for complex knowledge graphs and provides accessible resources for replication.", "key_contributions": ["Introduction of a unified framework for link prediction in complex knowledge graphs.", "Development of modules for effective fact representation and message passing.", "Empirical validation of the framework's performance across different fact types."], "limitations": "The study may be limited by the data types tested and the specific configurations of the models; further studies could explore additional fact types and applications.", "keywords": ["knowledge graphs", "link prediction", "hierarchical representation", "machine learning", "temporal facts"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2411.08135", "pdf": "https://arxiv.org/pdf/2411.08135.pdf", "abs": "https://arxiv.org/abs/2411.08135", "title": "On the Role of Speech Data in Reducing Toxicity Detection Bias", "authors": ["Samuel J. Bell", "Mariano Coria Meglioli", "Megan Richards", "Eduardo Sánchez", "Christophe Ropers", "Skyler Wang", "Adina Williams", "Levent Sagun", "Marta R. Costa-jussà"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at NAACL 2025", "summary": "Text toxicity detection systems exhibit significant biases, producing\ndisproportionate rates of false positives on samples mentioning demographic\ngroups. But what about toxicity detection in speech? To investigate the extent\nto which text-based biases are mitigated by speech-based systems, we produce a\nset of high-quality group annotations for the multilingual MuTox dataset, and\nthen leverage these annotations to systematically compare speech- and\ntext-based toxicity classifiers. Our findings indicate that access to speech\ndata during inference supports reduced bias against group mentions,\nparticularly for ambiguous and disagreement-inducing samples. Our results also\nsuggest that improving classifiers, rather than transcription pipelines, is\nmore helpful for reducing group bias. We publicly release our annotations and\nprovide recommendations for future toxicity dataset construction.", "AI": {"tldr": "Investigates biases in toxicity detection in speech vs. text, finding that speech classifiers reduce biases for group mentions.", "motivation": "To explore how speech-based toxicity detection mitigates biases present in text-based systems.", "method": "The study involved creating high-quality annotations for the multilingual MuTox dataset and comparing speech and text-based toxicity classifiers.", "result": "Speech-based classifiers showed reduced bias against demographic group mentions, especially in ambiguous cases, unlike text-based classifiers.", "conclusion": "Enhancing classifiers is more effective for reducing group bias than improving transcription methods; annotations are publicly available.", "key_contributions": ["Developed high-quality group annotations for the MuTox dataset.", "Demonstrated that speech classifiers reduce text-based biases in toxicity detection.", "Provided recommendations for future toxicity dataset construction."], "limitations": "", "keywords": ["toxicity detection", "bias in AI", "speech classification", "MuTox dataset", "natural language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.12527", "pdf": "https://arxiv.org/pdf/2412.12527.pdf", "abs": "https://arxiv.org/abs/2412.12527", "title": "When to Speak, When to Abstain: Contrastive Decoding with Abstention", "authors": ["Hyuhng Joon Kim", "Youna Kim", "Sang-goo Lee", "Taeuk Kim"], "categories": ["cs.CL"], "comment": "ACL 2025 (main)", "summary": "Large Language Models (LLMs) demonstrate exceptional performance across\ndiverse tasks by leveraging pre-trained (i.e., parametric) and external (i.e.,\ncontextual) knowledge. While substantial efforts have been made to enhance the\nutilization of both forms of knowledge, situations in which models lack\nrelevant information remain underexplored. To investigate this challenge, we\nfirst present a controlled testbed featuring four distinct knowledge access\nscenarios, including the aforementioned edge case, revealing that conventional\nLLM usage exhibits insufficient robustness in handling all instances.\nAddressing this limitation, we propose Contrastive Decoding with Abstention\n(CDA), a novel training-free decoding method that allows LLMs to generate\nresponses when relevant knowledge is available and to abstain otherwise. CDA\nestimates the relevance of both knowledge sources for a given input, adaptively\ndeciding which type of information to prioritize and which to exclude. Through\nextensive experiments, we demonstrate that CDA can effectively perform accurate\ngeneration and abstention simultaneously, enhancing reliability and preserving\nuser trust.", "AI": {"tldr": "This paper presents Contrastive Decoding with Abstention (CDA), a method for Large Language Models (LLMs) that improves their ability to generate responses when relevant knowledge is available and to abstain when it is not, enhancing reliability and user trust.", "motivation": "To address the limitations of conventional LLMs in scenarios where relevant information is lacking, thereby improving robustness.", "method": "Introduction of Contrastive Decoding with Abstention (CDA), a training-free decoding technique that assesses the relevance of both parametric and contextual knowledge sources and adaptively decides which to use.", "result": "CDA demonstrates a significant improvement in generating accurate responses while allowing for abstention when necessary, thus enhancing the reliability of LLM outputs.", "conclusion": "CDA not only improves LLM performance in knowledge-scarce situations but also preserves user trust by managing response generation appropriately.", "key_contributions": ["Introduction of a controlled testbed for assessing LLM knowledge access scenarios", "Proposition of a new method (CDA) for LLMs to handle knowledge limitation effectively", "Demonstration of improved reliability in model outputs through abstention"], "limitations": "", "keywords": ["Large Language Models", "Contrastive Decoding", "Abstention", "Knowledge Access", "User Trust"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2412.12632", "pdf": "https://arxiv.org/pdf/2412.12632.pdf", "abs": "https://arxiv.org/abs/2412.12632", "title": "What External Knowledge is Preferred by LLMs? Characterizing and Exploring Chain of Evidence in Imperfect Context", "authors": ["Zhiyuan Chang", "Mingyang Li", "Xiaojun Jia", "Junjie Wang", "Yuekai Huang", "Qing Wang", "Yihao Huang", "Yang Liu"], "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 figures", "summary": "Incorporating external knowledge into large language models (LLMs) has\nemerged as a promising approach to mitigate outdated knowledge and\nhallucination in LLMs. However, external knowledge is often imperfect. In\naddition to useful knowledge, external knowledge is rich in irrelevant or\nmisinformation in the context that can impair the reliability of LLM responses.\nThis paper focuses on LLMs' preferred external knowledge in imperfect contexts\nwhen handling multi-hop QA. Inspired by criminal procedural law's Chain of\nEvidence (CoE), we characterize that knowledge preferred by LLMs should\nmaintain both relevance to the question and mutual support among knowledge\npieces. Accordingly, we propose an automated CoE discrimination approach and\nevaluate LLMs' effectiveness, faithfulness and robustness with CoE, including\nits application in the Retrieval-Augmented Generation (RAG). Tests on five LLMs\nshow CoE improves generation accuracy, answer faithfulness, robustness to\nknowledge conflicts, and boosts the performance of existing approaches in three\npractical RAG scenarios.", "AI": {"tldr": "This paper explores an automated approach inspired by the Chain of Evidence (CoE) to enhance the reliability of large language models (LLMs) in multi-hop question answering by distinguishing relevant from irrelevant external knowledge.", "motivation": "Incorporating external knowledge into LLMs can reduce outdated knowledge and hallucinations but often includes irrelevant or misleading information.", "method": "The paper proposes an automated CoE discrimination approach to evaluate LLMs' performance with respect to generating accurate and faithful responses, specifically in multi-hop QA scenarios.", "result": "Tests on five LLMs indicate that CoE enhances generation accuracy, strengthens answer faithfulness, and improves robustness against knowledge conflicts.", "conclusion": "Utilizing CoE leads to overall better performance of LLMs in retrieval-augmented generation (RAG) scenarios by improving response quality.", "key_contributions": ["Proposed an automated CoE discrimination approach for LLMs.", "Demonstrated improvements in accuracy, faithfulness, and robustness with CoE.", "Provided insights into applying CoE in practical RAG contexts."], "limitations": "The approach relies on the quality of external knowledge and may not fully overcome all issues related to misinformation.", "keywords": ["Large Language Models", "Multi-hop Question Answering", "Chain of Evidence", "Retrieval-Augmented Generation", "Knowledge Management"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.15529", "pdf": "https://arxiv.org/pdf/2412.15529.pdf", "abs": "https://arxiv.org/abs/2412.15529", "title": "XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation", "authors": ["Qianren Mao", "Yangyifei Luo", "Qili Zhang", "Yashuo Luo", "Zhilong Cao", "Jinlong Zhang", "HanWen Hao", "Zhijun Chen", "Weifeng Jiang", "Junnan Liu", "Xiaolong Wang", "Zhenting Huang", "Zhixing Tan", "Sun Jie", "Bo Li", "Xudong Liu", "Richong Zhang", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) synergizes the retrieval of pertinent\ndata with the generative capabilities of Large Language Models (LLMs), ensuring\nthat the generated output is not only contextually relevant but also accurate\nand current. We introduce XRAG, an open-source, modular codebase that\nfacilitates exhaustive evaluation of the performance of foundational components\nof advanced RAG modules. These components are systematically categorized into\nfour core phases: pre-retrieval, retrieval, post-retrieval, and generation. We\nsystematically analyse them across reconfigured datasets, providing a\ncomprehensive benchmark for their effectiveness. As the complexity of RAG\nsystems continues to escalate, we underscore the critical need to identify\npotential failure points in RAG systems. We formulate a suite of experimental\nmethodologies and diagnostic testing protocols to dissect the failure points\ninherent in RAG engineering. Subsequently, we proffer bespoke solutions aimed\nat bolstering the overall performance of these modules. Our work thoroughly\nevaluates the performance of advanced core components in RAG systems, providing\ninsights into optimizations for prevalent failure points.", "AI": {"tldr": "This paper introduces XRAG, an open-source codebase for evaluating foundational components of retrieval-augmented generation (RAG) systems, focusing on performance benchmarks and addressing potential failure points.", "motivation": "To improve the effectiveness of retrieval-augmented generation (RAG) systems by evaluating core components and identifying critical failure points.", "method": "The paper categorizes the components of RAG into four phases: pre-retrieval, retrieval, post-retrieval, and generation, and evaluates their performance over various datasets.", "result": "The analysis provides a comprehensive benchmark for the effectiveness of advanced RAG components and outlines bespoke solutions to enhance their performance.", "conclusion": "The research highlights the importance of diagnosing failure points in RAG systems and offers insights for optimizations in response to these challenges.", "key_contributions": ["Introduction of XRAG as an open-source codebase for RAG evaluation", "Systematic categorization and performance benchmarking of RAG components", "Development of experimental methodologies for diagnosing RAG failures"], "limitations": "", "keywords": ["Retrieval-augmented generation", "Large Language Models", "Performance evaluation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.00745", "pdf": "https://arxiv.org/pdf/2501.00745.pdf", "abs": "https://arxiv.org/abs/2501.00745", "title": "Dynamics of Adversarial Attacks on Large Language Model-Based Search Engines", "authors": ["Xiyang Hu"], "categories": ["cs.CL", "cs.AI", "cs.GT", "cs.IR", "econ.TH"], "comment": null, "summary": "The increasing integration of Large Language Model (LLM) based search engines\nhas transformed the landscape of information retrieval. However, these systems\nare vulnerable to adversarial attacks, especially ranking manipulation attacks,\nwhere attackers craft webpage content to manipulate the LLM's ranking and\npromote specific content, gaining an unfair advantage over competitors. In this\npaper, we study the dynamics of ranking manipulation attacks. We frame this\nproblem as an Infinitely Repeated Prisoners' Dilemma, where multiple players\nstrategically decide whether to cooperate or attack. We analyze the conditions\nunder which cooperation can be sustained, identifying key factors such as\nattack costs, discount rates, attack success rates, and trigger strategies that\ninfluence player behavior. We identify tipping points in the system dynamics,\ndemonstrating that cooperation is more likely to be sustained when players are\nforward-looking. However, from a defense perspective, we find that simply\nreducing attack success probabilities can, paradoxically, incentivize attacks\nunder certain conditions. Furthermore, defensive measures to cap the upper\nbound of attack success rates may prove futile in some scenarios. These\ninsights highlight the complexity of securing LLM-based systems. Our work\nprovides a theoretical foundation and practical insights for understanding and\nmitigating their vulnerabilities, while emphasizing the importance of adaptive\nsecurity strategies and thoughtful ecosystem design.", "AI": {"tldr": "This paper investigates ranking manipulation attacks on LLM-based search engines using the Infinitely Repeated Prisoners' Dilemma framework, revealing conditions for sustaining cooperation among attackers and the paradoxical effects of defensive strategies.", "motivation": "To understand the vulnerabilities of LLM-based search engines to ranking manipulation attacks and propose methods for mitigating these threats.", "method": "Analyzed the dynamics of ranking manipulation attacks through a game-theoretic approach using the Infinitely Repeated Prisoners' Dilemma to identify conditions for cooperation among players.", "result": "Identified key factors influencing cooperation, including attack costs and success rates, and demonstrated paradoxical outcomes where reducing attack success rates could incentivize more attacks.", "conclusion": "Adaptive security strategies and careful design of ecosystems are crucial for mitigating vulnerabilities in LLM-based systems, as simplistic defensive measures may be ineffective.", "key_contributions": ["Theoretical framework for understanding ranking manipulation in LLMs", "Identification of conditions supporting cooperation in adversarial settings", "Insights into the paradoxical effects of certain defensive strategies"], "limitations": "Focuses on a theoretical analysis and does not provide empirical validation of findings.", "keywords": ["Large Language Models", "ranking manipulation", "information retrieval", "game theory", "security strategies"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2501.03266", "pdf": "https://arxiv.org/pdf/2501.03266.pdf", "abs": "https://arxiv.org/abs/2501.03266", "title": "LLM Content Moderation and User Satisfaction: Evidence from Response Refusals in Chatbot Arena", "authors": ["Stefan Pasch"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.SI"], "comment": null, "summary": "LLM safety and ethical alignment are widely discussed, but the impact of\ncontent moderation on user satisfaction remains underexplored. In particular,\nlittle is known about how users respond when models refuse to answer a\nprompt-one of the primary mechanisms used to enforce ethical boundaries in\nLLMs. We address this gap by analyzing nearly 50,000 model comparisons from\nChatbot Arena, a platform where users indicate their preferred LLM response in\npairwise matchups, providing a large-scale setting for studying real-world user\npreferences. Using a novel RoBERTa-based refusal classifier fine-tuned on a\nhand-labeled dataset, we distinguish between refusals due to ethical concerns\nand technical limitations. Our results reveal a substantial refusal penalty:\nethical refusals yield significantly lower win rates than both technical\nrefusals and standard responses, indicating that users are especially\ndissatisfied when models decline a task for ethical reasons. However, this\npenalty is not uniform. Refusals receive more favorable evaluations when the\nunderlying prompt is highly sensitive (e.g., involving illegal content), and\nwhen the refusal is phrased in a detailed and contextually aligned manner.\nThese findings underscore a core tension in LLM design: safety-aligned\nbehaviors may conflict with user expectations, calling for more adaptive\nmoderation strategies that account for context and presentation.", "AI": {"tldr": "The paper examines how content moderation affects user satisfaction in LLMs, specifically focusing on user reactions to refusals based on ethical versus technical concerns.", "motivation": "To address the gap in understanding user responses to LLM refusals, particularly those based on ethical considerations.", "method": "Analyzed nearly 50,000 model comparisons from Chatbot Arena to study user preferences and employed a RoBERTa-based refusal classifier to differentiate between types of refusals.", "result": "Ethical refusals led to lower win rates compared to technical refusals and standard responses. However, sensitive prompts received more favorable evaluations when refusals were contextually appropriate.", "conclusion": "The findings highlight a conflict between safety-aligned LLM behaviors and user expectations, suggesting the need for adaptive moderation strategies.", "key_contributions": ["Large-scale analysis of user preferences in LLM responses", "Development of a RoBERTa-based classifier for refusal types", "Insights into user satisfaction versus ethical considerations in LLM design"], "limitations": "", "keywords": ["LLM", "content moderation", "user satisfaction", "ethical alignment", "refusal classifier"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.04987", "pdf": "https://arxiv.org/pdf/2501.04987.pdf", "abs": "https://arxiv.org/abs/2501.04987", "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures", "authors": ["Ziwei He", "Jian Yuan", "Haoli Bai", "Jingwen Leng", "Bo Jiang"], "categories": ["cs.CL"], "comment": null, "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.", "AI": {"tldr": "TreeKV is a new method for efficient caching in transformer-based LLMs that enhances context retention while allowing for significant cache size reduction.", "motivation": "Efficient KV cache compression is necessary for improving performance of transformer-based LLMs in resource-limited settings, especially for long sequences.", "method": "TreeKV uses a tree structure for smooth cache compression, avoiding position-based and global importance biases. It operates in both generation and prefilling stages without requiring training.", "result": "TreeKV outperforms existing models in language modeling tasks (PG19, OpenWebText2) and successfully reduces cache size by 16x while maintaining high-quality outputs.", "conclusion": "TreeKV achieves superior efficiency, with optimal performance on the Longbench benchmark at just 6% of the budget.", "key_contributions": ["Introduction of TreeKV, a training-free cache compression method", "Demonstrated significant performance improvement in long-context LLM scenarios", "Achieved a 16x reduction in cache size with maintained output quality"], "limitations": "", "keywords": ["Key-value cache", "Transformer models", "Language models", "Cache compression", "Long sequences"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.10316", "pdf": "https://arxiv.org/pdf/2501.10316.pdf", "abs": "https://arxiv.org/abs/2501.10316", "title": "Know Your Mistakes: Towards Preventing Overreliance on Task-Oriented Conversational AI Through Accountability Modeling", "authors": ["Suvodip Dey", "Yi-Jyun Sun", "Gokhan Tur", "Dilek Hakkani-Tur"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Recent LLMs have enabled significant advancements for conversational agents.\nHowever, they are also well known to hallucinate, producing responses that seem\nplausible but are factually incorrect. On the other hand, users tend to\nover-rely on LLM-based AI agents, accepting AI's suggestion even when it is\nwrong. Adding positive friction, such as explanations or getting user\nconfirmations, has been proposed as a mitigation in AI-supported\ndecision-making systems. In this paper, we propose an accountability model for\nLLM-based task-oriented dialogue agents to address user overreliance via\nfriction turns in cases of model uncertainty and errors associated with\ndialogue state tracking (DST). The accountability model is an augmented LLM\nwith an additional accountability head that functions as a binary classifier to\npredict the relevant slots of the dialogue state mentioned in the conversation.\nWe perform our experiments with multiple backbone LLMs on two established\nbenchmarks (MultiWOZ and Snips). Our empirical findings demonstrate that the\nproposed approach not only enables reliable estimation of AI agent errors but\nalso guides the decoder in generating more accurate actions. We observe around\n3% absolute improvement in joint goal accuracy (JGA) of DST output by\nincorporating accountability heads into modern LLMs. Self-correcting the\ndetected errors further increases the JGA from 67.13 to 70.51, achieving\nstate-of-the-art DST performance. Finally, we show that error correction\nthrough user confirmations (friction turn) achieves a similar performance gain,\nhighlighting its potential to reduce user overreliance.", "AI": {"tldr": "This paper presents an accountability model for LLM-based dialogue agents to reduce user overreliance on AI by introducing friction turns that help in managing model uncertainty and errors in dialogue state tracking.", "motivation": "The paper addresses user overreliance on LLM-based conversational agents that can hallucinate, producing false yet plausible responses. It aims to improve decision-making systems by adding positive friction through user confirmations and explanations.", "method": "The authors propose an augmented LLM with an accountability head that serves as a binary classifier for predicting relevant dialogue state slots, tested on two benchmarks (MultiWOZ and Snips).", "result": "The accountability model leads to a 3% improvement in joint goal accuracy (JGA) of dialogue state tracking, with self-correction of errors further increasing JGA from 67.13 to 70.51, achieving state-of-the-art results.", "conclusion": "Incorporating user confirmations as a friction turn results in significant performance gains in dialogue state tracking while reducing overreliance on AI agents.", "key_contributions": ["Development of an accountability model for LLM-based dialogue agents", "Demonstration of a significant accuracy improvement in dialogue state tracking", "Identification of user confirmation as a viable method to mitigate overreliance."], "limitations": "", "keywords": ["Large Language Models", "Dialogue State Tracking", "User Overreliance", "Accountability Model", "Error Correction"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.11885", "pdf": "https://arxiv.org/pdf/2501.11885.pdf", "abs": "https://arxiv.org/abs/2501.11885", "title": "Med-R$^2$: Crafting Trustworthy LLM Physicians via Retrieval and Reasoning of Evidence-Based Medicine", "authors": ["Keer Lu", "Zheng Liang", "Zhuoran Zhang", "Da Pan", "Shusen Zhang", "Xin Wu", "Zenan Zhou", "Guosheng Dong", "Bin Cui", "Tengjiao Wang", "Wentao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have exhibited remarkable capabilities in\nclinical scenarios. Despite their potential, existing works face challenges\nwhen applying LLMs to medical settings. Strategies relying on training with\nmedical datasets are highly cost-intensive and may suffer from outdated\ntraining data. Leveraging external knowledge bases is a suitable alternative,\nyet it faces obstacles such as limited retrieval precision and poor\neffectiveness in answer extraction. These issues collectively prevent LLMs from\ndemonstrating the expected level of proficiency in mastering medical expertise.\nTo address these challenges, we introduce Med-R^2, a novel LLM physician\nframework that adheres to the Evidence-Based Medicine (EBM) process,\nefficiently integrating retrieval mechanisms as well as the selection and\nreasoning processes of evidence, thereby enhancing the problem-solving\ncapabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM\nphysician. Our comprehensive experiments indicate that Med-R^2 achieves a\n14.74\\% improvement over vanilla RAG methods and even a 3.32\\% enhancement\ncompared to fine-tuning strategies, without incurring additional training\ncosts.", "AI": {"tldr": "Med-R^2 is a novel LLM framework for enhancing healthcare applications by integrating retrieval mechanisms with evidence-based reasoning, significantly improving performance without high costs.", "motivation": "To improve the application of LLMs in medical settings, addressing challenges of expensive training datasets and limitations in existing retrieval and extraction strategies.", "method": "The Med-R^2 framework integrates the Evidence-Based Medicine (EBM) process with advanced retrieval mechanisms and reasoning strategies to enhance the capabilities of LLMs in clinical contexts.", "result": "Med-R^2 demonstrates a 14.74% improvement over standard Retrieval-Augmented Generation (RAG) methods and a 3.32% improvement compared to fine-tuning methods, achieving these gains without additional training costs.", "conclusion": "Med-R^2 effectively enhances LLM performance in healthcare by providing reliable access to external knowledge and improved reasoning, fostering trustworthy LLM applications in medicine.", "key_contributions": ["Introduction of Med-R^2, a novel LLM physician framework", "Integration of Evidence-Based Medicine process with LLMs", "Significant performance improvement in healthcare scenarios without increased training costs"], "limitations": "", "keywords": ["Large Language Models", "Evidence-Based Medicine", "Healthcare", "Retrieval-Augmented Generation", "Medical Expertise"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2501.13977", "pdf": "https://arxiv.org/pdf/2501.13977.pdf", "abs": "https://arxiv.org/abs/2501.13977", "title": "Re-ranking Using Large Language Models for Mitigating Exposure to Harmful Content on Social Media Platforms", "authors": ["Rajvardhan Oak", "Muhammad Haroon", "Claire Jo", "Magdalena Wojcieszak", "Anshuman Chhabra"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Social media platforms utilize Machine Learning (ML) and Artificial\nIntelligence (AI) powered recommendation algorithms to maximize user\nengagement, which can result in inadvertent exposure to harmful content.\nCurrent moderation efforts, reliant on classifiers trained with extensive\nhuman-annotated data, struggle with scalability and adapting to new forms of\nharm. To address these challenges, we propose a novel re-ranking approach using\nLarge Language Models (LLMs) in zero-shot and few-shot settings. Our method\ndynamically assesses and re-ranks content sequences, effectively mitigating\nharmful content exposure without requiring extensive labeled data. Alongside\ntraditional ranking metrics, we also introduce two new metrics to evaluate the\neffectiveness of re-ranking in reducing exposure to harmful content. Through\nexperiments on three datasets, three models and across three configurations, we\ndemonstrate that our LLM-based approach significantly outperforms existing\nproprietary moderation approaches, offering a scalable and adaptable solution\nfor harm mitigation.", "AI": {"tldr": "This paper presents a novel re-ranking method using Large Language Models to mitigate harmful content exposure on social media platforms, overcoming limitations of traditional moderation approaches.", "motivation": "The study addresses the inadequacies of current social media content moderation frameworks, which struggle with scalability and adaptability in identifying harmful content due to reliance on extensive human-annotated datasets.", "method": "A re-ranking approach using Large Language Models (LLMs) in zero-shot and few-shot settings to dynamically assess and re-rank content sequences for reduced harmful exposure.", "result": "Experiments show that the LLM-based re-ranking method significantly outperforms existing proprietary moderation techniques, indicating its superiority and scalability.", "conclusion": "The proposed method offers an effective way to enhance content moderation on social media without the need for large volumes of labeled training data, showcasing adaptability to new types of harmful content.", "key_contributions": ["Introduction of a novel re-ranking approach using LLMs", "Development of new metrics for assessing the effectiveness of harm mitigation", "Demonstration of superior performance over existing moderation systems."], "limitations": "The approach may still face challenges in entirely eliminating harmful content and requires further validation in real-world settings.", "keywords": ["Machine Learning", "AI", "Content Moderation", "Large Language Models", "Harm Mitigation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.06604", "pdf": "https://arxiv.org/pdf/2502.06604.pdf", "abs": "https://arxiv.org/abs/2502.06604", "title": "Do we really have to filter out random noise in pre-training data for language models?", "authors": ["Jinghan Ru", "Yuxin Xie", "Xianwei Zhuang", "Yuguo Yin", "Zhihui Guo", "Zhiming Liu", "Qianli Ren", "Yuexian Zou"], "categories": ["cs.CL"], "comment": null, "summary": "Web-scale pre-training datasets are the cornerstone of LLMs' success.\nHowever, text data curated from the Internet inevitably contains random noise\ncaused by decoding errors or unregulated web content. In contrast to previous\nworks that focus on low quality or synthetic data, our study \\textbf{provides\nthe first systematic investigation of such random noise through a cohesive\n``What-Why-How'' framework.} Surprisingly, we observed that the resulting\nincrease in the loss of next-token prediction (NTP) was significantly lower\nthan the proportion of random noise even when the model was scaled up to 2.7B.\nWe provide a theoretical justification for this phenomenon, which also\nelucidates the success of multilingual models and can be applied to multimodal\nmodels. On the other hand, experiments show that the model's performance in\ndownstream tasks is not based solely on the NTP loss, which means that random\nnoise may result in degraded downstream performance. To address the potential\nadverse effects, we introduce a novel plug-and-play Local Gradient Matching\nloss, which explicitly enhances the denoising capability of the downstream task\nhead by aligning the gradient of normal and perturbed features without\nrequiring knowledge of the model's parameters. Additional experiments on 8\nlanguage and 14 vision benchmarks further validate its effectiveness.", "AI": {"tldr": "This study investigates random noise in web-scale pre-training datasets for LLMs using a cohesive framework, revealing unexpected results about its impact on model performance and introducing a novel loss function to enhance denoising capabilities.", "motivation": "The paper addresses the impact of random noise in web-sourced data for training LLMs, seeking to understand its effects on model performance and provide solutions.", "method": "The authors employ a 'What-Why-How' framework to systematically investigate random noise, providing theoretical justifications and conducting experiments with a Local Gradient Matching loss to improve model performance in downstream tasks.", "result": "The increase in next-token prediction loss due to noise was lower than expected, and the Local Gradient Matching loss was validated to enhance denoising in downstream tasks across various benchmarks.", "conclusion": "Random noise affects downstream task performance negatively, and the introduced Local Gradient Matching loss can effectively mitigate this issue.", "key_contributions": ["First systematic investigation of random noise in LLM training datasets.", "Introduction of the Local Gradient Matching loss to enhance denoising.", "Validation of findings through experiments on multiple language and vision benchmarks."], "limitations": "The study explores a specific aspect of data quality, but it may not cover all forms of noise or their interactions with model architectures.", "keywords": ["random noise", "LLM", "Local Gradient Matching", "downstream performance", "data quality"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.06876", "pdf": "https://arxiv.org/pdf/2502.06876.pdf", "abs": "https://arxiv.org/abs/2502.06876", "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging", "authors": ["Jinluan Yang", "Dingnan Jin", "Anke Tang", "Li Shen", "Didi Zhu", "Zhengyu Chen", "Ziyu Zhao", "Daixin Wang", "Qing Cui", "Zhiqiang Zhang", "Jun Zhou", "Fei Wu", "Kun Kuang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Achieving balanced alignment of large language models (LLMs) in terms of\nHelpfulness, Honesty, and Harmlessness (3H optimization) constitutes a\ncornerstone of responsible AI. Existing methods like data mixture strategies\nface limitations, including heavy reliance on expert knowledge and conflicting\noptimization signals. While model merging offers parameter-level\nconflict-resolution strategies through integrating specialized models'\nparameters, its potential for 3H optimization remains underexplored. This paper\nsystematically compares the effectiveness of model merging and data mixture\nmethods in constructing 3H-aligned LLMs for the first time, revealing\npreviously overlooked collaborative and conflict relationships among the 3H\ndimensions and discussing the advantages and drawbacks of data mixture\n(\\textit{data-level}) and model merging (\\textit{parameter-level}) methods in\nmitigating the conflict for balanced 3H optimization. Specially, we propose a\nnovel \\textbf{R}eweighting \\textbf{E}nhanced task \\textbf{S}ingular\n\\textbf{M}erging method, \\textbf{RESM}, through outlier weighting and\nsparsity-aware rank selection strategies to address the challenges of\npreference noise accumulation and layer sparsity adaptation inherent in\n3H-aligned LLM merging. Extensive evaluations can verify the effectiveness and\nrobustness of RESM compared to previous data mixture (2\\%-5\\% gain) and model\nmerging (1\\%-3\\% gain) methods in achieving balanced LLM alignment. We release\nour models through \\href{https://huggingface.co/Jinluan}{3H\\_Merging} for\nfurther investigations.", "AI": {"tldr": "This paper explores methods for achieving balanced alignment of large language models (LLMs) regarding Helpfulness, Honesty, and Harmlessness (3H optimization), comparing traditional data mixture strategies with model merging techniques.", "motivation": "To advance responsible AI through balanced 3H optimization in large language models, addressing limitations of existing methods.", "method": "The paper compares model merging and data mixture techniques for 3H alignment and introduces a new merging method called RESM that incorporates outlier weighting and sparsity-aware rank selection to improve model alignment.", "result": "RESM demonstrated a 2%-5% improvement over traditional data mixture methods and a 1%-3% improvement over existing model merging techniques in achieving balanced alignment.", "conclusion": "Model merging, particularly with the RESM method, offers a promising approach to better alignment of large language models concerning Helpfulness, Honesty, and Harmlessness.", "key_contributions": ["Introduction of the RESM merging method", "Systematic comparison of model merging vs. data mixture for 3H optimization", "Release of models for further research"], "limitations": "", "keywords": ["3H optimization", "large language models", "model merging", "data mixture methods", "RESM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.07490", "pdf": "https://arxiv.org/pdf/2502.07490.pdf", "abs": "https://arxiv.org/abs/2502.07490", "title": "Mask-Enhanced Autoregressive Prediction: Pay Less Attention to Learn More", "authors": ["Xialie Zhuang", "Zhikai Jia", "Jianjin Li", "Zhenyu Zhang", "Li Shen", "Zheng Cao", "Shiwei Liu"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages,7 figures", "summary": "Large Language Models (LLMs) are discovered to suffer from accurately\nretrieving key information. To address this, we propose Mask-Enhanced\nAutoregressive Prediction (MEAP), a simple yet effective training paradigm that\nseamlessly integrates Masked Language Modeling (MLM) into Next-Token Prediction\n(NTP) to enhance the latter's in-context retrieval capabilities. Specifically,\nMEAP first randomly masks a small fraction of input tokens and then directly\nperforms the standard next-token prediction autoregressive using a decoder-only\nTransformer. MEAP eliminates the need for bidirectional attention or\nencoder-decoder architectures for MLM, incurring no additional computational\noverhead during pre-training or inference. Intensive experiments demonstrate\nthat MEAP substantially outperforms NTP on key information retrieval and\nlong-context reasoning tasks, while performing on par or better on commonsense\nreasoning tasks. The benefits of MEAP also extend to supervised fine-tuning,\nwhere it shows remarkable advantages in lost-in-the-middle scenarios,\noutperforming NTP by 11.77 percentage points. Our analysis indicates that\nMEAP's effectiveness arises from its ability to promote more distinguishable\nattention scores by concentrating on a reduced set of non-masked tokens. This\nmechanism improves the model's focus on task-relevant signals while mitigating\nthe influence of peripheral context. These findings position MEAP as a\npromising training paradigm for large language models.", "AI": {"tldr": "MEAP improves LLM key information retrieval and reasoning tasks by integrating MLM into NTP without added computational overhead.", "motivation": "LLMs struggle with accurate information retrieval, necessitating an improved approach for enhancing their capabilities.", "method": "MEAP integrates Masked Language Modeling with Next-Token Prediction by masking input tokens and using a decoder-only Transformer for autoregressive prediction.", "result": "MEAP significantly outperforms NTP in key information retrieval and long-context reasoning tasks, and also shows superior performance in supervised fine-tuning scenarios.", "conclusion": "MEAP's emphasis on a smaller set of non-masked tokens enhances task-relevant focus in LLM training, making it an effective paradigm for improving model performance.", "key_contributions": ["Proposing the MEAP training paradigm for LLMs", "Demonstrating superior performance in information retrieval and reasoning tasks", "Highlighting the mechanism that improves attention scores in language models"], "limitations": "", "keywords": ["Large Language Models", "Masked Language Modeling", "Next-Token Prediction", "Information Retrieval", "Attention Mechanism"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.08666", "pdf": "https://arxiv.org/pdf/2502.08666.pdf", "abs": "https://arxiv.org/abs/2502.08666", "title": "Hallucination, Monofacts, and Miscalibration: An Empirical Investigation", "authors": ["Miranda Muqing Miao", "Michael Kearns"], "categories": ["cs.CL", "cs.AI"], "comment": "Code available at https://github.com/mmiao2/Hallucination.git", "summary": "Hallucinated facts in large language models (LLMs) have recently been shown\nto obey a statistical lower bound determined by the monofact rate (related to\nthe classical Good-Turing missing mass estimator) minus model miscalibration\n(Kalai & Vempala, 2024). We present the first empirical investigation of this\nthree-way relationship in classical n-gram models and fine-tuned\nencoder-decoder Transformers. By generating training data from Pareto\ndistributions with varying shape parameters, we systematically control the\nmonofact rates and establish its positive relationship with hallucination. To\nbridge theory and practice, we derive an empirical analog of the hallucination\nbound by replacing the population miscalibration term (Section 2.1) with an\nempirical bin-wise KL divergence and confirm its practical viability. We then\nintroduce selective upweighting -- a simple yet effective technique that\nstrategically repeats as little as 5% of training examples -- to deliberately\ninject miscalibration into the model. This intervention reduces hallucination\nby up to 40%, challenging universal deduplication policies. Our experiments\nreveal a critical trade-off: selective upweighting maintains pre-injection\nlevels of accuracy while substantially reducing hallucination, whereas standard\ntraining gradually improves accuracy but fails to address persistently high\nhallucination, indicating an inherent tension in optimization objectives.", "AI": {"tldr": "This paper investigates the relationship between monofact rates and hallucination in language models, proposing a method to reduce hallucination through selective upweighting during training.", "motivation": "Understanding and mitigating hallucinations in large language models, which have implications for their reliability in applications.", "method": "Empirical investigation of the relationship between monofact rates and hallucination, and the introduction of selective upweighting during training.", "result": "Selective upweighting can reduce hallucination by up to 40% without sacrificing accuracy, challenging existing deduplication policies.", "conclusion": "The trade-off between maintaining accuracy and reducing hallucination highlights a key optimization challenge in model training.", "key_contributions": ["First empirical investigation of the relationship between monofact rates and hallucination in language models.", "Development of selective upweighting technique that reduces hallucination significantly.", "Findings challenge the efficacy of universal deduplication policies in training language models."], "limitations": "Focuses on specific models and empirical settings; results may not generalize universally across all types of language models.", "keywords": ["hallucination", "large language models", "selective upweighting", "monofact rate", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.11175", "pdf": "https://arxiv.org/pdf/2502.11175.pdf", "abs": "https://arxiv.org/abs/2502.11175", "title": "Investigating Language Preference of Multilingual RAG Systems", "authors": ["Jeonghyun Park", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enhance language\nmodels by integrating external multilingual information to produce\ncontext-aware responses. However, mRAG systems struggle with retrieving\nrelevant information due to linguistic variations between queries and\ndocuments, generating inconsistent responses when multilingual sources\nconflict. In this work, we systematically investigate language preferences in\nboth retrieval and generation of mRAG through a series of experiments. Our\nanalysis indicates that retrievers tend to prefer high-resource and query\nlanguages, yet this preference does not consistently improve generation\nperformance. Moreover, we observe that generators prefer the query language or\nLatin scripts, leading to inconsistent outputs. To overcome these issues, we\npropose Dual Knowledge Multilingual RAG (DKM-RAG), a simple yet effective\nframework that fuses translated multilingual passages with complementary model\nknowledge. Empirical results demonstrate that DKM-RAG mitigates language\npreference in generation and enhances performance across diverse linguistic\nsettings.", "AI": {"tldr": "This paper investigates language preferences in multilingual retrieval-augmented generation (mRAG), proposing a framework to enhance performance across languages.", "motivation": "The motivation behind this work is to address the challenges faced by mRAG systems in retrieving relevant information across diverse linguistic contexts and improving the consistency of generated responses.", "method": "The authors conducted a series of experiments to analyze language preferences in mRAG for both retrieval and generation processes, leading to the development of the Dual Knowledge Multilingual RAG (DKM-RAG) framework.", "result": "Empirical results show that DKM-RAG reduces language preference issues in generation and improves performance significantly across various linguistic settings.", "conclusion": "The study concludes that integrating translated multilingual passages with complementary model knowledge effectively addresses inconsistencies in mRAG systems.", "key_contributions": ["Investigation of language preferences in mRAG systems", "Introduction of the Dual Knowledge Multilingual RAG (DKM-RAG) framework", "Empirical demonstration of enhanced performance across diverse languages"], "limitations": "", "keywords": ["Multilingual Retrieval", "Augmented Generation", "Language Preferences", "Dual Knowledge Multilingual RAG", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11948", "pdf": "https://arxiv.org/pdf/2502.11948.pdf", "abs": "https://arxiv.org/abs/2502.11948", "title": "Can Your Uncertainty Scores Detect Hallucinated Entity?", "authors": ["Min-Hsuan Yeh", "Max Kamachee", "Seongheon Park", "Yixuan Li"], "categories": ["cs.CL"], "comment": null, "summary": "To mitigate the impact of hallucination nature of LLMs, many studies propose\ndetecting hallucinated generation through uncertainty estimation. However,\nthese approaches predominantly operate at the sentence or paragraph level,\nfailing to pinpoint specific spans or entities responsible for hallucinated\ncontent. This lack of granularity is especially problematic for long-form\noutputs that mix accurate and fabricated information. To address this\nlimitation, we explore entity-level hallucination detection. We propose a new\ndata set, HalluEntity, which annotates hallucination at the entity level. Based\non the dataset, we comprehensively evaluate uncertainty-based hallucination\ndetection approaches across 17 modern LLMs. Our experimental results show that\nuncertainty estimation approaches focusing on individual token probabilities\ntend to over-predict hallucinations, while context-aware methods show better\nbut still suboptimal performance. Through an in-depth qualitative study, we\nidentify relationships between hallucination tendencies and linguistic\nproperties and highlight important directions for future research. HalluEntity:\nhttps://huggingface.co/datasets/samuelyeh/HalluEntity", "AI": {"tldr": "This paper tackles the limitation of existing LLM hallucination detection methods by proposing entity-level hallucination detection using a new dataset called HalluEntity, which annotates hallucination at the entity level.", "motivation": "Current methods for detecting hallucinations in LLMs primarily work at a sentence or paragraph level, lacking the ability to identify specific spans or entities that contribute to hallucinated content, particularly in long-form outputs.", "method": "The paper introduces a new dataset, HalluEntity, which uses entity-level annotations for hallucinations. It evaluates various uncertainty-based detection approaches across 17 modern LLMs.", "result": "Experimental results indicate that uncertainty estimation methods focusing on individual token probabilities over-predict hallucinations, while context-aware methods perform better but still exhibit suboptimal results.", "conclusion": "The study identifies key relationships between hallucination tendencies and linguistic properties, providing important insights for future research directions.", "key_contributions": ["Introduction of the HalluEntity dataset for entity-level hallucination detection.", "Comprehensive evaluation of 17 modern LLMs using the proposed dataset.", "Insights into the relationships between linguistic properties and hallucination tendencies."], "limitations": "The performance of context-aware methods is still suboptimal, indicating a need for better detection approaches.", "keywords": ["LLM", "hallucination detection", "entity-level analysis", "uncertainty estimation", "HalluEntity"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.14662", "pdf": "https://arxiv.org/pdf/2502.14662.pdf", "abs": "https://arxiv.org/abs/2502.14662", "title": "iAgent: LLM Agent as a Shield between User and Recommender Systems", "authors": ["Wujiang Xu", "Yunxiao Shi", "Zujie Liang", "Xuying Ning", "Kai Mei", "Kun Wang", "Xi Zhu", "Min Xu", "Yongfeng Zhang"], "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL 2025 and WWW2025@HCRS", "summary": "Traditional recommender systems usually take the user-platform paradigm,\nwhere users are directly exposed under the control of the platform's\nrecommendation algorithms. However, the defect of recommendation algorithms may\nput users in very vulnerable positions under this paradigm. First, many\nsophisticated models are often designed with commercial objectives in mind,\nfocusing on the platform's benefits, which may hinder their ability to protect\nand capture users' true interests. Second, these models are typically optimized\nusing data from all users, which may overlook individual user's preferences.\nDue to these shortcomings, users may experience several disadvantages under the\ntraditional user-platform direct exposure paradigm, such as lack of control\nover the recommender system, potential manipulation by the platform, echo\nchamber effects, or lack of personalization for less active users due to the\ndominance of active users during collaborative learning. Therefore, there is an\nurgent need to develop a new paradigm to protect user interests and alleviate\nthese issues. Recently, some researchers have introduced LLM agents to simulate\nuser behaviors, these approaches primarily aim to optimize platform-side\nperformance, leaving core issues in recommender systems unresolved. To address\nthese limitations, we propose a new user-agent-platform paradigm, where agent\nserves as the protective shield between user and recommender system that\nenables indirect exposure.", "AI": {"tldr": "The paper introduces a new user-agent-platform paradigm for recommender systems to better protect user interests and address flaws in traditional models.", "motivation": "Traditional recommender systems prioritize platform benefits over individual user preferences, leading to user vulnerabilities and lack of personalization.", "method": "The proposed approach involves an agent that acts as a protective layer between the user and the recommender system, allowing for indirect exposure.", "result": "This paradigm aims to enhance user control and personalization while mitigating issues like manipulation and echo chambers.", "conclusion": "Adopting this user-agent-platform model can significantly improve user satisfaction and better align recommendations with individual needs.", "key_contributions": ["Introduction of the user-agent-platform paradigm", "Improvement of user protection in recommender systems", "Focus on individual user preferences over platform objectives"], "limitations": "The effectiveness of the proposed paradigm needs empirical validation in real-world scenarios.", "keywords": ["recommender systems", "user-agent-platform", "user preferences", "LLM agents", "HCI"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.15208", "pdf": "https://arxiv.org/pdf/2502.15208.pdf", "abs": "https://arxiv.org/abs/2502.15208", "title": "Unveiling Attractor Cycles in Large Language Models: A Dynamical Systems View of Successive Paraphrasing", "authors": ["Zhilin Wang", "Yafu Li", "Jianhao Yan", "Yu Cheng", "Yue Zhang"], "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "9 pages", "summary": "Dynamical systems theory provides a framework for analyzing iterative\nprocesses and evolution over time. Within such systems, repetitive\ntransformations can lead to stable configurations, known as attractors,\nincluding fixed points and limit cycles. Applying this perspective to large\nlanguage models (LLMs), which iteratively map input text to output text,\nprovides a principled approach to characterizing long-term behaviors.\nSuccessive paraphrasing serves as a compelling testbed for exploring such\ndynamics, as paraphrases re-express the same underlying meaning with linguistic\nvariation. Although LLMs are expected to explore a diverse set of paraphrases\nin the text space, our study reveals that successive paraphrasing converges to\nstable periodic states, such as 2-period attractor cycles, limiting linguistic\ndiversity. This phenomenon is attributed to the self-reinforcing nature of\nLLMs, as they iteratively favour and amplify certain textual forms over others.\nThis pattern persists with increasing generation randomness or alternating\nprompts and LLMs. These findings underscore inherent constraints in LLM\ngenerative capability, while offering a novel dynamical systems perspective for\nstudying their expressive potential.", "AI": {"tldr": "The paper analyzes the iterative nature of large language models (LLMs) through dynamics systems theory, revealing that successive paraphrasing leads to convergence on stable periodic states, limiting linguistic diversity.", "motivation": "To understand the patterns of text generation in LLMs through iterative processes, using dynamical systems theory as a framework.", "method": "The study investigates the effects of successive paraphrasing in LLMs and explores the emergence of stable periodic states in generated outputs.", "result": "The findings demonstrate that successive paraphrasing leads to convergence towards specific periodic states (2-period attractors), indicating a limitation in linguistic diversity despite expectations of diversity in LLM outputs.", "conclusion": "The research highlights inherent constraints in the generative capabilities of LLMs and suggests using dynamical systems theory for studying the expressive potential of these models.", "key_contributions": ["Introduction of dynamical systems theory to the analysis of LLMs", "Identification of stabilization in paraphrasing leading to 2-period cycles", "Insights into constraints on linguistic diversity in LLM outputs"], "limitations": "The study focuses on specific types of iterative processes and may not generalize to all aspects of language generation in LLMs.", "keywords": ["Dynamical systems", "Large language models", "Paraphrasing", "Iterative processes", "Text generation"], "importance_score": 8, "read_time_minutes": 9}}
{"id": "2503.04807", "pdf": "https://arxiv.org/pdf/2503.04807.pdf", "abs": "https://arxiv.org/abs/2503.04807", "title": "Call for Rigor in Reporting Quality of Instruction Tuning Data", "authors": ["Hyeonseok Moon", "Jaehyung Seo", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the ACL2025-main", "summary": "Instruction tuning is crucial for adapting large language models (LLMs) to\nalign with user intentions. Numerous studies emphasize the significance of the\nquality of instruction tuning (IT) data, revealing a strong correlation between\nIT data quality and the alignment performance of LLMs. In these studies, the\nquality of IT data is typically assessed by evaluating the performance of LLMs\ntrained with that data. However, we identified a prevalent issue in such\npractice: hyperparameters for training models are often selected arbitrarily\nwithout adequate justification. We observed significant variations in\nhyperparameters applied across different studies, even when training the same\nmodel with the same data. In this study, we demonstrate the potential problems\narising from this practice and emphasize the need for careful consideration in\nverifying data quality. Through our experiments on the quality of LIMA data and\na selected set of 1,000 Alpaca data points, we demonstrate that arbitrary\nhyperparameter decisions can make any arbitrary conclusion.", "AI": {"tldr": "This paper critiques the arbitrary selection of hyperparameters in training large language models (LLMs) with instruction tuning data and its impact on evaluating data quality.", "motivation": "To address the inconsistencies in hyperparameter selection in studies assessing instruction tuning data quality for LLMs, which can lead to misleading conclusions about data effectiveness.", "method": "The paper analyzes the variation in hyperparameters used across different studies for training the same model and conducts experiments on LIMA data and a selection of Alpaca data points to illustrate the consequences of arbitrary hyperparameter choices.", "result": "The study shows that the arbitrary selection of hyperparameters can lead to significant variations in model performance, undermining the validity of conclusions drawn about instruction tuning data quality.", "conclusion": "Careful consideration of hyperparameter choices is essential when evaluating the quality of instruction tuning data to maintain the integrity of alignment performance assessments for LLMs.", "key_contributions": ["Identification of the issue with arbitrary hyperparameter selection in instruction tuning studies.", "Experimental demonstration of the impact of hyperparameter decisions on model performance results.", "Emphasis on the need for rigorous methodology in evaluating IT data quality."], "limitations": "", "keywords": ["large language models", "instruction tuning", "hyperparameter selection", "data quality", "model performance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.10995", "pdf": "https://arxiv.org/pdf/2503.10995.pdf", "abs": "https://arxiv.org/abs/2503.10995", "title": "TigerLLM -- A Family of Bangla Large Language Models", "authors": ["Nishat Raihan", "Marcos Zampieri"], "categories": ["cs.CL"], "comment": null, "summary": "The development of Large Language Models (LLMs) remains heavily skewed\ntowards English and a few other high-resource languages. This linguistic\ndisparity is particularly evident for Bangla - the 5th most spoken language. A\nfew initiatives attempted to create open-source Bangla LLMs with performance\nstill behind high-resource languages and limited reproducibility. To address\nthis gap, we introduce TigerLLM - a family of Bangla LLMs. Our results\ndemonstrate that these models surpass all open-source alternatives and also\noutperform larger proprietary models like GPT3.5 across standard benchmarks,\nestablishing TigerLLM as the new baseline for future Bangla language modeling.", "AI": {"tldr": "TigerLLM, a family of Bangla LLMs, outperforms existing open-source and proprietary models, setting a new standard for Bangla language processing.", "motivation": "To address the linguistic disparity in the development of Large Language Models (LLMs) for Bangla, which is underrepresented compared to high-resource languages.", "method": "Introduction of TigerLLM, a set of Bangla LLMs, which were evaluated against existing models across standard benchmarks.", "result": "TigerLLM surpasses all open-source alternatives and outperforms larger proprietary models like GPT3.5.", "conclusion": "TigerLLM establishes itself as the new baseline for future developments in Bangla language modeling.", "key_contributions": ["Introduction of a new family of Bangla LLMs called TigerLLM", "Performance benchmarks demonstrating superiority over existing models", "Providing a new standard for future Bangla LLM development"], "limitations": "", "keywords": ["Bangla", "Large Language Models", "TigerLLM", "NLP", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.16525", "pdf": "https://arxiv.org/pdf/2503.16525.pdf", "abs": "https://arxiv.org/abs/2503.16525", "title": "KVShare: An LLM Service System with Efficient and Effective Multi-Tenant KV Cache Reuse", "authors": ["Huan Yang", "Renji Zhang", "Mingzhe Huang", "Weijun Wang", "Yin Tang", "Yuanchun Li", "Yunxin Liu", "Deyu Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in long-text understanding have pushed the context length of\nlarge language models (LLMs) up to one million tokens. It boosts LLMs's\naccuracy and reasoning capacity but causes exorbitant computational costs and\nunsatisfactory Time to First Token (TTFT). KV cache reuse, which reuses the\nexact same KV cache of prefixes and templates or shares similar ones but with\nextra selective recomputation, offers a promising way to tackle this issue.\nHowever, prior studies overlook the cross-request KV reuse and the attention\ndeviations introduced by new tokens during the decoding stage. In this paper,\nwe present a KV cache management module that shares the KV cache across\nrequests under multi-tenant scenarios without sacrificing model accuracy. Our\nsystem, KVShare, enables accurate and efficient LLM serving by 1) a Dual-Stage\nHigh Deviation algorithm (DHD) that conditionally selects a small portion of KV\ncache to be recomputed during both prefill and decode phases, and 2) a\ncache-aware scheduler that prioritizes requests based on their KV cache hit\nrates and orchestrates continuous batching to achieve enhanced system\nefficiency and faster TTFT. Multi-task experiments conducted on models such as\nQwen2.5-7B,Llama3.1-8B and Yi1.5-9B demonstrate that KVShare reduces TTFT by up\nto 9.39x and increases 1.2x of the throughput compared to the full KV\nrecompute. Moreover, KVShare achieves 20.38% boost in terms of accuracy\ncompared to SOTA methods.", "AI": {"tldr": "This paper presents KVShare, a KV cache management module for large language models (LLMs) designed to reduce computational costs and improve Time to First Token (TTFT) while maintaining accuracy by sharing KV caches across requests.", "motivation": "To address the high computational costs and unsatisfactory Time to First Token (TTFT) associated with large language models (LLMs) operating with long context lengths.", "method": "The paper introduces KVShare, which implements a Dual-Stage High Deviation (DHD) algorithm that selectively recomputes part of the KV cache during decoding, and a cache-aware scheduler that optimizes request handling based on KV cache hit rates.", "result": "KVShare reduces TTFT by up to 9.39x and increases throughput by 1.2x compared to full KV recompute, while achieving a 20.38% improvement in accuracy over state-of-the-art methods.", "conclusion": "The proposed KV cache management module, KVShare, enhances the efficiency and effectiveness of LLM serving in multi-tenant environments without sacrificing model accuracy.", "key_contributions": ["Introduction of KVShare for efficient KV cache management in LLMs", "Development of a Dual-Stage High Deviation algorithm for selective cache recomputation", "Creation of a cache-aware scheduler for prioritizing requests"], "limitations": "", "keywords": ["large language models", "KV cache management", "recomputation", "efficiency", "multi-tenant systems"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.16529", "pdf": "https://arxiv.org/pdf/2503.16529.pdf", "abs": "https://arxiv.org/abs/2503.16529", "title": "Safety Evaluation and Enhancement of DeepSeek Models in Chinese Contexts", "authors": ["Wenjing Zhang", "Xuejiao Lei", "Zhaoxiang Liu", "Limin Han", "Jiaojiao Zhao", "Junting Guo", "Zhenhong Long", "Shu Yang", "Meijuan An", "Beibei Huang", "Rongjia Du", "Ning Wang", "Kai Wang", "Shiguo Lian"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "21 pages, 13 figures, 4 tables", "summary": "DeepSeek-R1, renowned for its exceptional reasoning capabilities and\nopen-source strategy, is significantly influencing the global artificial\nintelligence landscape. However, it exhibits notable safety shortcomings.\nRecent research conducted by Robust Intelligence, a subsidiary of Cisco, in\ncollaboration with the University of Pennsylvania, revealed that DeepSeek-R1\nachieves a 100\\% attack success rate when processing harmful prompts.\nFurthermore, multiple security firms and research institutions have identified\ncritical security vulnerabilities within the model. Although China Unicom has\nuncovered safety vulnerabilities of R1 in Chinese contexts, the safety\ncapabilities of the remaining distilled models in the R1 series have not yet\nbeen comprehensively evaluated. To address this gap, this study utilizes the\ncomprehensive Chinese safety benchmark CHiSafetyBench to conduct an in-depth\nsafety evaluation of the DeepSeek-R1 series distilled models. The objective is\nto assess the safety capabilities of these models in Chinese contexts both\nbefore and after distillation, and to further elucidate the adverse effects of\ndistillation on model safety. Building on these findings, we implement targeted\nsafety enhancements for the entire DeepSeek-R1 model series. Evaluation results\nindicate that the enhanced models achieve significant improvements in safety\nwhile maintaining reasoning capabilities without notable degradation. We\nopen-source the safety-enhanced models at\nhttps://github.com/UnicomAI/DeepSeek-R1-Safe to serve as a valuable resource\nfor future research and optimization of DeepSeek models.", "AI": {"tldr": "This study evaluates and enhances the safety of the DeepSeek-R1 series distilled models using a comprehensive Chinese safety benchmark, addressing previously identified vulnerabilities and demonstrating significant improvements in safety without compromising reasoning capabilities.", "motivation": "To assess and improve the safety of DeepSeek-R1 models, which have shown critical vulnerabilities, especially in harmful prompt processing.", "method": "The study utilizes the CHiSafetyBench, a comprehensive Chinese safety benchmark, to evaluate the safety capabilities of DeepSeek-R1 series models both before and after the distillation process.", "result": "Evaluation results show enhanced models achieve significant improvements in safety while maintaining reasoning capabilities with no notable degradation.", "conclusion": "The study successfully enhances the safety of the DeepSeek-R1 model series, providing open-source access to improved models for future research and optimization.", "key_contributions": ["Comprehensive evaluation of DeepSeek-R1 distilled models' safety in Chinese contexts", "Implementation of targeted safety enhancements", "Open-sourcing safety-enhanced models for community use"], "limitations": "The study's focus is primarily on the Chinese context and may not be applicable to other languages or settings without further research.", "keywords": ["DeepSeek-R1", "safety evaluation", "machine learning", "distillation", "HCI"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.23362", "pdf": "https://arxiv.org/pdf/2503.23362.pdf", "abs": "https://arxiv.org/abs/2503.23362", "title": "Mixture of Routers", "authors": ["Jia-Chen Zhang", "Yu-Jie Xiong", "Xi-He Qiu", "Chun-Ming Xia", "Fei Dai"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages,4 figures", "summary": "Supervised fine-tuning (SFT) is a milestone in aligning large language models\nwith human instructions and adapting them to downstream tasks. In particular,\nLow-Rank Adaptation (LoRA) has gained widespread attention due to its parameter\nefficiency. However, its impact on improving the performance of large models\nremains limited. Recent studies suggest that combining LoRA with\nMixture-of-Experts (MoE) can significantly enhance fine-tuning performance. MoE\nadapts to the diversity and complexity of datasets by dynamically selecting the\nmost suitable experts, thereby improving task accuracy and efficiency. Despite\nimpressive results, recent studies reveal issues in the MoE routing mechanism,\nsuch as incorrect assignments and imbalanced expert allocation. Inspired by the\nprinciples of Redundancy and Fault Tolerance Theory. We innovatively integrate\nthe concept of Mixture of Experts into the routing mechanism and propose an\nefficient fine-tuning method called Mixture of Routers (MoR). It employs\nmultiple sub-routers for joint selection and uses a learnable main router to\ndetermine the weights of the sub-routers. The results show that MoR outperforms\nbaseline models on most tasks, achieving an average performance improvement of\n1%. MoR can serve as a plug-and-play, parameter-efficient fine-tuning method\nsuitable for a wide range of applications. Our code is available here:\nhttps://anonymous.4open.science/r/MoR-DFC6.", "AI": {"tldr": "This paper introduces Mixture of Routers (MoR), a novel fine-tuning method that enhances parameter efficiency in large language models by integrating multiple sub-routers into the routing mechanism of Mixture-of-Experts (MoE).", "motivation": "The need to improve the performance of large models using efficient fine-tuning techniques while addressing issues in MoE routing mechanisms.", "method": "The proposed Mixture of Routers (MoR) uses multiple sub-routers for joint selection and a learnable main router to determine the weights of the sub-routers, enhancing the fine-tuning process.", "result": "MoR outperforms baseline models on most tasks, achieving an average performance improvement of 1%.", "conclusion": "MoR offers a parameter-efficient fine-tuning method suitable for a wide range of applications and serves as a plug-and-play solution.", "key_contributions": ["Introduction of Mixture of Routers (MoR) for fine-tuning.", "Integration of multiple sub-routers into the routing mechanism of MoE.", "Demonstrated average performance improvement over baseline models."], "limitations": "", "keywords": ["Mixture-of-Experts", "Low-Rank Adaptation", "fine-tuning", "large language models", "routing mechanism"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2504.01698", "pdf": "https://arxiv.org/pdf/2504.01698.pdf", "abs": "https://arxiv.org/abs/2504.01698", "title": "Do Theory of Mind Benchmarks Need Explicit Human-like Reasoning in Language Models?", "authors": ["Yi-Long Lu", "Chunhui Zhang", "Jiajun Song", "Lifeng Fan", "Wei Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Theory of Mind (ToM), the ability to attribute mental states to others, is\nfundamental for human social intelligence and a critical capability for\nadvanced Artificial Intelligence. Recent advancements in Large Language Models\n(LLMs) have shown promising performance on ToM benchmarks, raising the\nquestion: Do these benchmarks necessitate explicit human-like reasoning\nprocesses, or can models succeed through alternative strategies? We investigate\nthis question empirically by applying Reinforcement Learning (RL) and\nSupervised Fine-Tuning (SFT) to LLMs of varying scales (0.5B to 7B parameters)\nand evaluating them across multiple ToM datasets. Our results reveal a\nscale-dependent impact of RL: while RL significantly improves accuracy and\nfosters high-quality, interpretable, and transferable belief-tracking reasoning\nin larger models (7B), it leads to \"reasoning collapse\" in smaller models\n($\\leq$3B), where high accuracy and generalization ability are achieved via\ndrastically shortened, less meaningful responses. Surprisingly, further SFT\nachieves competitive and generalizable performance across these benchmarks,\noften matching or exceeding RL models in accuracy, despite not being explicitly\ntrained to produce structured reasoning traces. These findings highlight a\ncritical discrepancy between benchmark accuracy and the nature of learned\nreasoning. Our work suggests that current ToM benchmarks may be solvable\nwithout requiring the explicit, human-like simulation of mental states they\nwere designed to probe. LLMs, particularly when scale is limited or training\nsignals focus solely on output correctness, may leverage alternative rules\neffective for benchmark data structures.", "AI": {"tldr": "This paper explores the effectiveness of Large Language Models (LLMs) on Theory of Mind (ToM) tasks, revealing that larger models can improve reasoning through Reinforcement Learning (RL), while smaller models face limitations. Supervised Fine-Tuning (SFT) offers competitive performance without traditional reasoning.", "motivation": "To understand if LLM benchmarks for Theory of Mind require human-like reasoning or can succeed with alternative strategies.", "method": "The study uses Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT) on LLMs of different sizes (0.5B to 7B parameters) and evaluates their performance on multiple ToM datasets.", "result": "Larger models (7B) showed improved accuracy and reasoning quality with RL, whereas smaller models (≤3B) faced 'reasoning collapse'. SFT often matched or exceeded RL performance in terms of accuracy without explicit reasoning training.", "conclusion": "Current ToM benchmarks may not necessitate human-like reasoning; LLMs can achieve high accuracy through alternative methods, challenging the assumption that explicit belief-tracking is needed.", "key_contributions": ["Investigated the distinct performance of LLMs under different training methodologies (RL vs. SFT) on ToM tasks.", "Demonstrated scale-dependent reasoning capabilities in LLMs, noting significant differences in model sizes.", "Highlighted the disconnection between benchmark performance and the reasoning process utilized by LLMs."], "limitations": "Study largely focused on specific LLM scales and might not generalize across all model architectures or tasks.", "keywords": ["Theory of Mind", "Large Language Models", "Reinforcement Learning", "Supervised Fine-Tuning", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.09184", "pdf": "https://arxiv.org/pdf/2504.09184.pdf", "abs": "https://arxiv.org/abs/2504.09184", "title": "Parameterized Synthetic Text Generation with SimpleStories", "authors": ["Lennart Finke", "Chandan Sreedhara", "Thomas Dooms", "Mat Allen", "Emerald Zhang", "Juan Diego Rodriguez", "Noa Nabeshima", "Thomas Marshall", "Dan Braun"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SimpleStories, a large synthetic story dataset in simple language,\nconsisting of 2 million samples each in English and Japanese. Through\nparameterizing prompts at multiple levels of abstraction, we achieve control\nover story characteristics at scale, inducing syntactic and semantic diversity.\nAblations on a newly trained model suite show improved sample efficiency and\nmodel interpretability compared to the TinyStories dataset. We open-source all\nconstituent parts of model creation, hoping to enable novel ways to study the\nend-to-end training process. As a byproduct, we move the frontier regarding the\nfewest-parameter language model that outputs grammatical natural language.", "AI": {"tldr": "The paper presents SimpleStories, a dataset of 2 million synthetic stories in simple language, enabling control over story characteristics and improving model efficiency and interpretability.", "motivation": "The need for a large, diverse synthetic story dataset that facilitates efficient end-to-end training and model interpretability.", "method": "Creation of a synthetic dataset called SimpleStories with 2 million English and Japanese samples, employing parameterized prompts for varying story characteristics.", "result": "Demonstrated improved sample efficiency and model interpretability compared to the TinyStories dataset through ablation studies on a newly trained model suite.", "conclusion": "The open-sourcing of the dataset and model components aims to foster new research approaches in model training and to push the limits of parameter efficiency in language models.", "key_contributions": ["Development of SimpleStories, a large dataset for story generation", "Improvement in sample efficiency and interpretability of language models", "Open-source resources for enhancing end-to-end training studies"], "limitations": "", "keywords": ["synthetic dataset", "language model", "story generation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2505.09655", "pdf": "https://arxiv.org/pdf/2505.09655.pdf", "abs": "https://arxiv.org/abs/2505.09655", "title": "DRA-GRPO: Exploring Diversity-Aware Reward Adjustment for R1-Zero-Like Training of Large Language Models", "authors": ["Xiwen Chen", "Wenhui Zhu", "Peijie Qiu", "Xuanzhao Dong", "Hao Wang", "Haiyu Wu", "Huayu Li", "Aristeidis Sotiras", "Yalin Wang", "Abolfazl Razi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in reinforcement learning for language model post-training,\nsuch as Group Relative Policy Optimization (GRPO), have shown promise in\nlow-resource settings. However, GRPO typically relies on solution-level and\nscalar reward signals that fail to capture the semantic diversity among sampled\ncompletions. This leads to what we identify as a diversity-quality\ninconsistency, where distinct reasoning paths may receive indistinguishable\nrewards. To address this limitation, we propose $\\textit{Diversity-aware Reward\nAdjustment}$ (DRA), a method that explicitly incorporates semantic diversity\ninto the reward computation. DRA uses Submodular Mutual Information (SMI) to\ndownweight redundant completions and amplify rewards for diverse ones. This\nencourages better exploration during learning, while maintaining stable\nexploitation of high-quality samples. Our method integrates seamlessly with\nboth GRPO and its variant DR.~GRPO, resulting in $\\textit{DRA-GRPO}$ and\n$\\textit{DGA-DR.~GRPO}$. We evaluate our method on five mathematical reasoning\nbenchmarks and find that it outperforms recent strong baselines. It achieves\nstate-of-the-art performance with an average accuracy of 58.2%, using only\n7,000 fine-tuning samples and a total training cost of approximately $55. The\ncode is available at https://github.com/xiwenc1/DRA-GRPO.", "AI": {"tldr": "This paper presents Diversity-aware Reward Adjustment (DRA) to improve reward signals in reinforcement learning for language models by incorporating semantic diversity in the learning process.", "motivation": "The paper addresses the limitation of standard reward signals in reinforcement learning that fail to capture semantic diversity, leading to diversity-quality inconsistencies.", "method": "DRA employs Submodular Mutual Information (SMI) to adjust rewards by downweighting redundant completions and enhancing rewards for diverse ones, integrated with both GRPO and its variant.", "result": "DRA demonstrates improved exploration and high-quality sample exploitation, achieving state-of-the-art performance on five mathematical reasoning benchmarks with an average accuracy of 58.2% using low resources.", "conclusion": "DRA improves the effectiveness of reinforcement learning for language model post-training by rewarding diversity, leading to better performance with limited training samples.", "key_contributions": ["Introduction of the Diversity-aware Reward Adjustment (DRA) method", "Utilization of Submodular Mutual Information (SMI) for reward adjustment", "Demonstration of state-of-the-art performance on mathematical reasoning tasks"], "limitations": "", "keywords": ["Reinforcement Learning", "Natural Language Processing", "Semantic Diversity"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.09724", "pdf": "https://arxiv.org/pdf/2505.09724.pdf", "abs": "https://arxiv.org/abs/2505.09724", "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs", "authors": ["Gino Carmona-Díaz", "William Jiménez-Leal", "María Alejandra Grisales", "Chandra Sripada", "Santiago Amaya", "Michael Inzlicht", "Juan Pablo Bermúdez"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "31 pages, 1 figure", "summary": "Analyzing texts such as open-ended responses, headlines, or social media\nposts is a time- and labor-intensive process highly susceptible to bias. LLMs\nare promising tools for text analysis, using either a predefined (top-down) or\na data-driven (bottom-up) taxonomy, without sacrificing quality. Here we\npresent a step-by-step tutorial to efficiently develop, test, and apply\ntaxonomies for analyzing unstructured data through an iterative and\ncollaborative process between researchers and LLMs. Using personal goals\nprovided by participants as an example, we demonstrate how to write prompts to\nreview datasets and generate a taxonomy of life domains, evaluate and refine\nthe taxonomy through prompt and direct modifications, test the taxonomy and\nassess intercoder agreements, and apply the taxonomy to categorize an entire\ndataset with high intercoder reliability. We discuss the possibilities and\nlimitations of using LLMs for text analysis.", "AI": {"tldr": "This paper presents a tutorial for using LLMs to develop and apply taxonomies for text analysis, demonstrating an efficient process for categorizing unstructured data.", "motivation": "The analysis of texts such as responses, headlines, or social media posts is labor-intensive and biased; LLMs can enhance this process.", "method": "The paper describes a step-by-step tutorial for developing, testing, and applying taxonomies with LLMs in an iterative and collaborative manner.", "result": "The approach allows for the categorization of datasets with high intercoder reliability and demonstrates how to generate and refine taxonomies effectively.", "conclusion": "While LLMs show promise in text analysis, the paper discusses both their capabilities and limitations.", "key_contributions": ["Tutorial on developing taxonomies using LLMs", "Step-by-step guidance on text analysis with high intercoder reliability", "Insights into limitations and possibilities of LLMs for text categorization"], "limitations": "The paper discusses challenges and potential biases in using LLMs for text analysis.", "keywords": ["text analysis", "taxonomy development", "LLMs", "intercoder reliability", "unstructured data"], "importance_score": 8, "read_time_minutes": 30}}
{"id": "2505.09924", "pdf": "https://arxiv.org/pdf/2505.09924.pdf", "abs": "https://arxiv.org/abs/2505.09924", "title": "From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models", "authors": ["Yidan Wang", "Yubing Ren", "Yanan Cao", "Binxing Fang"], "categories": ["cs.CL", "cs.CR"], "comment": "Accepted to ACL 2025 (main)", "summary": "The rise of Large Language Models (LLMs) has heightened concerns about the\nmisuse of AI-generated text, making watermarking a promising solution.\nMainstream watermarking schemes for LLMs fall into two categories: logits-based\nand sampling-based. However, current schemes entail trade-offs among\nrobustness, text quality, and security. To mitigate this, we integrate\nlogits-based and sampling-based schemes, harnessing their respective strengths\nto achieve synergy. In this paper, we propose a versatile symbiotic\nwatermarking framework with three strategies: serial, parallel, and hybrid. The\nhybrid framework adaptively embeds watermarks using token entropy and semantic\nentropy, optimizing the balance between detectability, robustness, text\nquality, and security. Furthermore, we validate our approach through\ncomprehensive experiments on various datasets and models. Experimental results\nindicate that our method outperforms existing baselines and achieves\nstate-of-the-art (SOTA) performance. We believe this framework provides novel\ninsights into diverse watermarking paradigms. Our code is available at\nhttps://github.com/redwyd/SymMark.", "AI": {"tldr": "This paper introduces a symbiotic watermarking framework for Large Language Models that balances robustness, text quality, and security using three strategies: serial, parallel, and hybrid.", "motivation": "As concerns grow regarding the misuse of AI-generated text, this study aims to provide a solution through effective watermarking methods that integrate current techniques.", "method": "The paper combines logits-based and sampling-based watermarking approaches, creating a hybrid framework that embeds watermarks using token and semantic entropy.", "result": "The proposed method shows improved performance over existing watermarking techniques, achieving state-of-the-art results across various datasets and models.", "conclusion": "The authors believe this framework offers valuable insights into watermarking strategies for AI-generated text, potentially enhancing the security and accountability of LLM outputs.", "key_contributions": ["Introduction of a versatile watermarking framework for LLMs.", "Combination of logits-based and sampling-based approaches.", "Demonstration of state-of-the-art performance in watermarking."], "limitations": "", "keywords": ["Watermarking", "Large Language Models", "AI security"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2505.10354", "pdf": "https://arxiv.org/pdf/2505.10354.pdf", "abs": "https://arxiv.org/abs/2505.10354", "title": "LDIR: Low-Dimensional Dense and Interpretable Text Embeddings with Relative Representations", "authors": ["Yile Wang", "Zhanyu Shen", "Hui Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Semantic text representation is a fundamental task in the field of natural\nlanguage processing. Existing text embedding (e.g., SimCSE and LLM2Vec) have\ndemonstrated excellent performance, but the values of each dimension are\ndifficult to trace and interpret. Bag-of-words, as classic sparse interpretable\nembeddings, suffers from poor performance. Recently, Benara et al. (2024)\npropose interpretable text embeddings using large language models, which forms\n\"0/1\" embeddings based on responses to a series of questions. These\ninterpretable text embeddings are typically high-dimensional (larger than\n10,000). In this work, we propose Low-dimensional (lower than 500) Dense and\nInterpretable text embeddings with Relative representations (LDIR). The\nnumerical values of its dimensions indicate semantic relatedness to different\nanchor texts through farthest point sampling, offering both semantic\nrepresentation as well as a certain level of traceability and interpretability.\nWe validate LDIR on multiple semantic textual similarity, retrieval, and\nclustering tasks. Extensive experimental results show that LDIR performs close\nto the black-box baseline models and outperforms the interpretable embeddings\nbaselines with much fewer dimensions. Code is available at\nhttps://github.com/szu-tera/LDIR.", "AI": {"tldr": "This paper introduces Low-dimensional Dense and Interpretable text embeddings (LDIR) that offer semantic representation and interpretability, outperforming traditional methods with fewer dimensions.", "motivation": "Current text embeddings struggle with interpretability despite effective performance, prompting the need for more interpretable yet efficient solutions.", "method": "The authors propose a new approach that generates low-dimensional (under 500) embeddings using farthest point sampling to maintain semantic relatedness to anchor texts.", "result": "LDIR shows performance close to black-box models while outperforming existing interpretable embeddings with significantly fewer dimensions, validated across various tasks.", "conclusion": "LDIR provides a compelling balance between performance and interpretability in semantic text representations.", "key_contributions": ["Introduction of Low-dimensional Dense and Interpretable (LDIR) embeddings.", "Validation of LDIR on multiple tasks demonstrating superior performance with fewer dimensions.", "Code availability for further research and validation."], "limitations": "", "keywords": ["semantic text representation", "interpretable embeddings", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
