{"id": "2506.00028", "pdf": "https://arxiv.org/pdf/2506.00028.pdf", "abs": "https://arxiv.org/abs/2506.00028", "title": "Visualization and Comparison of AOI Transitions with Force-Directed Graph Layout", "authors": ["Yuri Miyagi", "Nils Rodrigues", "Daniel Weiskopf", "Takayuki Itoh"], "categories": ["cs.HC"], "comment": null, "summary": "By analyzing the gaze trajectories of people viewing screens and\nadvertisements, we can determine what people are interested in. This knowledge\ncan be effective when recommending commercial products and services, and also,\nwhen improving advertisement design. Therefore, analysis and visualization of\neye gaze have been an active research topic. This paper proposes a new method\nfor visualizing patterns of the gaze trajectories of multiple people by (1)\nvisualizing patterns that move through multiple areas of interest (AOI) and (2)\nvisualizing differences among multiple gaze trajectories. The method first\nconstructs a hierarchical AOI structure to a Web page or an image, and uses\nthis structure to convert the trajectory into a sequence of symbols. We apply\nN-grams to the generated symbol sequences to extract transition patterns\nbetween AOIs. Finally, the method visualizes a list of the pattern extraction\nresults and the shapes of the characteristic elements. We present the\nvisualization of gaze trajectories for three examples of stimuli, and argue\nthat analysts can efficiently discover trends in gaze transitions between text\nand figures, as well as differences between participants of the eye-tracking\nexperiments.", "AI": {"tldr": "The paper presents a new method for visualizing gaze trajectories, enabling the analysis of interest patterns while viewing screens and advertisements.", "motivation": "To improve advertisement design and product recommendations by analyzing eye gaze patterns.", "method": "The proposed method constructs a hierarchical structure of Areas of Interest (AOI) to convert gaze trajectories into symbol sequences, utilizing N-grams to extract and visualize transition patterns between AOIs.", "result": "The method efficiently visualizes gaze transition trends and differences among multiple participants in eye-tracking experiments.", "conclusion": "Analysts can better understand gaze behaviors, enhancing insights into interaction patterns with visual stimuli.", "key_contributions": ["Development of a hierarchical AOI structure for gaze trajectory analysis", "Use of N-grams for pattern extraction in gaze data", "Effective visualization of gaze transition differences among participants"], "limitations": "", "keywords": ["gaze trajectory", "visualization", "advertisement design", "eye tracking", "N-grams"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.00241", "pdf": "https://arxiv.org/pdf/2506.00241.pdf", "abs": "https://arxiv.org/abs/2506.00241", "title": "Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department", "authors": ["Menglin Zhao", "Zhuorui Yong", "Ruijia Guan", "Kai-Wei Chang", "Adrian Haimovich", "Kei Ouchi", "Timothy Bickmore", "Bingsheng Yao", "Dakuo Wang", "Smit Desai"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Serious illness conversations (SICs), discussions between clinical care teams\nand patients with serious, life-limiting illnesses about their values, goals,\nand care preferences, are critical for patient-centered care. Without these\nconversations, patients often receive aggressive interventions that may not\nalign with their goals. Clinical care teams face significant barriers when\nconducting serious illness conversations with older adult patients in Emergency\nDepartment (ED) settings, where most older adult patients lack documented\ntreatment goals. To understand current practices and identify AI support\nopportunities, we conducted interviews with two domain experts and nine ED\nclinical care team members. Through thematic analysis, we characterized a\nfour-phase serious illness conversation workflow (identification, preparation,\nconduction, documentation) and identified key needs and challenges at each\nstage. Clinical care teams struggle with fragmented EHR data access, time\nconstraints, emotional preparation demands, and documentation burdens. While\nparticipants expressed interest in AI tools for information synthesis,\nconversational support, and automated documentation, they emphasized preserving\nhuman connection and clinical autonomy. We present design guidelines for AI\ntools supporting SIC workflows that fit within existing clinical practices.\nThis work contributes empirical understanding of ED-based serious illness\nconversations and provides design considerations for AI in high-stakes clinical\nenvironments.", "AI": {"tldr": "This paper explores AI support for serious illness conversations (SICs) in Emergency Departments (ED), identifying current workflows and challenges faced by clinical teams.", "motivation": "To enhance patient-centered care through serious illness conversations between clinicians and patients with serious illnesses, particularly in emergency settings where communication is crucial yet challenging.", "method": "Interviews with two domain experts and nine ED clinical care team members, followed by thematic analysis to characterize SIC workflows and identify needs and challenges.", "result": "The study identified a four-phase workflow for SICs and highlighted significant barriers such as EHR data access issues, time constraints, and the need for emotional preparation, alongside a desire for AI tools to assist in these areas.", "conclusion": "Design guidelines for AI tools supporting SIC workflows are proposed, ensuring they align with current clinical practices while maintaining human connection and clinician autonomy.", "key_contributions": ["Characterization of a four-phase SIC workflow", "Identification of barriers faced by clinical care teams during SICs", "Design guidelines for integrating AI into clinical practices supporting SICs"], "limitations": "Focus on a limited sample size from specific clinical settings may limit generalizability of findings.", "keywords": ["serious illness conversations", "AI in healthcare", "Emergency Department", "clinical workflows", "patient-centered care"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00376", "pdf": "https://arxiv.org/pdf/2506.00376.pdf", "abs": "https://arxiv.org/abs/2506.00376", "title": "Understanding Remote Communication between Grandparents and Grandchildren in Distributed Immigrant Families", "authors": ["Jiawen Stefanie Zhu", "Jian Zhao"], "categories": ["cs.HC", "cs.CY", "H.5.0"], "comment": "Poster at Graphics Interface 2025", "summary": "Grandparent-grandchild bonds are crucial for both parties. Many immigrant\nfamilies are geographically dispersed, and the grandparents and grandchildren\nneed to rely on remote communication to maintain their relationships. In\naddition to geographical separation, grandparents and grandchildren in such\nfamilies also face language and culture barriers during remote communication.\nThe associated challenges and needs remain understudied as existing research\nprimarily focuses on non-immigrant families or co-located immigrant families.\nTo address this gap, we conducted interviews with six Chinese immigrant\nfamilies in Canada. Our findings highlight unique challenges faced by immigrant\nfamilies during remote communication, such as amplified language and cultural\nbarriers due to geographic separation, and provide insights into how technology\ncan better support remote communication. This work offers empirical knowledge\nabout the communication needs of distributed immigrant families and provides\ndirections for future research and design to support grandparent-grandchild\nremote communication in these families.", "AI": {"tldr": "The paper explores the unique challenges faced by Chinese immigrant families in Canada during remote communication between grandparents and grandchildren, emphasizing language and cultural barriers.", "motivation": "To investigate how geographic separation affects the communication between immigrant grandparents and grandchildren, as existing research mainly focuses on non-immigrant or co-located families.", "method": "Interviews with six Chinese immigrant families in Canada were conducted to gather insights on their remote communication experiences.", "result": "The research identifies significant language and cultural barriers exacerbated by geographic distance, offering insights into how technology can bridge these gaps.", "conclusion": "The work enhances the understanding of the communication needs of dispersed immigrant families and suggests directions for future technological support in maintaining grandparent-grandchild relationships.", "key_contributions": ["Identifies specific challenges in remote communication for immigrant families.", "Highlights the impact of language and cultural barriers due to geographic separation.", "Provides empirical insights for the design of supportive technology."], "limitations": "The study is limited by its small sample size and focus on only one cultural group (Chinese immigrants in Canada).", "keywords": ["Remote Communication", "Immigrant Families", "Cultural Barriers", "Technology Support", "Grandparent-Grandchild Relations"], "importance_score": 3, "read_time_minutes": 10}}
{"id": "2506.00717", "pdf": "https://arxiv.org/pdf/2506.00717.pdf", "abs": "https://arxiv.org/abs/2506.00717", "title": "Vid2Coach: Transforming How-To Videos into Task Assistants", "authors": ["Mina Huh", "Zihui Xue", "Ujjaini Das", "Kumar Ashutosh", "Kristen Grauman", "Amy Pavel"], "categories": ["cs.HC", "cs.CV"], "comment": null, "summary": "People use videos to learn new recipes, exercises, and crafts. Such videos\nremain difficult for blind and low vision (BLV) people to follow as they rely\non visual comparison. Our observations of visual rehabilitation therapists\n(VRTs) guiding BLV people to follow how-to videos revealed that VRTs provide\nboth proactive and responsive support including detailed descriptions,\nnon-visual workarounds, and progress feedback. We propose Vid2Coach, a system\nthat transforms how-to videos into wearable camera-based assistants that\nprovide accessible instructions and mixed-initiative feedback. From the video,\nVid2Coach generates accessible instructions by augmenting narrated instructions\nwith demonstration details and completion criteria for each step. It then uses\nretrieval-augmented-generation to extract relevant non-visual workarounds from\nBLV-specific resources. Vid2Coach then monitors user progress with a camera\nembedded in commercial smart glasses to provide context-aware instructions,\nproactive feedback, and answers to user questions. BLV participants (N=8) using\nVid2Coach completed cooking tasks with 58.5\\% fewer errors than when using\ntheir typical workflow and wanted to use Vid2Coach in their daily lives.\nVid2Coach demonstrates an opportunity for AI visual assistance that strengthens\nrather than replaces non-visual expertise.", "AI": {"tldr": "Vid2Coach is a system designed to assist blind and low vision individuals by transforming how-to videos into accessible, wearable camera-based guidance, enhancing their learning experience in tasks like cooking.", "motivation": "To improve accessibility for blind and low vision (BLV) individuals using how-to videos, which are typically reliant on visual cues that they cannot perceive.", "method": "Vid2Coach utilizes a combination of narrated instructions and augmented details derived from videos. It employs retrieval-augmented generation to produce non-visual workarounds and monitors user progress using a camera in smart glasses to provide real-time feedback.", "result": "Users completing tasks with Vid2Coach made 58.5% fewer errors than with their usual methods, indicating significant improvement in task execution.", "conclusion": "Vid2Coach not only enhances accessibility but also represents a new approach to leveraging AI for visual assistance, maintaining the importance of non-visual expertise.", "key_contributions": ["Development of Vid2Coach for enhancing accessibility of how-to videos", "Integration of retrieval-augmented generation for non-visual guidance", "Demonstrated effectiveness through user studies with BLV individuals"], "limitations": "", "keywords": ["Blind and Low Vision", "Accessible Technology", "AI Assistance"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.00019", "pdf": "https://arxiv.org/pdf/2506.00019.pdf", "abs": "https://arxiv.org/abs/2506.00019", "title": "Amadeus-Verbo Technical Report: The powerful Qwen2.5 family models trained in Portuguese", "authors": ["William Alberto Cruz-Castañeda", "Marcellus Amadeus"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report introduces the experience of developing Amadeus Verbo, a family\nof large language models for Brazilian Portuguese. To handle diverse use cases,\nAmadeus Verbo includes base-tuned, merged, and instruction-tuned models in\nsizes of 0.5B, 1.5B, 3B, 7B, 14B, 32B, and 72B parameters. Thus, the main\nobjective is to show how easy it is to fine-tune foundation models to\ndemocratize the open-source development of Brazilian Portuguese LLMs when data\nand resources are available. Amadeus-Verbo family models are all available at\nHuggingFace at\nhttps://huggingface.co/collections/amadeusai/amadeus-verbo-qwen25-67cf2e7aae69ce2b3bcdcfda.", "AI": {"tldr": "Development of Amadeus Verbo, a family of Brazilian Portuguese LLMs with various model sizes.", "motivation": "To democratize the open-source development of Brazilian Portuguese language models by demonstrating ease of fine-tuning foundation models.", "method": "The report describes the creation of base-tuned, merged, and instruction-tuned models in multiple sizes, focusing on the process of obtaining and tuning these models.", "result": "Amadeus Verbo includes models ranging from 0.5B to 72B parameters, showcasing flexibility for different applications in Brazilian Portuguese.", "conclusion": "Easy fine-tuning of these models makes them accessible for various use cases, promoting broader use of Brazilian Portuguese LLMs.", "key_contributions": ["Introduction of Amadeus Verbo family of models", "Demonstration of fine-tuning process for LLMs in Brazilian Portuguese", "Availability of models on HuggingFace for public use"], "limitations": "", "keywords": ["large language models", "Brazilian Portuguese", "fine-tuning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.00791", "pdf": "https://arxiv.org/pdf/2506.00791.pdf", "abs": "https://arxiv.org/abs/2506.00791", "title": "CO-OPERA: A Human-AI Collaborative Playwriting Tool to Support Creative Storytelling for Interdisciplinary Drama Education", "authors": ["Xuejiao Ma", "Haibo Zhao", "Zinuo Guo", "Yijie Guo", "Guanhong Liu", "Bo Jiang"], "categories": ["cs.HC"], "comment": null, "summary": "Drama-in-education is an interdisciplinary instructional approach that\nintegrates subjects such as language, history, and psychology. Its core\ncomponent is playwriting. Based on need-finding interviews of 13 teachers, we\nfound that current general-purpose AI tools cannot effectively assist teachers\nand students during playwriting. Therefore, we propose CO-OPERA - a\ncollaborative playwriting tool integrating generative artificial intelligence\ncapabilities. In CO-OPERA, users can both expand their thinking through\ndiscussions with a tutor and converge their thinking by operating agents to\ngenerate script elements. Additionally, the system allows for iterative\nmodifications and regenerations based on user requirements. A system usability\ntest conducted with middle school students shows that our CO-OPERA helps users\nfocus on whole logical narrative development during playwriting. Our\nplaywriting examples and raw data for qualitative and quantitative analysis are\navailable at https://github.com/daisyinb612/CO-OPERA.", "AI": {"tldr": "CO-OPERA is a collaborative playwriting tool that uses generative AI to assist in the playwriting process, enhancing logical narrative development for students.", "motivation": "Current AI tools are ineffective in supporting teachers and students during playwriting, highlighting the need for a specialized solution.", "method": "The study involved need-finding interviews with 13 teachers and the development of the CO-OPERA tool, which integrates generative AI for collaborative playwriting.", "result": "A usability test with middle school students demonstrated that CO-OPERA enables users to better focus on the logical progression of their narratives during the writing process.", "conclusion": "CO-OPERA improves playwriting by facilitating discussion and iterative modifications, thereby enhancing the overall narrative development experience.", "key_contributions": ["Introduction of CO-OPERA as a generative AI tool for playwriting", "Usability validation through testing with middle school students", "Creation of a collaborative environment for narrative development"], "limitations": "The study's findings are based on a small sample size of 13 teachers and a single usability test with middle school students.", "keywords": ["collaborative playwriting", "generative AI", "education technology", "narrative development", "user usability"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.00022", "pdf": "https://arxiv.org/pdf/2506.00022.pdf", "abs": "https://arxiv.org/abs/2506.00022", "title": "Scaling Physical Reasoning with the PHYSICS Dataset", "authors": ["Shenghe Zheng", "Qianjia Cheng", "Junchi Yao", "Mengsong Wu", "haonan he", "Ning Ding", "Yu Cheng", "Shuyue Hu", "Lei Bai", "Dongzhan Zhou", "Ganqu Cui", "Peng Ye"], "categories": ["cs.CL", "cs.LG", "physics.ed-ph"], "comment": "Work on physical datasets", "summary": "Large Language Models (LLMs) have achieved remarkable progress on advanced\nreasoning tasks such as mathematics and coding competitions. Meanwhile,\nphysics, despite being both reasoning-intensive and essential to real-world\nunderstanding, received limited academic and industrial attention. This paper\nintroduces PHYSICS, a dataset containing 16,568 high-quality physics problems\nspanning subjects and difficulty levels, to facilitate this issue.\nSpecifically, PHYSICS is curated with exercises from over 100 textbooks through\na carefully designed pipeline for quality control. It covers five major physics\ndomains: Mechanics, Electromagnetism, Thermodynamics, Optics, and Modern\nPhysics. It also spans a wide range of difficulty levels, from high school to\ngraduate-level physics courses. To utilize the data for improving and\nevaluating the model's physical reasoning capabilities, we split the dataset\ninto training and test sets, and provide reasoning paths generated by powerful\nreasoning models for the training data to facilitate model training. In\naddition, for the evaluation part, we find that existing evaluation frameworks\nexhibit biases in aspects such as units, simplification, and precision in\nphysics domain. To balance efficiency and accuracy, we introduce a Rule+Model\nevaluation framework tailored to physics problems. Our evaluations on current\nstate-of-the-art open-source and proprietary models highlight the limitations\nof current models in handling physics-related tasks. We hope that our dataset\nand evaluation methodology will jointly advance the development of LLMs in the\nfield of physics.", "AI": {"tldr": "The paper introduces PHYSICS, a dataset with 16,568 physics problems aimed at improving LLMs' reasoning capabilities in physics.", "motivation": "To address the lack of high-quality datasets for physics that can enhance the reasoning capabilities of large language models (LLMs).", "method": "The dataset is curated from over 100 textbooks, covering a range of physics topics and difficulty levels, and includes reasoning paths for training models. It also proposes a new evaluation framework tailored to physics problems.", "result": "The PHYSICS dataset, along with the proposed evaluation framework, highlights the current limitations of state-of-the-art models in handling physics reasoning tasks.", "conclusion": "The dataset and evaluation method are expected to advance LLM development specifically in the field of physics.", "key_contributions": ["Introduction of the PHYSICS dataset with 16,568 high-quality physics problems.", "Development of a Rule+Model evaluation framework for physics tasks.", "Identification of biases in existing evaluation frameworks for physics."], "limitations": "The dataset is limited to physics and may not generalize to other reasoning-intensive disciplines.", "keywords": ["Large Language Models", "Physics", "Dataset", "Reasoning", "Evaluation Framework"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.01284", "pdf": "https://arxiv.org/pdf/2506.01284.pdf", "abs": "https://arxiv.org/abs/2506.01284", "title": "Fast SSVEP Detection Using a Calibration-Free EEG Decoding Framework", "authors": ["Chenlong Wang", "Jiaao Li", "Shuailei Zhang", "Wenbo Ding", "Xinlei Chen"], "categories": ["cs.HC"], "comment": null, "summary": "Steady-State Visual Evoked Potential is a brain response to visual stimuli\nflickering at constant frequencies. It is commonly used in brain-computer\ninterfaces for direct brain-device communication due to their simplicity,\nminimal training data, and high information transfer rate. Traditional methods\nsuffer from poor performance due to reliance on prior knowledge, while deep\nlearning achieves higher accuracy but requires substantial high-quality\ntraining data for precise signal decoding. In this paper, we propose a\ncalibration-free EEG signal decoding framework for fast SSVEP detection. Our\nframework integrates Inter-Trial Remixing & Context-Aware Distribution\nAlignment data augmentation for EEG signals and employs a compact architecture\nof small fully connected layers, effectively addressing the challenge of\nlimited EEG data availability. Additionally, we propose an Adaptive Spectrum\nDenoise Module that operates in the frequency domain based on global features,\nrequiring only linear complexity to reduce noise in EEG data and improve data\nquality. For calibration-free classification experiments on short EEG signals\nfrom three public datasets, our framework demonstrates statistically\nsignificant accuracy advantages(p<0.05) over existing methods in the majority\nof cases, while requiring at least 52.7% fewer parameters and 29.9% less\ninference time. By eliminating the need for user-specific calibration, this\nadvancement significantly enhances the usability of BCI systems, accelerating\ntheir commercialization and widespread adoption in real-world applications.", "AI": {"tldr": "Proposed a novel calibration-free EEG signal decoding framework enhancing SSVEP detection for brain-computer interfaces (BCI) with superior accuracy and reduced resource requirements.", "motivation": "Growth in brain-computer interfaces necessitates improved SSVEP detection methods that are both accurate and require minimal user-specific calibration.", "method": "Integrated Inter-Trial Remixing & Context-Aware Distribution Alignment data augmentation techniques and a compact architecture of fully connected layers, alongside an Adaptive Spectrum Denoise Module for improved EEG data quality.", "result": "Demonstrated significant accuracy improvements over traditional methods with at least 52.7% fewer parameters and 29.9% less inference time in calibration-free classification on short EEG signals from three datasets.", "conclusion": "The proposed framework enhances BCI usability, facilitating broader application and commercialization, as it eliminates the need for extensive individual calibration.", "key_contributions": ["Calibration-free EEG signal decoding framework", "Integration of innovative data augmentation techniques for EEG signals", "Adaptive Spectrum Denoise Module for improved data quality"], "limitations": "", "keywords": ["SSVEP", "brain-computer interfaces", "EEG signal decoding", "data augmentation", "deep learning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.00027", "pdf": "https://arxiv.org/pdf/2506.00027.pdf", "abs": "https://arxiv.org/abs/2506.00027", "title": "From Mathematical Reasoning to Code: Generalization of Process Reward Models in Test-Time Scaling", "authors": ["Zhengyu Chen", "Yudong Wang", "Teng Xiao", "Ruochen Zhou", "Xuesheng Yang", "Wei Wang", "Zhifang Sui", "Jingang Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in improving the reasoning capabilities of Large Language\nModels have underscored the efficacy of Process Reward Models (PRMs) in\naddressing intermediate errors through structured feedback mechanisms. This\nstudy analyzes PRMs from multiple perspectives, including training\nmethodologies, scalability, and generalization capabilities. We investigate the\ninterplay between pre-training and reward model training FLOPs to assess their\ninfluence on PRM efficiency and accuracy in complex reasoning tasks. Our\nanalysis reveals a pattern of diminishing returns in performance with\nincreasing PRM scale, highlighting the importance of balancing model size and\ncomputational cost. Furthermore, the diversity of training datasets\nsignificantly impacts PRM performance, emphasizing the importance of diverse\ndata to enhance both accuracy and efficiency. We further examine test-time\nscaling strategies, identifying Monte Carlo Tree Search as the most effective\nmethod when computational resources are abundant, while Best-of-N Sampling\nserves as a practical alternative under resource-limited conditions. Notably,\nour findings indicate that PRMs trained on mathematical datasets exhibit\nperformance comparable to those tailored for code generation, suggesting robust\ncross-domain generalization. Employing a gradient-based metric, we observe that\nPRMs exhibit a preference for selecting responses with similar underlying\npatterns, further informing their optimization.", "AI": {"tldr": "The study analyzes Process Reward Models (PRMs) to improve their reasoning capabilities, examining their training methodologies, scalability, and generalization in complex tasks while emphasizing the role of diverse datasets and efficient test-time scaling strategies.", "motivation": "To investigate the efficacy of Process Reward Models in improving reasoning capabilities of Large Language Models through structured feedback mechanisms.", "method": "The study analyzes PRMs from perspectives of training methodologies, computational efficiency, and generalization capabilities, focusing on the influence of pre-training and reward model training FLOPs, as well as assessing different test-time scaling strategies.", "result": "The analysis reveals a diminishing returns pattern in performance with increased PRM scale, highlights the significance of diverse training datasets, and identifies effective test-time scaling methods, such as Monte Carlo Tree Search and Best-of-N Sampling.", "conclusion": "Balancing model size and computational cost is crucial for optimizing PRMs, and their performance on distinct tasks shows potential for cross-domain generalization, especially in mathematical datasets and code generation.", "key_contributions": ["Examination of the interplay between model scale and training FLOPs on PRM efficiency.", "Identification of test-time scaling strategies for optimizing complex reasoning tasks.", "Demonstration of cross-domain generalization abilities of PRMs trained on diverse datasets."], "limitations": "", "keywords": ["Process Reward Models", "Large Language Models", "Reasoning", "Generalization", "Test-time scaling"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01287", "pdf": "https://arxiv.org/pdf/2506.01287.pdf", "abs": "https://arxiv.org/abs/2506.01287", "title": "How Problematic are Suspenseful Interactions?", "authors": ["Alarith Uhde"], "categories": ["cs.HC"], "comment": "14 pages, 3 figures", "summary": "Current \"social acceptability\" guidelines for interactive technologies advise\nagainst certain, seemingly problematic forms of interaction. Specifically,\n\"suspenseful\" interactions, characterized by visible manipulations and\ninvisible effects, are generally considered be problematic. However, the\nempirical grounding for this claim is surprisingly weak. To test its validity,\nthis paper presents a controlled replication study (n = 281) of the\n\"suspensefulness effect\". Although it could be statistically replicated with\ntwo out of three social acceptability measures, effect sizes were small (r =<\n.2), and all compared forms of interaction, including the suspenseful one, had\nhigh absolute social acceptability scores. Thus, despite the slight negative\neffect, suspenseful interactions seem less problematic in the overall scheme of\nthings. We discuss alternative approaches to improve the social acceptability\nof interactive technology, and recommend to more closely engage with their\nspecific social situatedness.", "AI": {"tldr": "This paper investigates the social acceptability of suspenseful interactions in interactive technologies, finding them less problematic than previously thought despite some negative perceptions.", "motivation": "To challenge the weak empirical support for the social acceptability guidelines that deem suspenseful interactions problematic.", "method": "A controlled replication study was conducted with 281 participants to assess the 'suspensefulness effect' across different social acceptability measures.", "result": "The study found that suspenseful interactions were statistically replicated with two out of three social acceptability measures, although effect sizes were small and overall acceptability scores were high.", "conclusion": "Despite some slight negative effects on social acceptability, suspenseful interactions are not as problematic as suggested by current guidelines, calling for more context-aware evaluations of technology.", "key_contributions": ["Controlled replication study of the suspensefulness effect", "Quantitative analysis of social acceptability measures", "Recommendations for improving social acceptability in technology"], "limitations": "Effect sizes were small, indicating that while there is a slight negative perception, it is not significant in the broader context of social acceptability.", "keywords": ["social acceptability", "suspenseful interactions", "interactive technology", "human-computer interaction", "replication study"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.00042", "pdf": "https://arxiv.org/pdf/2506.00042.pdf", "abs": "https://arxiv.org/abs/2506.00042", "title": "Enhancing Tool Learning in Large Language Models with Hierarchical Error Checklists", "authors": ["Yue Cui", "Liuyi Yao", "Shuchang Tao", "Weijie Shi", "Yaliang Li", "Bolin Ding", "Xiaofang Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced natural language\nprocessing, particularly through the integration of external tools and APIs.\nHowever, their effectiveness is frequently hampered by parameter mis-filling\nduring tool calling. In this paper, we propose the Hierarchical Tool Error\nChecklist (HiTEC) framework to systematically diagnose and mitigate\ntool-calling errors without relying on extensive real-world interactions. HiTEC\nintroduces a two-tiered approach: a global error checklist that identifies\ncommon, cross-tool issues, and a local error checklist that targets\ntool-specific and contextual failures. Building on this structure, we propose\ntwo deployments: HiTEC-In Context Learning (HiTEC-ICL) and\nHiTEC-Kahneman-Tversky Optimization (HiTEC-KTO). HiTEC-ICL embeds the global\nchecklist in the initial prompts and leverages a two-round conversational\ninteraction to dynamically refine parameter handling, while HiTEC-KTO generates\nhigh-quality negative examples to drive fine-tuning via preference-based\noptimization. Extensive experiments across five public datasets demonstrate\nthat our framework significantly improves parameter-filling accuracy and\ntool-calling success rates compared to baseline methods.", "AI": {"tldr": "The HiTEC framework systematically addresses tool-calling errors in LLMs to enhance natural language processing effectiveness.", "motivation": "To mitigate parameter mis-filling during tool calling in large language models, which hampers their effectiveness.", "method": "The paper proposes the Hierarchical Tool Error Checklist (HiTEC), which includes a global error checklist for common issues and a local error checklist for tool-specific failures. It introduces two deployments: HiTEC-In Context Learning (HiTEC-ICL) and HiTEC-Kahneman-Tversky Optimization (HiTEC-KTO).", "result": "The HiTEC framework significantly improves parameter-filling accuracy and tool-calling success rates across five public datasets.", "conclusion": "The proposed approach offers a systematic method to diagnose and mitigate tool-calling errors in LLMs, enhancing their performance without extensive real-world interactions.", "key_contributions": ["Introduction of the HiTEC framework for tool error diagnosis in LLMs", "Development of HiTEC-ICL and HiTEC-KTO for parameter handling and fine-tuning", "Experimental validation showing improved outcomes compared to baselines"], "limitations": "", "keywords": ["large language models", "tool-calling errors", "Hierarchical Tool Error Checklist", "natural language processing", "parameter filling"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01395", "pdf": "https://arxiv.org/pdf/2506.01395.pdf", "abs": "https://arxiv.org/abs/2506.01395", "title": "NoRe: Augmenting Journaling Experience with Generative AI for Music Creation", "authors": ["Joonyoung Park", "Hyewon Cho", "Hyehyun Chu", "Yeeun Lee", "Hajin Lim"], "categories": ["cs.HC"], "comment": "Accepted by ACM DIS 2025; 20 pages, 6 figures, 8 tables", "summary": "Journaling has long been recognized for fostering emotional awareness and\nself-reflection, and recent advancements in generative AI offer new\nopportunities to create personalized music that can enhance these practices. In\nthis study, we explore how AI-generated music can augment the journaling\nexperience. Through a formative study, we examined journal writers' writing\npatterns, purposes, emotional regulation strategies, and the design\nrequirements for the system that augments journaling experience by\njournal-based AI-generated music. Based on these insights, we developed NoRe, a\nsystem that transforms journal entries into personalized music using generative\nAI. In a seven-day in-the-wild study (N=15), we investigated user engagement\nand perceived emotional effectiveness through system logs, surveys, and\ninterviews. Our findings suggest that journal-based music generation could\nsupport emotional reflection and provide vivid reminiscence of daily\nexperiences. Drawing from these findings, we discuss design implications for\ntailoring music to journal writers' emotional states and preferences.", "AI": {"tldr": "This study investigates the use of AI-generated music to enhance journaling practices, presenting a system called NoRe that personalizes music based on journal entries to support emotional reflection.", "motivation": "To explore how generative AI can augment emotional awareness and self-reflection in journaling through personalized music.", "method": "A formative study was conducted to analyze journal writers' behaviors and needs, leading to the development of NoRe, a system that creates personalized music from journal entries. A seven-day in-the-wild study was performed with 15 participants to evaluate engagement and emotional effectiveness.", "result": "The findings indicate that AI-generated music can enhance emotional reflection and help users reminisce about daily experiences effectively.", "conclusion": "Journal-based music generation has the potential to support emotional reflection and offers design implications for aligning music with users' emotional states and preferences.", "key_contributions": ["Development of NoRe, a system transforming journal entries into personalized music using AI.", "Insights into journal writers' emotional regulation and design requirements for an AI music generation system.", "Empirical evidence from user studies supporting the emotional impact of AI-generated music on journaling."], "limitations": "Small sample size (N=15) and the duration of the study may limit generalizability of findings.", "keywords": ["AI-generated music", "journaling", "emotional reflection", "human-computer interaction", "personalization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00061", "pdf": "https://arxiv.org/pdf/2506.00061.pdf", "abs": "https://arxiv.org/abs/2506.00061", "title": "Unraveling SITT: Social Influence Technique Taxonomy and Detection with LLMs", "authors": ["Wiktoria Mieleszczenko-Kowszewicz", "Beata Bajcar", "Aleksander Szczęsny", "Maciej Markiewicz", "Jolanta Babiak", "Berenika Dyczek", "Przemysław Kazienko"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we present the Social Influence Technique Taxonomy (SITT), a\ncomprehensive framework of 58 empirically grounded techniques organized into\nnine categories, designed to detect subtle forms of social influence in textual\ncontent. We also investigate the LLMs ability to identify various forms of\nsocial influence. Building on interdisciplinary foundations, we construct the\nSITT dataset -- a 746-dialogue corpus annotated by 11 experts in Polish and\ntranslated into English -- to evaluate the ability of LLMs to identify these\ntechniques. Using a hierarchical multi-label classification setup, we benchmark\nfive LLMs, including GPT-4o, Claude 3.5, Llama-3.1, Mixtral, and PLLuM. Our\nresults show that while some models, notably Claude 3.5, achieved moderate\nsuccess (F1 score = 0.45 for categories), overall performance of models remains\nlimited, particularly for context-sensitive techniques. The findings\ndemonstrate key limitations in current LLMs' sensitivity to nuanced linguistic\ncues and underscore the importance of domain-specific fine-tuning. This work\ncontributes a novel resource and evaluation example for understanding how LLMs\ndetect, classify, and potentially replicate strategies of social influence in\nnatural dialogues.", "AI": {"tldr": "This paper introduces the Social Influence Technique Taxonomy (SITT), a framework for detecting social influence in text, and evaluates LLMs on their ability to identify these techniques using a annotated dialogue corpus.", "motivation": "To create a comprehensive framework that helps in detecting subtle forms of social influence in textual content and to evaluate the capabilities of large language models (LLMs) in this domain.", "method": "A hierarchical multi-label classification setup was used to benchmark five LLMs on a 746-dialogue corpus annotated for social influence techniques.", "result": "Claude 3.5 showed moderate success with an F1 score of 0.45, but overall performance of the models was limited, particularly for context-sensitive techniques.", "conclusion": "Current LLMs struggle with sensitivity to nuanced linguistic cues in identifying social influence, highlighting a need for domain-specific fine-tuning.", "key_contributions": ["Introduction of the Social Influence Technique Taxonomy (SITT) with 58 techniques.", "Creation of the SITT dataset for evaluating LLMs.", "Benchmarking of five LLMs on the developed dataset with results indicating limitations in their capabilities."], "limitations": "LLMs show limited sensitivity to nuanced linguistic cues, especially for context-sensitive techniques.", "keywords": ["social influence", "large language models", "dialogue corpus", "classification", "linguistic cues"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.01836", "pdf": "https://arxiv.org/pdf/2506.01836.pdf", "abs": "https://arxiv.org/abs/2506.01836", "title": "Your Interface, Your Control: Adapting Takeover Requests for Seamless Handover in Semi-Autonomous Vehicles", "authors": ["Amr Gomaa", "Simon Engel", "Elena Meiser", "Abdulrahman Mohamed Selim", "Tobias Jungbluth", "Aeneas Leon Sommer", "Sarah Kohlmann", "Michael Barz", "Maurice Rekrut", "Michael Feld", "Daniel Sonntag", "Antonio Krüger"], "categories": ["cs.HC"], "comment": null, "summary": "With the automotive industry transitioning towards conditionally automated\ndriving, takeover warning systems are crucial for ensuring safe collaborative\ndriving between users and semi-automated vehicles. However, previous work has\nfocused on static warning systems that do not accommodate different driver\nstates. Therefore, we propose an adaptive takeover warning system that is\npersonalised to drivers, enhancing their experience and safety. We conducted\ntwo user studies investigating semi-autonomous driving scenarios in rural and\nurban environments while participants performed non-driving-related tasks such\nas text entry and visual search. We investigated the effects of varying time\nbudgets and head-up versus head-down displays for takeover requests on drivers'\nsituational awareness and mental state. Through our statistical and clustering\nanalyses, we propose strategies for designing adaptable takeover systems, e.g.,\nusing longer time budgets and head-up displays for non-hazardous takeover\nevents in high-complexity environments while using shorter time budgets and\nhead-down displays for hazardous events in low-complexity environments.", "AI": {"tldr": "Proposes an adaptive takeover warning system for semi-automated vehicles that personalizes warnings based on driver states to enhance safety and situational awareness.", "motivation": "To enhance safety in conditionally automated driving by personalizing takeover warning systems to accommodate different driver states.", "method": "Conducted two user studies in rural and urban settings with participants performing non-driving-related tasks while measuring the effectiveness of various display types and timing for takeover warnings.", "result": "Identified effective strategies for designing adaptable takeover systems, recommending longer time budgets and head-up displays for non-hazardous events in complex environments, and shorter budgets with head-down displays for hazardous events in simpler environments.", "conclusion": "An adaptive system can improve driver safety and engagement by tailoring warning protocols to specific driving contexts and mental states.", "key_contributions": ["Development of a personalized adaptive takeover warning system", "Insights from user studies on driver states and display types", "Recommendations for designing safer semi-automated driving systems"], "limitations": "Results may vary across different driver profiles and environments outside of those studied.", "keywords": ["Automated driving", "User study", "Takeover warning system", "Human-Computer Interaction", "Driver safety"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.00064", "pdf": "https://arxiv.org/pdf/2506.00064.pdf", "abs": "https://arxiv.org/abs/2506.00064", "title": "Mis-prompt: Benchmarking Large Language Models for Proactive Error Handling", "authors": ["Jiayi Zeng", "Yizhe Feng", "Mengliang He", "Wenhui Lei", "Wei Zhang", "Zeming Liu", "Xiaoming Shi", "Aimin Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant advancements in\nerror handling. Current error-handling works are performed in a passive manner,\nwith explicit error-handling instructions. However, in real-world scenarios,\nexplicit error-handling instructions are usually unavailable. In this paper,\nour work identifies this challenge as how to conduct proactive error handling\nwithout explicit error handling instructions. To promote further research, this\nwork introduces a new benchmark, termed Mis-prompt, consisting of four\nevaluation tasks, an error category taxonomy, and a new evaluation dataset.\nFurthermore, this work analyzes current LLMs' performance on the benchmark, and\nthe experimental results reveal that current LLMs show poor performance on\nproactive error handling, and SFT on error handling instances improves LLMs'\nproactive error handling capabilities. The dataset will be publicly available.", "AI": {"tldr": "This paper addresses the challenge of proactive error handling in large language models (LLMs) without explicit instructions and introduces the Mis-prompt benchmark for evaluation.", "motivation": "Current error handling in LLMs relies on explicit instructions, which are often unavailable in real-world scenarios. This paper seeks to address the gap in proactive error handling.", "method": "The authors introduce a new benchmark called Mis-prompt, which includes four evaluation tasks, an error category taxonomy, and a new evaluation dataset to analyze LLM performance on proactive error handling.", "result": "Experimental results indicate that current LLMs perform poorly in proactive error handling but demonstrate improved capabilities when fine-tuned on error handling instances.", "conclusion": "The paper concludes that there is a need for proactive error handling in LLMs and the Mis-prompt benchmark will facilitate further research in this area.", "key_contributions": ["Introduction of the Mis-prompt benchmark for evaluating proactive error handling in LLMs.", "Development of an error category taxonomy and a new evaluation dataset.", "Analysis of current LLMs' performance, revealing significant gaps in proactive error handling capabilities."], "limitations": "The paper primarily focuses on the evaluation aspect and does not propose a specific new approach or mechanism for proactive error handling itself.", "keywords": ["large language models", "error handling", "benchmark", "proactive error handling", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00065", "pdf": "https://arxiv.org/pdf/2506.00065.pdf", "abs": "https://arxiv.org/abs/2506.00065", "title": "You Prefer This One, I Prefer Yours: Using Reference Words is Harder Than Vocabulary Words for Humans and Multimodal Language Models", "authors": ["Dota Tianai Dong", "Yifan Luo", "Po-Ya Angela Wang", "Asli Ozyurek", "Paula Rubio-Fernandez"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages", "summary": "Multimodal language models (MLMs) increasingly communicate in human-like\nways, yet their ability to use reference words remains largely overlooked\ndespite their ubiquity in everyday communication. Our study addresses this gap\nby comparing human and MLM use of three word classes with increasing cognitive\ndemands: vocabulary words, possessive pronouns (`mine' vs `yours'), and\ndemonstrative pronouns (`this one' vs `that one'). Evaluating seven\nstate-of-the-art MLMs against human participants, we observe a clear difficulty\nhierarchy: while MLMs approach human-level performance on the vocabulary task,\nthey show substantial deficits with possessives and demonstratives. Our\nanalysis reveals these difficulties stem from limitations in perspective-taking\nand spatial reasoning. Although prompt engineering improved model performance\non possessive use, demonstrative use remained well below human-level\ncompetence. These findings provide theoretical and empirical evidence that\nproducing grammatical forms requiring pragmatics and social cognition remains a\nclear challenge in current NLP systems.", "AI": {"tldr": "This study compares human and MLM performance on reference word usage, revealing gaps in MLMs' ability to use possessive and demonstrative pronouns.", "motivation": "To investigate the largely overlooked ability of multimodal language models (MLMs) to use reference words in comparison to human communication.", "method": "The study evaluates seven state-of-the-art MLMs against human participants using three word classes (vocabulary words, possessive pronouns, demonstrative pronouns) with varying cognitive demands.", "result": "MLMs performed well on vocabulary tasks, approaching human-level competence, but struggled significantly with possessive and demonstrative pronouns, revealing deficits in perspective-taking and spatial reasoning.", "conclusion": "Producing grammatical forms requiring pragmatics and social cognition presents a clear challenge for current NLP systems, despite improvements through prompt engineering.", "key_contributions": ["Identification of gaps in MLMs' use of reference words", "Analysis of difficulties stemming from perspective-taking and spatial reasoning", "Empirical evaluation of MLMs against human performance across different cognitive tasks"], "limitations": "", "keywords": ["Multimodal language models", "pronouns", "NLP", "cognition", "human communication"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2506.00068", "pdf": "https://arxiv.org/pdf/2506.00068.pdf", "abs": "https://arxiv.org/abs/2506.00068", "title": "Probing Politico-Economic Bias in Multilingual Large Language Models: A Cultural Analysis of Low-Resource Pakistani Languages", "authors": ["Afrozah Nadeem", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are increasingly shaping public discourse, yet\ntheir politico-economic biases remain underexamined in non-Western and\nlow-resource multilingual contexts. This paper presents a systematic analysis\nof political bias in 13 state-of-the-art LLMs across five low-resource\nlanguages spoken in Pakistan: Urdu, Punjabi, Sindhi, Balochi, and Pashto. We\npropose a novel framework that integrates an adapted Political Compass Test\n(PCT) with a multi-level framing analysis. Our method combines quantitative\nassessment of political orientation across economic (left-right) and social\n(libertarian-authoritarian) axes with qualitative analysis of framing through\ncontent, style, and emphasis. We further contextualize this analysis by\naligning prompts with 11 key socio-political themes relevant to Pakistani\nsociety. Our results reveal that LLMs predominantly align with liberal-left\nvalues, echoing Western training data influences, but exhibit notable shifts\ntoward authoritarian framing in regional languages, suggesting strong cultural\nmodulation effects. We also identify consistent model-specific bias signatures\nand language-conditioned variations in ideological expression. These findings\nshow the urgent need for culturally grounded, multilingual bias auditing\nframeworks.", "AI": {"tldr": "This paper analyzes political bias in LLMs across low-resource languages in Pakistan, proposing a framework that combines quantitative and qualitative methods to understand biases.", "motivation": "To address the underexamined political biases of LLMs in non-Western and low-resource multilingual contexts, particularly focusing on their influence in Pakistan.", "method": "A novel framework that integrates an adapted Political Compass Test with multi-level framing analysis, combining quantitative assessments of political orientation with qualitative content analysis.", "result": "The analysis shows that LLMs predominantly align with liberal-left values influenced by Western data, but display authoritarian framing variations in regional languages, revealing cultural modulation effects.", "conclusion": "There is an urgent need for culturally grounded, multilingual bias auditing frameworks due to the findings of model-specific bias signatures and language-conditioned ideological expressions.", "key_contributions": ["Introduced a novel framework for analyzing political bias in LLMs in low-resource languages.", "Revealed shifts toward authoritarian framing in regional languages compared to Western influences.", "Identified consistent model-specific bias signatures."], "limitations": "", "keywords": ["Language Models", "Political Bias", "Multilingual Analysis", "Cultural Modulation", "Bias Auditing"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.00069", "pdf": "https://arxiv.org/pdf/2506.00069.pdf", "abs": "https://arxiv.org/abs/2506.00069", "title": "Evaluating the Sensitivity of LLMs to Prior Context", "authors": ["Robert Hankache", "Kingsley Nketia Acheampong", "Liang Song", "Marek Brynda", "Raad Khraishi", "Greig A. Cowan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in multi-turn\ndialogue and other sustained interactive scenarios, it is essential to\nunderstand how extended context affects their performance. Popular benchmarks,\nfocusing primarily on single-turn question answering (QA) tasks, fail to\ncapture the effects of multi-turn exchanges. To address this gap, we introduce\na novel set of benchmarks that systematically vary the volume and nature of\nprior context. We evaluate multiple conventional LLMs, including GPT, Claude,\nand Gemini, across these benchmarks to measure their sensitivity to contextual\nvariations. Our findings reveal that LLM performance on multiple-choice\nquestions can degrade dramatically in multi-turn interactions, with performance\ndrops as large as 73% for certain models. Even highly capable models such as\nGPT-4o exhibit up to a 32% decrease in accuracy. Notably, the relative\nperformance of larger versus smaller models is not always predictable.\nMoreover, the strategic placement of the task description within the context\ncan substantially mitigate performance drops, improving the accuracy by as much\nas a factor of 3.5. These findings underscore the need for robust strategies to\ndesign, evaluate, and mitigate context-related sensitivity in LLMs.", "AI": {"tldr": "This paper introduces a novel set of benchmarks for evaluating large language models (LLMs) in multi-turn dialogue scenarios, revealing significant performance drops due to contextual variations.", "motivation": "There is a gap in current benchmarks that primarily focus on single-turn question answering, necessitating an evaluation of LLMs in multi-turn contexts.", "method": "The authors systematically vary the volume and nature of prior context in their benchmarks and evaluate multiple conventional LLMs on these tasks.", "result": "Performance drops were observed, with some models experiencing up to a 73% decrease in multi-turn interactions. Even advanced models like GPT-4o showed a 32% accuracy decline. Strategic task description placement improved accuracy significantly.", "conclusion": "Robust strategies are essential for designing and evaluating LLMs to address context-related sensitivities that impact performance.", "key_contributions": ["Introduction of novel benchmarks for multi-turn dialogue evaluation", "Measurement of LLM performance sensitivity to contextual variations", "Insights on the strategic placement of task descriptions to mitigate performance drops"], "limitations": "The study focuses primarily on traditional LLMs and may not capture the full spectrum of newer models.", "keywords": ["large language models", "multi-turn dialogue", "context sensitivity", "performance evaluation", "task description"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2506.00077", "pdf": "https://arxiv.org/pdf/2506.00077.pdf", "abs": "https://arxiv.org/abs/2506.00077", "title": "Gaussian mixture models as a proxy for interacting language models", "authors": ["Edward Wang", "Tianyu Wang", "Avanti Athreya", "Vince Lyzinski", "Carey E. Priebe"], "categories": ["cs.CL", "cs.LG", "stat.ML", "62R07"], "comment": null, "summary": "Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.", "AI": {"tldr": "This paper presents interacting Gaussian mixture models (GMMs) as an alternative to large language models (LLMs) for studying human behavior in social sciences, demonstrating their potential effectiveness and computational advantages.", "motivation": "The need for alternative models to LLMs in social science experiments where large-scale experiments are infeasible, while also addressing the computational expense of LLMs.", "method": "Introduced interacting Gaussian mixture models (GMMs) and compared their performance against experimental simulations of LLMs that rely on feedback from other LLMs.", "result": "Interacting GMMs successfully captured significant dynamics of interacting LLMs, highlighting similarities and differences in behavior between the two models.", "conclusion": "GMMs offer computational advantages and insights into human behavior modeling, with discussions on potential modifications and future research avenues.", "key_contributions": ["Introduction of interacting GMMs as a feasible alternative to LLMs for studying human interactions.", "Demonstration of the effectiveness of GMMs in capturing dynamics relevant to social sciences.", "Identification of key differences and similarities between GMMs and LLMs."], "limitations": "Computational constraints of GMMs in larger-scale applications need to be addressed in future research.", "keywords": ["Gaussian mixture models", "large language models", "human behavior", "social sciences", "interacting systems"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00085", "pdf": "https://arxiv.org/pdf/2506.00085.pdf", "abs": "https://arxiv.org/abs/2506.00085", "title": "COSMIC: Generalized Refusal Direction Identification in LLM Activations", "authors": ["Vincent Siu", "Nicholas Crispino", "Zihao Yu", "Sam Pan", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) encode behaviors such as refusal within their\nactivation space, yet identifying these behaviors remains a significant\nchallenge. Existing methods often rely on predefined refusal templates\ndetectable in output tokens or require manual analysis. We introduce\n\\textbf{COSMIC} (Cosine Similarity Metrics for Inversion of Concepts), an\nautomated framework for direction selection that identifies viable steering\ndirections and target layers using cosine similarity - entirely independent of\nmodel outputs. COSMIC achieves steering performance comparable to prior methods\nwithout requiring assumptions about a model's refusal behavior, such as the\npresence of specific refusal tokens. It reliably identifies refusal directions\nin adversarial settings and weakly aligned models, and is capable of steering\nsuch models toward safer behavior with minimal increase in false refusals,\ndemonstrating robustness across a wide range of alignment conditions.", "AI": {"tldr": "COSMIC is an automated framework for identifying refusal behaviors in Large Language Models (LLMs) using cosine similarity without relying on model outputs or predefined templates.", "motivation": "Identifying refusal behaviors in LLMs is challenging, with existing methods relying on predefined templates or manual analysis. A more automated and robust approach is needed.", "method": "COSMIC uses cosine similarity metrics to select steering directions and target layers for identifying refusal behaviors, independent of existing model output assumptions.", "result": "COSMIC matches the steering performance of prior methods and effectively identifies refusal behaviors in both adversarial settings and weakly aligned models with minimal false refusals.", "conclusion": "COSMIC demonstrates robustness across various alignment conditions, providing a more reliable method for steering LLMs towards safer behaviors.", "key_contributions": ["Introduction of the COSMIC framework for refusal behavior detection in LLMs", "Elimination of the need for predefined refusal templates or specific token assumptions", "Demonstration of effectiveness in adversarial and weakly aligned model scenarios"], "limitations": "", "keywords": ["Large Language Models", "refusal behavior", "cosine similarity", "alignment", "safety"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00087", "pdf": "https://arxiv.org/pdf/2506.00087.pdf", "abs": "https://arxiv.org/abs/2506.00087", "title": "SwitchLingua: The First Large-Scale Multilingual and Multi-Ethnic Code-Switching Dataset", "authors": ["Peng Xie", "Xingyuan Liu", "Tsz Wai Chan", "Yequan Bie", "Yangqiu Song", "Yang Wang", "Hao Chen", "Kani Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Code-switching (CS) is the alternating use of two or more languages within a\nconversation or utterance, often influenced by social context and speaker\nidentity. This linguistic phenomenon poses challenges for Automatic Speech\nRecognition (ASR) systems, which are typically designed for a single language\nand struggle to handle multilingual inputs. The growing global demand for\nmultilingual applications, including Code-Switching ASR (CSASR), Text-to-Speech\n(CSTTS), and Cross-Lingual Information Retrieval (CLIR), highlights the\ninadequacy of existing monolingual datasets.\n  Although some code-switching datasets exist, most are limited to bilingual\nmixing within homogeneous ethnic groups, leaving a critical need for a\nlarge-scale, diverse benchmark akin to ImageNet in computer vision.\n  To bridge this gap, we introduce \\textbf{LinguaMaster}, a multi-agent\ncollaboration framework specifically designed for efficient and scalable\nmultilingual data synthesis. Leveraging this framework, we curate\n\\textbf{SwitchLingua}, the first large-scale multilingual and multi-ethnic\ncode-switching dataset, including: (1) 420K CS textual samples across 12\nlanguages, and (2) over 80 hours of audio recordings from 174 speakers\nrepresenting 18 countries/regions and 63 racial/ethnic backgrounds, based on\nthe textual data. This dataset captures rich linguistic and cultural diversity,\noffering a foundational resource for advancing multilingual and multicultural\nresearch. Furthermore, to address the issue that existing ASR evaluation\nmetrics lack sensitivity to code-switching scenarios, we propose the\n\\textbf{Semantic-Aware Error Rate (SAER)}, a novel evaluation metric that\nincorporates semantic information, providing a more accurate and context-aware\nassessment of system performance.", "AI": {"tldr": "Introduction of LinguaMaster and SwitchLingua, a large-scale multilingual code-switching dataset and a novel evaluation metric for ASR systems.", "motivation": "The growing global demand for multilingual applications requires addressing the inadequacies of existing monolingual datasets for ASR systems, specifically in handling code-switching.", "method": "The paper introduces LinguaMaster, a multi-agent collaboration framework for multilingual data synthesis, leading to the creation of SwitchLingua, a large-scale dataset with diverse linguistic and ethnic representation.", "result": "SwitchLingua includes 420K CS textual samples and over 80 hours of audio data from 174 speakers, enhancing resources for multilingual and multicultural research.", "conclusion": "This work provides a foundational resource for future advancement in code-switching ASR and proposes SAER, an evaluation metric that improves performance assessment in code-switching scenarios.", "key_contributions": ["Introduction of LinguaMaster for data synthesis", "Creation of SwitchLingua dataset with 420K CS samples", "Proposal of Semantic-Aware Error Rate (SAER) for ASR evaluation"], "limitations": "", "keywords": ["Code-switching", "Automatic Speech Recognition", "Multilingual dataset", "Semantic-Aware Error Rate", "Linguistic diversity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00195", "pdf": "https://arxiv.org/pdf/2506.00195.pdf", "abs": "https://arxiv.org/abs/2506.00195", "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences", "authors": ["Mingqian Zheng", "Wenjia Hu", "Patrick Zhao", "Motahhare Eslami", "Jena D. Hwang", "Faeze Brahman", "Carolyn Rose", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Current LLMs are trained to refuse potentially harmful input queries\nregardless of whether users actually had harmful intents, causing a tradeoff\nbetween safety and user experience. Through a study of 480 participants\nevaluating 3,840 query-response pairs, we examine how different refusal\nstrategies affect user perceptions across varying motivations. Our findings\nreveal that response strategy largely shapes user experience, while actual user\nmotivation has negligible impact. Partial compliance -- providing general\ninformation without actionable details -- emerges as the optimal strategy,\nreducing negative user perceptions by over 50% to flat-out refusals.\nComplementing this, we analyze response patterns of 9 state-of-the-art LLMs and\nevaluate how 6 reward models score different refusal strategies, demonstrating\nthat models rarely deploy partial compliance naturally and reward models\ncurrently undervalue it. This work demonstrates that effective guardrails\nrequire focusing on crafting thoughtful refusals rather than detecting intent,\noffering a path toward AI safety mechanisms that ensure both safety and\nsustained user engagement.", "AI": {"tldr": "This paper examines how refusal strategies in LLMs affect user experience, finding that partial compliance is the most effective strategy.", "motivation": "The study addresses the tradeoff between safety and user experience in LLM responses to potentially harmful queries.", "method": "Conducted a study with 480 participants evaluating 3,840 query-response pairs to assess the impact of refusal strategies.", "result": "Partial compliance reduces negative user perceptions by over 50% compared to flat-out refusals; state-of-the-art LLMs seldom use this strategy and reward models undervalue it.", "conclusion": "Effective AI safety mechanisms should focus on crafting thoughtful refusals rather than solely detecting user intent.", "key_contributions": ["Identified the impact of refusal strategies on user perception", "Demonstrated that partial compliance is the optimal response strategy", "Revealed shortcomings in the deployment of refusal strategies by LLMs and their scoring by reward models"], "limitations": "", "keywords": ["LLM", "refusal strategies", "user experience", "AI safety", "partial compliance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00088", "pdf": "https://arxiv.org/pdf/2506.00088.pdf", "abs": "https://arxiv.org/abs/2506.00088", "title": "HD-NDEs: Neural Differential Equations for Hallucination Detection in LLMs", "authors": ["Qing Li", "Jiahui Geng", "Zongxiong Chen", "Derui Zhu", "Yuxia Wang", "Congbo Ma", "Chenyang Lyu", "Fakhri Karray"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In recent years, large language models (LLMs) have made remarkable\nadvancements, yet hallucination, where models produce inaccurate or non-factual\nstatements, remains a significant challenge for real-world deployment. Although\ncurrent classification-based methods, such as SAPLMA, are highly efficient in\nmitigating hallucinations, they struggle when non-factual information arises in\nthe early or mid-sequence of outputs, reducing their reliability. To address\nthese issues, we propose Hallucination Detection-Neural Differential Equations\n(HD-NDEs), a novel method that systematically assesses the truthfulness of\nstatements by capturing the full dynamics of LLMs within their latent space.\nOur approaches apply neural differential equations (Neural DEs) to model the\ndynamic system in the latent space of LLMs. Then, the sequence in the latent\nspace is mapped to the classification space for truth assessment. The extensive\nexperiments across five datasets and six widely used LLMs demonstrate the\neffectiveness of HD-NDEs, especially, achieving over 14% improvement in AUC-ROC\non the True-False dataset compared to state-of-the-art techniques.", "AI": {"tldr": "The paper presents a novel method, Hallucination Detection-Neural Differential Equations (HD-NDEs), to effectively detect hallucinations in large language models (LLMs) by capturing their latent space dynamics.", "motivation": "To address the significant challenge of hallucination in LLMs, particularly when non-factual information appears early in outputs.", "method": "The approach utilizes neural differential equations to model the dynamics in the latent space of LLMs, mapping sequences into the classification space for truth assessment.", "result": "Extensive experiments showed that HD-NDEs improve AUC-ROC scores by over 14% on the True-False dataset compared to existing methods.", "conclusion": "HD-NDEs significantly enhance the reliability of LLM outputs by systematically assessing statement truthfulness, addressing limitations of previous techniques.", "key_contributions": ["Introduction of Hallucination Detection-Neural Differential Equations (HD-NDEs) as a new method for assessing truthfulness in LLMs", "Demonstration of improved performance on multiple datasets compared to state-of-the-art techniques", "Innovative use of neural differential equations to model dynamics in LLM latent spaces"], "limitations": "", "keywords": ["Hallucination Detection", "Neural Differential Equations", "Large Language Models", "Truthfulness Assessment", "Machine Learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00103", "pdf": "https://arxiv.org/pdf/2506.00103.pdf", "abs": "https://arxiv.org/abs/2506.00103", "title": "Writing-Zero: Bridge the Gap Between Non-verifiable Problems and Verifiable Rewards", "authors": ["Xun Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has enabled large\nlanguage models (LLMs) to achieve remarkable breakthroughs in reasoning tasks\nwith objective ground-truth answers, such as mathematics and code generation.\nHowever, a significant gap remains for non-verifiable tasks, like creative\nwriting and open-ended dialogue, where quality assessment is inherently\nsubjective and lacks definitive references. Existing approaches for these\ndomains often rely on scalar reward models trained with human preferences,\nwhich suffer from limited generalization and are prone to reward hacking, such\nas over-explanation and length bias. In this work, we propose a unified\nRLVR-based training paradigm that bridges the gap between non-verifiable tasks\nand verifiable rewards. We introduce a writing-principle-based pairwise\nGenerative Reward Model (GenRM) and a novel Bootstrapped Relative Policy\nOptimization (BRPO) algorithm. The pairwise writing GenRM leverages\nself-principled critique to transform subjective assessments into reliable,\nverifiable rewards, while BRPO enables dynamic, reference-free pairwise\ncomparison by leveraging a bootstrapped response as temporary reference from\nwithin group rollouts during RL training. Our approach empowers LLMs to develop\nrobust writing capabilities without supervised fine-tuning, as demonstrated by\nWriting-Zero, which shows consistent improvement and strong resistance to\nreward hacking compared to scalar reward baselines. Furthermore, our method\nachieves competitive results on both in-house and open-source writing\nbenchmarks. Our findings suggest the potential to unify rule-based,\nreference-based, and reference-free reward modeling under the RLVR framework,\nthus paving the way for a comprehensive and scalable RL training paradigm\napplicable across all language tasks.", "AI": {"tldr": "This paper presents a unified RLVR-based training paradigm to enhance large language models' (LLMs) capabilities in non-verifiable creative writing through a novel pairwise Generative Reward Model and Bootstrapped Relative Policy Optimization algorithm.", "motivation": "To address the challenges of subjective quality assessment in creative writing and open-ended dialogue tasks, which traditional reward models struggle with due to their reliance on limited human preferences.", "method": "A pairwise Generative Reward Model (GenRM) transforms subjective quality assessments into verifiable rewards, while the Bootstrapped Relative Policy Optimization (BRPO) algorithm facilitates dynamic pairwise comparisons using bootstrapped responses during RL training.", "result": "The proposed approach, demonstrated through Writing-Zero, shows significant improvements in writing capabilities and robustness against reward hacking, outperforming scalar reward models on various writing benchmarks.", "conclusion": "The proposed unified RLVR framework offers a comprehensive approach to reward modeling that can enhance LLM performance across a wide range of language tasks, combining rule-based, reference-based, and reference-free methodologies.", "key_contributions": ["Introduced a pairwise Generative Reward Model for verifiable reward generation in subjective tasks.", "Proposed Bootstrapped Relative Policy Optimization for dynamic reinforcement learning without fixed references.", "Demonstrated competitive performance on writing benchmarks compared to traditional reward models."], "limitations": "", "keywords": ["Reinforcement Learning", "Generative Reward Model", "Large Language Models", "Human-Computer Interaction", "Creative Writing"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.00134", "pdf": "https://arxiv.org/pdf/2506.00134.pdf", "abs": "https://arxiv.org/abs/2506.00134", "title": "Spurious Correlations and Beyond: Understanding and Mitigating Shortcut Learning in SDOH Extraction with Large Language Models", "authors": ["Fardin Ahsan Sakib", "Ziwei Zhu", "Karen Trister Grace", "Meliha Yetisgen", "Ozlem Uzuner"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Social determinants of health (SDOH) extraction from clinical text is\ncritical for downstream healthcare analytics. Although large language models\n(LLMs) have shown promise, they may rely on superficial cues leading to\nspurious predictions. Using the MIMIC portion of the SHAC (Social History\nAnnotation Corpus) dataset and focusing on drug status extraction as a case\nstudy, we demonstrate that mentions of alcohol or smoking can falsely induce\nmodels to predict current/past drug use where none is present, while also\nuncovering concerning gender disparities in model performance. We further\nevaluate mitigation strategies - such as prompt engineering and\nchain-of-thought reasoning - to reduce these false positives, providing\ninsights into enhancing LLM reliability in health domains.", "AI": {"tldr": "This paper investigates the extraction of social determinants of health using large language models, revealing potential biases and proposing solutions to improve model reliability.", "motivation": "The importance of accurately extracting social determinants of health from clinical text to enable effective healthcare analytics.", "method": "Analysis of the MIMIC portion of the SHAC dataset focusing on drug status extraction, examining the correlation between alcohol/smoking mentions and false predictions of drug use, while exploring prompt engineering and reasoning strategies as mitigation techniques.", "result": "Findings reveal that LLMs can be misled by superficial cues like mentions of alcohol and smoking, leading to inaccurate predictions about drug use, and identify gender disparities in model performance.", "conclusion": "The study highlights the necessity of enhancing LLM reliability in health domains through specific strategies to mitigate false positives and biases in predictions.", "key_contributions": ["Identification of spurious predictions in LLMs related to drug use based on superficial cues.", "Discovery of gender disparities in model performance for SDOH extraction.", "Evaluation of mitigation strategies like prompt engineering to improve model reliability."], "limitations": "Focus on a specific case study may limit generalizability; other forms of bias not assessed.", "keywords": ["Social Determinants of Health", "LLMs", "Healthcare Analytics", "Bias in AI", "Prompt Engineering"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00386", "pdf": "https://arxiv.org/pdf/2506.00386.pdf", "abs": "https://arxiv.org/abs/2506.00386", "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training", "authors": ["Keyeun Lee", "Seolhee Lee", "Esther Hehsun Kim", "Yena Ko", "Jinsu Eun", "Dahee Kim", "Hyewon Cho", "Haiyi Zhu", "Robert E. Kraut", "Eunyoung Suh", "Eun-mee Kim", "Hajin Lim"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 Findings, 34 pages, 9 figures", "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training.", "AI": {"tldr": "Adaptive-VP is a dialogue generation framework using large language models to enhance virtual patient interactions by adapting to trainee communication skills in real-time, improving nursing communication training.", "motivation": "To improve nursing communication training with scalable, adaptive virtual patient systems that respond effectively to trainee interactions.", "method": "The framework constructs flexible clinical scenarios and uses a modular system to assess and adapt VP behavior based on trainee input during simulations.", "result": "Automated evaluations indicated that the communication skill assessment mirrored real-world proficiency, and expert reviews confirmed more natural interactions compared to existing methods.", "conclusion": "Adaptive-VP shows promise as a scalable tool for effective nursing communication training, enhancing virtual patient interactions' realism and adaptability.", "key_contributions": ["Development of a dialogue generation framework for virtual patients using LLMs.", "Real-time adaptation of virtual patient behavior based on trainee responses.", "Automated assessment of communication skills validated by expert evaluation."], "limitations": "The framework's performance may vary depending on the quality and scope of the training scenarios created.", "keywords": ["adaptive virtual patients", "communication training", "large language models", "nursing education", "real-time interaction"], "importance_score": 9, "read_time_minutes": 34}}
{"id": "2506.00137", "pdf": "https://arxiv.org/pdf/2506.00137.pdf", "abs": "https://arxiv.org/abs/2506.00137", "title": "LaMP-QA: A Benchmark for Personalized Long-form Question Answering", "authors": ["Alireza Salemi", "Hamed Zamani"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Personalization is essential for question answering systems that are\nuser-centric. Despite its importance, personalization in answer generation has\nbeen relatively underexplored. This is mainly due to lack of resources for\ntraining and evaluating personalized question answering systems. We address\nthis gap by introducing LaMP-QA -- a benchmark designed for evaluating\npersonalized long-form answer generation. The benchmark covers questions from\nthree major categories: (1) Arts & Entertainment, (2) Lifestyle & Personal\nDevelopment, and (3) Society & Culture, encompassing over 45 subcategories in\ntotal. To assess the quality and potential impact of the LaMP-QA benchmark for\npersonalized question answering, we conduct comprehensive human and automatic\nevaluations, to compare multiple evaluation strategies for evaluating generated\npersonalized responses and measure their alignment with human preferences.\nFurthermore, we benchmark a number of non-personalized and personalized\napproaches based on open-source and proprietary large language models (LLMs).\nOur results show that incorporating the personalized context provided leads to\nperformance improvements of up to 39%. The benchmark is publicly released to\nsupport future research in this area.", "AI": {"tldr": "LaMP-QA is a benchmark for evaluating personalized long-form answer generation in question answering systems.", "motivation": "To address the lack of resources for training and evaluating personalized question answering systems, which are essential for user-centric applications.", "method": "The paper introduces LaMP-QA, a benchmark covering diverse topics, and conducts human and automatic evaluations to assess various strategies for personalized answer generation.", "result": "The incorporation of personalized context in answers led to performance improvements of up to 39%.", "conclusion": "LaMP-QA will support future research in personalized question answering by providing a publicly available benchmark.", "key_contributions": ["Introduction of LaMP-QA benchmark for personalized question answering", "Comprehensive evaluation of multiple personalized response generation methods", "Demonstration of significant performance improvements with personalized context"], "limitations": "", "keywords": ["personalization", "question answering", "benchmark", "large language models", "evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00145", "pdf": "https://arxiv.org/pdf/2506.00145.pdf", "abs": "https://arxiv.org/abs/2506.00145", "title": "Vedavani: A Benchmark Corpus for ASR on Vedic Sanskrit Poetry", "authors": ["Sujeet Kumar", "Pretam Ray", "Abhinay Beerukuri", "Shrey Kamoji", "Manoj Balaji Jagadeeshan", "Pawan Goyal"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Sanskrit, an ancient language with a rich linguistic heritage, presents\nunique challenges for automatic speech recognition (ASR) due to its phonemic\ncomplexity and the phonetic transformations that occur at word junctures,\nsimilar to the connected speech found in natural conversations. Due to these\ncomplexities, there has been limited exploration of ASR in Sanskrit,\nparticularly in the context of its poetic verses, which are characterized by\nintricate prosodic and rhythmic patterns. This gap in research raises the\nquestion: How can we develop an effective ASR system for Sanskrit, particularly\none that captures the nuanced features of its poetic form? In this study, we\nintroduce Vedavani, the first comprehensive ASR study focused on Sanskrit Vedic\npoetry. We present a 54-hour Sanskrit ASR dataset, consisting of 30,779\nlabelled audio samples from the Rig Veda and Atharva Veda. This dataset\ncaptures the precise prosodic and rhythmic features that define the language.\nWe also benchmark the dataset on various state-of-the-art multilingual speech\nmodels.$^{1}$ Experimentation revealed that IndicWhisper performed the best\namong the SOTA models.", "AI": {"tldr": "This paper introduces Vedavani, the first comprehensive automatic speech recognition (ASR) study for Sanskrit Vedic poetry, presenting a 54-hour annotated dataset and benchmarking it against state-of-the-art multilingual speech models.", "motivation": "The complexity of Sanskrit, especially its poetic forms, poses unique challenges for automatic speech recognition, leading to limited research in this area.", "method": "A 54-hour ASR dataset containing 30,779 labelled audio samples from the Rig Veda and Atharva Veda was created, focusing on capturing the prosodic and rhythmic features of Sanskrit poetry and benchmarked on various multilingual speech models.", "result": "The benchmarking of the dataset revealed that the IndicWhisper model outperformed other state-of-the-art models in recognizing Sanskrit poetry.", "conclusion": "Creating a dedicated ASR system for Sanskrit, particularly for its poetic forms, is feasible, and the Vedavani dataset serves as a significant resource for further research in this area.", "key_contributions": ["Development of the first ASR system specifically for Sanskrit Vedic poetry.", "Introduction of a unique dataset with 54 hours of audio to improve ASR for an ancient language.", "Benchmarking against state-of-the-art models to highlight performance gains."], "limitations": "", "keywords": ["Sanskrit", "Automatic Speech Recognition", "Vedic Poetry", "Dataset", "IndicWhisper"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.00583", "pdf": "https://arxiv.org/pdf/2506.00583.pdf", "abs": "https://arxiv.org/abs/2506.00583", "title": "The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation", "authors": ["Yuhang Zhou", "Yimin Xiao", "Wei Ai", "Ge Gao"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 3 figures", "summary": "Social media platforms have become central to modern communication, yet they\nalso harbor offensive content that challenges platform safety and inclusivity.\nWhile prior research has primarily focused on textual indicators of offense,\nthe role of emojis, ubiquitous visual elements in online discourse, remains\nunderexplored. Emojis, despite being rarely offensive in isolation, can acquire\nharmful meanings through symbolic associations, sarcasm, and contextual misuse.\nIn this work, we systematically examine emoji contributions to offensive\nTwitter messages, analyzing their distribution across offense categories and\nhow users exploit emoji ambiguity. To address this, we propose an LLM-powered,\nmulti-step moderation pipeline that selectively replaces harmful emojis while\npreserving the tweet's semantic intent. Human evaluations confirm our approach\neffectively reduces perceived offensiveness without sacrificing meaning. Our\nanalysis also reveals heterogeneous effects across offense types, offering\nnuanced insights for online communication and emoji moderation.", "AI": {"tldr": "This paper explores the role of emojis in offensive Twitter messages and proposes an LLM-based moderation system to mitigate their harmful effects while maintaining semantic intent.", "motivation": "To address the gap in understanding how emojis contribute to the offensiveness of social media content, especially on platforms like Twitter where they are widely used.", "method": "The study systematically examines the distribution of emojis in offensive tweets, analyzes their contextual meanings, and implements a multi-step LLM-powered moderation pipeline to replace harmful emojis.", "result": "Human evaluations demonstrate that the proposed approach effectively reduces perceived offensiveness of tweets without losing their intended meanings, while revealing heterogeneous impacts across different types of offensive content.", "conclusion": "The findings provide insights into emoji use in online communication and contribute to developing strategies for effective moderation on social media.", "key_contributions": ["Systematic analysis of emojis in offensive communication on Twitter", "Development of an LLM-powered moderation pipeline for emoji replacement", "Insights into the contextual misuse of emojis and their varied impact across offense types"], "limitations": "", "keywords": ["emojis", "offensive content", "Twitter", "moderation", "human-computer interaction"], "importance_score": 7, "read_time_minutes": 18}}
{"id": "2506.00160", "pdf": "https://arxiv.org/pdf/2506.00160.pdf", "abs": "https://arxiv.org/abs/2506.00160", "title": "Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement", "authors": ["Qihui Fan", "Enfu Nan", "Wenbo Li", "Lei Lu", "Pu Zhao", "Yanzhi Wang"], "categories": ["cs.CL"], "comment": null, "summary": "The growing popularity of social deduction game systems for both business\napplications and AI research has greatly benefited from the rapid advancements\nin Large Language Models (LLMs), which now demonstrate stronger reasoning and\npersuasion capabilities. Especially with the raise of DeepSeek R1 and V3\nmodels, LLMs should enable a more engaging experience for human players in\nLLM-agent-based social deduction games like Werewolf. Previous works either\nfine-tuning, advanced prompting engineering, or additional experience pool to\nachieve engaging text-format Werewolf game experience. We propose a novel yet\nstraightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)\nmodels designed for enhanced compatibility with various LLM models, and\nimproved user engagement. We argue with ever enhancing LLM reasoning, extra\ncomponents will be unnecessary in the case of Werewolf.", "AI": {"tldr": "This paper presents a novel LLM-based Werewolf game system that utilizes tuned TTS models for better engagement without the need for additional components, leveraging the improved reasoning abilities of LLMs.", "motivation": "The paper is motivated by the advancements in LLMs and their application in creating engaging social deduction games, specifically to enhance user interaction without complexity.", "method": "The authors propose an LLM-based Werewolf game system that employs tuned Text-to-Speech models aimed at improving compatibility and user engagement, simplifying the game design.", "result": "The proposed system shows that with the advancements in LLM reasoning capabilities, the traditional need for complex additional systems is reduced, fostering a more engaging gameplay experience.", "conclusion": "The findings suggest that a straightforward implementation using LLMs can lead to improved player engagement in social deduction games without the necessity of complex designs or additional tools.", "key_contributions": ["Development of an LLM-based Werewolf game system", "Integration of tuned TTS models for better engagement", "Reduction of complexity in game design by leveraging LLM capabilities."], "limitations": "", "keywords": ["Large Language Models", "social deduction games", "Text-to-Speech", "user engagement", "Werewolf"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00195", "pdf": "https://arxiv.org/pdf/2506.00195.pdf", "abs": "https://arxiv.org/abs/2506.00195", "title": "Let Them Down Easy! Contextual Effects of LLM Guardrails on User Perceptions and Preferences", "authors": ["Mingqian Zheng", "Wenjia Hu", "Patrick Zhao", "Motahhare Eslami", "Jena D. Hwang", "Faeze Brahman", "Carolyn Rose", "Maarten Sap"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Current LLMs are trained to refuse potentially harmful input queries\nregardless of whether users actually had harmful intents, causing a tradeoff\nbetween safety and user experience. Through a study of 480 participants\nevaluating 3,840 query-response pairs, we examine how different refusal\nstrategies affect user perceptions across varying motivations. Our findings\nreveal that response strategy largely shapes user experience, while actual user\nmotivation has negligible impact. Partial compliance -- providing general\ninformation without actionable details -- emerges as the optimal strategy,\nreducing negative user perceptions by over 50% to flat-out refusals.\nComplementing this, we analyze response patterns of 9 state-of-the-art LLMs and\nevaluate how 6 reward models score different refusal strategies, demonstrating\nthat models rarely deploy partial compliance naturally and reward models\ncurrently undervalue it. This work demonstrates that effective guardrails\nrequire focusing on crafting thoughtful refusals rather than detecting intent,\noffering a path toward AI safety mechanisms that ensure both safety and\nsustained user engagement.", "AI": {"tldr": "This paper explores how different refusal strategies in LLMs affect user perceptions and engagement, revealing that partial compliance is the most effective strategy.", "motivation": "To address the tradeoff between LLM safety and user experience when refusing harmful queries.", "method": "A study involving 480 participants evaluated 3,840 query-response pairs to analyze the effects of different refusal strategies on user perceptions.", "result": "The study found that response strategy significantly impacts user experience, with partial compliance reducing negative perceptions by over 50% compared to outright refusals.", "conclusion": "Effective AI safety mechanisms should prioritize thoughtful refusals over intent detection to enhance user engagement while ensuring safety.", "key_contributions": ["Identified the impact of refusal strategies on user perceptions in LLMs", "Demonstrated that partial compliance is the optimal refusal strategy", "Provided insights into the shortcomings of current reward models in evaluating LLM responses"], "limitations": "The study may not encompass all types of harmful queries or user motivations outside the sample.", "keywords": ["LLM", "refusal strategies", "user experience", "AI safety", "partial compliance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00200", "pdf": "https://arxiv.org/pdf/2506.00200.pdf", "abs": "https://arxiv.org/abs/2506.00200", "title": "Structuring Radiology Reports: Challenging LLMs with Lightweight Models", "authors": ["Johannes Moll", "Louisa Fay", "Asfandyar Azhar", "Sophie Ostmeier", "Tim Lueth", "Sergios Gatidis", "Curtis Langlotz", "Jean-Benoit Delbrouck"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Radiology reports are critical for clinical decision-making but often lack a\nstandardized format, limiting both human interpretability and machine learning\n(ML) applications. While large language models (LLMs) have shown strong\ncapabilities in reformatting clinical text, their high computational\nrequirements, lack of transparency, and data privacy concerns hinder practical\ndeployment. To address these challenges, we explore lightweight encoder-decoder\nmodels (<300M parameters)-specifically T5 and BERT2BERT-for structuring\nradiology reports from the MIMIC-CXR and CheXpert Plus datasets. We benchmark\nthese models against eight open-source LLMs (1B-70B), adapted using prefix\nprompting, in-context learning (ICL), and low-rank adaptation (LoRA)\nfinetuning. Our best-performing lightweight model outperforms all LLMs adapted\nusing prompt-based techniques on a human-annotated test set. While some\nLoRA-finetuned LLMs achieve modest gains over the lightweight model on the\nFindings section (BLEU 6.4%, ROUGE-L 4.8%, BERTScore 3.6%, F1-RadGraph 1.1%,\nGREEN 3.6%, and F1-SRR-BERT 4.3%), these improvements come at the cost of\nsubstantially greater computational resources. For example, LLaMA-3-70B\nincurred more than 400 times the inference time, cost, and carbon emissions\ncompared to the lightweight model. These results underscore the potential of\nlightweight, task-specific models as sustainable and privacy-preserving\nsolutions for structuring clinical text in resource-constrained healthcare\nsettings.", "AI": {"tldr": "This paper evaluates lightweight models for structuring radiology reports, showing they outperform larger LLMs while being more resource-efficient.", "motivation": "To improve the standardization of radiology reports for better clinical decision-making and overcome the limitations of large language models in terms of computational requirements and data privacy.", "method": "The study benchmarks lightweight encoder-decoder models (T5 and BERT2BERT) against eight open-source LLMs (1B-70B) using prefix prompting, in-context learning, and low-rank adaptation for structuring radiology reports.", "result": "The best-performing lightweight model outperforms all adapted LLMs in human-annotated testing, though some adapted models achieve modest gains with much higher resource requirements.", "conclusion": "Lightweight, task-specific models provide a sustainable and privacy-preserving approach to structuring clinical text, especially in resource-constrained healthcare environments.", "key_contributions": ["Demonstrated the efficacy of lightweight models for clinical text structuring.", "Highlighted sustainability in model deployment compared to large LLMs.", "Provided a comprehensive benchmark of lightweight and large models in radiology report structuring."], "limitations": "The study's findings may not generalize to non-radiology clinical texts or other languages as it primarily focused on radiology reports.", "keywords": ["radiology reports", "machine learning", "lightweight models", "clinical decision-making", "health informatics"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00204", "pdf": "https://arxiv.org/pdf/2506.00204.pdf", "abs": "https://arxiv.org/abs/2506.00204", "title": "Structure-Aware Fill-in-the-Middle Pretraining for Code", "authors": ["Linyuan Gong", "Alvin Cheung", "Mostafa Elhoushi", "Sida Wang"], "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "14 pages", "summary": "Fill-in-the-Middle (FIM) is a common pretraining method for code LLMs, where\nmodels complete code segments given surrounding context. However, existing LLMs\ntreat code as plain text and mask random character spans. We propose and\nevaluate AST-FIM, a pretraining strategy that leverages Abstract Syntax Trees\n(ASTs) to mask complete syntactic structures at scale, ensuring coherent\ntraining examples better aligned with universal code structures and common code\nediting patterns such as blocks, expressions, or functions. To evaluate\nreal-world fill-in-the-middle (FIM) programming tasks, we introduce\nReal-FIM-Eval, a benchmark derived from 30,000+ GitHub commits across 12\nlanguages. On infilling tasks, experiments on 1B and 8B parameter models show\nthat AST-FIM is particularly beneficial for real-world code editing as it\noutperforms standard random-character FIM by up to 5 pts on standard FIM\nbenchmarks. Our code is publicly available at\nhttps://github.com/gonglinyuan/ast_fim.", "AI": {"tldr": "AST-FIM is a new pretraining strategy for code LLMs that leverages Abstract Syntax Trees to enhance code completion tasks by masking complete syntactic structures instead of random character spans.", "motivation": "Existing LLMs mask random character spans, which does not effectively utilize the syntactic structure of code. This can lead to incoherent training examples.", "method": "AST-FIM uses Abstract Syntax Trees to mask complete code structures during the pretraining process, providing a more coherent context for models.", "result": "AST-FIM outperforms traditional random-character FIM methods by up to 5 points on standard benchmarks tailored for fill-in-the-middle programming tasks.", "conclusion": "AST-FIM significantly improves the performance of code LLMs on real-world code editing tasks and is validated through a comprehensive benchmark, Real-FIM-Eval.", "key_contributions": ["Introduction of AST-FIM for pretraining code LLMs", "Development of Real-FIM-Eval benchmark from 30,000+ GitHub commits", "Demonstrated performance increase of AST-FIM over traditional methods"], "limitations": "", "keywords": ["code LLMs", "pretraining", "Abstract Syntax Trees", "fill-in-the-middle", "Real-FIM-Eval"], "importance_score": 7, "read_time_minutes": 14}}
{"id": "2506.00210", "pdf": "https://arxiv.org/pdf/2506.00210.pdf", "abs": "https://arxiv.org/abs/2506.00210", "title": "REIC: RAG-Enhanced Intent Classification at Scale", "authors": ["Ziji Zhang", "Michael Yang", "Zhiyu Chen", "Yingying Zhuang", "Shu-Ting Pi", "Qun Liu", "Rajashekar Maragoud", "Vy Nguyen", "Anurag Beniwal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Accurate intent classification is critical for efficient routing in customer\nservice, ensuring customers are connected with the most suitable agents while\nreducing handling times and operational costs. However, as companies expand\ntheir product lines, intent classification faces scalability challenges due to\nthe increasing number of intents and variations in taxonomy across different\nverticals. In this paper, we introduce REIC, a Retrieval-augmented generation\nEnhanced Intent Classification approach, which addresses these challenges\neffectively. REIC leverages retrieval-augmented generation (RAG) to dynamically\nincorporate relevant knowledge, enabling precise classification without the\nneed for frequent retraining. Through extensive experiments on real-world\ndatasets, we demonstrate that REIC outperforms traditional fine-tuning,\nzero-shot, and few-shot methods in large-scale customer service settings. Our\nresults highlight its effectiveness in both in-domain and out-of-domain\nscenarios, demonstrating its potential for real-world deployment in adaptive\nand large-scale intent classification systems.", "AI": {"tldr": "This paper presents REIC, an enhanced intent classification method using retrieval-augmented generation to improve scalability and accuracy in customer service applications.", "motivation": "The need for efficient customer service routing and the challenges posed by scaling intent classification as product lines expand.", "method": "REIC incorporates retrieval-augmented generation (RAG) to dynamically use relevant knowledge for intent classification, reducing the need for retraining.", "result": "REIC outperforms traditional methods such as fine-tuning, zero-shot, and few-shot approaches in both in-domain and out-of-domain scenarios.", "conclusion": "REIC demonstrates strong performance in intent classification, suitable for adaptive deployment in customer service environments.", "key_contributions": ["Introduction of REIC for intent classification", "Utilization of retrieval-augmented generation for scalability", "Demonstration of superior performance on real-world datasets"], "limitations": "", "keywords": ["intent classification", "customer service", "retrieval-augmented generation", "scalability", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00232", "pdf": "https://arxiv.org/pdf/2506.00232.pdf", "abs": "https://arxiv.org/abs/2506.00232", "title": "ComposeRAG: A Modular and Composable RAG for Corpus-Grounded Multi-Hop Question Answering", "authors": ["Ruofan Wu", "Youngwon Lee", "Fan Shu", "Danmei Xu", "Seung-won Hwang", "Zhewei Yao", "Yuxiong He", "Feng Yan"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems are increasingly diverse, yet\nmany suffer from monolithic designs that tightly couple core functions like\nquery reformulation, retrieval, reasoning, and verification. This limits their\ninterpretability, systematic evaluation, and targeted improvement, especially\nfor complex multi-hop question answering. We introduce ComposeRAG, a novel\nmodular abstraction that decomposes RAG pipelines into atomic, composable\nmodules. Each module, such as Question Decomposition, Query Rewriting,\nRetrieval Decision, and Answer Verification, acts as a parameterized\ntransformation on structured inputs/outputs, allowing independent\nimplementation, upgrade, and analysis. To enhance robustness against errors in\nmulti-step reasoning, ComposeRAG incorporates a self-reflection mechanism that\niteratively revisits and refines earlier steps upon verification failure.\nEvaluated on four challenging multi-hop QA benchmarks, ComposeRAG consistently\noutperforms strong baselines in both accuracy and grounding fidelity.\nSpecifically, it achieves up to a 15% accuracy improvement over\nfine-tuning-based methods and up to a 5% gain over reasoning-specialized\npipelines under identical retrieval conditions. Crucially, ComposeRAG\nsignificantly enhances grounding: its verification-first design reduces\nungrounded answers by over 10% in low-quality retrieval settings, and by\napproximately 3% even with strong corpora. Comprehensive ablation studies\nvalidate the modular architecture, demonstrating distinct and additive\ncontributions from each component. These findings underscore ComposeRAG's\ncapacity to deliver flexible, transparent, scalable, and high-performing\nmulti-hop reasoning with improved grounding and interpretability.", "AI": {"tldr": "ComposeRAG introduces a modular abstraction for Retrieval-Augmented Generation systems, enhancing multi-hop question answering through decomposed functions and a self-reflection mechanism for error handling, demonstrating significant performance improvements.", "motivation": "To overcome limitations in existing RAG systems, which struggle with interpretability and improvement due to tightly coupled designs.", "method": "ComposeRAG decomposes RAG pipelines into atomic modules for functions like query reformulation and answer verification, allowing for independent testing and upgrades, and includes a self-reflection mechanism to refine answers upon error verification.", "result": "ComposeRAG outperforms existing methods by achieving up to a 15% accuracy improvement and significantly enhances grounding, reducing ungrounded answers by over 10% in low-quality settings.", "conclusion": "The modular design of ComposeRAG provides a flexible and scalable approach to multi-hop reasoning, enhancing performance and interpretability.", "key_contributions": ["Introduction of a modular architecture for RAG systems", "Significant improvements in accuracy and grounding fidelity", "Validation through comprehensive ablation studies"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Multi-hop Question Answering", "Modular Architecture", "Self-reflection Mechanism", "Grounding"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.13109", "pdf": "https://arxiv.org/pdf/2409.13109.pdf", "abs": "https://arxiv.org/abs/2409.13109", "title": "Visualizationary: Automating Design Feedback for Visualization Designers using LLMs", "authors": ["Sungbok Shin", "Sanghyun Hong", "Niklas Elmqvist"], "categories": ["cs.HC"], "comment": "Minor Revision Status, IEEE Transactions on Visualization and\n  Computer Graphics", "summary": "Interactive visualization editors empower users to author visualizations\nwithout writing code, but do not provide guidance on the art and craft of\neffective visual communication. In this paper, we explore the potential of\nusing an off-the-shelf large language models (LLMs) to provide actionable and\ncustomized feedback to visualization designers. Our implementation,\nVISUALIZATIONARY, demonstrates how ChatGPT can be used for this purpose through\ntwo key components: a preamble of visualization design guidelines and a suite\nof perceptual filters that extract salient metrics from a visualization image.\nWe present findings from a longitudinal user study involving 13 visualization\ndesigners-6 novices, 4 intermediates, and 3 experts-who authored a new\nvisualization from scratch over several days. Our results indicate that\nproviding guidance in natural language via an LLM can aid even seasoned\ndesigners in refining their visualizations. All our supplemental materials are\navailable at https://osf.io/v7hu8.", "AI": {"tldr": "This paper explores using large language models to provide feedback to visualization designers, demonstrating the implementation of an interactive tool called VISUALIZATIONARY.", "motivation": "To empower visualization designers with actionable, customized feedback that improves visual communication without requiring code.", "method": "The paper introduces an implementation called VISUALIZATIONARY that utilizes ChatGPT to offer design guidelines and perceptual filters to help design visualizations effectively.", "result": "User study results show that both novice and experienced visualization designers benefitted from using the LLM-guided feedback, leading to refined visualizations over the course of several days.", "conclusion": "The study suggests that natural language guidance via LLMs can significantly support visualization designers in enhancing their work.", "key_contributions": ["Demonstration of LLM application in visualization design", "Development of perceptual filters for extracting metrics", "User study validating effectiveness of LLM feedback"], "limitations": "", "keywords": ["Visualization", "Large Language Models", "Human-Computer Interaction", "Design Feedback", "User Study"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00235", "pdf": "https://arxiv.org/pdf/2506.00235.pdf", "abs": "https://arxiv.org/abs/2506.00235", "title": "MedOrch: Medical Diagnosis with Tool-Augmented Reasoning Agents for Flexible Extensibility", "authors": ["Yexiao He", "Ang Li", "Boyi Liu", "Zhewei Yao", "Yuxiong He"], "categories": ["cs.CL"], "comment": null, "summary": "Healthcare decision-making represents one of the most challenging domains for\nArtificial Intelligence (AI), requiring the integration of diverse knowledge\nsources, complex reasoning, and various external analytical tools. Current AI\nsystems often rely on either task-specific models, which offer limited\nadaptability, or general language models without grounding with specialized\nexternal knowledge and tools. We introduce MedOrch, a novel framework that\norchestrates multiple specialized tools and reasoning agents to provide\ncomprehensive medical decision support. MedOrch employs a modular, agent-based\narchitecture that facilitates the flexible integration of domain-specific tools\nwithout altering the core system. Furthermore, it ensures transparent and\ntraceable reasoning processes, enabling clinicians to meticulously verify each\nintermediate step underlying the system's recommendations. We evaluate MedOrch\nacross three distinct medical applications: Alzheimer's disease diagnosis,\nchest X-ray interpretation, and medical visual question answering, using\nauthentic clinical datasets. The results demonstrate MedOrch's competitive\nperformance across these diverse medical tasks. Notably, in Alzheimer's disease\ndiagnosis, MedOrch achieves an accuracy of 93.26%, surpassing the\nstate-of-the-art baseline by over four percentage points. For predicting\nAlzheimer's disease progression, it attains a 50.35% accuracy, marking a\nsignificant improvement. In chest X-ray analysis, MedOrch exhibits superior\nperformance with a Macro AUC of 61.2% and a Macro F1-score of 25.5%. Moreover,\nin complex multimodal visual question answering (Image+Table), MedOrch achieves\nan accuracy of 54.47%. These findings underscore MedOrch's potential to advance\nhealthcare AI by enabling reasoning-driven tool utilization for multimodal\nmedical data processing and supporting intricate cognitive tasks in clinical\ndecision-making.", "AI": {"tldr": "MedOrch is a novel AI framework for healthcare decision-making that integrates specialized tools and reasoning agents, demonstrating promising performance in medical applications like Alzheimer's diagnosis and chest X-ray interpretation.", "motivation": "The paper addresses the limitations of current AI systems in healthcare decision-making, highlighting the need for adaptability and integration of specialized knowledge and reasoning.", "method": "MedOrch employs a modular, agent-based architecture allowing the integration of various domain-specific tools while maintaining traceability and transparency in its reasoning processes.", "result": "MedOrch achieves competitive performance across various medical tasks, with a 93.26% accuracy in Alzheimer's diagnosis and significant results in chest X-ray analysis and multimodal visual question answering.", "conclusion": "The findings suggest that MedOrch enhances healthcare AI capabilities by facilitating reasoning-driven use of specialized tools for complex medical decision-making.", "key_contributions": ["Introduction of MedOrch framework for healthcare AI", "Modular architecture for integrating domain-specific tools", "Demonstrated superior performance in key medical tasks."], "limitations": "", "keywords": ["Healthcare AI", "Medical decision support", "Modular architecture"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2410.03448", "pdf": "https://arxiv.org/pdf/2410.03448.pdf", "abs": "https://arxiv.org/abs/2410.03448", "title": "\"Cold, Calculated, and Condescending\": How AI Identifies and Explains Ableism Compared to Disabled People", "authors": ["Mahika Phutane", "Ananya Seelam", "Aditya Vashistha"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "People with disabilities (PwD) regularly encounter ableist hate and\nmicroaggressions online. These spaces are generally moderated by machine\nlearning models, but little is known about how effectively AI models identify\nableist speech and how well their judgments align with PwD. To investigate\nthis, we curated a first-of-its-kind dataset of 200 social media comments\ntargeted towards PwD, and prompted state-of-the art AI models (i.e., Toxicity\nClassifiers, LLMs) to score toxicity and ableism for each comment, and explain\ntheir reasoning. Then, we recruited 190 participants to similarly rate and\nexplain the harm, and evaluate LLM explanations. Our mixed-methods analysis\nhighlighted a major disconnect: AI underestimated toxicity compared to PwD\nratings, while its ableism assessments were sporadic and varied. Although LLMs\nidentified some biases, its explanations were flawed--they lacked nuance, made\nincorrect assumptions, and appeared judgmental instead of educational. Going\nforward, we discuss challenges and opportunities in designing moderation\nsystems for ableism, and advocate for the involvement of intersectional\ndisabled perspectives in AI.", "AI": {"tldr": "This study examines the effectiveness of AI models in moderating online ableist speech against persons with disabilities (PwD) by comparing AI ratings with human assessments.", "motivation": "To understand how well AI models identify ableist speech and align their judgments with those of persons with disabilities, given the prevalence of ableism online.", "method": "The study involved curating a dataset of 200 social media comments aimed at PwD and comparing scores of toxicity and ableism provided by state-of-the-art AI models to those of 190 participants who also rated and explained their perspectives on the comments.", "result": "AI models underestimated toxicity compared to PwD evaluations and had inconsistent ableism assessments; their explanations lacked nuance and were often flawed.", "conclusion": "The study highlights the need for more nuanced AI moderation systems that incorporate perspectives from disabled individuals to effectively address ableism in online spaces.", "key_contributions": ["First dataset focusing on ableist speech against PwD", "Comparison of AI model performance to human ratings", "Highlighting the need for intersectional input in AI moderation design"], "limitations": "The study's dataset is limited to 200 comments and may not reflect broader online discourse; AI model limitations in understanding context were also noted.", "keywords": ["ableism", "machine learning", "human-computer interaction", "toxicity", "disability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00250", "pdf": "https://arxiv.org/pdf/2506.00250.pdf", "abs": "https://arxiv.org/abs/2506.00250", "title": "PersianMedQA: Language-Centric Evaluation of LLMs in the Persian Medical Domain", "authors": ["Mohammad Javad Ranjbar Kalahroodi", "Amirhossein Sheikholselami", "Sepehr Karimi", "Sepideh Ranjbar Kalahroodi", "Heshaam Faili", "Azadeh Shakery"], "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable performance on a wide\nrange of NLP benchmarks, often surpassing human-level accuracy. However, their\nreliability in high-stakes domains such as medicine, particularly in\nlow-resource languages, remains underexplored. In this work, we introduce\nPersianMedQA, a large-scale, expert-validated dataset of multiple-choice\nPersian medical questions, designed to evaluate LLMs across both Persian and\nEnglish. We benchmark over 40 state-of-the-art models, including\ngeneral-purpose, Persian fine-tuned, and medical LLMs, in zero-shot and\nchain-of-thought (CoT) settings. Our results show that closed-source general\nmodels (e.g., GPT-4.1) consistently outperform all other categories, achieving\n83.3% accuracy in Persian and 80.7% in English, while Persian fine-tuned models\nsuch as Dorna underperform significantly (e.g., 35.9% in Persian), often\nstruggling with both instruction-following and domain reasoning. We also\nanalyze the impact of translation, showing that while English performance is\ngenerally higher, Persian responses are sometimes more accurate due to cultural\nand clinical contextual cues. Finally, we demonstrate that model size alone is\ninsufficient for robust performance without strong domain or language\nadaptation. PersianMedQA provides a foundation for evaluating multilingual and\nculturally grounded medical reasoning in LLMs. The PersianMedQA dataset can be\naccessed at:\nhttps://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA](https://huggingface.co/datasets/MohammadJRanjbar/PersianMedQA", "AI": {"tldr": "Introduction of PersianMedQA, a dataset to evaluate LLMs in medical contexts for Persian and English.", "motivation": "To explore the reliability of LLMs in high-stakes medical domains, particularly for low-resource languages like Persian.", "method": "Benchmarking over 40 state-of-the-art models including general-purpose and language-specific LLMs in various testing settings.", "result": "Closed-source general models performed best, with GPT-4.1 achieving 83.3% accuracy in Persian, while Persian fine-tuned models significantly underperformed.", "conclusion": "Model size is not sufficient for robust performance; domain-specific adaptation is critical. The PersianMedQA dataset offers a means to evaluate multilingual medical reasoning.", "key_contributions": ["Introduced PersianMedQA dataset for medical question answering in Persian and English.", "Benchmarking of over 40 LLMs in zero-shot and chain-of-thought settings.", "Analysis of translation impact on model performance."], "limitations": "Results may vary based on cultural and clinical contextual cues that are not universally applicable.", "keywords": ["Large Language Models", "PersianMedQA", "Medical Questions", "Multilingual Evaluation", "Cultural Context"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.02805", "pdf": "https://arxiv.org/pdf/2502.02805.pdf", "abs": "https://arxiv.org/abs/2502.02805", "title": "Data-driven Causal Discovery for Pedestrians-Autonomous Personal Mobility Vehicle Interactions with eHMIs: From Psychological States to Walking Behaviors", "authors": ["Hailong Liu", "Yang Li", "Toshihiro Hiraoka", "Takahiro Wada"], "categories": ["cs.HC"], "comment": null, "summary": "Autonomous personal mobility vehicle (APMV) is a new type of small smart\nvehicle designed for mixed-traffic environments, including interactions with\npedestrians. To enhance the interaction experience between pedestrians and\nAPMVs and to prevent potential risks, it is crucial to investigate pedestrians'\nwalking behaviors when interacting with APMVs and to understand the\npsychological processes underlying these behaviors. This study aims to\ninvestigate the causal relationships between subjective evaluations of\npedestrians and their walking behaviors during interactions with an APMV\nequipped with an external human-machine interface (eHMI). An experiment of\npedestrian-APMV interaction was conducted with 42 pedestrian participants, in\nwhich various eHMIs on the APMV were designed to induce participants to\nexperience different levels of subjective evaluations and generate the\ncorresponding walking behaviors. Based on the hypothesized model of the\npedestrian's cognition-decision-behavior process, the results of causal\ndiscovery align with the previously proposed model. Furthermore, this study\nfurther analyzes the direct and total causal effects of each factor and\ninvestigates the causal processes affecting several important factors in the\nfield of human-vehicle interaction, such as situation awareness, trust in\nvehicle, risk perception, hesitation in decision making, and walking behaviors.", "AI": {"tldr": "This study investigates the interactions between pedestrians and autonomous personal mobility vehicles (APMVs) through external human-machine interfaces (eHMIs), focusing on pedestrian walking behaviors and psychological processes.", "motivation": "To improve the interaction experience between pedestrians and APMVs, and to prevent potential risks associated with these interactions.", "method": "An experiment was conducted with 42 pedestrian participants, who interacted with APMVs equipped with various eHMIs designed to evoke different subjective evaluations and walking behaviors.", "result": "The study found causal relationships between pedestrians' subjective evaluations (such as situation awareness and trust in the vehicle) and their walking behaviors during APMV interactions, supporting a previously proposed cognitive-behavioral model.", "conclusion": "Understanding the psychological processes and causal relationships can enhance the design of eHMIs to improve safety and interaction between pedestrians and APMVs.", "key_contributions": ["Identification of key psychological factors influencing pedestrian behavior during APMV interactions.", "Demonstration of effective experimental design for studying human-vehicle interactions.", "Establishment of a causal model linking subjective evaluations to pedestrian walking behaviors."], "limitations": "The study is limited by the sample size and the specific context of the experiment, which may affect the generalizability of the findings.", "keywords": ["autonomous vehicles", "human-machine interaction", "pedestrian behavior", "eHMI", "cognitive processes"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.00253", "pdf": "https://arxiv.org/pdf/2506.00253.pdf", "abs": "https://arxiv.org/abs/2506.00253", "title": "Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race", "authors": ["Lihao Sun", "Chengzhi Mao", "Valentin Hofmann", "Xuechunzi Bai"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accpeted to ACL 2025 Main Conferencce", "summary": "Although value-aligned language models (LMs) appear unbiased in explicit bias\nevaluations, they often exhibit stereotypes in implicit word association tasks,\nraising concerns about their fair usage. We investigate the mechanisms behind\nthis discrepancy and find that alignment surprisingly amplifies implicit bias\nin model outputs. Specifically, we show that aligned LMs, unlike their\nunaligned counterparts, overlook racial concepts in early internal\nrepresentations when the context is ambiguous. Not representing race likely\nfails to activate safety guardrails, leading to unintended biases. Inspired by\nthis insight, we propose a new bias mitigation strategy that works by\nincentivizing the representation of racial concepts in the early model layers.\nIn contrast to conventional mitigation methods of machine unlearning, our\ninterventions find that steering the model to be more aware of racial concepts\neffectively mitigates implicit bias. Similar to race blindness in humans,\nignoring racial nuances can inadvertently perpetuate subtle biases in LMs.", "AI": {"tldr": "Aligned language models can inadvertently amplify implicit biases due to their lack of representation of racial concepts, necessitating a new mitigation strategy that incentivizes awareness of these concepts.", "motivation": "To explore why value-aligned language models exhibit implicit bias despite appearing unbiased in explicit evaluations.", "method": "The authors analyze the internal representations of aligned and unaligned language models, particularly focusing on how these models handle racial concepts in ambiguous contexts.", "result": "Aligned language models overlook racial concepts in early representations, failing to activate safety guardrails, which leads to increased implicit bias; new strategies for bias mitigation are proposed that encourage representation of racial concepts.", "conclusion": "Developing interventions that make models more aware of racial concepts can effectively mitigate implicit bias, avoiding the pitfalls of ignorance associated with race blindness.", "key_contributions": ["Discovery that alignment amplifies implicit bias in language models", "Proposed novel bias mitigation strategy focusing on early representations of racial concepts", "Insights into the mechanisms of implicit bias in language models and their internal representation handling."], "limitations": "", "keywords": ["Bias Mitigation", "Language Models", "Implicit Bias", "Racial Concepts", "Fairness"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.16456", "pdf": "https://arxiv.org/pdf/2503.16456.pdf", "abs": "https://arxiv.org/abs/2503.16456", "title": "Position: Beyond Assistance - Reimagining LLMs as Ethical and Adaptive Co-Creators in Mental Health Care", "authors": ["Abeer Badawi", "Md Tahmid Rahman Laskar", "Jimmy Xiangji Huang", "Shaina Raza", "Elham Dolatabadi"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "This position paper argues for a fundamental shift in how Large Language\nModels (LLMs) are integrated into the mental health care domain. We advocate\nfor their role as co-creators rather than mere assistive tools. While LLMs have\nthe potential to enhance accessibility, personalization, and crisis\nintervention, their adoption remains limited due to concerns about bias,\nevaluation, over-reliance, dehumanization, and regulatory uncertainties. To\naddress these challenges, we propose two structured pathways: SAFE-i\n(Supportive, Adaptive, Fair, and Ethical Implementation) Guidelines for ethical\nand responsible deployment, and HAAS-e (Human-AI Alignment and Safety\nEvaluation) Framework for multidimensional, human-centered assessment. SAFE-i\nprovides a blueprint for data governance, adaptive model engineering, and\nreal-world integration, ensuring LLMs align with clinical and ethical\nstandards. HAAS-e introduces evaluation metrics that go beyond technical\naccuracy to measure trustworthiness, empathy, cultural sensitivity, and\nactionability. We call for the adoption of these structured approaches to\nestablish a responsible and scalable model for LLM-driven mental health\nsupport, ensuring that AI complements, rather than replaces, human expertise.", "AI": {"tldr": "This paper advocates for LLMs in mental health care to be co-creators, emphasizing ethical guidelines and evaluation frameworks to enhance their responsible use.", "motivation": "The paper highlights the limited adoption of LLMs in mental health care due to concerns about bias, evaluation, and ethical implications, and argues for their reframing as co-creators instead of just assistive tools.", "method": "The authors propose two structured pathways: SAFE-i Guidelines for ethical deployment and HAAS-e Framework for human-centered evaluation of LLMs in mental health.", "result": "Implementing the SAFE-i Guidelines and HAAS-e Framework is expected to enhance the ethical integration of LLMs, ensuring they support mental health care without compromising clinical standards.", "conclusion": "The paper calls for the adoption of these structured approaches to create a responsible and scalable model for using LLMs in mental health care, ensuring AI complements human expertise.", "key_contributions": ["Introduction of SAFE-i Guidelines for ethical LLM use in mental health", "Development of the HAAS-e Framework for evaluating LLM impact", "Proposing a new perspective of LLMs as co-creators in mental health care"], "limitations": "", "keywords": ["Large Language Models", "Mental Health Care", "Ethical Guidelines", "Human-AI Alignment", "Evaluation Metrics"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00256", "pdf": "https://arxiv.org/pdf/2506.00256.pdf", "abs": "https://arxiv.org/abs/2506.00256", "title": "The Impact of Disability Disclosure on Fairness and Bias in LLM-Driven Candidate Selection", "authors": ["Mahammed Kamruzzaman", "Gene Louis Kim"], "categories": ["cs.CL"], "comment": "Accepted at The 38th International FLAIRS Conference (FLAIRS\n  2025)(main)", "summary": "As large language models (LLMs) become increasingly integrated into hiring\nprocesses, concerns about fairness have gained prominence. When applying for\njobs, companies often request/require demographic information, including\ngender, race, and disability or veteran status. This data is collected to\nsupport diversity and inclusion initiatives, but when provided to LLMs,\nespecially disability-related information, it raises concerns about potential\nbiases in candidate selection outcomes. Many studies have highlighted how\ndisability can impact CV screening, yet little research has explored the\nspecific effect of voluntarily disclosed information on LLM-driven candidate\nselection. This study seeks to bridge that gap. When candidates shared\nidentical gender, race, qualifications, experience, and backgrounds, and sought\njobs with minimal employment rate gaps between individuals with and without\ndisabilities (e.g., Cashier, Software Developer), LLMs consistently favored\ncandidates who disclosed that they had no disability. Even in cases where\ncandidates chose not to disclose their disability status, the LLMs were less\nlikely to select them compared to those who explicitly stated they did not have\na disability.", "AI": {"tldr": "The study investigates biases in large language models (LLMs) during candidate selection based on voluntarily disclosed demographic information, particularly focusing on disability status.", "motivation": "With the increasing use of LLMs in hiring, there are growing concerns about fairness, particularly regarding the impact of demographic disclosures on candidate selection outcomes.", "method": "The study compares selection outcomes of candidates with identical qualifications who either disclosed having a disability or not in various job applications with minimal employment rate gaps.", "result": "LLMs consistently favored candidates who disclosed they had no disability over similar candidates who either disclosed a disability or chose not to disclose.", "conclusion": "The findings highlight significant biases in LLMs regarding candidates' disability status, suggesting that voluntary disclosure negatively affects selection chances for candidates with disabilities.", "key_contributions": ["Identified biases in LLM candidate selection based on disability disclosure.", "Provided empirical evidence of the impact of demographic information on job applications.", "Called attention to the need for fairer LLM integration in hiring practices."], "limitations": "The study focuses specifically on disability disclosure, which may not encompass all forms of demographic bias.", "keywords": ["large language models", "hiring bias", "demographic information", "disability", "candidate selection"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.16512", "pdf": "https://arxiv.org/pdf/2503.16512.pdf", "abs": "https://arxiv.org/abs/2503.16512", "title": "Multimodal Sensing and Machine Learning to Compare Printed and Verbal Assembly Instructions Delivered by a Social Robot", "authors": ["Ruchik Mishra", "Laksita Prasanna", "Adair Adair", "Dan O Popa"], "categories": ["cs.HC", "cs.RO"], "comment": "Accepted to IEEE CASE 2025", "summary": "In this paper, we compare a manual assembly task communicated to workers\nusing both printed and robot-delivered instructions. The comparison was made\nusing physiological signals (blood volume pulse (BVP) and electrodermal\nactivity (EDA)) collected from individuals during an experimental study. In\naddition, we also collected responses of individuals using the NASA Task Load\nIndex (TLX) survey. Furthermore, we mapped the collected physiological signals\nto the responses of participants for NASA TLX to predict their workload. For\nboth the classification problems, we compare the performance of Convolutional\nNeural Networks (CNNs) and Long-Short-Term Memory (LSTM) models. Results show\nthat for our CNN-based approach using multimodal data (both BVP and EDA) gave\nbetter results than using just BVP (approx. 8.38% more) and EDA (approx 20.49%\nmore). Our LSTM-based model too had better results when we used multimodal data\n(approx 8.38% more than just BVP and 6.70% more than just EDA). Overall, CNNs\nperformed better than LSTMs for classifying physiologies for paper vs\nrobot-based instruction by 7.72%. The CNN-based model was able to give better\nclassification results (approximately 17.83% more on an average across all\nresponses of the NASA TLX) within a few minutes of training compared to the\nLSTM-based models.", "AI": {"tldr": "This paper compares printed and robot-delivered instructions in a manual assembly task using physiological signals and machine learning models.", "motivation": "To explore the effectiveness of different instruction delivery methods (printed vs robot) on worker performance and workload as indicated by physiological signals.", "method": "Physiological signals (BVP and EDA) were collected during a manual assembly task, and the NASA Task Load Index survey responses were mapped to these signals. CNN and LSTM models were employed for classification tasks to predict workload and compare performance between instruction types.", "result": "CNN models using multimodal data (BVP and EDA) outperformed those using single signals, showing approximately 17.83% better classification results across all NASA TLX responses compared to LSTMs, with CNNs outperforming LSTMs in overall performance by 7.72%.", "conclusion": "Utilizing multimodal data enhances classification performance in distinguishing workload based on instruction type, with CNNs being the superior model in this context.", "key_contributions": ["Demonstrated the effectiveness of using physiological signals for workload prediction in assembly tasks", "Provided a comparative analysis of printed vs robot-delivered instructions using machine learning models", "Highlighted the superiority of CNN over LSTM in processing physiological signals for task classification."], "limitations": "", "keywords": ["human-computer interaction", "machine learning", "physiological signals", "workload prediction", "CNN", "LSTM"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00264", "pdf": "https://arxiv.org/pdf/2506.00264.pdf", "abs": "https://arxiv.org/abs/2506.00264", "title": "MultiHoax: A Dataset of Multi-hop False-Premise Questions", "authors": ["Mohammadamin Shafiei", "Hamidreza Saffari", "Nafise Sadat Moosavi"], "categories": ["cs.CL"], "comment": null, "summary": "As Large Language Models are increasingly deployed in high-stakes domains,\ntheir ability to detect false assumptions and reason critically is crucial for\nensuring reliable outputs. False-premise questions (FPQs) serve as an important\nevaluation method by exposing cases where flawed assumptions lead to incorrect\nresponses. While existing benchmarks focus on single-hop FPQs, real-world\nreasoning often requires multi-hop inference, where models must verify\nconsistency across multiple reasoning steps rather than relying on\nsurface-level cues. To address this gap, we introduce MultiHoax, a benchmark\nfor evaluating LLMs' ability to handle false premises in complex, multi-step\nreasoning tasks. Our dataset spans seven countries and ten diverse knowledge\ncategories, using Wikipedia as the primary knowledge source to enable factual\nreasoning across regions. Experiments reveal that state-of-the-art LLMs\nstruggle to detect false premises across different countries, knowledge\ncategories, and multi-hop reasoning types, highlighting the need for improved\nfalse premise detection and more robust multi-hop reasoning capabilities in\nLLMs.", "AI": {"tldr": "Introduction of MultiHoax, a benchmark for evaluating LLMs on false-premise questions in multi-hop reasoning tasks.", "motivation": "The increasing deployment of Large Language Models in critical domains necessitates a mechanism to ensure their outputs are reliable, particularly in detecting false assumptions that can lead to incorrect responses.", "method": "A benchmark called MultiHoax was developed to assess LLMs on their ability to manage false premises through multi-step reasoning, utilizing a dataset sourced from Wikipedia across various countries and knowledge categories.", "result": "State-of-the-art LLMs were found to struggle with detecting false premises across different contexts and multi-hop reasoning tasks, revealing significant shortcomings in their reasoning capabilities.", "conclusion": "The findings underline the urgency for enhancements in LLMs' false premise detection and their multi-hop reasoning abilities to improve accuracy in practical applications.", "key_contributions": ["Introduction of a new benchmark (MultiHoax) for evaluating multi-hop reasoning in LLMs.", "Exposure of the limitations of current state-of-the-art LLMs in detecting false premises.", "Provision of a diverse dataset for future research on LLM reasoning capabilities."], "limitations": "Limited to the contexts and categories represented in the dataset; may not cover all possible reasoning challenges.", "keywords": ["Large Language Models", "false premises", "multi-hop reasoning", "benchmark", "knowledge categories"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.18792", "pdf": "https://arxiv.org/pdf/2503.18792.pdf", "abs": "https://arxiv.org/abs/2503.18792", "title": "REALM: A Dataset of Real-World LLM Use Cases", "authors": ["Jingwen Cheng", "Kshitish Ghate", "Wenyue Hua", "William Yang Wang", "Hong Shen", "Fei Fang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "11 pages, 3 figures", "summary": "Large Language Models (LLMs), such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles.", "AI": {"tldr": "The paper introduces REALM, a comprehensive dataset of LLM use cases, highlighting diverse applications and user demographics.", "motivation": "To enhance understanding of LLM real-world applications and their impact on different user demographics.", "method": "The study involved collecting over 94,000 LLM use cases from Reddit and news articles, categorizing applications, and analyzing the correlation with user occupations.", "result": "REALM provides a valuable dataset that reveals insights on LLM usage across various domains and user types, highlighting the breadth of LLM application in different societal contexts.", "conclusion": "By integrating extensive real-world data, REALM sets a foundation for further exploration of how LLMs are adopted and utilized across different sectors and demographics.", "key_contributions": ["Introduction of the REALM dataset comprising over 94,000 LLM use cases.", "Analysis of LLM applications categorized by user demographics.", "Insights into the relationship between user occupations and types of LLM applications."], "limitations": "", "keywords": ["Large Language Models", "LLM applications", "user demographics", "dataset", "REALM"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00267", "pdf": "https://arxiv.org/pdf/2506.00267.pdf", "abs": "https://arxiv.org/abs/2506.00267", "title": "CASPER: A Large Scale Spontaneous Speech Dataset", "authors": ["Cihan Xiao", "Ruixing Liang", "Xiangyu Zhang", "Mehmet Emre Tiryaki", "Veronica Bae", "Lavanya Shankar", "Rong Yang", "Ethan Poon", "Emmanuel Dupoux", "Sanjeev Khudanpur", "Leibny Paola Garcia Perera"], "categories": ["cs.CL"], "comment": null, "summary": "The success of large language models has driven interest in developing\nsimilar speech processing capabilities. However, a key challenge is the\nscarcity of high-quality spontaneous speech data, as most existing datasets\ncontain scripted dialogues. To address this, we present a novel pipeline for\neliciting and recording natural dialogues and release our Stage 1 dataset with\n200+ hours of spontaneous speech. Our approach fosters fluid, natural\nconversations while encouraging a diverse range of topics and interactive\nexchanges. Unlike traditional methods, it facilitates genuine interactions,\nproviding a reproducible framework for future data collection. This paper\nintroduces our dataset and methodology, laying the groundwork for addressing\nthe shortage of spontaneous speech data. We plan to expand this dataset in\nfuture stages, offering a growing resource for the research community.", "AI": {"tldr": "This paper presents a novel pipeline for eliciting and recording natural dialogues, along with a dataset of 200+ hours of spontaneous speech to address the shortage of such data in speech processing.", "motivation": "There is a lack of high-quality spontaneous speech data necessary for developing advanced speech processing capabilities, as existing datasets mainly consist of scripted dialogues.", "method": "The authors developed a reproducible framework for collecting natural, unscripted dialogues that encourages diverse topics and interactive exchanges.", "result": "They released a Stage 1 dataset containing over 200 hours of spontaneous speech, which offers genuine interactions and serves as a foundation for future expansions.", "conclusion": "The paper lays the groundwork for tackling the scarcity of spontaneous speech data and aims to expand the dataset in subsequent stages for continued research use.", "key_contributions": ["Introduction of a novel pipeline for spontaneous dialogue collection", "Release of a Stage 1 dataset with 200+ hours of spontaneous speech data", "Provision of a reproducible framework for future data collection"], "limitations": "", "keywords": ["speech processing", "spontaneous speech", "dataset", "natural dialogues", "dialogue system"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861.pdf", "abs": "https://arxiv.org/abs/2504.13861", "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "35 pages, 13 figures, 7 tables", "summary": "Though Large Vision-Language Models (LVLMs) are being actively explored in\nmedicine, their ability to conduct telemedicine consultations combining\naccurate diagnosis with professional dialogue remains underexplored. In this\npaper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark),\nan open-source framework for simulating and evaluating LVLM-driven telemedical\nconsultations. 3MDBench simulates patient variability through four\ntemperament-based Patient Agents and an Assessor Agent that jointly evaluate\ndiagnostic accuracy and dialogue quality. It includes 3013 cases across 34\ndiagnoses drawn from real-world telemedicine interactions, combining textual\nand image-based data. The experimental study compares diagnostic strategies for\npopular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and\nQwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal\nreasoning improves F1 score by 6.5% over non-dialogue settings, highlighting\nthe importance of context-aware, information-seeking questioning. Moreover,\ninjecting predictions from a diagnostic convolutional network into the LVLM's\ncontext boosts F1 by up to 20%. Source code is available at\nhttps://anonymous.4open.science/r/3mdbench_acl-0511.", "AI": {"tldr": "This paper presents 3MDBench, an open-source framework for simulating LVLM-driven telemedicine consultations, highlighting the importance of multimodal dialogue in improving diagnostic accuracy.", "motivation": "To explore the potential of Large Vision-Language Models in telemedicine and address the gap in their evaluation for accurate diagnosis and professional dialogue.", "method": "3MDBench includes 3013 cases across 34 diagnoses with simulations involving four temperament-based Patient Agents and an Assessor Agent for evaluating diagnostic accuracy and dialogue quality.", "result": "The study shows that multimodal dialogue with internal reasoning improves F1 score by 6.5%, and injecting predictions from a convolutional network into the LVLM's context boosts F1 by up to 20%.", "conclusion": "The framework demonstrates that context-aware dialogue significantly enhances the performance of LVLMs in telehealth scenarios.", "key_contributions": ["Introduction of 3MDBench framework for telemedicine evaluations", "Simulation of patient variability with diverse Patient Agents", "Demonstration of performance gains through multimodal dialogue"], "limitations": "", "keywords": ["Large Vision-Language Models", "telemedicine", "multimodal dialogue"], "importance_score": 9, "read_time_minutes": 35}}
{"id": "2506.00277", "pdf": "https://arxiv.org/pdf/2506.00277.pdf", "abs": "https://arxiv.org/abs/2506.00277", "title": "Hierarchical Level-Wise News Article Clustering via Multilingual Matryoshka Embeddings", "authors": ["Hans W. A. Hanley", "Zakir Durumeric"], "categories": ["cs.CL", "cs.AI", "cs.SI"], "comment": "Accepted to The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Contextual large language model embeddings are increasingly utilized for\ntopic modeling and clustering. However, current methods often scale poorly,\nrely on opaque similarity metrics, and struggle in multilingual settings. In\nthis work, we present a novel, scalable, interpretable, hierarchical, and\nmultilingual approach to clustering news articles and social media data. To do\nthis, we first train multilingual Matryoshka embeddings that can determine\nstory similarity at varying levels of granularity based on which subset of the\ndimensions of the embeddings is examined. This embedding model achieves\nstate-of-the-art performance on the SemEval 2022 Task 8 test dataset (Pearson\n$\\rho$ = 0.816). Once trained, we develop an efficient hierarchical clustering\nalgorithm that leverages the hierarchical nature of Matryoshka embeddings to\nidentify unique news stories, narratives, and themes. We conclude by\nillustrating how our approach can identify and cluster stories, narratives, and\noverarching themes within real-world news datasets.", "AI": {"tldr": "This paper presents a novel approach for clustering multilingual news articles and social media data using interpretable, hierarchical embeddings that address scalability and similarity issues.", "motivation": "To improve clustering of news articles and social media data by overcoming the limitations of current methods, such as poor scalability and reliance on opaque similarity metrics.", "method": "Introducing multilingual Matryoshka embeddings that assess story similarity at different granularity levels, followed by an efficient hierarchical clustering algorithm that utilizes the hierarchical nature of these embeddings.", "result": "Achieves state-of-the-art performance on the SemEval 2022 Task 8 dataset with a Pearson correlation coefficient of 0.816.", "conclusion": "The proposed method successfully identifies and clusters unique news stories, narratives, and overarching themes in real-world datasets.", "key_contributions": ["Development of multilingual Matryoshka embeddings for story similarity", "Hierarchical clustering algorithm leveraging embedding structure", "Demonstrated effectiveness on real-world news data"], "limitations": "", "keywords": ["multilingual embeddings", "clustering", "news articles", "social media", "hierarchical"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822.pdf", "abs": "https://arxiv.org/abs/2504.14822", "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted as ACL 2025 (main)", "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review.", "AI": {"tldr": "InsightAgent is an interactive AI agent that utilizes large language models to enhance the efficiency and quality of systematic reviews in healthcare by partitioning literature semantically and providing visual feedback mechanisms.", "motivation": "Systematic reviews are crucial for evidence-based healthcare but are hampered by labor-intensive processes and lack of effective automatic summarization methods.", "method": "The authors developed InsightAgent, which segments literature based on semantics and uses a multi-agent framework that allows users to visualize and interact with the agent, offering real-time feedback.", "result": "User studies show that InsightAgent improves the quality of systematic reviews by 27.2% (79.7% of human-written quality) and enhances user satisfaction by 34.4%, reducing the time required for completion to about 1.5 hours.", "conclusion": "InsightAgent significantly streamlines the process of systematic reviews, making it more efficient while maintaining high quality and user satisfaction.", "key_contributions": ["Introduction of a human-centered interactive AI agent for systematic reviews.", "Effective multi-agent design for literature processing.", "Intuitive visualizations that enhance user interaction and feedback."], "limitations": "", "keywords": ["systematic reviews", "AI in healthcare", "interactive AI agents", "large language models", "user studies"], "importance_score": 9, "read_time_minutes": 7}}
{"id": "2506.00288", "pdf": "https://arxiv.org/pdf/2506.00288.pdf", "abs": "https://arxiv.org/abs/2506.00288", "title": "Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation", "authors": ["Ahmed Elhady", "Eneko Agirre", "Mikel Artetxe"], "categories": ["cs.CL", "cs.AI"], "comment": "To appear in ACL 2025 Main", "summary": "Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future.", "AI": {"tldr": "This paper investigates the role of English data in continued pretraining (CPT) for large language models (LLMs) and introduces methods to enhance language adaptation.", "motivation": "To explore the impact of including English data in the continued pretraining of LLMs for new languages, particularly its influence on the model's capabilities in the target language.", "method": "The authors conduct experiments to analyze the effect of English data inclusion on validation perplexity and downstream capabilities. They also introduce a language-agnostic benchmark for in-context learning (ICL) and apply curriculum learning and exponential moving average (EMA) of weights to mitigate issues arising from the lack of English data.", "result": "Including English data does not affect validation perplexity but is crucial for the emergence of downstream capabilities in the target language. The study reveals catastrophic forgetting when English is excluded early on in CPT.", "conclusion": "The findings improve understanding of how emergent abilities develop in language adaptation through CPT and suggest methods to enhance this process in future models.", "key_contributions": ["Introduced a language-agnostic benchmark for in-context learning (ICL).", "Identified the critical role of English data in continued pretraining for language adaptation.", "Proposed curriculum learning and EMA as methods to alleviate negative impacts of excluding English data."], "limitations": "The study primarily focuses on the inclusion of English data and may not address effects of other languages or data types comprehensively.", "keywords": ["continued pretraining", "large language models", "language adaptation", "in-context learning", "catastrophic forgetting"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2505.22906", "pdf": "https://arxiv.org/pdf/2505.22906.pdf", "abs": "https://arxiv.org/abs/2505.22906", "title": "HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding", "authors": ["Emmanuel Anaya González", "Raven Rothkopf", "Sorin Lerner", "Nadia Polikarpova"], "categories": ["cs.HC", "cs.AI", "cs.PL"], "comment": "10 pages, 6 figures", "summary": "While AI programming tools hold the promise of increasing programmers'\ncapabilities and productivity to a remarkable degree, they often exclude users\nfrom essential decision-making processes, causing many to effectively \"turn off\ntheir brains\" and over-rely on solutions provided by these systems. These\nbehaviors can have severe consequences in critical domains, like software\nsecurity. We propose Human-in-the-loop Decoding, a novel interaction technique\nthat allows users to observe and directly influence LLM decisions during code\ngeneration, in order to align the model's output with their personal\nrequirements. We implement this technique in HiLDe, a code completion assistant\nthat highlights critical decisions made by the LLM and provides local\nalternatives for the user to explore. In a within-subjects study (N=18) on\nsecurity-related tasks, we found that HiLDe led participants to generate\nsignificantly fewer vulnerabilities and better align code generation with their\ngoals compared to a traditional code completion assistant.", "AI": {"tldr": "This paper presents HiLDe, an AI-driven code completion assistant that integrates users into the decision-making loop, significantly enhancing code security and alignment with user goals during programming.", "motivation": "The increasing reliance on AI programming tools can lead to a lack of user involvement in decision-making, risking critical issues such as software security. This study aims to address this problem by enabling user interaction with LLM outputs.", "method": "The authors propose the Human-in-the-loop Decoding technique, implemented in the HiLDe assistant, which allows users to observe and influence code generation decisions made by the LLM.", "result": "In a study with 18 participants focused on security-related tasks, HiLDe resulted in fewer generated vulnerabilities and better alignment of code with user goals compared to traditional assistants.", "conclusion": "Integrating users into the coding process through the HiLDe assistant may enhance software security and the relevance of AI-generated code, encouraging more critical engagement with AI tools.", "key_contributions": ["Introduction of Human-in-the-loop Decoding for AI programming.", "Implementation of HiLDe code completion assistant as a case study.", "Empirical evidence showing improved security outcomes and user alignment in coding tasks."], "limitations": "The study is limited by its small sample size of 18 participants and focuses on specific security-related tasks, which may not generalize to all programming scenarios.", "keywords": ["Human-in-the-loop", "code completion", "AI programming tools", "software security", "LLM interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00290", "pdf": "https://arxiv.org/pdf/2506.00290.pdf", "abs": "https://arxiv.org/abs/2506.00290", "title": "DLM-One: Diffusion Language Models for One-Step Sequence Generation", "authors": ["Tianqi Chen", "Shujian Zhang", "Mingyuan Zhou"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "This paper introduces DLM-One, a score-distillation-based framework for\none-step sequence generation with continuous diffusion language models (DLMs).\nDLM-One eliminates the need for iterative refinement by aligning the scores of\na student model's outputs in the continuous token embedding space with the\nscore function of a pretrained teacher DLM. We investigate whether DLM-One can\nachieve substantial gains in sampling efficiency for language modeling. Through\ncomprehensive experiments on DiffuSeq -- a representative continuous DLM -- we\nshow that DLM-One achieves up to ~500x speedup in inference time while\nmaintaining competitive performance on benchmark text generation tasks used to\nevaluate the teacher models. We further analyze the method's empirical behavior\nacross multiple datasets, providing initial insights into its generality and\npractical applicability. Our findings position one-step diffusion as a\npromising direction for efficient, high-quality language generation and broader\nadoption of continuous diffusion models operating in embedding space for\nnatural language processing.", "AI": {"tldr": "DLM-One is a framework for one-step sequence generation using continuous diffusion language models, achieving significant sampling efficiency and speedup in inference time.", "motivation": "To improve the efficiency of language modeling by introducing a score-distillation framework that eliminates the need for iterative refinement.", "method": "DLM-One aligns the outputs of a student model with the pretrained teacher DLM's score function in continuous token embedding space.", "result": "DLM-One achieves up to ~500x speedup in inference time while maintaining competitive performance on benchmark text generation tasks.", "conclusion": "One-step diffusion is a promising direction for efficient, high-quality language generation and broader application of continuous diffusion models in NLP.", "key_contributions": ["Introduction of the DLM-One framework", "Significant improvements in sampling efficiency", "Initial insights into the generality and practical applicability of one-step diffusion"], "limitations": "", "keywords": ["language modeling", "diffusion models", "NLP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00304", "pdf": "https://arxiv.org/pdf/2506.00304.pdf", "abs": "https://arxiv.org/abs/2506.00304", "title": "Can LLMs Understand Unvoiced Speech? Exploring EMG-to-Text Conversion with LLMs", "authors": ["Payal Mohapatra", "Akash Pandey", "Xiaoyuan Zhang", "Qi Zhu"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "Unvoiced electromyography (EMG) is an effective communication tool for\nindividuals unable to produce vocal speech. However, most prior methods rely on\npaired voiced and unvoiced EMG signals, along with speech data, for EMG-to-text\nconversion, which is not practical for such individuals. Given the rise of\nlarge language models (LLMs) in speech recognition, we explore their potential\nto understand unvoiced speech. To this end, we address the challenge of\nlearning from unvoiced EMG alone and propose a novel EMG adaptor module that\nmaps EMG features into an LLM's input space, achieving an average word error\nrate (WER) of 0.49 on a closed-vocabulary unvoiced EMG-to-text task. Even with\na conservative data availability of just six minutes, our approach improves\nperformance over specialized models by nearly 20%. While LLMs have been shown\nto be extendable to new language modalities -- such as audio -- understanding\narticulatory biosignals like unvoiced EMG remains more challenging. This work\ntakes a crucial first step toward enabling LLMs to comprehend unvoiced speech\nusing surface EMG.", "AI": {"tldr": "Exploration of using large language models for unvoiced EMG-to-text conversion without paired voiced data.", "motivation": "To improve communication for individuals unable to produce vocal speech by enabling LLMs to understand unvoiced EMG signals.", "method": "Proposed a novel EMG adaptor module to map EMG features into an LLM's input space, achieving a low word error rate (WER).", "result": "Achieved an average WER of 0.49 on a closed-vocabulary unvoiced EMG-to-text task, improving performance by nearly 20% over specialized models with limited data.", "conclusion": "This work is a crucial step towards enabling LLMs to comprehend unvoiced speech, showing potential for practical applications in communication aids.", "key_contributions": ["Novel EMG adaptor module for LLM input mapping", "Significant reduction in word error rate for unvoiced EMG-to-text", "First attempt to apply LLMs to interpret unvoiced articulatory biosignals"], "limitations": "The approach is tested on limited data; further research needed for scalability.", "keywords": ["EMG", "Large Language Models", "Unvoiced Speech", "Speech Recognition", "Health Informatics"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.00307", "pdf": "https://arxiv.org/pdf/2506.00307.pdf", "abs": "https://arxiv.org/abs/2506.00307", "title": "Lossless Token Sequence Compression via Meta-Tokens", "authors": ["John Harvill", "Ziwei Fan", "Hao Wang", "Yizhou Sun", "Hao Ding", "Luke Huan", "Anoop Deoras"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 8 figures", "summary": "Existing work on prompt compression for Large Language Models (LLM) focuses\non lossy methods that try to maximize the retention of semantic information\nthat is relevant to downstream tasks while significantly reducing the sequence\nlength. In this paper, we introduce a task-agnostic lossless compression\ntechnique similar to LZ77 that makes it possible to reduce the input token\nsequence length on average by 27\\% and 18\\% for the two evaluation tasks\nexplored here. Given that we use transformer-based LLMs, this equates to 47\\%\nand 33\\% less encoding computation, respectively, due to the quadratic nature\nof attention. The token sequence transformation is trivial to reverse and\nhighlights that no semantic information is lost in the process. We evaluate our\nproposed approach on two tasks that require strict preservation of\nsemantics/syntax and demonstrate that existing lossy compression methods\nperform poorly in this setting. We find that our lossless compression technique\nproduces only a small gap in performance compared to using the uncompressed\ninput and posit that larger models and an expanded computing budget would\nlikely erase the gap entirely.", "AI": {"tldr": "Introduces a lossless compression technique for Large Language Models that reduces input token sequence length while retaining all semantic information.", "motivation": "Existing methods for prompt compression focus on lossy techniques, which can lead to loss of important semantic information during compression.", "method": "A task-agnostic lossless compression technique inspired by LZ77 is proposed, allowing for significant reduction in input token sequence length and computation requirements during encoding.", "result": "Achieved an average reduction of 27% and 18% in token sequence length for two tasks, leading to 47% and 33% less encoding computation, respectively.", "conclusion": "The proposed technique shows strong potential for preserving semantics/syntax without loss, and larger models may further compensate for the slight performance gap observed compared to uncompressed inputs.", "key_contributions": ["Introduces a novel lossless compression method for LLMs", "Demonstrates significant sequence length reduction", "Maintains semantic information without loss"], "limitations": "The study evaluates only two tasks; performance may vary across different types of tasks not examined here.", "keywords": ["Large Language Models", "Lossless Compression", "Prompt Compression", "Semantic Preservation", "Transformer Models"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2506.00312", "pdf": "https://arxiv.org/pdf/2506.00312.pdf", "abs": "https://arxiv.org/abs/2506.00312", "title": "An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3", "authors": ["Brendan Sands", "Yining Wang", "Chenhao Xu", "Yuxuan Zhou", "Lai Wei", "Rohitash Chandra"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been prominent in various tasks, including\ntext generation and summarisation. The applicability of LLMs to the generation\nof product reviews is gaining momentum, paving the way for the generation of\nmovie reviews. In this study, we propose a framework that generates movie\nreviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate\ntheir performance by comparing the generated outputs with IMDb user reviews. We\nuse movie subtitles and screenplays as input to the LLMs and investigate how\nthey affect the quality of reviews generated. We review the LLM-based movie\nreviews in terms of vocabulary, sentiment polarity, similarity, and thematic\nconsistency in comparison to IMDB user reviews. The results demonstrate that\nLLMs are capable of generating syntactically fluent and structurally complete\nmovie reviews. Nevertheless, there is still a noticeable gap in emotional\nrichness and stylistic coherence between LLM-generated and IMDb reviews,\nsuggesting that further refinement is needed to improve the overall quality of\nmovie review generation. We provided a survey-based analysis where participants\nwere told to distinguish between LLM and IMDb user reviews. The results show\nthat LLM-generated reviews are difficult to distinguish from IMDB user reviews.\nWe found that DeepSeek-V3 produced the most balanced reviews, closely matching\nIMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0\ncaptured negative emotions better but showed excessive emotional intensity.", "AI": {"tldr": "This study proposes a framework for generating movie reviews using large language models (LLMs) and compares their performance against IMDb user reviews.", "motivation": "The growing interest in applying LLMs for generating product and movie reviews.", "method": "A framework utilizing three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0) to generate movie reviews based on input from movie subtitles and screenplays.", "result": "LLM-generated reviews are syntactically fluent but lack emotional richness and stylistic coherence compared to IMDb reviews; DeepSeek-V3 produced the most balanced reviews.", "conclusion": "While LLMs can generate structurally complete movie reviews, improvements are needed in emotional depth and coherence.", "key_contributions": ["Proposed framework for LLMs to generate movie reviews", "Comparison of three different LLMs in generating reviews", "Survey-based analysis on the indistinguishability of LLM-generated reviews from IMDb reviews"], "limitations": "Need for further refinement in emotional richness and stylistic coherence of LLM-generated reviews.", "keywords": ["Large Language Models", "Movie Reviews", "Sentiment Analysis", "Natural Language Processing", "User-generated Content"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.01617", "pdf": "https://arxiv.org/pdf/2412.01617.pdf", "abs": "https://arxiv.org/abs/2412.01617", "title": "If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 (main)", "summary": "Warning: this paper discusses content related, but not limited to, violence,\nsex, and suicide. Loneliness, or the lack of fulfilling relationships,\nsignificantly impacts a person's mental and physical well-being and is\nprevalent worldwide. Previous research suggests that large language models\n(LLMs) may help mitigate loneliness. However, we argue that the use of\nwidespread LLMs in services like ChatGPT is more prevalent--and riskier, as\nthey are not designed for this purpose. To explore this, we analysed user\ninteractions with ChatGPT outside of its marketed use as a task-oriented\nassistant. In dialogues classified as lonely, users frequently (37%) sought\nadvice or validation, and received good engagement. However, ChatGPT failed in\nsensitive scenarios, like responding appropriately to suicidal ideation or\ntrauma. We also observed a 35% higher incidence of toxic content, with women\nbeing 22x more likely to be targeted than men. Our findings underscore ethical\nand legal questions about this technology, and note risks like radicalisation\nor further isolation. We conclude with recommendations to research and industry\nto address loneliness.", "AI": {"tldr": "This paper examines the risks of using large language models like ChatGPT to address loneliness, revealing inadequate responses to sensitive topics and heightened incidents of toxic interactions.", "motivation": "To explore the implications of using large language models for mitigating loneliness and the potential risks associated with their use.", "method": "Analysis of user interactions with ChatGPT, specifically dialogues classified as lonely, to identify patterns and outcomes in user responses.", "result": "37% of users sought advice or validation in lonely dialogues, with good engagement, but ChatGPT failed in sensitive scenarios like suicidal ideation, with a 35% higher incidence of toxic content.", "conclusion": "The use of LLMs in addressing loneliness raises ethical and legal concerns, with risks such as radicalisation and isolation; recommendations for research and industry are provided.", "key_contributions": ["Identification of the inadequacies of LLMs in addressing mental health issues like loneliness.", "Quantification of toxic content and biases present in user interactions with LLMs.", "Recommendations for improving LLM applications in sensitive contexts."], "limitations": "The study primarily focuses on interactions with ChatGPT and may not generalize to other settings or models.", "keywords": ["loneliness", "large language models", "ChatGPT", "toxic content", "mental health"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00319", "pdf": "https://arxiv.org/pdf/2506.00319.pdf", "abs": "https://arxiv.org/abs/2506.00319", "title": "SkillVerse : Assessing and Enhancing LLMs with Tree Evaluation", "authors": ["Yufei Tian", "Jiao Sun", "Nanyun Peng", "Zizhao Zhang"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "As language models evolve to tackle complex, multifaceted tasks, their\nevaluation must adapt to capture this intricacy. A granular, skill-specific\nunderstanding of model capabilities can empower researchers to make informed\nmodel development plans. In this paper, we introduce SkillVerse, an\nunsupervised tree-structured diagnosis framework for understanding model\nproficiency in specific abilities. With LLM as a judge, SkillVerse first\ncritiques the model responses, and then organizes them into a hierarchical\nstructure termed dendrogram. Given proficiency at arbitrary levels of\ngranularity, SkillVerse is flexible to produce insights of behaviors of modern\nlarge models. We also demonstrate its efficacy in two downstream tasks: 1)\nimproving model in-context learning by 25% using a tree-search algorithm to\nselect more informative few-shot demonstrations, and 2) accurately predicting\nnew model weaknesses with a 55% success rate, 22% higher than without\nSkillVerse.", "AI": {"tldr": "SkillVerse is an unsupervised framework for evaluating language model capabilities through a hierarchical analysis of model responses.", "motivation": "To develop a more nuanced evaluation method for complex language model capabilities to guide effective model development.", "method": "SkillVerse critiques model responses using a tree-structured approach, organizing insights into a hierarchical dendrogram.", "result": "SkillVerse enhances in-context learning by 25% and improves prediction of model weaknesses by 22%.", "conclusion": "The framework provides a flexible, insightful understanding of large language models' capabilities.", "key_contributions": ["Introduction of the SkillVerse framework for model evaluation", "Demonstration of improved in-context learning", "Enhanced prediction accuracy of model weaknesses"], "limitations": "", "keywords": ["language models", "evaluation framework", "SkillVerse", "machine learning", "neural networks"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00331", "pdf": "https://arxiv.org/pdf/2506.00331.pdf", "abs": "https://arxiv.org/abs/2506.00331", "title": "TreeRare: Syntax Tree-Guided Retrieval and Reasoning for Knowledge-Intensive Question Answering", "authors": ["Boyi Zhang", "Zhuo Liu", "Hangfeng He"], "categories": ["cs.CL"], "comment": null, "summary": "In real practice, questions are typically complex and knowledge-intensive,\nrequiring Large Language Models (LLMs) to recognize the multifaceted nature of\nthe question and reason across multiple information sources. Iterative and\nadaptive retrieval, where LLMs decide when and what to retrieve based on their\nreasoning, has been shown to be a promising approach to resolve complex,\nknowledge-intensive questions. However, the performance of such retrieval\nframeworks is limited by the accumulation of reasoning errors and misaligned\nretrieval results. To overcome these limitations, we propose TreeRare (Syntax\nTree-Guided Retrieval and Reasoning), a framework that utilizes syntax trees to\nguide information retrieval and reasoning for question answering. Following the\nprinciple of compositionality, TreeRare traverses the syntax tree in a\nbottom-up fashion, and in each node, it generates subcomponent-based queries\nand retrieves relevant passages to resolve localized uncertainty. A\nsubcomponent question answering module then synthesizes these passages into\nconcise, context-aware evidence. Finally, TreeRare aggregates the evidence\nacross the tree to form a final answer. Experiments across five question\nanswering datasets involving ambiguous or multi-hop reasoning demonstrate that\nTreeRare achieves substantial improvements over existing state-of-the-art\nmethods.", "AI": {"tldr": "TreeRare is a novel framework that utilizes syntax trees to improve information retrieval and reasoning for answering complex questions using LLMs.", "motivation": "There is a need for improved performance in handling complex, knowledge-intensive questions with LLMs due to limitations caused by reasoning errors and misaligned retrieval results.", "method": "TreeRare employs syntax trees to guide information retrieval, generating subcomponent-based queries at each node and synthesizing relevant passages into context-aware evidence.", "result": "TreeRare shows substantial improvements over state-of-the-art methods across five question answering datasets involving ambiguous or multi-hop reasoning.", "conclusion": "The proposed framework effectively resolves localized uncertainties by utilizing a structured approach to retrieval and reasoning, enhancing the accuracy of complex question answering.", "key_contributions": ["Introduction of the TreeRare framework for guided retrieval using syntax trees", "Improved performance on ambiguous and multi-hop reasoning tasks", "Demonstration of compositionality in question answering through subcomponent queries"], "limitations": "", "keywords": ["Large Language Models", "information retrieval", "question answering", "syntax trees", "reasoning"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2501.17182", "pdf": "https://arxiv.org/pdf/2501.17182.pdf", "abs": "https://arxiv.org/abs/2501.17182", "title": "Dialogue Systems for Emotional Support via Value Reinforcement", "authors": ["Juhee Kim", "Chunghu Mok", "Jisun Lee", "Hyang Sook Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "I.2.7"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "Emotional support dialogue systems aim to reduce help-seekers' distress and\nhelp them overcome challenges. While human values$\\unicode{x2013}$core beliefs\nthat shape an individual's priorities$\\unicode{x2013}$are increasingly\nemphasized in contemporary psychological therapy for their role in fostering\ninternal transformation and long-term emotional well-being, their integration\ninto emotional support systems remains underexplored. To bridge this gap, we\npresent a value-driven method for training emotional support dialogue systems\ndesigned to reinforce positive values in seekers. Notably, our model identifies\nwhich values to reinforce at each turn and how to do so, by leveraging online\nsupport conversations from Reddit. We evaluate the method across support\nskills, seekers' emotional intensity, and value reinforcement. Our method\nconsistently outperforms various baselines, effectively exploring and eliciting\nvalues from seekers. Additionally, leveraging crowd knowledge from Reddit\nsignificantly enhances its effectiveness. Therapists highlighted its ability to\nvalidate seekers' challenges and emphasize positive aspects of their\nsituations$\\unicode{x2013}$both crucial elements of value reinforcement. Our\nwork, being the first to integrate value reinforcement into emotional support\nsystems, demonstrates its promise and establishes a foundation for future\nresearch.", "AI": {"tldr": "This paper introduces a method for training emotional support dialogue systems using values from psychological therapy to enhance emotional well-being.", "motivation": "To integrate human values into emotional support dialogue systems, which is essential for reducing distress and promoting long-term emotional well-being in help-seekers.", "method": "A value-driven approach leveraging Reddit conversations was developed to train the system, identifying which values to reinforce and how to elicit them in real-time during interactions.", "result": "The proposed method consistently outperformed various baseline models in promoting positive values, effectively exploring and eliciting values from seekers, and was validated positively by therapists.", "conclusion": "Integrating value reinforcement into emotional support systems shows promise and lays the groundwork for further research in this area.", "key_contributions": ["First integration of value reinforcement in emotional support systems.", "Utilization of crowd knowledge from Reddit to enhance system effectiveness.", "Evaluation across multiple dimensions like support skills and emotional intensity."], "limitations": "", "keywords": ["emotional support", "dialogue systems", "human values", "value reinforcement", "psychological therapy"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00332", "pdf": "https://arxiv.org/pdf/2506.00332.pdf", "abs": "https://arxiv.org/abs/2506.00332", "title": "Disentangling Codemixing in Chats: The NUS ABC Codemixed Corpus", "authors": ["Svetlana Churina", "Akshat Gupta", "Insyirah Mujtahid", "Kokil Jaidka"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Code-mixing involves the seamless integration of linguistic elements from\nmultiple languages within a single discourse, reflecting natural multilingual\ncommunication patterns. Despite its prominence in informal interactions such as\nsocial media, chat messages and instant-messaging exchanges, there has been a\nlack of publicly available corpora that are author-labeled and suitable for\nmodeling human conversations and relationships. This study introduces the first\nlabeled and general-purpose corpus for understanding code-mixing in context\nwhile maintaining rigorous privacy and ethical standards. Our live project will\ncontinuously gather, verify, and integrate code-mixed messages into a\nstructured dataset released in JSON format, accompanied by detailed metadata\nand linguistic statistics. To date, it includes over 355,641 messages spanning\nvarious code-mixing patterns, with a primary focus on English, Mandarin, and\nother languages. We expect the Codemix Corpus to serve as a foundational\ndataset for research in computational linguistics, sociolinguistics, and NLP\napplications.", "AI": {"tldr": "Introduction of the Codemix Corpus, a labeled dataset for studying code-mixing in multilingual communication.", "motivation": "To address the lack of publicly available, author-labeled corpora for modeling code-mixing in human conversations, particularly in informal contexts.", "method": "The study involves the continuous collection, verification, and integration of code-mixed messages into a structured dataset, with a focus on maintaining privacy and ethical standards.", "result": "The Codemix Corpus contains over 355,641 messages representing various code-mixing patterns, primarily focusing on English and Mandarin.", "conclusion": "The Codemix Corpus aims to provide a foundational dataset for advancing research in computational linguistics, sociolinguistics, and NLP applications.", "key_contributions": ["First labeled corpus for code-mixing", "Includes over 355,641 messages", "Structured dataset released in JSON format with metadata"], "limitations": "", "keywords": ["code-mixing", "corpus", "multilingual communication", "NLP", "sociolinguistics"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.00334", "pdf": "https://arxiv.org/pdf/2506.00334.pdf", "abs": "https://arxiv.org/abs/2506.00334", "title": "Beyond Context to Cognitive Appraisal: Emotion Reasoning as a Theory of Mind Benchmark for Large Language Models", "authors": ["Gerard Christopher Yeo", "Kokil Jaidka"], "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Datasets used for emotion recognition tasks typically contain overt cues that\ncan be used in predicting the emotions expressed in a text. However, one\nchallenge is that texts sometimes contain covert contextual cues that are rich\nin affective semantics, which warrant higher-order reasoning abilities to infer\nemotional states, not simply the emotions conveyed. This study advances beyond\nsurface-level perceptual features to investigate how large language models\n(LLMs) reason about others' emotional states using contextual information,\nwithin a Theory-of-Mind (ToM) framework. Grounded in Cognitive Appraisal\nTheory, we curate a specialized ToM evaluation dataset1 to assess both forward\nreasoning - from context to emotion- and backward reasoning - from emotion to\ninferred context. We showed that LLMs can reason to a certain extent, although\nthey are poor at associating situational outcomes and appraisals with specific\nemotions. Our work highlights the need for psychological theories in the\ntraining and evaluation of LLMs in the context of emotion reasoning.", "AI": {"tldr": "This study explores how large language models reason about emotional states using contextual information, highlighting the challenges and limitations in emotion recognition tasks.", "motivation": "To investigate the ability of large language models (LLMs) to reason about emotions using covert contextual cues, moving beyond overt cues typically used in emotion recognition.", "method": "We developed a specialized Theory-of-Mind (ToM) evaluation dataset and conducted tests on LLMs to assess their reasoning capabilities in both forward and backward contexts related to emotions.", "result": "LLMs demonstrated some reasoning abilities regarding emotional states but struggled with associating situational appraisals with specific emotions.", "conclusion": "There is a necessity for integrating psychological theories in the training and evaluation of LLMs for improved emotion reasoning.", "key_contributions": ["Creation of a specialized ToM evaluation dataset for emotion recognition", "Insights into LLMs' reasoning abilities regarding emotions", "Highlighting the importance of psychological theories in LLM training and evaluation"], "limitations": "LLMs showed poor performance in correlating situational outcomes and appraisals with specific emotions.", "keywords": ["emotional states", "large language models", "contextual information", "emotion reasoning", "Theory-of-Mind"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00338", "pdf": "https://arxiv.org/pdf/2506.00338.pdf", "abs": "https://arxiv.org/abs/2506.00338", "title": "OWSM v4: Improving Open Whisper-Style Speech Models via Data Scaling and Cleaning", "authors": ["Yifan Peng", "Shakeel Muhammad", "Yui Sudo", "William Chen", "Jinchuan Tian", "Chyi-Jiunn Lin", "Shinji Watanabe"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "The Open Whisper-style Speech Models (OWSM) project has developed a series of\nfully open speech foundation models using academic-scale resources, but their\ntraining data remains insufficient. This work enhances OWSM by integrating\nYODAS, a large-scale web-crawled dataset with a Creative Commons license.\nHowever, incorporating YODAS is nontrivial due to its wild nature, which\nintroduces challenges such as incorrect language labels and audio-text\nmisalignments. To address this, we develop a scalable data-cleaning pipeline\nusing public toolkits, yielding a dataset with 166,000 hours of speech across\n75 languages. Our new series of OWSM v4 models, trained on this curated dataset\nalongside existing OWSM data, significantly outperform previous versions on\nmultilingual benchmarks. Our models even match or surpass frontier industrial\nmodels like Whisper and MMS in multiple scenarios. We will publicly release the\ncleaned YODAS data, pre-trained models, and all associated scripts via the\nESPnet toolkit.", "AI": {"tldr": "The OWSM project enhances open speech models by incorporating a large-scale web dataset (YODAS), addressing data quality issues with a new cleaning pipeline, resulting in improved multilingual model performance.", "motivation": "To enhance OWSM by integrating a large-scale dataset and improve the quality of speech models for better multilingual support.", "method": "Developed a scalable data-cleaning pipeline to address issues from YODAS, resulting in a curated dataset with 166,000 hours of speech across 75 languages.", "result": "OWSM v4 models trained on the cleaned dataset significantly outperform previous models and match or surpass industrial benchmarks like Whisper and MMS.", "conclusion": "The OWSM project will publicly release the cleaned YODAS data, pre-trained models, and associated scripts for the community via the ESPnet toolkit.", "key_contributions": ["Integrated YODAS dataset for improved speech models", "Developed a scalable data-cleaning pipeline", "Achieved performance on par with top industrial models"], "limitations": "", "keywords": ["Speech Models", "Multilingual Benchmarking", "Data-Cleaning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2503.13975", "pdf": "https://arxiv.org/pdf/2503.13975.pdf", "abs": "https://arxiv.org/abs/2503.13975", "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark", "authors": ["Omar Shaikh", "Hussein Mozannar", "Gagan Bansal", "Adam Fourney", "Eric Horvitz"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025, 16 pages, 5 figures", "summary": "Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, we find that early grounding failures predict later\ninteraction breakdowns. Building on these insights, we introduce Rifts, a\nbenchmark derived from publicly available LLM interaction data containing\nsituations where LLMs fail to initiate grounding. We note that current frontier\nmodels perform poorly on Rifts, highlighting the need to reconsider how we\ntrain and prompt LLMs for human interaction. To this end, we develop a\npreliminary intervention aimed at mitigating grounding failures.", "AI": {"tldr": "The paper examines grounding challenges in human-LLM interactions, revealing LLMs' limitations in initiating clarifications and follow-ups, and introduces a benchmark to address these issues.", "motivation": "To understand and improve mutual understanding in human-LLM conversations, which is crucial for effective interaction, especially in high-stakes scenarios.", "method": "Analysis of datasets (WildChat, MultiWOZ, Bing Chat) to create a taxonomy of grounding acts, and models for annotating and predicting grounding behavior.", "result": "LLMs were less likely to initiate clarifications and follow-ups compared to humans, with early grounding failures predicting interaction breakdowns.", "conclusion": "Current LLMs need better training and prompting strategies to improve their grounding capabilities in conversations.", "key_contributions": ["Development of a taxonomy of grounding acts", "Introduction of the Rifts benchmark for assessing grounding issues", "Preliminary intervention proposed to mitigate grounding failures."], "limitations": "", "keywords": ["Human-Computer Interaction", "Language Models", "Grounding", "Conversational Agents", "Benchmarking"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2506.00344", "pdf": "https://arxiv.org/pdf/2506.00344.pdf", "abs": "https://arxiv.org/abs/2506.00344", "title": "Efficient Latent Semantic Clustering for Scaling Test-Time Computation of LLMs", "authors": ["Sungjae Lee", "Hoyoung Kim", "Jeongyeon Hwang", "Eunhyeok Park", "Jungseul Ok"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Scaling test-time computation--generating and analyzing multiple or\nsequential outputs for a single input--has become a promising strategy for\nimproving the reliability and quality of large language models (LLMs), as\nevidenced by advances in uncertainty quantification and multi-step reasoning. A\nkey shared component is semantic clustering, which groups outputs that differ\nin form but convey the same meaning. Semantic clustering enables estimation of\nthe distribution over the semantics of outputs and helps avoid redundant\nexploration of reasoning paths. However, existing approaches typically rely on\nexternal models, which introduce substantial computational overhead and often\nfail to capture context-aware semantics. We propose Latent Semantic Clustering\n(LSC), a lightweight and context-sensitive method that leverages the generator\nLLM's internal hidden states for clustering, eliminating the need for external\nmodels. Our extensive experiment across various LLMs and datasets shows that\nLSC significantly improves the computational efficiency of test-time scaling\nwhile maintaining or exceeding the performance of existing methods.", "AI": {"tldr": "Latent Semantic Clustering (LSC) improves the efficiency of test-time computation for LLMs by using internal hidden states for clustering, eliminating the need for external models.", "motivation": "To enhance the reliability and quality of LLMs through improved test-time computation by addressing the limitations of existing semantic clustering methods that rely on external models.", "method": "LSC employs the internal hidden states of the generator LLM for semantic clustering, which reduces computational overhead and captures context-aware semantics effectively.", "result": "LSC demonstrates significant improvement in computational efficiency during test-time scaling while maintaining or exceeding the performance levels of existing methods.", "conclusion": "LSC offers a lightweight solution to enhance the clustering process in LLM outputs without the downsides of external model reliance, paving the way for better resource utilization in LLM applications.", "key_contributions": ["Introduction of Latent Semantic Clustering (LSC) method for LLMs.", "Demonstration of enhanced computational efficiency during test-time scaling.", "Elimination of external model dependency for semantic clustering."], "limitations": "", "keywords": ["Latent Semantic Clustering", "large language models", "test-time computation", "semantic clustering", "context-sensitive methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00381", "pdf": "https://arxiv.org/pdf/2506.00381.pdf", "abs": "https://arxiv.org/abs/2506.00381", "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic Reconstruction of Continuous Language from Human Intracranial EEG", "authors": ["Siavash Shams", "Richard Antonello", "Gavin Mischler", "Stephan Bickel", "Ashesh Mehta", "Nima Mesgarani"], "categories": ["cs.CL", "eess.AS", "eess.SP"], "comment": "Accepted at Interspeech 2025 Code at\n  https://github.com/SiavashShams/neuro2semantic", "summary": "Decoding continuous language from neural signals remains a significant\nchallenge in the intersection of neuroscience and artificial intelligence. We\nintroduce Neuro2Semantic, a novel framework that reconstructs the semantic\ncontent of perceived speech from intracranial EEG (iEEG) recordings. Our\napproach consists of two phases: first, an LSTM-based adapter aligns neural\nsignals with pre-trained text embeddings; second, a corrector module generates\ncontinuous, natural text directly from these aligned embeddings. This flexible\nmethod overcomes the limitations of previous decoding approaches and enables\nunconstrained text generation. Neuro2Semantic achieves strong performance with\nas little as 30 minutes of neural data, outperforming a recent state-of-the-art\nmethod in low-data settings. These results highlight the potential for\npractical applications in brain-computer interfaces and neural decoding\ntechnologies.", "AI": {"tldr": "A novel framework, Neuro2Semantic, decodes semantic content from neural signals using an LSTM-based model for text generation from iEEG recordings.", "motivation": "Decoding language from neural signals is a significant challenge in neuroscience and AI, with potential applications in brain-computer interfaces.", "method": "The framework uses an LSTM-based adapter to align iEEG neural signals with pre-trained text embeddings, followed by a corrector module to generate continuous, natural text.", "result": "Neuro2Semantic outperforms a state-of-the-art method in low-data settings, achieving strong performance with only 30 minutes of neural data.", "conclusion": "This framework shows promise for practical applications in neural decoding technologies and brain-computer interfaces.", "key_contributions": ["Introduction of Neuro2Semantic framework for semantic decoding from neural signals", "Use of LSTM-based adapter for aligning neural signals with text embeddings", "Demonstration of strong performance in low-data scenarios"], "limitations": "", "keywords": ["neural signals", "language decoding", "brain-computer interfaces"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00386", "pdf": "https://arxiv.org/pdf/2506.00386.pdf", "abs": "https://arxiv.org/abs/2506.00386", "title": "Adaptive-VP: A Framework for LLM-Based Virtual Patients that Adapts to Trainees' Dialogue to Facilitate Nurse Communication Training", "authors": ["Keyeun Lee", "Seolhee Lee", "Esther Hehsun Kim", "Yena Ko", "Jinsu Eun", "Dahee Kim", "Hyewon Cho", "Haiyi Zhu", "Robert E. Kraut", "Eunyoung Suh", "Eun-mee Kim", "Hajin Lim"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025 Findings, 34 pages, 9 figures", "summary": "Effective communication training is essential to preparing nurses for\nhigh-quality patient care. While standardized patient (SP) simulations provide\nvaluable experiential learning, they are often costly and inflexible. Virtual\npatient (VP) systems offer a scalable alternative, but most fail to adapt to\nthe varying communication skills of trainees. In particular, when trainees\nrespond ineffectively, VPs should escalate in hostility or become\nuncooperative--yet this level of adaptive interaction remains largely\nunsupported. To address this gap, we introduce Adaptive-VP, a VP dialogue\ngeneration framework that leverages large language models (LLMs) to dynamically\nadapt VP behavior based on trainee input. The framework features a pipeline for\nconstructing clinically grounded yet flexible VP scenarios and a modular system\nfor assessing trainee communication and adjusting VP responses in real time,\nwhile ensuring learner safety. We validated Adaptive-VP by simulating\nchallenging patient conversations. Automated evaluation using a corpus from\npracticing nurses showed that our communication skill evaluation mechanism\nreflected real-world proficiency levels. Expert nurses further confirmed that\nAdaptive-VP produced more natural and realistic interactions than existing\napproaches, demonstrating its potential as a scalable and effective tool for\nnursing communication training.", "AI": {"tldr": "Adaptive-VP is a VP dialogue generation framework that utilizes LLMs to dynamically adjust patient interactions based on nursing trainees' communication skills for effective training.", "motivation": "To enhance nursing communication training by providing a scalable and adaptable solution that responds to trainees' varying skills, overcoming limitations of traditional standardized patient simulations.", "method": "The framework employs a pipeline to create flexible VP scenarios and a modular system to assess trainee communication and modify VP responses in real time.", "result": "Validation showed Adaptive-VP facilitates challenging patient conversations effectively, with automated evaluation correlating with real-world nursing proficiency levels and expert reviews affirming its realistic interactions.", "conclusion": "Adaptive-VP presents a promising tool for nursing education, offering adaptive communication training that scales and improves learning outcomes.", "key_contributions": ["Introduction of Adaptive-VP framework using LLMs for scalable communication training", "Real-time adjustment of VP responses based on trainee input", "Validation through expert evaluations demonstrating improved interaction realism"], "limitations": "", "keywords": ["virtual patient", "adaptive learning", "communication training", "large language models", "nursing education"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00391", "pdf": "https://arxiv.org/pdf/2506.00391.pdf", "abs": "https://arxiv.org/abs/2506.00391", "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for Text-to-SQL", "authors": ["Ge Qu", "Jinyang Li", "Bowen Qin", "Xiaolong Li", "Nan Huo", "Chenhao Ma", "Reynold Cheng"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main", "summary": "Current self-correction approaches in text-to-SQL face two critical\nlimitations: 1) Conventional self-correction methods rely on recursive\nself-calls of LLMs, resulting in multiplicative computational overhead, and 2)\nLLMs struggle to implement effective error detection and correction for\ndeclarative SQL queries, as they fail to demonstrate the underlying reasoning\npath. In this work, we propose SHARE, an SLM-based Hierarchical Action\ncorREction assistant that enables LLMs to perform more precise error\nlocalization and efficient correction. SHARE orchestrates three specialized\nSmall Language Models (SLMs) in a sequential pipeline, where it first\ntransforms declarative SQL queries into stepwise action trajectories that\nreveal underlying reasoning, followed by a two-phase granular refinement. We\nfurther propose a novel hierarchical self-evolution strategy for data-efficient\ntraining. Experimental results demonstrate that SHARE effectively enhances\nself-correction capabilities while proving robust across various LLMs.\nFurthermore, our comprehensive analysis shows that SHARE maintains strong\nperformance even in low-resource training settings, which is particularly\nvaluable for text-to-SQL applications with data privacy constraints.", "AI": {"tldr": "SHARE proposes a novel approach for self-correction in text-to-SQL applications, addressing computational overhead and error detection limitations of conventional LLM methods.", "motivation": "Current self-correction methods for text-to-SQL are inefficient and struggle with effective error detection, necessitating a new solution.", "method": "SHARE utilizes a sequential pipeline of three specialized Small Language Models (SLMs) to transform SQL queries into action trajectories, followed by two-phase refinement and a hierarchical self-evolution training strategy.", "result": "Experimental results show that SHARE significantly improves self-correction capabilities and performs robustly across various LLMs, even in low-resource settings.", "conclusion": "SHARE enhances error localization and correction, making it valuable for text-to-SQL applications, especially under data privacy constraints.", "key_contributions": ["Introduces SHARE, a novel hierarchical correction mechanism for text-to-SQL.", "Employs specialized SLMs for better error localization and correction.", "Demonstrates robust performance in low-resource training scenarios."], "limitations": "", "keywords": ["text-to-SQL", "self-correction", "Small Language Models", "error detection", "LLMs"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00396", "pdf": "https://arxiv.org/pdf/2506.00396.pdf", "abs": "https://arxiv.org/abs/2506.00396", "title": "Speculative Reward Model Boosts Decision Making Ability of LLMs Cost-Effectively", "authors": ["Jiawei Gu", "Shangsong Liang"], "categories": ["cs.CL"], "comment": "ACL2025 Oral (Industry Track)", "summary": "Effective decision-making in Large Language Models (LLMs) is essential for\nhandling intricate tasks. However, existing approaches prioritize performance\nbut often overlook the balance between effectiveness and computational cost. To\naddress this, we first introduce the 3E Criteria to systematically assess the\ncost-effectiveness of search strategies, revealing that existing methods often\ntrade significant efficiency for marginal performance gains. To improve LLM\ndecision-making while maintaining efficiency, we propose the Speculative Reward\nModel (SRM), a plug-and-play framework that seamlessly integrates with existing\nsearch strategies. Specifically, SRM employs an external reward assigner to\npredict optimal actions, reducing reliance on LLMs' internal self-evaluation.\nAnd a speculative verification mechanism is used to prune suboptimal choices\nand guide the search toward more promising steps. We evaluate SRM on several\ncomplex decision-making tasks including mathematical reasoning, planning and\nnumerical reasoning in specialized domains. Experimental results show that SRM\nreduces costs to 1/10 of the original search framework on average while\nmaintaining effectiveness.", "AI": {"tldr": "This paper introduces the Speculative Reward Model (SRM) to enhance decision-making efficiency in Large Language Models (LLMs) while balancing performance and computational cost.", "motivation": "To address the inefficiencies in existing decision-making strategies for LLMs which prioritize performance over cost-effectiveness.", "method": "The paper proposes the 3E Criteria to assess search strategies and introduces the SRM, which integrates an external reward assigner and a speculative verification mechanism to optimize LLM decision-making.", "result": "SRM reduces decision-making costs to 1/10 of the original search framework while maintaining effectiveness across various complex tasks.", "conclusion": "The proposed SRM significantly improves efficiency and effectiveness in LLM decision-making for complex tasks.", "key_contributions": ["Introduction of the 3E Criteria for assessing search strategies.", "Development of the Speculative Reward Model (SRM) for enhancing efficiency in decision-making.", "Validation of SRM's effectiveness in reducing decision-making costs across complex domains."], "limitations": "", "keywords": ["Large Language Models", "Decision-Making", "Cost-Effectiveness", "Speculative Reward Model", "Search Strategies"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00400", "pdf": "https://arxiv.org/pdf/2506.00400.pdf", "abs": "https://arxiv.org/abs/2506.00400", "title": "Scaling Textual Gradients via Sampling-Based Momentum", "authors": ["Zixin Ding", "Junyuan Hong", "Jiachen T. Wang", "Zinan Lin", "Zhangyang Wang", "Yuxin Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As prompts play an increasingly critical role in large language models\n(LLMs), optimizing textual prompts has become a crucial challenge. The Textual\nGradient Descent (TGD) framework has emerged as a promising data-driven\napproach that iteratively refines textual prompts using LLM - suggested updates\n(or textual gradients) over minibatches of training samples. In this paper, we\nempirically demonstrate that scaling the number of training examples initially\nimproves but later degrades TGD's performance across multiple downstream NLP\ntasks. However, while data scaling improves results for most tasks, it also\nsignificantly increases the computational cost when leveraging LLMs. To address\nthis, we draw inspiration from numerical gradient descent and propose Textual\nStochastic Gradient Descent with Momentum (TSGD-M) - a method that facilitates\nscalable in-context learning by reweighting prompt sampling based on past batch\ndistributions. Across nine NLP tasks spanning three domains - including\nBIG-Bench Hard (BBH), natural language understanding tasks, and reasoning tasks\n- TSGD-M significantly outperforms TGD baselines that do not incorporate\nreweighted sampling, while also reducing variance in most tasks.", "AI": {"tldr": "The paper proposes Textual Stochastic Gradient Descent with Momentum (TSGD-M) to optimize prompt refinement processes in LLMs, addressing the trade-off between data scaling and computational cost.", "motivation": "As large language models (LLMs) become dominant in NLP applications, the optimization of textual prompts is vital for improving model performance.", "method": "The authors introduce the TSGD-M framework which reweights prompt sampling based on previous batch distributions, inspired by numerical gradient descent techniques.", "result": "TSGD-M significantly outperforms the baseline TGD methods across nine NLP tasks while also reducing computational variance compared to TGD.", "conclusion": "The proposed TSGD-M method allows for more scalable in-context learning, offering improved efficiency and effectiveness over previous approaches.", "key_contributions": ["Introduction of Textual Stochastic Gradient Descent with Momentum (TSGD-M)", "Empirical demonstration of performance trade-offs with data scaling", "Significant performance improvements across multiple NLP tasks with reduced variance"], "limitations": "The study does not explore the impact of TSGD-M on tasks outside the selected nine, nor does it fully address computational limits of LLMs when using large datasets.", "keywords": ["Textual Gradient Descent", "large language models", "prompt optimization", "NLP tasks", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00402", "pdf": "https://arxiv.org/pdf/2506.00402.pdf", "abs": "https://arxiv.org/abs/2506.00402", "title": "Causal Structure Discovery for Error Diagnostics of Children's ASR", "authors": ["Vishwanath Pratap Singh", "Md. Sahidullah", "Tomi Kinnunen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Children's automatic speech recognition (ASR) often underperforms compared to\nthat of adults due to a confluence of interdependent factors: physiological\n(e.g., smaller vocal tracts), cognitive (e.g., underdeveloped pronunciation),\nand extrinsic (e.g., vocabulary limitations, background noise). Existing\nanalysis methods examine the impact of these factors in isolation, neglecting\ninterdependencies-such as age affecting ASR accuracy both directly and\nindirectly via pronunciation skills. In this paper, we introduce a causal\nstructure discovery to unravel these interdependent relationships among\nphysiology, cognition, extrinsic factors, and ASR errors. Then, we employ\ncausal quantification to measure each factor's impact on children's ASR. We\nextend the analysis to fine-tuned models to identify which factors are\nmitigated by fine-tuning and which remain largely unaffected. Experiments on\nWhisper and Wav2Vec2.0 demonstrate the generalizability of our findings across\ndifferent ASR systems.", "AI": {"tldr": "This paper explores the interdependent factors affecting children's automatic speech recognition (ASR) performance, introducing a causal structure discovery method and quantifying the impact of various factors.", "motivation": "Children's ASR performance lags behind adults due to physiological, cognitive, and extrinsic factors, which are often analyzed in isolation rather than collectively.", "method": "Causal structure discovery is employed to understand the interdependencies among physiological, cognitive, and extrinsic factors influencing ASR errors. Causal quantification measures the impact of these factors, and the analysis is extended to fine-tuned models to assess the influence of fine-tuning on these factors.", "result": "Experiments with ASR models such as Whisper and Wav2Vec2.0 show the generalizability of the findings, highlighting which factors are mitigated through fine-tuning and which persist.", "conclusion": "Understanding the interdependencies of different factors provides insights into improving children's ASR performance, suggesting targeted approaches for model fine-tuning.", "key_contributions": ["Introduction of causal structure discovery in analyzing ASR factors", "Quantification of interdependent impacts on ASR errors", "Generalizability assessment across multiple ASR systems"], "limitations": "", "keywords": ["automatic speech recognition", "children", "causal analysis", "fine-tuning", "speech performance"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.00413", "pdf": "https://arxiv.org/pdf/2506.00413.pdf", "abs": "https://arxiv.org/abs/2506.00413", "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding", "authors": ["Daniel Israel", "Guy Van den Broeck", "Aditya Grover"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": "10 pages, 5 figures", "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.", "AI": {"tldr": "This paper presents adaptive parallel decoding (APD) for improving the generation speed of large language models (LLMs) by dynamically adjusting token sampling in parallel, achieving higher throughput with minimal quality loss.", "motivation": "The bottleneck in generation speed of LLMs due to autoregressive decoding leads to a need for methods that can enhance performance without sacrificing quality.", "method": "APD introduces a mixture approach that combines dLLM marginal probabilities with the joint probability from a small autoregressive model, optimizing through key parameters and techniques such as KV caching.", "result": "APD significantly improves throughput on downstream tasks while maintaining nearly the same level of quality as autoregressive models.", "conclusion": "APD offers a flexible solution for balancing throughput and quality in token generation for LLMs, with three adjustable parameters to optimize performance.", "key_contributions": ["Introduction of adaptive parallel decoding (APD) for token generation.", "Demonstration of improved throughput with minimal quality loss.", "Optimal tradeoff parameters for performance tuning."], "limitations": "", "keywords": ["adaptive parallel decoding", "large language models", "throughput", "autoregressive models", "diffusion models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00418", "pdf": "https://arxiv.org/pdf/2506.00418.pdf", "abs": "https://arxiv.org/abs/2506.00418", "title": "Dual Debiasing for Noisy In-Context Learning for Text Generation", "authors": ["Siqi Liang", "Sumyeong Ahn", "Paramveer S. Dhillon", "Jiayu Zhou"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted by 2025 ACL Findings", "summary": "In context learning (ICL) relies heavily on high quality demonstrations drawn\nfrom large annotated corpora. Existing approaches detect noisy annotations by\nranking local perplexities, presuming that noisy samples yield higher\nperplexities than their clean counterparts. However, this assumption breaks\ndown when the noise ratio is high and many demonstrations are flawed. We\nreexamine the perplexity based paradigm for text generation under noisy\nannotations, highlighting two sources of bias in perplexity: the annotation\nitself and the domain specific knowledge inherent in large language models\n(LLMs). To overcome these biases, we introduce a dual debiasing framework that\nuses synthesized neighbors to explicitly correct perplexity estimates, yielding\na robust Sample Cleanliness Score. This metric uncovers absolute sample\ncleanliness regardless of the overall corpus noise level. Extensive experiments\ndemonstrate our method's superior noise detection capabilities and show that\nits final ICL performance is comparable to that of a fully clean demonstration\ncorpus. Moreover, our approach remains robust even when noise ratios are\nextremely high.", "AI": {"tldr": "The paper introduces a dual debiasing framework to improve noise detection in context learning by correcting perplexity estimates, yielding a robust Sample Cleanliness Score.", "motivation": "To address the limitations of existing methods in detecting noisy annotations in context learning due to the breakdown of assumptions related to perplexity in the presence of high noise ratios.", "method": "The authors propose a dual debiasing framework that uses synthesized neighbors to correct perplexity estimates, leading to the development of a Sample Cleanliness Score.", "result": "Extensive experiments show that the proposed method offers superior noise detection capabilities and allows for ICL performance comparable to that of a clean demonstration corpus, even under high noise conditions.", "conclusion": "The dual debiasing framework and the Sample Cleanliness Score provide a reliable mechanism for uncovering absolute sample cleanliness in noisy annotated corpora.", "key_contributions": ["Introduction of a dual debiasing framework for noise detection", "Development of a robust Sample Cleanliness Score", "Demonstrated superior noise detection capabilities compared to existing methods"], "limitations": "", "keywords": ["context learning", "noise detection", "perplexity", "sample cleanliness", "language models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00421", "pdf": "https://arxiv.org/pdf/2506.00421.pdf", "abs": "https://arxiv.org/abs/2506.00421", "title": "Enabling Chatbots with Eyes and Ears: An Immersive Multimodal Conversation System for Dynamic Interactions", "authors": ["Jihyoung Jang", "Minwook Bae", "Minji Kim", "Dilek Hakkani-Tur", "Hyounghun Kim"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 (32 pages); Project website: https://m3c-dataset.github.io/", "summary": "As chatbots continue to evolve toward human-like, real-world, interactions,\nmultimodality remains an active area of research and exploration. So far,\nefforts to integrate multimodality into chatbots have primarily focused on\nimage-centric tasks, such as visual dialogue and image-based instructions,\nplacing emphasis on the \"eyes\" of human perception while neglecting the \"ears\",\nnamely auditory aspects. Moreover, these studies often center around static\ninteractions that focus on discussing the modality rather than naturally\nincorporating it into the conversation, which limits the richness of\nsimultaneous, dynamic engagement. Furthermore, while multimodality has been\nexplored in multi-party and multi-session conversations, task-specific\nconstraints have hindered its seamless integration into dynamic, natural\nconversations. To address these challenges, this study aims to equip chatbots\nwith \"eyes and ears\" capable of more immersive interactions with humans. As\npart of this effort, we introduce a new multimodal conversation dataset,\nMultimodal Multi-Session Multi-Party Conversation ($M^3C$), and propose a novel\nmultimodal conversation model featuring multimodal memory retrieval. Our model,\ntrained on the $M^3C$, demonstrates the ability to seamlessly engage in\nlong-term conversations with multiple speakers in complex, real-world-like\nsettings, effectively processing visual and auditory inputs to understand and\nrespond appropriately. Human evaluations highlight the model's strong\nperformance in maintaining coherent and dynamic interactions, demonstrating its\npotential for advanced multimodal conversational agents.", "AI": {"tldr": "This study presents a new multimodal conversation model that incorporates both visual and auditory inputs for dynamic human-like interactions in chatbots.", "motivation": "To enhance the integration of multimodality in chatbots by considering both visual and auditory aspects for more immersive interactions.", "method": "A novel multimodal conversation model is proposed, trained on a new dataset called Multimodal Multi-Session Multi-Party Conversation ($M^3C$), which allows for processing of dynamic, real-world-like conversations with multiple speakers.", "result": "The model shows strong performance in maintaining coherent and dynamic interactions during multimodal conversations, effectively processing visual and auditory inputs.", "conclusion": "The study demonstrates the potential for advanced multimodal conversational agents capable of realistic, immersive interactions with humans.", "key_contributions": ["Introduction of the $M^3C$ dataset for multimodal conversations", "Development of a novel multimodal conversation model with memory retrieval capabilities", "Demonstrated strong performance in dynamic human-like interactions using visual and auditory inputs."], "limitations": "", "keywords": ["multimodal conversation", "chatbots", "auditory interactions", "visual inputs", "dynamic engagement"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00422", "pdf": "https://arxiv.org/pdf/2506.00422.pdf", "abs": "https://arxiv.org/abs/2506.00422", "title": "DYNAC: Dynamic Vocabulary based Non-Autoregressive Contextualization for Speech Recognition", "authors": ["Yui Sudo", "Yosuke Fukumoto", "Muhammad Shakeel", "Yifan Peng", "Chyi-Jiunn Lin", "Shinji Watanabe"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Contextual biasing (CB) improves automatic speech recognition for rare and\nunseen phrases. Recent studies have introduced dynamic vocabulary, which\nrepresents context phrases as expandable tokens in autoregressive (AR) models.\nThis method improves CB accuracy but with slow inference speed. While dynamic\nvocabulary can be applied to non-autoregressive (NAR) models, such as\nconnectionist temporal classification (CTC), the conditional independence\nassumption fails to capture dependencies between static and dynamic tokens.\nThis paper proposes DYNAC (Dynamic Vocabulary-based NAR Contextualization), a\nself-conditioned CTC method that integrates dynamic vocabulary into\nintermediate layers. Conditioning the encoder on dynamic vocabulary, DYNAC\neffectively captures dependencies between static and dynamic tokens while\nreducing the real-time factor (RTF). Experimental results show that DYNAC\nreduces RTF by 81% with a 0.1-point degradation in word error rate on the\nLibriSpeech 960 test-clean set.", "AI": {"tldr": "DYNAC is a self-conditioned CTC method that enhances contextual biasing in non-autoregressive models by integrating dynamic vocabulary, resulting in significantly improved inference speed.", "motivation": "To improve automatic speech recognition for rare and unseen phrases and to address the slow inference speed of dynamic vocabulary in autoregressive models.", "method": "DYNAC proposes a self-conditioned CTC method that integrates dynamic vocabulary into intermediate layers of the model, allowing the encoder to effectively capture dependencies between static and dynamic tokens.", "result": "DYNAC achieves an 81% reduction in real-time factor with only a 0.1-point degradation in word error rate on the LibriSpeech 960 test-clean set.", "conclusion": "The proposed DYNAC method enhances the performance of non-autoregressive models for speech recognition tasks by effectively utilizing dynamic vocabulary, achieving faster inference without significantly compromising accuracy.", "key_contributions": ["Introduction of DYNAC for integrating dynamic vocabulary in non-autoregressive models", "Demonstration of significant improvement in inference speed", "Reduction in real-time factor while maintaining accuracy in speech recognition tasks"], "limitations": "", "keywords": ["speech recognition", "dynamic vocabulary", "non-autoregressive models", "conditional temporal classification", "contextual biasing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.00425", "pdf": "https://arxiv.org/pdf/2506.00425.pdf", "abs": "https://arxiv.org/abs/2506.00425", "title": "Inter-Passage Verification for Multi-evidence Multi-answer QA", "authors": ["Bingsen Chen", "Shengjie Wang", "Xi Ye", "Chen Zhao"], "categories": ["cs.CL"], "comment": "19 pages, 6 figures, to appear in ACL 2025 Findings", "summary": "Multi-answer question answering (QA), where questions can have many valid\nanswers, presents a significant challenge for existing retrieval-augmented\ngeneration-based QA systems, as these systems struggle to retrieve and then\nsynthesize a large number of evidence passages. To tackle these challenges, we\npropose a new multi-answer QA framework -- Retrieval-augmented Independent\nReading with Inter-passage Verification (RI$^2$VER). Our framework retrieves a\nlarge set of passages and processes each passage individually to generate an\ninitial high-recall but noisy answer set. Then we propose a new inter-passage\nverification pipeline that validates every candidate answer through (1)\nVerification Question Generation, (2) Gathering Additional Evidence, and (3)\nVerification with inter-passage synthesis. Evaluations on the QAMPARI and RoMQA\ndatasets demonstrate that our framework significantly outperforms existing\nbaselines across various model sizes, achieving an average F1 score improvement\nof 11.17%. Further analysis validates that our inter-passage verification\npipeline enables our framework to be particularly beneficial for questions\nrequiring multi-evidence synthesis.", "AI": {"tldr": "The paper presents RI²VER, a multi-answer QA framework addressing challenges in retrieving and synthesizing multiple evidence passages to improve answer accuracy.", "motivation": "Current retrieval-augmented QA systems struggle with generating multi-answer responses due to difficulties in managing and synthesizing numerous evidence passages.", "method": "The proposed framework retrieves multiple passages and initially generates a noisy answer set which is refined through an inter-passage verification process involving question generation, evidence gathering, and synthesis verification.", "result": "RI²VER shows significant performance improvements over existing methods, with an average F1 score increase of 11.17% on the QAMPARI and RoMQA datasets.", "conclusion": "The inter-passage verification approach enhances the framework's ability to synthesize multiple pieces of evidence effectively, making it valuable for complex multi-answer questions.", "key_contributions": ["Introduction of the RI²VER framework for multi-answer QA.", "Development of an inter-passage verification pipeline to enhance answer accuracy.", "Empirical validation showing significant performance gains over existing baselines."], "limitations": "The framework may depend heavily on the quality of retrieved passages and could struggle with overly complex queries beyond its designed scope.", "keywords": ["Multi-answer QA", "Inter-passage Verification", "Retrieval-augmented generation", "Evidence synthesis", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00445", "pdf": "https://arxiv.org/pdf/2506.00445.pdf", "abs": "https://arxiv.org/abs/2506.00445", "title": "G2S: A General-to-Specific Learning Framework for Temporal Knowledge Graph Forecasting with Large Language Models", "authors": ["Long Bai", "Zixuan Li", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng", "Tat-Seng Chua"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Forecasting over Temporal Knowledge Graphs (TKGs) which predicts future facts\nbased on historical ones has received much attention. Recent studies have\nintroduced Large Language Models (LLMs) for this task to enhance the models'\ngeneralization abilities. However, these models perform forecasting via\nsimultaneously learning two kinds of entangled knowledge in the TKG: (1)\ngeneral patterns, i.e., invariant temporal structures shared across different\nscenarios; and (2) scenario information, i.e., factual knowledge engaged in\nspecific scenario, such as entities and relations. As a result, the learning\nprocesses of these two kinds of knowledge may interfere with each other, which\npotentially impact the generalization abilities of the models. To enhance the\ngeneralization ability of LLMs on this task, in this paper, we propose a\nGeneral-to-Specific learning framework (G2S) that disentangles the learning\nprocesses of the above two kinds of knowledge. In the general learning stage,\nwe mask the scenario information in different TKGs and convert it into\nanonymous temporal structures. After training on these structures, the model is\nable to capture the general patterns across different TKGs. In the specific\nlearning stage, we inject the scenario information into the structures via\neither in-context learning or fine-tuning modes. Experimental results show that\nG2S effectively improves the generalization abilities of LLMs.", "AI": {"tldr": "This paper proposes a General-to-Specific learning framework (G2S) for forecasting over Temporal Knowledge Graphs (TKGs) that enhances the generalization abilities of Large Language Models (LLMs) by disentangling general patterns and scenario information.", "motivation": "To enhance the forecasting capabilities of LLMs on Temporal Knowledge Graphs (TKGs) by improving model generalization through a novel learning framework.", "method": "The proposed G2S framework consists of two stages: general learning, where scenario information is masked to focus on capturing general patterns, and specific learning, where scenario information is reintroduced to refine predictions.", "result": "Experimental results demonstrate that G2S significantly improves the generalization abilities of LLMs in forecasting tasks over TKGs.", "conclusion": "The G2S framework successfully disentangles general and specific knowledge, leading to better performance in TKG forecasts.", "key_contributions": ["Introduction of the G2S framework for TKG forecasting", "Disentanglement of general patterns and scenario-specific information", "Demonstration of improved generalization capabilities of LLMs"], "limitations": "", "keywords": ["Temporal Knowledge Graphs", "Large Language Models", "Forecasting", "General-to-Specific Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00448", "pdf": "https://arxiv.org/pdf/2506.00448.pdf", "abs": "https://arxiv.org/abs/2506.00448", "title": "Fact-Controlled Diagnosis of Hallucinations in Medical Text Summarization", "authors": ["Suhas BN", "Han-Chin Shing", "Lei Xu", "Mitch Strong", "Jon Burnsky", "Jessica Ofor", "Jordan R. Mason", "Susan Chen", "Sundararajan Srinivasan", "Chaitanya Shivade", "Jack Moriarty", "Joseph Paul Cohen"], "categories": ["cs.CL"], "comment": "https://github.com/amazon-science/acibench-hallucination-annotations", "summary": "Hallucinations in large language models (LLMs) during summarization of\npatient-clinician dialogues pose significant risks to patient care and clinical\ndecision-making. However, the phenomenon remains understudied in the clinical\ndomain, with uncertainty surrounding the applicability of general-domain\nhallucination detectors. The rarity and randomness of hallucinations further\ncomplicate their investigation. In this paper, we conduct an evaluation of\nhallucination detection methods in the medical domain, and construct two\ndatasets for the purpose: A fact-controlled Leave-N-out dataset -- generated by\nsystematically removing facts from source dialogues to induce hallucinated\ncontent in summaries; and a natural hallucination dataset -- arising\norganically during LLM-based medical summarization. We show that general-domain\ndetectors struggle to detect clinical hallucinations, and that performance on\nfact-controlled hallucinations does not reliably predict effectiveness on\nnatural hallucinations. We then develop fact-based approaches that count\nhallucinations, offering explainability not available with existing methods.\nNotably, our LLM-based detectors, which we developed using fact-controlled\nhallucinations, generalize well to detecting real-world clinical\nhallucinations. This research contributes a suite of specialized metrics\nsupported by expert-annotated datasets to advance faithful clinical\nsummarization systems.", "AI": {"tldr": "This paper evaluates hallucination detection methods specifically in the medical domain, showing that existing general-domain detectors are ineffective and presenting new fact-based approaches to improve detection accuracy in clinical summarization by using specialized metrics and expert-annotated datasets.", "motivation": "To address the risks posed by hallucinations in large language models (LLMs) during patient-clinician dialogue summarization, which can affect patient care and clinical decision-making.", "method": "The authors constructed two datasets for their analysis: one induced hallucinations by removing facts from source dialogues (fact-controlled) and another that included naturally occurring hallucinations during LLM summarization. They evaluated existing detection methods and developed their own fact-based approaches.", "result": "The analysis revealed that general-domain hallucination detectors perform poorly in detecting clinical hallucinations, and the performance on fact-controlled scenarios does not accurately predict efficacy on natural scenarios. Their new LLM-based detectors showed good generalization to real-world clinical hallucinations.", "conclusion": "The research provides a framework for improving the detection of hallucinations in clinical summarization, enhancing the reliability of LLMs in medical applications through specialized metrics and datasets.", "key_contributions": ["Evaluation of hallucination detection methods in the medical domain", "Creation of two specific datasets for studying hallucinations", "Development of explainable, fact-based hallucination detection approaches"], "limitations": "The rarity and randomness of hallucinations can complicate comprehensive investigations, and the generalization of findings to all clinical contexts may require further validation.", "keywords": ["hallucinations", "large language models", "medical summarization", "detection methods", "fact-based approaches"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00469", "pdf": "https://arxiv.org/pdf/2506.00469.pdf", "abs": "https://arxiv.org/abs/2506.00469", "title": "Massively Multilingual Adaptation of Large Language Models Using Bilingual Translation Data", "authors": ["Shaoxiong Ji", "Zihao Li", "Jaakko Paavola", "Indraneil Paul", "Hengyu Luo", "Jörg Tiedemann"], "categories": ["cs.CL"], "comment": "EMMA-500 Gen 2; refer to Gen 1 in arXiv:2409.17892", "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.", "AI": {"tldr": "This paper examines the role of bilingual translation data in the multilingual continual pre-training of the Llama3 model family, introducing the MaLA corpus and the EMMA-500 model suite which enhance performance, especially for low-resource languages.", "motivation": "To understand the impact of bilingual translation data on the performance of massively multilingual models during continual pre-training.", "method": "Constructed the MaLA bilingual translation corpus with over 2,500 language pairs, developed the EMMA-500 model suite from Llama 3 models pre-trained on diverse data mixes.", "result": "Bilingual data significantly improves language transfer and model performance, notably for low-resource languages, demonstrated through evaluations on 7 tasks and 12 benchmarks.", "conclusion": "The findings support the inclusion of bilingual translation data in multilingual language adaptations and highlight the potential for improved models, with access to open-sourced resources for future research.", "key_contributions": ["Introduction of the MaLA bilingual translation corpus.", "Development of the EMMA-500 model suite for 500 languages.", "Demonstration of enhanced performance using bilingual data during continual pre-training."], "limitations": "", "keywords": ["multilingual pre-training", "bilingual data", "language adaptation", "Llama 3", "low-resource languages"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.00479", "pdf": "https://arxiv.org/pdf/2506.00479.pdf", "abs": "https://arxiv.org/abs/2506.00479", "title": "EffiVLM-BENCH: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Vision-Language Models", "authors": ["Zekun Wang", "Minghua Ma", "Zexin Wang", "Rongchuan Mu", "Liping Shan", "Ming Liu", "Bing Qin"], "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": "ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have achieved remarkable success, yet\ntheir significant computational demands hinder practical deployment. While\nefforts to improve LVLM efficiency are growing, existing methods lack\ncomprehensive evaluation across diverse backbones, benchmarks, and metrics. In\nthis work, we systematically evaluate mainstream acceleration techniques for\nLVLMs, categorized into token and parameter compression. We introduce\nEffiVLM-Bench, a unified framework for assessing not only absolute performance\nbut also generalization and loyalty, while exploring Pareto-optimal trade-offs.\nOur extensive experiments and in-depth analyses offer insights into optimal\nstrategies for accelerating LVLMs. We open-source code and recipes for\nEffiVLM-Bench to foster future research.", "AI": {"tldr": "This paper evaluates acceleration techniques for Large Vision-Language Models (LVLMs) and introduces EffiVLM-Bench, a framework for performance assessment.", "motivation": "The study addresses the high computational demands of LVLMs which impede their deployment in practical applications, highlighting the need for efficient evaluation methodologies.", "method": "The authors systematically evaluate various acceleration techniques for LVLMs, focusing on token and parameter compression, and introduce a framework called EffiVLM-Bench for performance assessment.", "result": "The comprehensive evaluations provide insights into optimal strategies for accelerating LVLMs, revealing trade-offs in performance across different benchmarks and metrics.", "conclusion": "The paper concludes with the recognition of significant findings in acceleration techniques and makes EffiVLM-Bench available as open-source to promote further research.", "key_contributions": ["Systematic evaluation of acceleration techniques for LVLMs.", "Introduction of EffiVLM-Bench as a comprehensive assessment framework.", "Insights on performance trade-offs in LVLMs under various configurations."], "limitations": "", "keywords": ["Large Vision-Language Models", "acceleration techniques", "performance evaluation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.00481", "pdf": "https://arxiv.org/pdf/2506.00481.pdf", "abs": "https://arxiv.org/abs/2506.00481", "title": "PVP: An Image Dataset for Personalized Visual Persuasion with Persuasion Strategies, Viewer Characteristics, and Persuasiveness Ratings", "authors": ["Junseo Kim", "Jongwook Han", "Dongmin Choi", "Jongwook Yoon", "Eun-Ju Lee", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main. Code and dataset are released at:\n  https://github.com/holi-lab/PVP_Personalized_Visual_Persuasion", "summary": "Visual persuasion, which uses visual elements to influence cognition and\nbehaviors, is crucial in fields such as advertising and political\ncommunication. With recent advancements in artificial intelligence, there is\ngrowing potential to develop persuasive systems that automatically generate\npersuasive images tailored to individuals. However, a significant bottleneck in\nthis area is the lack of comprehensive datasets that connect the persuasiveness\nof images with the personal information about those who evaluated the images.\nTo address this gap and facilitate technological advancements in personalized\nvisual persuasion, we release the Personalized Visual Persuasion (PVP) dataset,\ncomprising 28,454 persuasive images across 596 messages and 9 persuasion\nstrategies. Importantly, the PVP dataset provides persuasiveness scores of\nimages evaluated by 2,521 human annotators, along with their demographic and\npsychological characteristics (personality traits and values). We demonstrate\nthe utility of our dataset by developing a persuasive image generator and an\nautomated evaluator, and establish benchmark baselines. Our experiments reveal\nthat incorporating psychological characteristics enhances the generation and\nevaluation of persuasive images, providing valuable insights for personalized\nvisual persuasion.", "AI": {"tldr": "This paper introduces the Personalized Visual Persuasion (PVP) dataset containing 28,454 persuasive images and demonstrates its utility in generating personalized persuasive images using AI.", "motivation": "There is a growing demand for persuasive systems that can generate tailored images, but existing datasets linking image persuasiveness with evaluator characteristics are lacking.", "method": "The authors released the PVP dataset with images evaluated by human annotators, creating a persuasive image generator and automated evaluator to test its application.", "result": "Experiments show that using psychological characteristics significantly improves the generation and evaluation of persuasive images.", "conclusion": "The PVP dataset facilitates advancements in personalized visual persuasion and provides insights for further research in this area.", "key_contributions": ["Release of the Personalized Visual Persuasion (PVP) dataset with comprehensive data on persuasive images.", "Development of a persuasive image generator and automated evaluator.", "Establishment of benchmark baselines for evaluating persuasive images."], "limitations": "", "keywords": ["Visual Persuasion", "Dataset", "Artificial Intelligence", "Personalization", "Psychological Characteristics"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00483", "pdf": "https://arxiv.org/pdf/2506.00483.pdf", "abs": "https://arxiv.org/abs/2506.00483", "title": "Auto-Patching: Enhancing Multi-Hop Reasoning in Language Models", "authors": ["Aviv Jan", "Dean Tahory", "Omer Talmi", "Omar Abo Mokh"], "categories": ["cs.CL", "cs.LG"], "comment": "8 pages, 5 figures", "summary": "Multi-hop questions still stump large language models (LLMs), which struggle\nto link information across multiple reasoning steps. We introduce Auto-Patch, a\nnovel method that dynamically patches hidden states during inference to enhance\nmulti-hop reasoning in LLMs. Building on the PatchScopes framework, Auto-Patch\nselectively modifies internal representations using a learned classifier.\nEvaluated on the MuSiQue dataset, Auto-Patch improves the solve rate from\n18.45\\% (baseline) to 23.63~$\\pm$~0.7\\% (3 runs), narrowing the gap to\nChain-of-Thought prompting (27.44\\%). Our results highlight the potential of\ndynamic hidden state interventions for advancing complex reasoning in LLMs.", "AI": {"tldr": "Auto-Patch dynamically modifies hidden states in LLMs to enhance multi-hop reasoning, improving performance on the MuSiQue dataset.", "motivation": "Large language models struggle with multi-hop questions that require linking information through several reasoning steps.", "method": "The Auto-Patch method employs a learned classifier to selectively modify internal representations of LLMs during inference.", "result": "Auto-Patch improves the solve rate from a baseline of 18.45% to 23.63%, showing significant progress towards the Chain-of-Thought prompting baseline of 27.44%.", "conclusion": "Dynamic interventions in hidden states are promising for improving complex reasoning capabilities in large language models.", "key_contributions": ["Introduction of Auto-Patch for enhancing multi-hop reasoning in LLMs", "Improvement of solve rates on the MuSiQue dataset", "Demonstration of dynamic hidden state interventions' potential in LLMs."], "limitations": "", "keywords": ["large language models", "multi-hop reasoning", "hidden state intervention"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2506.00488", "pdf": "https://arxiv.org/pdf/2506.00488.pdf", "abs": "https://arxiv.org/abs/2506.00488", "title": "Synergizing LLMs with Global Label Propagation for Multimodal Fake News Detection", "authors": ["Shuguo Hu", "Jun Hu", "Huaiwen Zhang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) can assist multimodal fake news detection by\npredicting pseudo labels. However, LLM-generated pseudo labels alone\ndemonstrate poor performance compared to traditional detection methods, making\ntheir effective integration non-trivial. In this paper, we propose Global Label\nPropagation Network with LLM-based Pseudo Labeling (GLPN-LLM) for multimodal\nfake news detection, which integrates LLM capabilities via label propagation\ntechniques. The global label propagation can utilize LLM-generated pseudo\nlabels, enhancing prediction accuracy by propagating label information among\nall samples. For label propagation, a mask-based mechanism is designed to\nprevent label leakage during training by ensuring that training nodes do not\npropagate their own labels back to themselves. Experimental results on\nbenchmark datasets show that by synergizing LLMs with label propagation, our\nmodel achieves superior performance over state-of-the-art baselines.", "AI": {"tldr": "This paper introduces GLPN-LLM, a method for multimodal fake news detection that uses LLM-generated pseudo labels and integrates them through a global label propagation network.", "motivation": "To improve fake news detection by effectively integrating LLM-generated pseudo labels, which traditionally perform poorly on their own compared to traditional methods.", "method": "The paper proposes a Global Label Propagation Network with LLM-based Pseudo Labeling (GLPN-LLM) that uses label propagation techniques to enhance prediction accuracy by utilizing pseudo labels generated by LLMs.", "result": "The experimental results on benchmark datasets demonstrate that the GLPN-LLM model achieves better performance compared to existing state-of-the-art detection methods.", "conclusion": "By synergizing LLMs with label propagation techniques, the GLPN-LLM model significantly enhances the accuracy of multimodal fake news detection.", "key_contributions": ["Introduction of a novel GLPN-LLM framework for fake news detection.", "Integration of LLMs via label propagation techniques.", "Design of a mask-based mechanism to prevent label leakage."], "limitations": "", "keywords": ["Large Language Models", "fake news detection", "label propagation", "multimodal learning", "pseudo labeling"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00507", "pdf": "https://arxiv.org/pdf/2506.00507.pdf", "abs": "https://arxiv.org/abs/2506.00507", "title": "Exploring In-context Example Generation for Machine Translation", "authors": ["Dohyun Lee", "Seungil Chad Lee", "Chanwoo Yang", "Yujin Baek", "Jaegul Choo"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have demonstrated strong performance across\nvarious tasks, leveraging their exceptional in-context learning ability with\nonly a few examples. Accordingly, the selection of optimal in-context examples\nhas been actively studied in the field of machine translation. However, these\nstudies presuppose the presence of a demonstration pool with human-annotated\npairs, making them less applicable to low-resource languages where such an\nassumption is challenging to meet. To overcome this limitation, this paper\nexplores the research direction of in-context example generation for machine\ntranslation. Specifically, we propose Demonstration Augmentation for\nTranslation (DAT), a simple yet effective approach that generates example pairs\nwithout relying on any external resources. This method builds upon two prior\ncriteria, relevance and diversity, which have been highlighted in previous work\nas key factors for in-context example selection. Through experiments and\nanalysis on low-resource languages where human-annotated pairs are scarce, we\nshow that DAT achieves superior translation quality compared to the baselines.\nFurthermore, we investigate the potential of progressively accumulating\ngenerated pairs during test time to build and reuse a demonstration pool. Our\nimplementation is publicly available at https://github.com/aiclaudev/DAT.", "AI": {"tldr": "This paper proposes Demonstration Augmentation for Translation (DAT), a method for generating in-context examples for machine translation without external resources, particularly targeting low-resource languages.", "motivation": "The paper addresses the challenge of selecting optimal in-context examples for machine translation in low-resource languages where human-annotated pairs are scarce.", "method": "DAT generates example pairs based on criteria of relevance and diversity, and it explores progressively accumulating generated pairs during test time to enhance translation performance.", "result": "Experiments show that DAT yields superior translation quality compared to baselines in low-resource settings.", "conclusion": "The research demonstrates that effectively generating in-context examples can significantly improve machine translation outcomes for low-resource languages.", "key_contributions": ["Introduction of DAT for generating in-context examples without external resources.", "Demonstration of improved translation performance in low-resource languages.", "Exploration of accumulating generated pairs during test time to optimize translation quality."], "limitations": "", "keywords": ["machine translation", "large language models", "in-context learning", "low-resource languages", "demonstration augmentation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00509", "pdf": "https://arxiv.org/pdf/2506.00509.pdf", "abs": "https://arxiv.org/abs/2506.00509", "title": "Goal-Aware Identification and Rectification of Misinformation in Multi-Agent Systems", "authors": ["Zherui Li", "Yan Mi", "Zhenhong Zhou", "Houcheng Jiang", "Guibin Zhang", "Kun Wang", "Junfeng Fang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model-based Multi-Agent Systems (MASs) have demonstrated\nstrong advantages in addressing complex real-world tasks. However, due to the\nintroduction of additional attack surfaces, MASs are particularly vulnerable to\nmisinformation injection. To facilitate a deeper understanding of\nmisinformation propagation dynamics within these systems, we introduce\nMisinfoTask, a novel dataset featuring complex, realistic tasks designed to\nevaluate MAS robustness against such threats. Building upon this, we propose\nARGUS, a two-stage, training-free defense framework leveraging goal-aware\nreasoning for precise misinformation rectification within information flows.\nOur experiments demonstrate that in challenging misinformation scenarios, ARGUS\nexhibits significant efficacy across various injection attacks, achieving an\naverage reduction in misinformation toxicity of approximately 28.17% and\nimproving task success rates under attack by approximately 10.33%. Our code and\ndataset is available at: https://github.com/zhrli324/ARGUS.", "AI": {"tldr": "The paper introduces MisinfoTask, a dataset for studying misinformation in Multi-Agent Systems (MASs), and presents ARGUS, a defense framework that effectively mitigates misinformation propagation.", "motivation": "To address the vulnerability of Multi-Agent Systems to misinformation injection, which arises from the complexity of real-world tasks they handle.", "method": "MisinfoTask dataset is created to analyze misinformation dynamics, and ARGUS is a training-free framework that uses goal-aware reasoning to rectify misinformation in MAS information flows.", "result": "ARGUS significantly reduces misinformation toxicity by approximately 28.17% and improves task success rates under misinformation attack by about 10.33%.", "conclusion": "The methods and dataset contribute to understanding and improving the resilience of Multi-Agent Systems against misinformation.", "key_contributions": ["Introduction of MisinfoTask dataset for evaluating misinformation in MASs.", "Development of ARGUS, a goal-aware reasoning framework for misinformation rectification.", "Demonstration of ARGUS's efficacy against various misinformation injection attacks."], "limitations": "", "keywords": ["Multi-Agent Systems", "Misinformation", "Defense Framework", "Goal-aware Reasoning", "Dataset"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00514", "pdf": "https://arxiv.org/pdf/2506.00514.pdf", "abs": "https://arxiv.org/abs/2506.00514", "title": "Evaluating the Evaluation of Diversity in Commonsense Generation", "authors": ["Tianhui Zhang", "Bei Peng", "Danushka Bollegala"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "In commonsense generation, given a set of input concepts, a model must\ngenerate a response that is not only commonsense bearing, but also capturing\nmultiple diverse viewpoints. Numerous evaluation metrics based on form- and\ncontent-level overlap have been proposed in prior work for evaluating the\ndiversity of a commonsense generation model. However, it remains unclear as to\nwhich metrics are best suited for evaluating the diversity in commonsense\ngeneration. To address this gap, we conduct a systematic meta-evaluation of\ndiversity metrics for commonsense generation. We find that form-based diversity\nmetrics tend to consistently overestimate the diversity in sentence sets, where\neven randomly generated sentences are assigned overly high diversity scores. We\nthen use an Large Language Model (LLM) to create a novel dataset annotated for\nthe diversity of sentences generated for a commonsense generation task, and use\nit to conduct a meta-evaluation of the existing diversity evaluation metrics.\nOur experimental results show that content-based diversity evaluation metrics\nconsistently outperform the form-based counterparts, showing high correlations\nwith the LLM-based ratings. We recommend that future work on commonsense\ngeneration should use content-based metrics for evaluating the diversity of\ntheir outputs.", "AI": {"tldr": "The paper systematically evaluates the effectiveness of diversity metrics for commonsense generation and recommends using content-based metrics over form-based ones.", "motivation": "To identify the most effective evaluation metrics for measuring diversity in commonsense generation, as prior metrics tend to overestimate diversity.", "method": "Conduct a meta-evaluation of existing diversity metrics using a newly created dataset annotated for sentence diversity, comparing form-based and content-based metrics.", "result": "Content-based diversity metrics consistently outperformed form-based metrics and showed high correlations with LLM-generated diversity ratings.", "conclusion": "Future commonsense generation research should prioritize content-based evaluation metrics for diversity.", "key_contributions": ["Systematic meta-evaluation of commonsense generation diversity metrics.", "Creation of a novel LLM-annotated dataset for diversity assessment.", "Demonstration that content-based metrics are superior to form-based metrics."], "limitations": "", "keywords": ["commonsense generation", "diversity metrics", "Large Language Model", "evaluation metrics", "content-based metrics"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00519", "pdf": "https://arxiv.org/pdf/2506.00519.pdf", "abs": "https://arxiv.org/abs/2506.00519", "title": "CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention", "authors": ["Yuxi Sun", "Aoqi Zuo", "Wei Gao", "Jing Ma"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Association for Computational Linguistics Findings (ACL)\n  2025", "summary": "Large Language Models (LLMs) often exhibit knowledge disparities across\nlanguages. Encouraging LLMs to \\textit{abstain} when faced with knowledge gaps\nis a promising strategy to reduce hallucinations in multilingual settings.\nCurrent abstention strategies for multilingual scenarios primarily rely on\ngenerating feedback in various languages using LLMs and performing\nself-reflection. However, these methods can be adversely impacted by\ninaccuracies and biases in the generated feedback. To address this, from a\ncausal perspective, we introduce \\textit{CausalAbstain}, a method that helps\nLLMs determine whether to utilize multiple generated feedback responses and how\nto identify the most useful ones. Extensive experiments demonstrate that\n\\textit{CausalAbstain} effectively selects helpful feedback and enhances\nabstention decisions with interpretability in both native language\n(\\textsc{Casual-native}) and multilingual (\\textsc{Causal-multi}) settings,\noutperforming strong baselines on two benchmark datasets covering encyclopedic\nand commonsense knowledge QA tasks. Our code and data are open-sourced at\nhttps://github.com/peachch/CausalAbstain.", "AI": {"tldr": "CausalAbstain introduces a method to improve language models' abstention decisions in multilingual settings by effectively selecting useful feedback.", "motivation": "The paper addresses knowledge disparities in Large Language Models (LLMs) across languages and the need to reduce hallucinations in multilingual contexts.", "method": "CausalAbstain is a causal approach that helps LLMs determine how to utilize and identify useful generated feedback responses for abstention.", "result": "Extensive experiments show that CausalAbstain selects helpful feedback and enhances abstention decisions in both native and multilingual settings, outperforming strong baselines on two benchmark datasets.", "conclusion": "CausalAbstain improves interpretability and effectiveness of LLM abstention methods by leveraging causal relationships in feedback selection.", "key_contributions": ["Introduces the CausalAbstain method for feedback selection in multilingual LLM settings.", "Demonstrates improved abstention decisions in LLMs using extensive experiments.", "Open-sourced code and data for further research and application."], "limitations": "", "keywords": ["Large Language Models", "multilingual", "abstention", "CausalAbstain", "feedback selection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00527", "pdf": "https://arxiv.org/pdf/2506.00527.pdf", "abs": "https://arxiv.org/abs/2506.00527", "title": "Retrieval-Augmented Generation Systems for Intellectual Property via Synthetic Multi-Angle Fine-tuning", "authors": ["Runtao Ren", "Jian Ma", "Jianxi Luo"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems in the Intellectual Property\n(IP) field often struggle with diverse user queries, including colloquial\nexpressions, spelling errors, and ambiguous terminology, leading to inaccurate\nretrieval and suboptimal responses. To address this challenge, we propose\nMulti-Angle Question Generation and Retrieval Fine-Tuning Method (MQG-RFM), a\nnovel framework that leverages large language models (LLMs) to simulate varied\nuser inquiries and fine-tunes retrieval models to align semantically equivalent\nbut linguistically diverse questions. Unlike complex architectural\nmodifications, MQG-RFM adopts a lightweight Data-to-Tune paradigm, combining\nprompt-engineered query generation with hard negative mining to enhance\nretrieval robustness without costly infrastructure changes. Experimental\nresults on a Taiwan patent Q&A dataset show 185.62% improvement in retrieval\naccuracy on the Patent Consultation dataset and 262.26% improvement on the\nNovel Patent Technology Report dataset, with 14.22% and 53.58% improvements in\ngeneration quality over the baselines, respectively. By bridging the gap\nbetween user intent and system comprehension through semantic-aware retrieval\noptimization, MQG-RFM offers a practical, scalable approach for rapid,\ncost-effective deployment among small and medium-sized agencies seeking\nreliable patent intelligence solutions. Additionally, our proposed method has\nalready been adopted by ScholarMate, the largest professional research social\nnetworking platform in China, to support real-world development and deployment.\nA demo version of the instantiated is available at\nhttps://github.com/renruntao/patent_rag.", "AI": {"tldr": "The paper introduces MQG-RFM, a method that improves retrieval accuracy in RAG systems for patent queries by leveraging LLMs to generate diverse user inquiries and fine-tune retrieval models.", "motivation": "RAG systems in the IP field struggle with diverse user queries, leading to inaccurate retrieval and responses.", "method": "MQG-RFM uses large language models to simulate varied user inquiries and fine-tunes retrieval models without complex architectural changes, employing a Data-to-Tune paradigm and hard negative mining.", "result": "Experimental results show significant improvements: 185.62% in retrieval accuracy on the Patent Consultation dataset and 262.26% on the Novel Patent Technology Report dataset.", "conclusion": "MQG-RFM effectively optimizes semantic-aware retrieval, offering a scalable solution for small and medium-sized agencies in need of reliable patent intelligence.", "key_contributions": ["Introduction of the MQG-RFM framework for improved query handling.", "Demonstrated significant retrieval and generation performance gains on patent datasets.", "Real-world implementation by ScholarMate, enhancing patent intelligence solutions."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Multi-Angle Question Generation", "Semantic-aware retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00536", "pdf": "https://arxiv.org/pdf/2506.00536.pdf", "abs": "https://arxiv.org/abs/2506.00536", "title": "Decoupling Reasoning and Knowledge Injection for In-Context Knowledge Editing", "authors": ["Changyue Wang", "Weihang Su", "Qingyao Ai", "Yujia Zhou", "Yiqun Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge editing aims to efficiently update Large Language Models (LLMs) by\nmodifying specific knowledge without retraining the entire model. Among\nknowledge editing approaches, in-context editing (ICE) offers a lightweight\nsolution by injecting new knowledge directly into the input context, leaving\nmodel parameters unchanged. However, existing ICE approaches do not explicitly\nseparate the newly injected knowledge from the model's original reasoning\nprocess. This entanglement often results in conflicts between external updates\nand internal parametric knowledge, undermining the consistency and accuracy of\nthe reasoning path.In this work, we conduct preliminary experiments to examine\nhow parametric knowledge influences reasoning path planning. We find that the\nmodel's reasoning is tightly coupled with its internal knowledge, and that\nnaively injecting new information without adapting the reasoning path often\nleads to performance degradation, particularly in multi-hop tasks. To this end,\nwe propose DecKER, a novel ICE framework that decouples reasoning from\nknowledge editing by generating a masked reasoning path and then resolving\nknowledge edits via hybrid retrieval and model-based validation. Experiments on\nmulti-hop QA benchmarks show that DecKER significantly outperforms existing ICE\nmethods by mitigating knowledge conflicts and preserving reasoning consistency.\nOur code is available at: https://github.com/bebr2/DecKER .", "AI": {"tldr": "DecKER is a new framework for knowledge editing in LLMs that separates reasoning from knowledge updates, improving performance in multi-hop tasks.", "motivation": "The need to update LLMs efficiently without retraining, while maintaining consistency in reasoning when injecting new knowledge.", "method": "DecKER employs a masked reasoning path strategy to decouple internal reasoning processes from external knowledge edits, using hybrid retrieval and model-based validation.", "result": "DecKER significantly outperforms existing ICE methods in multi-hop QA benchmarks, by reducing knowledge conflicts and maintaining reasoning consistency.", "conclusion": "The DecKER framework enhances the knowledge editing process in LLMs and shows promising results in improving reasoning paths for complex tasks.", "key_contributions": ["Introduction of the DecKER framework for decoupled knowledge editing", "Demonstrated improvement in multi-hop reasoning tasks", "Code availability for further research and implementation"], "limitations": "", "keywords": ["Knowledge Editing", "Large Language Models", "In-Context Editing", "DecKER", "Multi-hop Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00539", "pdf": "https://arxiv.org/pdf/2506.00539.pdf", "abs": "https://arxiv.org/abs/2506.00539", "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation", "authors": ["Ruihan Yang", "Yikai Zhang", "Aili Chen", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Deqing Yang", "Yanghua Xiao"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.", "AI": {"tldr": "The paper proposes ARIA, a method for reducing reward variance in reinforcement learning for large language models by projecting actions into a low-dimensional intention space.", "motivation": "Large language models struggle with sparse rewards in open-ended tasks, resulting in high variance during reinforcement learning.", "method": "ARIA aggregates rewards in intention space, projecting natural language actions from a high-dimensional joint token distribution into a low-dimensional intention space for better clustering and reward assignment.", "result": "ARIA reduces policy gradient variance and improves performance by an average of 9.95% across four downstream tasks compared to existing RL baselines.", "conclusion": "The method demonstrates effective training of language agents by ensuring more robust reward signals, leading to improved policy optimization.", "key_contributions": ["Introduction of ARIA for reward aggregation in intention space", "Significant reduction in reward variance", "Improvement in task performance across various benchmarks"], "limitations": "", "keywords": ["large language models", "reinforcement learning", "reward aggregation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00549", "pdf": "https://arxiv.org/pdf/2506.00549.pdf", "abs": "https://arxiv.org/abs/2506.00549", "title": "Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages", "authors": ["Hyangsuk Min", "Yuho Lee", "Minjeong Ban", "Jiaqi Deng", "Nicole Hee-Yeon Kim", "Taewon Yun", "Hang Su", "Jason Cai", "Hwanjun Song"], "categories": ["cs.CL", "cs.AI"], "comment": "34 pages, 6 figures", "summary": "Evaluation frameworks for text summarization have evolved in terms of both\ndomain coverage and metrics. However, existing benchmarks still lack\ndomain-specific assessment criteria, remain predominantly English-centric, and\nface challenges with human annotation due to the complexity of reasoning. To\naddress these, we introduce MSumBench, which provides a multi-dimensional,\nmulti-domain evaluation of summarization in English and Chinese. It also\nincorporates specialized assessment criteria for each domain and leverages a\nmulti-agent debate system to enhance annotation quality. By evaluating eight\nmodern summarization models, we discover distinct performance patterns across\ndomains and languages. We further examine large language models as summary\nevaluators, analyzing the correlation between their evaluation and\nsummarization capabilities, and uncovering systematic bias in their assessment\nof self-generated summaries. Our benchmark dataset is publicly available at\nhttps://github.com/DISL-Lab/MSumBench.", "AI": {"tldr": "This paper introduces MSumBench, a benchmark for multi-domain and multi-dimensional evaluation of text summarization in English and Chinese, addressing limitations of existing frameworks.", "motivation": "Existing text summarization evaluation frameworks are limited by domain specificity, language coverage, and the complexity of human annotation.", "method": "MSumBench integrates specialized criteria for different domains and employs a multi-agent debate system to improve the quality of annotations while evaluating several summarization models.", "result": "Distinct performance patterns were found across domains and languages, and biases in large language models were identified regarding their evaluation of summaries they self-generate.", "conclusion": "The benchmark dataset aims to enhance the quality of summarization evaluations and is made publicly available for further research.", "key_contributions": ["Introduction of a multi-dimensional and multi-domain evaluation framework for summarization", "Incorporation of specialized assessment criteria tailored to different domains", "Use of a multi-agent debate system to improve annotation quality"], "limitations": "", "keywords": ["text summarization", "evaluation framework", "MSumBench", "multi-agent debate system", "large language models"], "importance_score": 7, "read_time_minutes": 34}}
{"id": "2506.00551", "pdf": "https://arxiv.org/pdf/2506.00551.pdf", "abs": "https://arxiv.org/abs/2506.00551", "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation", "authors": ["Ming Wang", "Peidong Wang", "Lin Wu", "Xiaocui Yang", "Daling Wang", "Shi Feng", "Yuxin Chen", "Bixuan Wang", "Yifei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Constrained by the cost and ethical concerns of involving real seekers in\nAI-driven mental health, researchers develop LLM-based conversational agents\n(CAs) with tailored configurations, such as profiles, symptoms, and scenarios,\nto simulate seekers. While these efforts advance AI in mental health, achieving\nmore realistic seeker simulation remains hindered by two key challenges:\ndynamic evolution and multi-session memory. Seekers' mental states often\nfluctuate during counseling, which typically spans multiple sessions. To\naddress this, we propose AnnaAgent, an emotional and cognitive dynamic agent\nsystem equipped with tertiary memory. AnnaAgent incorporates an emotion\nmodulator and a complaint elicitor trained on real counseling dialogues,\nenabling dynamic control of the simulator's configurations. Additionally, its\ntertiary memory mechanism effectively integrates short-term and long-term\nmemory across sessions. Evaluation results, both automated and manual,\ndemonstrate that AnnaAgent achieves more realistic seeker simulation in\npsychological counseling compared to existing baselines. The ethically reviewed\nand screened code can be found on https://github.com/sci-m-wang/AnnaAgent.", "AI": {"tldr": "AnnaAgent, a dynamic emotional and cognitive agent system, simulates seekers in AI-driven mental health counseling by addressing key challenges in realistic simulation.", "motivation": "To enhance the realism of seeker simulation in AI-driven mental health applications while addressing ethical concerns of using real seekers.", "method": "AnnaAgent employs an emotion modulator and a complaint elicitor, trained on authentic counseling dialogues, integrating a tertiary memory mechanism for dynamic control across sessions.", "result": "Evaluation results show that AnnaAgent provides a more realistic simulation of seekers during psychological counseling than existing baseline models.", "conclusion": "AnnaAgent represents a significant advancement in emotional and cognitive simulation for AI-driven mental health interfaces, improving multi-session interaction realism.", "key_contributions": ["Development of AnnaAgent with a dynamic emotional and cognitive framework", "Implementation of tertiary memory for better multi-session continuity", "Use of real counseling dialogues for training the agent's response behavior"], "limitations": "", "keywords": ["AI-driven mental health", "conversational agents", "LLM", "psychological counseling", "dynamic agent system"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.00583", "pdf": "https://arxiv.org/pdf/2506.00583.pdf", "abs": "https://arxiv.org/abs/2506.00583", "title": "The Hidden Language of Harm: Examining the Role of Emojis in Harmful Online Communication and Content Moderation", "authors": ["Yuhang Zhou", "Yimin Xiao", "Wei Ai", "Ge Gao"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 3 figures", "summary": "Social media platforms have become central to modern communication, yet they\nalso harbor offensive content that challenges platform safety and inclusivity.\nWhile prior research has primarily focused on textual indicators of offense,\nthe role of emojis, ubiquitous visual elements in online discourse, remains\nunderexplored. Emojis, despite being rarely offensive in isolation, can acquire\nharmful meanings through symbolic associations, sarcasm, and contextual misuse.\nIn this work, we systematically examine emoji contributions to offensive\nTwitter messages, analyzing their distribution across offense categories and\nhow users exploit emoji ambiguity. To address this, we propose an LLM-powered,\nmulti-step moderation pipeline that selectively replaces harmful emojis while\npreserving the tweet's semantic intent. Human evaluations confirm our approach\neffectively reduces perceived offensiveness without sacrificing meaning. Our\nanalysis also reveals heterogeneous effects across offense types, offering\nnuanced insights for online communication and emoji moderation.", "AI": {"tldr": "This paper investigates the role of emojis in offensive Twitter messages and proposes an LLM-powered moderation pipeline to address emoji-related offensiveness.", "motivation": "The study aims to fill the gap in existing research on the impact of emojis in online communication and their potential to convey harmful meanings.", "method": "A systematic examination of emoji contributions to offensive messages on Twitter, analyzing distribution across offense categories and the exploitation of emoji ambiguity; development of a multi-step moderation pipeline using LLMs to replace harmful emojis.", "result": "Human evaluations show that the proposed moderation approach effectively reduces perceived offensiveness of tweets while maintaining their semantic intent.", "conclusion": "The paper provides nuanced insights into emoji moderation and highlights varied effects across different types of offensive content.", "key_contributions": ["Systematic study of emojis in offensive communication on social media", "Proposal of a multi-step LLM-powered moderation pipeline", "Insights into the contextual misuse of emojis in conveying offense"], "limitations": "The study focuses on Twitter and may not generalize to other platforms or forms of communication.", "keywords": ["emoji moderation", "offensive content", "human-computer interaction", "social media analysis", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.00585", "pdf": "https://arxiv.org/pdf/2506.00585.pdf", "abs": "https://arxiv.org/abs/2506.00585", "title": "Entriever: Energy-based Retriever for Knowledge-Grounded Dialog Systems", "authors": ["Yucheng Cai", "Ke Li", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "categories": ["cs.CL"], "comment": "Accepted by ACL2025 Findings", "summary": "A retriever, which retrieves relevant knowledge pieces from a knowledge base\ngiven a context, is an important component in many natural language processing\n(NLP) tasks. Retrievers have been introduced in knowledge-grounded dialog\nsystems to improve knowledge acquisition. In knowledge-grounded dialog systems,\nwhen conditioning on a given context, there may be multiple relevant and\ncorrelated knowledge pieces. However, knowledge pieces are usually assumed to\nbe conditionally independent in current retriever models. To address this\nissue, we propose Entriever, an energy-based retriever. Entriever directly\nmodels the candidate retrieval results as a whole instead of modeling the\nknowledge pieces separately, with the relevance score defined by an energy\nfunction. We explore various architectures of energy functions and different\ntraining methods for Entriever, and show that Entriever substantially\noutperforms the strong cross-encoder baseline in knowledge retrieval tasks.\nFurthermore, we show that in semi-supervised training of knowledge-grounded\ndialog systems, Entriever enables effective scoring of retrieved knowledge\npieces and significantly improves end-to-end performance of dialog systems.", "AI": {"tldr": "Entriever is an energy-based retriever for knowledge-grounded dialog systems that models candidate retrieval results collectively, leading to improved performance over traditional methods.", "motivation": "Current retriever models assume that knowledge pieces are conditionally independent, which may limit their effectiveness in knowledge-grounded dialog systems.", "method": "Entriever models the candidate retrieval results as a whole using an energy function to define relevance scores, exploring various architectures and training methods.", "result": "Entriever significantly outperforms a strong cross-encoder baseline in knowledge retrieval tasks and improves end-to-end performance in semi-supervised training of dialog systems.", "conclusion": "Entriever provides a more effective approach to knowledge retrieval in dialog systems by collectively scoring relevant knowledge pieces.", "key_contributions": ["Introduction of an energy-based retriever that models knowledge pieces collectively.", "Demonstration of superior performance in knowledge retrieval tasks compared to existing models.", "Enhancement of end-to-end performance in dialog systems during semi-supervised training."], "limitations": "", "keywords": ["energy-based retriever", "knowledge-grounded dialog systems", "NLP", "knowledge retrieval", "semi-supervised training"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00608", "pdf": "https://arxiv.org/pdf/2506.00608.pdf", "abs": "https://arxiv.org/abs/2506.00608", "title": "PAKTON: A Multi-Agent Framework for Question Answering in Long Legal Agreements", "authors": ["Petros Raptopoulos", "Giorgos Filandrianos", "Maria Lymperaiou", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": null, "summary": "Contract review is a complex and time-intensive task that typically demands\nspecialized legal expertise, rendering it largely inaccessible to non-experts.\nMoreover, legal interpretation is rarely straightforward-ambiguity is\npervasive, and judgments often hinge on subjective assessments. Compounding\nthese challenges, contracts are usually confidential, restricting their use\nwith proprietary models and necessitating reliance on open-source alternatives.\nTo address these challenges, we introduce PAKTON: a fully open-source,\nend-to-end, multi-agent framework with plug-and-play capabilities. PAKTON is\ndesigned to handle the complexities of contract analysis through collaborative\nagent workflows and a novel retrieval-augmented generation (RAG) component,\nenabling automated legal document review that is more accessible, adaptable,\nand privacy-preserving. Experiments demonstrate that PAKTON outperforms both\ngeneral-purpose and pretrained models in predictive accuracy, retrieval\nperformance, explainability, completeness, and grounded justifications as\nevaluated through a human study and validated with automated metrics.", "AI": {"tldr": "PAKTON is an open-source multi-agent framework for automated contract review, improving accessibility and privacy in legal analysis.", "motivation": "Contract review is complex and often requires specialist legal knowledge, which makes it difficult for non-experts.", "method": "The paper presents PAKTON, an open-source framework that utilizes collaborative agent workflows and a retrieval-augmented generation (RAG) component for contract analysis.", "result": "PAKTON outperforms other models in predictive accuracy, retrieval performance, explainability, completeness, and grounded justifications as demonstrated through human evaluation and metrics.", "conclusion": "PAKTON provides an accessible and efficient solution for legal document review while ensuring privacy due to its open-source nature.", "key_contributions": ["Fully open-source framework for contract analysis", "Integration of multi-agent workflows for collaboration", "Novel retrieval-augmented generation (RAG) component for enhanced legal document review"], "limitations": "", "keywords": ["contract review", "open-source", "multi-agent", "RAG", "legal analysis"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2506.00612", "pdf": "https://arxiv.org/pdf/2506.00612.pdf", "abs": "https://arxiv.org/abs/2506.00612", "title": "Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge Graph Guided Distractor Generation", "authors": ["Running Yang", "Wenlong Deng", "Minghui Chen", "Yuyin Zhou", "Xiaoxiao Li"], "categories": ["cs.CL"], "comment": null, "summary": "Clinical tasks such as diagnosis and treatment require strong decision-making\nabilities, highlighting the importance of rigorous evaluation benchmarks to\nassess the reliability of large language models (LLMs). In this work, we\nintroduce a knowledge-guided data augmentation framework that enhances the\ndifficulty of clinical multiple-choice question (MCQ) datasets by generating\ndistractors (i.e., incorrect choices that are similar to the correct one and\nmay confuse existing LLMs). Using our KG-based pipeline, the generated choices\nare both clinically plausible and deliberately misleading. Our approach\ninvolves multi-step, semantically informed walks on a medical knowledge graph\nto identify distractor paths-associations that are medically relevant but\nfactually incorrect-which then guide the LLM in crafting more deceptive\ndistractors. We apply the designed knowledge graph guided distractor generation\n(KGGDG) pipline, to six widely used medical QA benchmarks and show that it\nconsistently reduces the accuracy of state-of-the-art LLMs. These findings\nestablish KGGDG as a powerful tool for enabling more robust and diagnostic\nevaluations of medical LLMs.", "AI": {"tldr": "The paper presents a knowledge-guided data augmentation framework that enhances clinical MCQ datasets by generating misleading distractors to evaluate the reliability of large language models.", "motivation": "To improve the evaluation benchmarks for clinical tasks like diagnosis and treatment by assessing the decision-making abilities of LLMs.", "method": "The study employs a knowledge graph-based approach to generate distractors that are clinically plausible but misleading, using multi-step semantic walks on medical knowledge graphs.", "result": "The KGGDG framework was applied to six medical QA benchmarks, successfully reducing the accuracy of state-of-the-art LLMs, indicating the need for more robust evaluation methods.", "conclusion": "The KGGDG approach is established as an effective tool for enhancing the rigor of evaluations of medical language models, providing more realistic tests of their performance in clinical decision-making tasks.", "key_contributions": ["Introduction of the KGGDG framework for generating misleading distractors in clinical MCQs.", "Demonstration of its effectiveness across six medical QA benchmarks.", "Highlighting the potential of knowledge graphs in improving the robustness of LLM evaluations."], "limitations": "", "keywords": ["large language models", "data augmentation", "clinical evaluation", "knowledge graph", "distractor generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00622", "pdf": "https://arxiv.org/pdf/2506.00622.pdf", "abs": "https://arxiv.org/abs/2506.00622", "title": "Improving Dialogue State Tracking through Combinatorial Search for In-Context Examples", "authors": ["Haesung Pyun", "Yoonah Park", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "In dialogue state tracking (DST), in-context learning comprises a retriever\nthat selects labeled dialogues as in-context examples and a DST model that uses\nthese examples to infer the dialogue state of the query dialogue. Existing\nmethods for constructing training data for retrievers suffer from three key\nlimitations: (1) the synergistic effect of examples is not considered, (2) the\nlinguistic characteristics of the query are not sufficiently factored in, and\n(3) scoring is not directly optimized for DST performance. Consequently, the\nretriever can fail to retrieve examples that would substantially improve DST\nperformance. To address these issues, we present CombiSearch, a method that\nscores effective in-context examples based on their combinatorial impact on DST\nperformance. Our evaluation on MultiWOZ shows that retrievers trained with\nCombiSearch surpass state-of-the-art models, achieving a 20x gain in data\nefficiency and generalizing well to the SGD dataset. Moreover, CombiSearch\nattains a 12% absolute improvement in the upper bound DST performance over\ntraditional approaches when no retrieval errors are assumed. This significantly\nincreases the headroom for practical DST performance while demonstrating that\nexisting methods rely on suboptimal data for retriever training.", "AI": {"tldr": "CombiSearch improves dialogue state tracking (DST) by using a retriever that scores in-context examples, addressing key limitations in existing methods.", "motivation": "Existing methods for training retrievers in dialogue state tracking fail to optimize for DST performance due to the lack of consideration for synergistic effects, linguistic characteristics, and performance-focused scoring.", "method": "CombiSearch scores effective in-context examples based on their combinatorial impact on DST performance to improve example retrieval.", "result": "Retrievers trained with CombiSearch exceeded state-of-the-art models, achieving a 20x gain in data efficiency and showing robustness on the SGD dataset, alongside a 12% absolute improvement in upper bound DST performance over traditional methods without retrieval errors.", "conclusion": "CombiSearch significantly enhances practical DST performance by addressing shortcomings in existing data-driven retriever training methods.", "key_contributions": ["Introduces CombiSearch for effective example scoring in DST", "Demonstrates 20x data efficiency improvement", "Achieves 12% performance improvement over traditional methods"], "limitations": "", "keywords": ["dialogue state tracking", "in-context learning", "CombiSearch", "machine learning", "data efficiency"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.00628", "pdf": "https://arxiv.org/pdf/2506.00628.pdf", "abs": "https://arxiv.org/abs/2506.00628", "title": "LID Models are Actually Accent Classifiers: Implications and Solutions for LID on Accented Speech", "authors": ["Niyati Bafna", "Matthew Wiesner"], "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "Prior research indicates that LID model performance significantly declines on\naccented speech; however, the specific causes, extent, and characterization of\nthese errors remain under-explored. (i) We identify a common failure mode on\naccented speech whereby LID systems often misclassify L2 accented speech as the\nspeaker's native language or a related language. (ii) We present evidence\nsuggesting that state-of-the-art models are invariant to permutations of short\nspans of speech, implying they classify on the basis of short phonotactic\nfeatures indicative of accent rather than language. Our analysis reveals a\nsimple method to enhance model robustness to accents through input chunking.\n(iii) We present an approach that integrates sequence-level information into\nour model without relying on monolingual ASR systems; this reduces\naccent-language confusion and significantly enhances performance on accented\nspeech while maintaining comparable results on standard LID.", "AI": {"tldr": "This paper investigates the performance decline of language identification (LID) models on accented speech and proposes methods to enhance their robustness.", "motivation": "The decline in LID model performance on accented speech is not well understood, prompting investigation into the causes and potential solutions.", "method": "The study identifies common misclassifications by LID systems on L2 accented speech and introduces a method for input chunking to improve model robustness, along with an approach that integrates sequence-level information.", "result": "The proposed methods reduce accent-language confusion and significantly improve performance on accented speech while maintaining comparable results on standard LID tasks.", "conclusion": "Enhancing LID systems to better handle accented speech is feasible through input chunking and sequence-level information integration, which can improve accuracy without sacrificing performance on non-accented speech.", "key_contributions": ["Identification of misclassification of accented speech by LID models", "Introduction of input chunking to improve robustness against accents", "Development of a sequence-level information integration approach"], "limitations": "", "keywords": ["language identification", "accented speech", "robustness", "sequence-level information", "input chunking"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00634", "pdf": "https://arxiv.org/pdf/2506.00634.pdf", "abs": "https://arxiv.org/abs/2506.00634", "title": "Social Construction of Urban Space: Understanding Neighborhood Boundaries Using Rental Listings", "authors": ["Adam Visokay", "Ruth Bagley", "Ian Kennedy", "Chris Hess", "Kyle Crowder", "Rob Voigt", "Denis Peskoff"], "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 4 tables", "summary": "Rental listings offer a unique window into how urban space is socially\nconstructed through language. We analyze Chicago Craigslist rental\nadvertisements from 2018 to 2024 to examine how listing agents characterize\nneighborhoods, identifying mismatches between institutional boundaries and\nneighborhood claims. Through manual and large language model annotation, we\nclassify unstructured listings from Craigslist according to their neighborhood.\nGeospatial analysis reveals three distinct patterns: properties with\nconflicting neighborhood designations due to competing spatial definitions,\nborder properties with valid claims to adjacent neighborhoods, and ``reputation\nlaundering\" where listings claim association with distant, desirable\nneighborhoods. Through topic modeling, we identify patterns that correlate with\nspatial positioning: listings further from neighborhood centers emphasize\ndifferent amenities than centrally-located units. Our findings demonstrate that\nnatural language processing techniques can reveal how definitions of urban\nspaces are contested in ways that traditional methods overlook.", "AI": {"tldr": "The paper analyzes Chicago Craigslist rental listings to explore how neighborhoods are constructed and represented through language, employing NLP techniques alongside geospatial analysis.", "motivation": "To understand how rental listings reflect and contest the social construction of urban space through language.", "method": "Manual and large language model annotation of Craigslist listings combined with geospatial and topic modeling analysis.", "result": "Identified three distinct patterns in how listings characterize neighborhoods, revealing conflicts between institutional boundaries and actual neighborhood claims, as well as variations in emphasized amenities based on spatial positioning.", "conclusion": "NLP techniques provide insights into contested definitions of urban spaces that traditional analysis methods may overlook.", "key_contributions": ["Use of NLP to analyze urban rental listings", "Identification of conflicting neighborhood designations", "Insights into spatially-dependent amenity emphasis"], "limitations": "", "keywords": ["Rental Listings", "Natural Language Processing", "Urban Studies", "Geospatial Analysis"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.00636", "pdf": "https://arxiv.org/pdf/2506.00636.pdf", "abs": "https://arxiv.org/abs/2506.00636", "title": "ViToSA: Audio-Based Toxic Spans Detection on Vietnamese Speech Utterances", "authors": ["Huy Ba Do", "Vy Le-Phuong Huynh", "Luan Thanh Nguyen"], "categories": ["cs.CL"], "comment": "Accepted for presentation at INTERSPEECH 2025", "summary": "Toxic speech on online platforms is a growing concern, impacting user\nexperience and online safety. While text-based toxicity detection is\nwell-studied, audio-based approaches remain underexplored, especially for\nlow-resource languages like Vietnamese. This paper introduces ViToSA\n(Vietnamese Toxic Spans Audio), the first dataset for toxic spans detection in\nVietnamese speech, comprising 11,000 audio samples (25 hours) with accurate\nhuman-annotated transcripts. We propose a pipeline that combines ASR and toxic\nspans detection for fine-grained identification of toxic content. Our\nexperiments show that fine-tuning ASR models on ViToSA significantly reduces\nWER when transcribing toxic speech, while the text-based toxic spans detection\n(TSD) models outperform existing baselines. These findings establish a novel\nbenchmark for Vietnamese audio-based toxic spans detection, paving the way for\nfuture research in speech content moderation.", "AI": {"tldr": "Introduction of ViToSA, a dataset for detecting toxic spans in Vietnamese speech, and a proposed pipeline for toxicity detection using ASR.", "motivation": "To address the growing concern of toxic speech on online platforms, especially for low-resource languages like Vietnamese, through audio-based approaches.", "method": "The paper proposes a pipeline that combines automatic speech recognition (ASR) and toxic spans detection (TSD) for identifying toxic content in Vietnamese audio samples.", "result": "Fine-tuning ASR models on the ViToSA dataset significantly reduces word error rate (WER) when transcribing toxic speech, and TSD models achieve better performance than existing baselines.", "conclusion": "The paper establishes ViToSA as a benchmark for Vietnamese audio-based toxic spans detection, facilitating future research in speech content moderation.", "key_contributions": ["Introduction of the first dataset for toxic spans detection in Vietnamese speech (ViToSA).", "Development of a pipeline integrating ASR and TSD for toxic content identification.", "Establishment of a benchmark for future research in audio-based toxicity detection."], "limitations": "", "keywords": ["toxic speech", "Vietnamese", "audio recognition", "machine learning", "speech moderation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.00637", "pdf": "https://arxiv.org/pdf/2506.00637.pdf", "abs": "https://arxiv.org/abs/2506.00637", "title": "Improving the Calibration of Confidence Scores in Text Generation Using the Output Distribution's Characteristics", "authors": ["Lorenzo Jaime Yu Flores", "Ori Ernst", "Jackie Chi Kit Cheung"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Well-calibrated model confidence scores can improve the usefulness of text\ngeneration models. For example, users can be prompted to review predictions\nwith low confidence scores, to prevent models from returning bad or potentially\ndangerous predictions. However, confidence metrics are not always well\ncalibrated in text generation. One reason is that in generation, there can be\nmany valid answers, which previous methods do not always account for. Hence, a\nconfident model could distribute its output probability among multiple\nsequences because they are all valid. We propose task-agnostic confidence\nmetrics suited to generation, which rely solely on the probabilities associated\nwith the model outputs without the need for further fine-tuning or heuristics.\nUsing these, we are able to improve the calibration of BART and Flan-T5 on\nsummarization, translation, and QA datasets.", "AI": {"tldr": "This paper introduces task-agnostic confidence metrics to enhance the calibration of text generation model confidence scores, improving model prediction reliability.", "motivation": "To overcome the issue of poorly calibrated confidence scores in text generation models, which can lead to the generation of incorrect or dangerous outputs.", "method": "The authors propose confidence metrics that are task-agnostic, relying solely on the output probabilities of the model without the need for additional fine-tuning or heuristics.", "result": "The proposed metrics improve the calibration of BART and Flan-T5 models when evaluated on summarization, translation, and QA datasets.", "conclusion": "By utilizing these confidence metrics, the authors demonstrate that it is possible to achieve better calibrated predictions in text generation tasks, enhancing overall model utility.", "key_contributions": ["Introduction of task-agnostic confidence metrics for text generation.", "Improved calibration of existing models such as BART and Flan-T5.", "No requirement for additional fine-tuning or heuristics to apply these metrics."], "limitations": "", "keywords": ["model confidence", "text generation", "calibration", "BART", "Flan-T5"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00643", "pdf": "https://arxiv.org/pdf/2506.00643.pdf", "abs": "https://arxiv.org/abs/2506.00643", "title": "SATA-BENCH: Select All That Apply Benchmark for Multiple Choice Questions", "authors": ["Weijie Xu", "Shixian Cui", "Xi Fang", "Chi Xue", "Stephanie Eckman", "Chandan Reddy"], "categories": ["cs.CL", "cs.AI", "68T01", "I.2.7"], "comment": "40 pages, 13 figures", "summary": "Large language models (LLMs) are increasingly evaluated on single-answer\nmultiple-choice tasks, yet many real-world problems require identifying all\ncorrect answers from a set of options. This capability remains underexplored.\nWe introduce SATA-BENCH, the first dedicated benchmark for evaluating LLMs on\nSelect All That Apply (SATA) questions across diverse domains, including\nreading comprehension, law, and biomedicine. Our evaluation of 27 open-source\nand proprietary models reveals a significant gap: even the strongest model\nachieves only 41.8% exact match, exposing LLMs' inability to reliably identify\nall correct answers. We find that this weakness stems from two core challenges:\nselection bias - models favor certain choices regardless of content, and count\nbias - models fail to predict the correct number of answers. To address these\nissues, we propose Choice Funnel, a decoding strategy that combines token\ndebiasing with adaptive thresholding to guide models toward complete and\naccurate selections. Choice Funnel achieves up to 29% higher exact match than\ncompetitive baselines while reducing inference cost by over 64%. Our findings\nexpose fundamental limitations in current LLMs and introduce a new framework\nfor diagnosing and improving multi-answer reasoning. We release SATA-BENCH and\nChoice Funnel to promote LLM development for robust decision-making in\nrealistic, multi-answer applications.", "AI": {"tldr": "The paper introduces SATA-BENCH, a benchmark for evaluating LLMs on Select All That Apply questions and proposes a decoding strategy called Choice Funnel to improve their performance.", "motivation": "There is an increasing need to evaluate large language models on their ability to identify all correct answers in multi-answer tasks, which is currently underexplored.", "method": "The authors conducted an evaluation of 27 models on SATA questions across various domains and proposed a new decoding strategy called Choice Funnel, which combines token debiasing with adaptive thresholding.", "result": "The evaluation revealed that existing models perform poorly, achieving only 41.8% exact match, with the proposed Choice Funnel improving this by up to 29% and reducing inference cost by over 64%.", "conclusion": "The findings highlight fundamental limitations in current LLMs and provide a new framework for enhancing multi-answer reasoning skills in LLMs.", "key_contributions": ["Introduction of SATA-BENCH, a new benchmark for multi-answer evaluation", "Development of Choice Funnel decoding strategy that improves exact match scores", "Documentation of performance gaps in existing LLMs on multi-answer tasks"], "limitations": "", "keywords": ["large language models", "SATA questions", "benchmarking", "multi-answer reasoning", "Choice Funnel"], "importance_score": 9, "read_time_minutes": 40}}
{"id": "2506.00644", "pdf": "https://arxiv.org/pdf/2506.00644.pdf", "abs": "https://arxiv.org/abs/2506.00644", "title": "Clinical Annotations for Automatic Stuttering Severity Assessment", "authors": ["Ana Rita Valente", "Rufael Marew", "Hawau Olamide Toyin", "Hamdan Al-Ali", "Anelise Bohnen", "Inma Becerra", "Elsa Marta Soares", "Goncalo Leal", "Hanan Aldarmaki"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Stuttering is a complex disorder that requires specialized expertise for\neffective assessment and treatment. This paper presents an effort to enhance\nthe FluencyBank dataset with a new stuttering annotation scheme based on\nestablished clinical standards. To achieve high-quality annotations, we hired\nexpert clinicians to label the data, ensuring that the resulting annotations\nmirror real-world clinical expertise. The annotations are multi-modal,\nincorporating audiovisual features for the detection and classification of\nstuttering moments, secondary behaviors, and tension scores. In addition to\nindividual annotations, we additionally provide a test set with highly reliable\nannotations based on expert consensus for assessing individual annotators and\nmachine learning models. Our experiments and analysis illustrate the complexity\nof this task that necessitates extensive clinical expertise for valid training\nand evaluation of stuttering assessment models.", "AI": {"tldr": "This paper enhances the FluencyBank dataset with a new stuttering annotation scheme, utilizing expert clinicians for high-quality, multi-modal annotations to assist in stuttering assessment and model training.", "motivation": "To improve the assessment and treatment of stuttering through a reliable dataset and annotation scheme.", "method": "Expert clinicians were hired to label data in a new stuttering annotation scheme, including audiovisual features for detecting and classifying stuttering moments and associated behaviors.", "result": "The paper presents a dataset with high-quality, multi-modal annotations and provides a test set for the evaluation of annotators and models, highlighting the complexity of stuttering assessment.", "conclusion": "The study underscores the necessity of clinical expertise in the creation of valid training datasets for stuttering assessment models.", "key_contributions": ["Development of a novel stuttering annotation scheme based on clinical standards.", "Creation of a multi-modal dataset incorporating audiovisual features.", "Provision of a reliable test set for benchmarking annotations and machine learning models."], "limitations": "", "keywords": ["stuttering", "dataset", "machine learning", "annotations", "clinical expertise"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.00649", "pdf": "https://arxiv.org/pdf/2506.00649.pdf", "abs": "https://arxiv.org/abs/2506.00649", "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction", "authors": ["Neil De La Fuente", "Oscar Sainz", "Iker García-Ferrero", "Eneko Agirre"], "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Information Extraction (IE) systems are traditionally domain-specific,\nrequiring costly adaptation that involves expert schema design, data\nannotation, and model training. While Large Language Models have shown promise\nin zero-shot IE, performance degrades significantly in unseen domains where\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\nautomatically defines domain-specific schemas, infers guidelines, and generates\nsynthetically labeled instances, allowing for better out-of-domain\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\ndemonstrate enhanced comprehension of complex, domain-specific annotation\nschemas. Code, models, and synthetic datasets are available at\nneilus03.github.io/guidex.com", "AI": {"tldr": "GUIDEX automates domain-specific schema definition and generates synthetic labeled instances to improve zero-shot Information Extraction (IE) performance in unseen domains.", "motivation": "To address the costly adaptation of Information Extraction systems to new domains and the performance drop in unseen domains with Large Language Models.", "method": "Introducing GUIDEX, which defines domain-specific schemas, infers guidelines, and generates synthetic labeled instances, fine-tuning Llama 3.1 for improved generalization.", "result": "Achieving a new state-of-the-art on seven zero-shot Named Entity Recognition benchmarks, with models gaining up to 7 F1 points without human-labeled data and nearly 2 F1 points higher when combined with it.", "conclusion": "GUIDEX significantly enhances the modeling comprehension of complex annotation schemas without the need for human annotation.", "key_contributions": ["Automated domain-specific schema definition", "Generation of synthetic labeled instances", "Significant improvement in zero-shot IE performance"], "limitations": "", "keywords": ["Information Extraction", "Large Language Models", "Zero-shot Learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.00658", "pdf": "https://arxiv.org/pdf/2506.00658.pdf", "abs": "https://arxiv.org/abs/2506.00658", "title": "Sarc7: Evaluating Sarcasm Detection and Generation with Seven Types and Emotion-Informed Techniques", "authors": ["Lang Xiong", "Raina Gao", "Alyssa Jeong", "Yicheng Fu", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sarcasm is a form of humor where expressions convey meanings opposite to\ntheir literal interpretations. Classifying and generating sarcasm using large\nlanguage models is vital for interpreting human communication. Sarcasm poses\nchallenges for computational models, due to its nuanced nature. We introduce\nSarc7, a benchmark that classifies 7 types of sarcasm: self-deprecating,\nbrooding, deadpan, polite, obnoxious, raging, and manic by annotating entries\nof the MUStARD dataset. Classification was evaluated using zero-shot, few-shot,\nchain-of-thought (CoT), and a novel emotion-based prompting technique. We\npropose an emotion-based generation method developed by identifying key\ncomponents of sarcasm-incongruity, shock value, and context dependency. Our\nclassification experiments show that Gemini 2.5, using emotion-based prompting,\noutperforms other setups with an F1 score of 0.3664. Human evaluators preferred\nour emotion-based prompting, with 38.46% more successful generations than\nzero-shot prompting.", "AI": {"tldr": "The paper introduces Sarc7, a benchmark for classifying seven types of sarcasm using large language models, focusing on emotion-based techniques for both classification and generation.", "motivation": "The study addresses the challenges of classifying and generating sarcasm, which is essential for understanding human communication.", "method": "The authors developed Sarc7 by annotating the MUStARD dataset and evaluated sarcasm classification using various techniques, including zero-shot, few-shot, chain-of-thought, and emotion-based prompting.", "result": "The emotion-based prompting method outperformed other approaches, achieving an F1 score of 0.3664 with positive feedback from human evaluators for its effectiveness in sarcasm generation.", "conclusion": "Emotion-based prompting proves to enhance the classification and generation of sarcasm in large language models significantly.", "key_contributions": ["Introduction of Sarc7 benchmark for sarcasm classification", "Evaluation of various prompting techniques for sarcasm classification and generation", "Demonstration of the effectiveness of emotion-based prompting method"], "limitations": "", "keywords": ["sarcasm", "large language models", "emotion-based prompting", "classification", "human communication"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.00668", "pdf": "https://arxiv.org/pdf/2506.00668.pdf", "abs": "https://arxiv.org/abs/2506.00668", "title": "SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues", "authors": ["Martin Kuo", "Jianyi Zhang", "Aolin Ding", "Louis DiValentin", "Amin Hass", "Benjamin F Morris", "Isaac Jacobson", "Randolph Linderman", "James Kiessling", "Nicolas Ramos", "Bhavna Gopal", "Maziyar Baran Pouyan", "Changwei Liu", "Hai Li", "Yiran Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Malicious attackers can exploit large language models (LLMs) by engaging them\nin multi-turn dialogues to achieve harmful objectives, posing significant\nsafety risks to society. To address this challenge, we propose a novel defense\nmechanism: SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues\n(STREAM). STREAM defends LLMs against multi-turn attacks while preserving their\nfunctional capabilities. Our approach involves constructing a human-annotated\ndataset, the Safety Reasoning Multi-turn Dialogues dataset, which is used to\nfine-tune a plug-and-play safety reasoning moderator. This model is designed to\nidentify malicious intent hidden within multi-turn conversations and alert the\ntarget LLM of potential risks. We evaluate STREAM across multiple LLMs against\nprevalent multi-turn attack strategies. Experimental results demonstrate that\nour method significantly outperforms existing defense techniques, reducing the\nAttack Success Rate (ASR) by 51.2%, all while maintaining comparable LLM\ncapability.", "AI": {"tldr": "The paper introduces a defense mechanism called STREAM to protect large language models from malicious multi-turn dialogue attacks while preserving their functionality.", "motivation": "To address safety risks posed by malicious attackers exploiting LLMs in multi-turn dialogues.", "method": "STREAM uses a human-annotated dataset to fine-tune a safety reasoning moderator that identifies malicious intents in dialogues.", "result": "STREAM significantly reduces the Attack Success Rate (ASR) by 51.2% while maintaining the performance of the LLM.", "conclusion": "The proposed method outperforms existing defense techniques, enhancing the safety of LLMs against multi-turn attacks.", "key_contributions": ["Introduction of SafeTy Reasoning Elicitation Alignment for Multi-Turn Dialogues (STREAM)", "Development of a human-annotated dataset for fine-tuning safety moderators", "Significant reduction in Attack Success Rate in evaluations across multiple LLMs"], "limitations": "", "keywords": ["large language models", "multi-turn dialogues", "safety reasoning", "attack defense", "human-annotated dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00671", "pdf": "https://arxiv.org/pdf/2506.00671.pdf", "abs": "https://arxiv.org/abs/2506.00671", "title": "DeepRAG: Integrating Hierarchical Reasoning and Process Supervision for Biomedical Multi-Hop QA", "authors": ["Yuelyu Ji", "Hang Zhang", "Shiven Verma", "Hui Ji", "Chun Li", "Yushui Han", "Yanshan Wang"], "categories": ["cs.CL"], "comment": null, "summary": "We propose DeepRAG, a novel framework that integrates DeepSeek hierarchical\nquestion decomposition capabilities with RAG Gym unified retrieval-augmented\ngeneration optimization using process level supervision. Targeting the\nchallenging MedHopQA biomedical question answering task, DeepRAG systematically\ndecomposes complex queries into precise sub-queries and employs concept level\nreward signals informed by the UMLS ontology to enhance biomedical accuracy.\nPreliminary evaluations on the MedHopQA dataset indicate that DeepRAG\nsignificantly outperforms baseline models, including standalone DeepSeek and\nRAG Gym, achieving notable improvements in both Exact Match and concept level\naccuracy.", "AI": {"tldr": "DeepRAG is a framework that enhances biomedical question answering by decomposing queries and optimizing retrieval-augmented generation with supervision.", "motivation": "The need for improved accuracy in biomedical question answering tasks, particularly in complex queries.", "method": "Integration of DeepSeek for hierarchical question decomposition with RAG Gym for unified retrieval-augmented generation optimization, using process level supervision.", "result": "DeepRAG outperforms baseline models on the MedHopQA dataset, achieving better Exact Match and concept level accuracy.", "conclusion": "The proposed DeepRAG framework effectively improves the accuracy of biomedical question answering by using structured query decomposition and ontology-informed rewards.", "key_contributions": ["Integration of DeepSeek with RAG Gym for biomedical QA tasks", "Use of UMLS ontology to inform reward signals", "Demonstrated significant performance improvement over baseline models"], "limitations": "", "keywords": ["DeepRAG", "biomedical question answering", "RAG Gym", "DeepSeek", "UMLS ontology"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.00694", "pdf": "https://arxiv.org/pdf/2506.00694.pdf", "abs": "https://arxiv.org/abs/2506.00694", "title": "Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments", "authors": ["Li Zhang", "Morgan Gray", "Jaromir Savelka", "Kevin D. Ashley"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50"], "comment": "11 pages, 7th Workshop on Automated Semantic Analysis of Information\n  in Legal Text, 16 June 2025, Chicago, IL", "summary": "Large Language Models (LLMs) demonstrate potential in complex legal tasks\nlike argument generation, yet their reliability remains a concern. Building\nupon pilot work assessing LLM generation of 3-ply legal arguments using human\nevaluation, this paper introduces an automated pipeline to evaluate LLM\nperformance on this task, specifically focusing on faithfulness (absence of\nhallucination), factor utilization, and appropriate abstention. We define\nhallucination as the generation of factors not present in the input case\nmaterials and abstention as the model's ability to refrain from generating\narguments when instructed and no factual basis exists. Our automated method\nemploys an external LLM to extract factors from generated arguments and\ncompares them against the ground-truth factors provided in the input case\ntriples (current case and two precedent cases). We evaluated eight distinct\nLLMs on three tests of increasing difficulty: 1) generating a standard 3-ply\nargument, 2) generating an argument with swapped precedent roles, and 3)\nrecognizing the impossibility of argument generation due to lack of shared\nfactors and abstaining. Our findings indicate that while current LLMs achieve\nhigh accuracy (over 90%) in avoiding hallucination on viable argument\ngeneration tests (Tests 1 & 2), they often fail to utilize the full set of\nrelevant factors present in the cases. Critically, on the abstention test (Test\n3), most models failed to follow instructions to stop, instead generating\nspurious arguments despite the lack of common factors. This automated pipeline\nprovides a scalable method for assessing these crucial LLM behaviors,\nhighlighting the need for improvements in factor utilization and robust\nabstention capabilities before reliable deployment in legal settings. Project\npage:\nhttps://github.com/lizhang-AIandLaw/Measuring-Faithfulness-and-Abstention.", "AI": {"tldr": "This paper presents an automated pipeline to evaluate Large Language Models (LLMs) in generating legal arguments, focusing on issues of hallucination, factor utilization, and abstention.", "motivation": "The reliability of LLMs in legal tasks is critical, and existing human evaluations revealed performance gaps that require an automated assessment method.", "method": "An automated pipeline uses an external LLM to extract factors from generated arguments, comparing them to ground-truth factors in case materials across three tests of varying complexity.", "result": "While LLMs showed over 90% accuracy in avoiding hallucinations, they struggled with utilizing relevant case factors and failed on the abstention test, generating inappropriate arguments instead of adhering to instructions.", "conclusion": "The findings stress the necessity for enhancements in factor utilization and abstention capabilities of LLMs before they can be reliably used in legal contexts.", "key_contributions": ["Development of an automated evaluation pipeline for LLMs in legal argument generation", "Empirical findings on LLMs' hallucination, factor utilization, and abstention in legal contexts", "Identification of critical gaps in current LLM capabilities for legal applications"], "limitations": "LLMs often generated irrelevant arguments despite clear instructions to abstain when no factual basis was present.", "keywords": ["Large Language Models", "legal argument generation", "abstention", "hallucination", "factor utilization"], "importance_score": 7, "read_time_minutes": 11}}
{"id": "2506.00713", "pdf": "https://arxiv.org/pdf/2506.00713.pdf", "abs": "https://arxiv.org/abs/2506.00713", "title": "From Argumentative Text to Argument Knowledge Graph: A New Framework for Structured Argumentation", "authors": ["Debarati Bhattacharjee", "Ashish Anand"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 7 figures", "summary": "This paper presents a framework to convert argumentative texts into argument\nknowledge graphs (AKG). Starting with basic annotations of argumentative\ncomponents (ACs) and argumentative relations (ARs), we enrich the information\nby constructing a knowledge base (KB) graph with metadata attributes for nodes.\nNext, we use premises and inference rules from the KB to form arguments by\napplying modus ponens. From these arguments, we create an AKG. The nodes and\nedges of the AKG have attributes that capture important argumentative features.\nWe also find missing inference rules by identifying markers. This makes it\npossible to identify undercut attacks that were previously undetectable in\nexisting datasets. The AKG gives a graphical view of the argumentative\nstructure that is easier to understand than theoretical formats. It also\nprepares the ground for future reasoning tasks, including checking the\ncoherence of arguments and identifying opportunities for revision. For this, it\nis important to find indirect relations, many of which are implicit. Our\nproposed AKG format, with annotated inference rules and modus ponens, will help\nreasoning models learn the implicit indirect relations that require inference\nover arguments and the relations between them.", "AI": {"tldr": "A framework to convert argumentative texts into argument knowledge graphs (AKG), enriching them with metadata and enabling detection of undercut attacks and reasoning tasks.", "motivation": "To improve the understanding and analysis of argumentative texts by transforming them into a more visually interpretable structure while facilitating reasoning tasks.", "method": "The framework begins with basic annotations of argumentative components and relations, constructs a knowledge base graph, applies inference rules like modus ponens, and identifies missing rules to form an argument knowledge graph.", "result": "The AKG enables easier interpretation of argumentative structures, identifies previously undetectable undercut attacks, and positions the framework for future reasoning tasks such as coherence checking.", "conclusion": "The proposed AKG format enhances reasoning models' ability to learn implicit indirect relations over arguments, supporting improved inference-related tasks.", "key_contributions": ["Development of a framework for generating argument knowledge graphs from texts", "Identification of previously undetectable undercut attacks", "Enhanced reasoning tasks through enriched argument structures"], "limitations": "", "keywords": ["Argument Knowledge Graph", "Machine Learning", "Natural Language Processing"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.00722", "pdf": "https://arxiv.org/pdf/2506.00722.pdf", "abs": "https://arxiv.org/abs/2506.00722", "title": "Chain-of-Thought Training for Open E2E Spoken Dialogue Systems", "authors": ["Siddhant Arora", "Jinchuan Tian", "Hayato Futami", "Jee-weon Jung", "Jiatong Shi", "Yosuke Kashiwagi", "Emiru Tsunoo", "Shinji Watanabe"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Unlike traditional cascaded pipelines, end-to-end (E2E) spoken dialogue\nsystems preserve full differentiability and capture non-phonemic information,\nmaking them well-suited for modeling spoken interactions. However, existing E2E\napproaches often require large-scale training data and generates responses\nlacking semantic coherence. We propose a simple yet effective strategy\nleveraging a chain-of-thought (CoT) formulation, ensuring that training on\nconversational data remains closely aligned with the multimodal language model\n(LM)'s pre-training on speech recognition~(ASR), text-to-speech synthesis\n(TTS), and text LM tasks. Our method achieves over 1.5 ROUGE-1 improvement over\nthe baseline, successfully training spoken dialogue systems on publicly\navailable human-human conversation datasets, while being compute-efficient\nenough to train on just 300 hours of public human-human conversation data, such\nas the Switchboard. We will publicly release our models and training code.", "AI": {"tldr": "This paper proposes an effective strategy for training end-to-end spoken dialogue systems using a chain-of-thought formulation, enhancing semantic coherence in responses with reduced training data requirements.", "motivation": "To address the limitations of existing end-to-end spoken dialogue systems that require large datasets and often generate semantically incoherent responses.", "method": "The authors introduce a chain-of-thought (CoT) formulation that aligns training with the pre-training tasks of multimodal language models, including ASR, TTS, and text LM tasks.", "result": "The proposed method achieved over 1.5 ROUGE-1 improvement compared to baselines when trained on 300 hours of conversation data, demonstrating both effectiveness and efficiency.", "conclusion": "This approach allows for effective training of spoken dialogue systems with much less data while maintaining semantic coherence, and the models will be made publicly available.", "key_contributions": ["Introduction of a chain-of-thought (CoT) formulation for training E2E spoken dialogue systems.", "Demonstrated training efficacy on 300 hours of data with significant performance improvements.", "Public release of models and training code."], "limitations": "", "keywords": ["end-to-end spoken dialogue systems", "chain-of-thought", "semantic coherence", "multimodal language model", "ASR"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.00726", "pdf": "https://arxiv.org/pdf/2506.00726.pdf", "abs": "https://arxiv.org/abs/2506.00726", "title": "Structured Gradient Guidance for Few-Shot Adaptation in Large Language Models", "authors": ["Hongye Zheng", "Yichen Wang", "Ray Pan", "Guiran Liu", "Binrong Zhu", "Hanlu Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a gradient-informed fine-tuning method for large language\nmodels under few-shot conditions. The goal is to enhance task adaptability and\ntraining stability when data is limited. The method builds on a base loss\nfunction and introduces two gradient-related regularization terms. The first\nenforces gradient direction consistency to guide parameter updates along\ntask-relevant directions and prevent drift. The second controls gradient\nmagnitude to avoid abnormal updates. Together, these components support a more\nefficient and stable optimization path. To further improve cross-task\ngeneralization, the method incorporates a gradient alignment mechanism. This\nmechanism measures the consistency between optimization directions of the\nsource and target tasks. It enhances fine-tuning performance in multi-task and\ncross-domain scenarios. Across various natural language understanding tasks,\nthe method outperforms existing fine-tuning strategies in average accuracy,\ngradient stability, and directional alignment. Empirical evaluations under\ndifferent sample sizes and domain-specific tasks confirm the method's\nrobustness and broad applicability in low-resource environments. In particular,\nthe method shows clear advantages in controlling parameter update paths. The\nresults demonstrate that a gradient-based fine-tuning framework can effectively\nleverage the representational power of large language models. It ensures\ntraining stability while reducing dependence on large volumes of labeled data.", "AI": {"tldr": "This paper introduces a gradient-informed fine-tuning method for large language models that improves task adaptability and training stability in few-shot settings by introducing gradient-related regularizations and a gradient alignment mechanism.", "motivation": "To enhance task adaptability and training stability for large language models when data is limited, especially in low-resource environments.", "method": "The method builds on a base loss function with two gradient-related regularization terms: one for gradient direction consistency and the other for controlling gradient magnitude. Additionally, a gradient alignment mechanism helps improve cross-task generalization.", "result": "The method outperforms existing fine-tuning strategies across various natural language understanding tasks in terms of average accuracy, gradient stability, and directional alignment.", "conclusion": "A gradient-based fine-tuning framework effectively leverages large language models' representational power, ensuring training stability while minimizing reliance on large labeled datasets.", "key_contributions": ["Gradient direction consistency regularization", "Gradient magnitude control", "Gradient alignment mechanism for cross-task generalization"], "limitations": "", "keywords": ["large language models", "fine-tuning", "few-shot learning", "gradient regularization", "cross-task generalization"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00737", "pdf": "https://arxiv.org/pdf/2506.00737.pdf", "abs": "https://arxiv.org/abs/2506.00737", "title": "Narrative Media Framing in Political Discourse", "authors": ["Yulia Otmakhova", "Lea Frermann"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Narrative frames are a powerful way of conceptualizing and communicating\ncomplex, controversial ideas, however automated frame analysis to date has\nmostly overlooked this framing device. In this paper, we connect elements of\nnarrativity with fundamental aspects of framing, and present a framework which\nformalizes and operationalizes such aspects. We annotate and release a data set\nof news articles in the climate change domain, analyze the dominance of\nnarrative frame components across political leanings, and test LLMs in their\nability to predict narrative frames and their components. Finally, we apply our\nframework in an unsupervised way to elicit components of narrative framing in a\nsecond domain, the COVID-19 crisis, where our predictions are congruent with\nprior theoretical work showing the generalizability of our approach.", "AI": {"tldr": "The paper presents a framework that connects narrativity and framing, analyzes narrative frames in climate change and COVID-19, and tests LLMs in predicting these frames.", "motivation": "Automated frame analysis has largely overlooked narrative frames, despite their significance in discussing complex ideas.", "method": "The authors propose a framework that operationalizes narrative and framing concepts, annotate a dataset of climate change news articles, and test LLMs for narrative frame prediction.", "result": "The analysis reveals the dominance of narrative frame components across different political leanings, and the framework successfully predicts narrative frames in the COVID-19 domain, aligning with existing theories.", "conclusion": "The approach demonstrates the generalizability of the framework across domains, providing insights into narrative framing's role in public discourse.", "key_contributions": ["Development of a framework connecting narrativity and framing", "Release of an annotated dataset of climate change articles", "Validation of LLMs in predicting narrative frames across domains."], "limitations": "The study may be limited by the specific domains analyzed and the generalizability of the findings beyond the tested topics.", "keywords": ["narrative frames", "framing analysis", "climate change", "COVID-19", "LLMs"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00739", "pdf": "https://arxiv.org/pdf/2506.00739.pdf", "abs": "https://arxiv.org/abs/2506.00739", "title": "DefenderBench: A Toolkit for Evaluating Language Agents in Cybersecurity Environments", "authors": ["Chiyu Zhang", "Marc-Alexandre Cote", "Michael Albada", "Anush Sankaran", "Jack W. Stokes", "Tong Wang", "Amir Abdi", "William Blum", "Muhammad Abdul-Mageed"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model (LLM) agents have shown impressive capabilities in human\nlanguage comprehension and reasoning, yet their potential in cybersecurity\nremains underexplored. We introduce DefenderBench, a practical, open-source\ntoolkit for evaluating language agents across offense, defense, and\ncybersecurity knowledge-based tasks. DefenderBench includes environments for\nnetwork intrusion, malicious content detection, code vulnerability analysis,\nand cybersecurity knowledge assessment. It is intentionally designed to be\naffordable and easily accessible for researchers while providing fair and\nrigorous assessment. We benchmark several state-of-the-art (SoTA) and popular\nLLMs, including both open- and closed-weight models, using a standardized\nagentic framework. Our results show that Claude-3.7-sonnet performs best with a\nDefenderBench score of 81.65, followed by Claude-3.7-sonnet-think with 78.40,\nwhile the best open-weight model, Llama 3.3 70B, is not far behind with a\nDefenderBench score of 71.81. DefenderBench's modular design allows seamless\nintegration of custom LLMs and tasks, promoting reproducibility and fair\ncomparisons. An anonymized version of DefenderBench is available at\nhttps://github.com/microsoft/DefenderBench.", "AI": {"tldr": "Introducing DefenderBench, a toolkit for evaluating language agents in cybersecurity tasks.", "motivation": "Exploring the potential of large language model agents in the field of cybersecurity, which remains largely underexplored.", "method": "Developed an open-source toolkit (DefenderBench) for evaluating LLMs on various cybersecurity tasks including network intrusion and malicious content detection, benchmarking several SoTA models within a standardized framework.", "result": "Benchmark results indicate that Claude-3.7-sonnet achieved the highest score of 81.65, with significant performances from other models as well, highlighting effectiveness across tasks.", "conclusion": "DefenderBench provides a modular and accessible platform for evaluating and comparing LLMs in cybersecurity, enhancing reproducibility and supporting diverse custom tasks.", "key_contributions": ["Introduction of DefenderBench toolkit for cybersecurity evaluation", "Benchmarking of various LLMs against standardized tasks", "Modular design promoting custom integration and comparison"], "limitations": "", "keywords": ["large language models", "cybersecurity", "evaluation toolkit", "machine learning", "AI applications"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2506.00740", "pdf": "https://arxiv.org/pdf/2506.00740.pdf", "abs": "https://arxiv.org/abs/2506.00740", "title": "Length Aware Speech Translation for Video Dubbing", "authors": ["Harveen Singh Chadha", "Aswin Shanmugam Subramanian", "Vikas Joshi", "Shubham Bansal", "Jian Xue", "Rupeshkumar Mehta", "Jinyu Li"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "This paper was accepted to Interspeech 2025", "summary": "In video dubbing, aligning translated audio with the source audio is a\nsignificant challenge. Our focus is on achieving this efficiently, tailored for\nreal-time, on-device video dubbing scenarios. We developed a phoneme-based\nend-to-end length-sensitive speech translation (LSST) model, which generates\ntranslations of varying lengths short, normal, and long using predefined tags.\nAdditionally, we introduced length-aware beam search (LABS), an efficient\napproach to generate translations of different lengths in a single decoding\npass. This approach maintained comparable BLEU scores compared to a baseline\nwithout length awareness while significantly enhancing synchronization quality\nbetween source and target audio, achieving a mean opinion score (MOS) gain of\n0.34 for Spanish and 0.65 for Korean, respectively.", "AI": {"tldr": "The paper presents a novel phoneme-based speech translation model for real-time video dubbing, addressing synchronization challenges between translated and source audio.", "motivation": "Aligning translated audio with the source audio in video dubbing is crucial yet challenging, especially for real-time, on-device applications.", "method": "We developed a length-sensitive speech translation (LSST) model that generates translations of varying lengths using predefined tags. We also introduced length-aware beam search (LABS) for efficient translation generation.", "result": "Our approach achieved comparable BLEU scores to a baseline without length awareness while significantly improving audio synchronization. We observed MOS gains of 0.34 for Spanish and 0.65 for Korean.", "conclusion": "The proposed LSST model and LABS technique enhance the quality and efficiency of video dubbing, making it suitable for real-time applications.", "key_contributions": ["Phoneme-based end-to-end speech translation model", "Length-aware beam search for multi-length translation", "Improved audio synchronization in translation"], "limitations": "", "keywords": ["speech translation", "video dubbing", "length-sensitive", "real-time", "synchronization"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.00741", "pdf": "https://arxiv.org/pdf/2506.00741.pdf", "abs": "https://arxiv.org/abs/2506.00741", "title": "Data Swarms: Optimizable Generation of Synthetic Evaluation Data", "authors": ["Shangbin Feng", "Yike Wang", "Weijia Shi", "Yulia Tsvetkov"], "categories": ["cs.CL"], "comment": null, "summary": "We propose Data Swarms, an algorithm to optimize the generation of synthetic\nevaluation data and advance quantitative desiderata of LLM evaluation. We first\ntrain a swarm of initial data generators using existing data, and define\nvarious evaluation objectives to reflect the desired properties of evaluation\n(e.g., generate more difficult problems for the evaluated models) and\nquantitatively evaluate data generators. We then employ particle swarm\noptimization to optimize the swarm of data generators, where they\ncollaboratively search through the model parameter space to find new generators\nthat advance these objectives. We further extend it to Adversarial Swarms,\nwhere the data generator swarm generates harder data while the test taker model\nswarm learns from such data, co-evolving dynamically for better data and models\nsimultaneously. Extensive experiments demonstrate that Data Swarms outperforms\neight data generation baselines across five evaluation objectives, while\nAdversarial Swarms produce more robust learning of synthetic data and stronger\ngeneralization. Further analysis reveals that Data Swarms successfully\noptimizes compositions of multiple evaluation objectives and generalizes to new\noff-the-shelf LLMs, unseen at optimization time.", "AI": {"tldr": "Data Swarms is an algorithm designed to optimize the generation of synthetic evaluation data for LLM evaluation, achieving better performance through collaborative swarm-based optimization techniques.", "motivation": "To improve the generation of synthetic evaluation data and meet quantitative evaluation objectives for LLM performance assessment.", "method": "The method uses particle swarm optimization to train a swarm of data generators, optimizing them based on evaluation objectives and later extending this to Adversarial Swarms for dynamic co-evolution of data generators and test models.", "result": "Data Swarms outperform eight existing data generation baselines across five evaluation objectives, while Adversarial Swarms improve robustness in learning from synthetic data.", "conclusion": "The proposed methods effectively optimize multiple evaluation objectives and demonstrate generalization capabilities with previously unseen LLMs.", "key_contributions": ["Introduction of Data Swarms for synthetic data generation optimization.", "Extension to Adversarial Swarms for dynamic model learning.", "Demonstrated effectiveness in solving evaluation problems for LLMs."], "limitations": "", "keywords": ["Data Swarms", "Adversarial Swarms", "LLM evaluation", "synthetic data generation", "particle swarm optimization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00743", "pdf": "https://arxiv.org/pdf/2506.00743.pdf", "abs": "https://arxiv.org/abs/2506.00743", "title": "Assortment of Attention Heads: Accelerating Federated PEFT with Head Pruning and Strategic Client Selection", "authors": ["Yeshwanth Venkatesha", "Souvik Kundu", "Priyadarshini Panda"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Parameter Efficient Fine-Tuning (PEFT) has become the de-facto approach in\nadapting Large Language Models (LLMs) for downstream tasks in Natural Language\nProcessing. However, its adoption in privacy-preserving distributed learning\nframeworks, such as Federated Learning (FL), remains relatively limited. This\nis mainly due to challenges specific to FL, such as resource-constrained\ndevices and diverse data distributions among clients. In this paper, we propose\nan efficient method to perform PEFT within the FL framework for Multi-Head\nAttention (MHA) based language models. We address the challenges through head\npruning, a novel head-specific weighted aggregation mechanism, and a client\nselection strategy. Head pruning minimizes training complexity within the\nclients, guided by the importance score computed based on the confidence of the\nattention head. Weighted aggregation of heads ensures the global model captures\ncrucial updates from diverse clients complementing our client selection\nstrategy. We show results on the MultiNLI benchmark along with 20 Newsgroups,\nXL-Sum, and E2E NLG datasets. We use the MultiNLI dataset and T5-small model\nwith LoRA as our PEFT method, attaining sparsity levels of up to 90%, resulting\nin a communication advantage of up to 1.8x and a reduction in training OPs of\n3.9x while maintaining the accuracy drop under 2%.", "AI": {"tldr": "This paper proposes an efficient method for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs) within the Federated Learning (FL) framework, addressing challenges related to resource constraints and diverse data distributions among clients.", "motivation": "To enable the adoption of PEFT techniques within Federated Learning contexts, which face unique challenges such as limited resources and heterogeneous data among clients.", "method": "The approach involves head pruning, a head-specific weighted aggregation mechanism, and a client selection strategy to efficiently perform PEFT on Multi-Head Attention based language models.", "result": "The proposed method demonstrated significant improvements on the MultiNLI, 20 Newsgroups, XL-Sum, and E2E NLG datasets, achieving up to 90% sparsity, a communication advantage of 1.8x, and a reduction in training operations of 3.9x with an accuracy drop of less than 2%.", "conclusion": "The method presents a viable solution for adapting LLMs in distributed environments, efficiently mitigating challenges in Federated Learning while preserving model accuracy.", "key_contributions": ["Introduction of head pruning for reducing training complexity in Federated Learning", "Development of a novel head-specific weighted aggregation mechanism", "Implementation of an effective client selection strategy"], "limitations": "The focus remains on specific types of LLMs and datasets, which may limit generalizability to other models or tasks.", "keywords": ["Parameter Efficient Fine-Tuning", "Federated Learning", "Multi-Head Attention", "Privacy-preserving", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.00748", "pdf": "https://arxiv.org/pdf/2506.00748.pdf", "abs": "https://arxiv.org/abs/2506.00748", "title": "Translate With Care: Addressing Gender Bias, Neutrality, and Reasoning in Large Language Model Translations", "authors": ["Pardis Sadat Zahraei", "Ali Emami"], "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Findings of ACL 2025", "summary": "Addressing gender bias and maintaining logical coherence in machine\ntranslation remains challenging, particularly when translating between natural\ngender languages, like English, and genderless languages, such as Persian,\nIndonesian, and Finnish. We introduce the Translate-with-Care (TWC) dataset,\ncomprising 3,950 challenging scenarios across six low- to mid-resource\nlanguages, to assess translation systems' performance. Our analysis of diverse\ntechnologies, including GPT-4, mBART-50, NLLB-200, and Google Translate,\nreveals a universal struggle in translating genderless content, resulting in\ngender stereotyping and reasoning errors. All models preferred masculine\npronouns when gender stereotypes could influence choices. Google Translate and\nGPT-4 showed particularly strong bias, favoring male pronouns 4-6 times more\nthan feminine ones in leadership and professional success contexts. Fine-tuning\nmBART-50 on TWC substantially resolved these biases and errors, led to strong\ngeneralization, and surpassed proprietary LLMs while remaining open-source.\nThis work emphasizes the need for targeted approaches to gender and semantic\ncoherence in machine translation, particularly for genderless languages,\ncontributing to more equitable and accurate translation systems.", "AI": {"tldr": "This paper presents the Translate-with-Care (TWC) dataset aimed at examining gender bias and logical coherence in machine translation across genderless languages. The study evaluates various translation models and identifies significant biases, proposing fine-tuning methods to improve performance.", "motivation": "To address the challenges of gender bias and logical coherence in machine translation, particularly for genderless languages.", "method": "The study introduces the TWC dataset, consisting of 3,950 scenarios, and analyzes translation models like GPT-4, mBART-50, NLLB-200, and Google Translate to assess their performance and biases.", "result": "Analysis shows all models favored masculine pronouns, with Google Translate and GPT-4 exhibiting significant gender bias. Fine-tuning mBART-50 on the TWC dataset improved bias and generalization.", "conclusion": "Emphasizes the need for targeted approaches to mitigate gender bias in machine translation systems for genderless languages, contributing to more equitable translations.", "key_contributions": ["Introduction of the TWC dataset for assessing gender bias in translation", "Demonstration of significant biases in major translation models", "Improvement of mBART-50 performance through fine-tuning on the TWC dataset"], "limitations": "", "keywords": ["gender bias", "machine translation", "natural language processing", "low-resource languages", "semantic coherence"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00759", "pdf": "https://arxiv.org/pdf/2506.00759.pdf", "abs": "https://arxiv.org/abs/2506.00759", "title": "Understanding and Mitigating Cross-lingual Privacy Leakage via Language-specific and Universal Privacy Neurons", "authors": ["Wenshuo Dong", "Qingsong Yang", "Shu Yang", "Lijie Hu", "Meng Ding", "Wanyu Lin", "Tianhang Zheng", "Di Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) trained on massive data capture rich information\nembedded in the training data. However, this also introduces the risk of\nprivacy leakage, particularly involving personally identifiable information\n(PII). Although previous studies have shown that this risk can be mitigated\nthrough methods such as privacy neurons, they all assume that both the\n(sensitive) training data and user queries are in English. We show that they\ncannot defend against the privacy leakage in cross-lingual contexts: even if\nthe training data is exclusively in one language, these (private) models may\nstill reveal private information when queried in another language. In this\nwork, we first investigate the information flow of cross-lingual privacy\nleakage to give a better understanding. We find that LLMs process private\ninformation in the middle layers, where representations are largely shared\nacross languages. The risk of leakage peaks when converted to a\nlanguage-specific space in later layers. Based on this, we identify\nprivacy-universal neurons and language-specific privacy neurons.\nPrivacy-universal neurons influence privacy leakage across all languages, while\nlanguage-specific privacy neurons are only related to specific languages. By\ndeactivating these neurons, the cross-lingual privacy leakage risk is reduced\nby 23.3%-31.6%.", "AI": {"tldr": "This paper investigates the risk of privacy leakage in Large Language Models (LLMs) during cross-lingual interactions and proposes deactivating specific neurons to mitigate this risk.", "motivation": "To address the risk of privacy leakage in LLMs when trained and queried in different languages, extending prior findings that assumed English-only contexts.", "method": "The authors analyze the information flow in LLMs to understand how private data is processed across layers, identifying privacy-universal and language-specific privacy neurons and evaluating their impact on cross-lingual privacy leakage.", "result": "Deactivating identified neurons can reduce the cross-lingual privacy leakage risk by 23.3%-31.6%.", "conclusion": "The study highlights the importance of considering cross-lingual contexts in LLM privacy mechanisms and offers a potential solution to mitigate the risk.", "key_contributions": ["Identification of privacy-universal and language-specific privacy neurons", "Analysis of information flow related to privacy leakage in LLMs", "Demonstration of risk reduction methods for cross-lingual queries"], "limitations": "", "keywords": ["Large Language Models", "Cross-Lingual Privacy", "Privacy Leakage", "Neurons", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.00773", "pdf": "https://arxiv.org/pdf/2506.00773.pdf", "abs": "https://arxiv.org/abs/2506.00773", "title": "Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models", "authors": ["Boheng Sheng", "Jiacheng Yao", "Meicong Zhang", "Guoxiu He"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often struggle to accurately read and comprehend\nextremely long texts. Current methods for improvement typically rely on\nsplitting long contexts into fixed-length chunks. However, fixed truncation\nrisks separating semantically relevant content, leading to ambiguity and\ncompromising accurate understanding. To overcome this limitation, we propose a\nstraightforward approach for dynamically separating and selecting chunks of\nlong context, facilitating a more streamlined input for LLMs. In particular, we\ncompute semantic similarities between adjacent sentences, using lower\nsimilarities to adaptively divide long contexts into variable-length chunks. We\nfurther train a question-aware classifier to select sensitive chunks that are\ncritical for answering specific questions. Experimental results on both\nsingle-hop and multi-hop question-answering benchmarks show that the proposed\napproach consistently outperforms strong baselines. Notably, it maintains\nrobustness across a wide range of input lengths, handling sequences of up to\n256k tokens. Our datasets and code are available at the following link:\nhttps://github.com/ECNU-Text-Computing/DCS", "AI": {"tldr": "This paper presents a method for dynamically separating and selecting chunks of long texts for large language models (LLMs) to enhance comprehension and accuracy in question-answering tasks.", "motivation": "Large language models face challenges in reading and understanding extremely long texts due to fixed-length chunking methods that often split semantically relevant content, leading to ambiguity.", "method": "The authors propose a method that computes semantic similarities between sentences to adaptively divide long contexts into variable-length chunks. A question-aware classifier is trained to select sensitive chunks critical for answering questions.", "result": "The experimental results demonstrate that the proposed approach consistently outperforms strong baseline methods on both single-hop and multi-hop question-answering benchmarks, maintaining robustness across various input lengths, including sequences of up to 256k tokens.", "conclusion": "The approach significantly improves LLMs' performance on long text comprehension, facilitating better question-answering capabilities without losing important semantic information.", "key_contributions": ["Dynamic separation of long text into variable-length chunks based on semantic similarities", "Training of a question-aware classifier to select critical chunks", "Demonstrated effectiveness on multiple question-answering benchmarks"], "limitations": "", "keywords": ["large language models", "semantic similarity", "question-answering", "text chunking", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00777", "pdf": "https://arxiv.org/pdf/2506.00777.pdf", "abs": "https://arxiv.org/abs/2506.00777", "title": "Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge", "authors": ["Md Tahmid Rahman Laskar", "Israt Jahan", "Elham Dolatabadi", "Chun Peng", "Enamul Hoque", "Jimmy Huang"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nbiomedical relation extraction, even in zero-shot scenarios. However,\nevaluating LLMs in this task remains challenging due to their ability to\ngenerate human-like text, often producing synonyms or abbreviations of\ngold-standard answers, making traditional automatic evaluation metrics\nunreliable. On the other hand, while human evaluation is more reliable, it is\ncostly and time-consuming, making it impractical for real-world applications.\nThis paper investigates the use of LLMs-as-the-Judge as an alternative\nevaluation method for biomedical relation extraction. We benchmark 8 LLMs as\njudges to evaluate the responses generated by 5 other LLMs across 3 biomedical\nrelation extraction datasets. Unlike other text-generation tasks, we observe\nthat LLM-based judges perform quite poorly (usually below 50% accuracy) in the\nbiomedical relation extraction task. Our findings reveal that it happens mainly\nbecause relations extracted by LLMs do not adhere to any standard format. To\naddress this, we propose structured output formatting for LLM-generated\nresponses that helps LLM-Judges to improve their performance by about 15% (on\naverage). We also introduce a domain adaptation technique to further enhance\nLLM-Judge performance by effectively transferring knowledge between datasets.\nWe release both our human-annotated and LLM-annotated judgment data (36k\nsamples in total) for public use here:\nhttps://github.com/tahmedge/llm_judge_biomedical_re.", "AI": {"tldr": "The paper investigates the use of LLMs as evaluators for biomedical relation extraction, revealing that traditional automatic evaluation is unreliable and human evaluation is impractical, with proposed improvements for LLM-based judges.", "motivation": "Evaluating LLMs in biomedical relation extraction is challenging due to their ability to produce varied outputs like synonyms, making traditional metrics unreliable.", "method": "The paper benchmarks 8 LLMs as judges evaluating outputs from 5 other LLMs on 3 biomedical relation extraction datasets, proposing structured output formatting and domain adaptation techniques to enhance evaluation accuracy.", "result": "LLM-based judges generally perform poorly (below 50% accuracy), primarily because extracted relations do not adhere to standard formats. Proposed structured formatting improves accuracy by about 15%.", "conclusion": "The findings highlight the limitations of LLMs as evaluators and suggest methods for improving their evaluation capabilities in biomedical tasks.", "key_contributions": ["Introduces LLMs-as-the-Judge for evaluating biomedical relation extraction.", "Proposes structured output formatting to enhance evaluation accuracy.", "Presents a domain adaptation technique for better knowledge transfer between datasets."], "limitations": "LLM judges performed below expected accuracy rates, indicating limitations in current evaluation methods for biomedical relations.", "keywords": ["Large Language Models", "Biomedical Relation Extraction", "Evaluation Methods"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00783", "pdf": "https://arxiv.org/pdf/2506.00783.pdf", "abs": "https://arxiv.org/abs/2506.00783", "title": "KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision", "authors": ["Rong Wu", "Pinlong Cai", "Jianbiao Mei", "Licheng Wen", "Tao Hu", "Xuemeng Yang", "Daocheng Fu", "Botian Shi"], "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 13 figures", "summary": "Large language models (LLMs) have made remarkable strides in various natural\nlanguage processing tasks, but their performance on complex reasoning problems\nremains hindered by a lack of explainability and trustworthiness. This issue,\noften manifesting as hallucinations or unattributable reasoning processes,\nlimits their applicability in complex reasoning scenarios. To address this, we\npropose Knowledge Graph-constrained Trajectory Reasoning Attribution and Chain\nExplanation Supervision (KG-TRACES), a novel framework that enhances the\nreasoning ability of LLMs through explicit supervision over reasoning paths and\nprocesses. KG-TRACES jointly supervises the model to: (1) predict symbolic\nrelation paths, (2) predict full triple-level reasoning paths, and (3) generate\nattribution-aware reasoning processes grounded in the reasoning paths. At\ninference phase, the model adapts to both KG-available and KG-unavailable\nscenarios, retrieving reasoning paths from a KG when possible or predicting\nplausible reasoning paths with only intrinsic knowledge when not. This design\nenables the model to reason in an explainable and source-attributable pattern.\nThrough extensive experiments on complex reasoning tasks, we demonstrate that\nKG-TRACES significantly outperforms existing SOTA: it improves Hits@1 by 1.6%\nand F1 by 4.7% on WebQSP, and achieves improvements of 4.8% in Hits@1 and 2.1%\nin F1 on CWQ. Moreover, we show its transferability to specialized domains such\nas medicine. By visualizing the intermediate steps of reasoning processes, we\nfurther show that the explicit supervision introduced by KG-TRACES leads to\nmore stable and goal-directed reasoning processes, aligning closely with\ncorrect answers. Code is available at https://github.com/Edaizi/KG-TRACES.", "AI": {"tldr": "Proposes KG-TRACES, a framework to enhance LLM reasoning via explicit supervision over reasoning paths, improving explainability and performance in complex reasoning tasks.", "motivation": "Addresses limitations of LLMs in complex reasoning due to lack of explainability and trustworthiness, which lead to hallucinations and unattributable reasoning.", "method": "KG-TRACES supervises models to predict symbolic relation paths, full triple-level reasoning paths, and generates attribution-aware reasoning processes, adapting to both KG-available and KG-unavailable scenarios.", "result": "KG-TRACES improves Hits@1 by 1.6% and F1 by 4.7% on WebQSP, and Hits@1 by 4.8% and F1 by 2.1% on CWQ, demonstrating effective reasoning in specialized domains like medicine.", "conclusion": "The framework enhances LLM performance by promoting explainable and source-attributable reasoning, with visualizations indicating more stable reasoning processes.", "key_contributions": ["Introduction of KG-TRACES for enhanced LLM explainability", "Demonstrated improvements in complex reasoning tasks", "Transferability to specialized domains like medicine."], "limitations": "", "keywords": ["Large Language Models", "Reasoning", "Knowledge Graph", "Explainability", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00784", "pdf": "https://arxiv.org/pdf/2506.00784.pdf", "abs": "https://arxiv.org/abs/2506.00784", "title": "Research Borderlands: Analysing Writing Across Research Cultures", "authors": ["Shaily Bhatt", "Tal August", "Maria Antoniak"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Improving cultural competence of language technologies is important. However\nmost recent works rarely engage with the communities they study, and instead\nrely on synthetic setups and imperfect proxies of culture. In this work, we\ntake a human-centered approach to discover and measure language-based cultural\nnorms, and cultural competence of LLMs. We focus on a single kind of culture,\nresearch cultures, and a single task, adapting writing across research\ncultures. Through a set of interviews with interdisciplinary researchers, who\nare experts at moving between cultures, we create a framework of structural,\nstylistic, rhetorical, and citational norms that vary across research cultures.\nWe operationalise these features with a suite of computational metrics and use\nthem for (a) surfacing latent cultural norms in human-written research papers\nat scale; and (b) highlighting the lack of cultural competence of LLMs, and\ntheir tendency to homogenise writing. Overall, our work illustrates the\nefficacy of a human-centered approach to measuring cultural norms in\nhuman-written and LLM-generated texts.", "AI": {"tldr": "This paper presents a human-centered approach to measure cultural competence in language technologies, specifically focusing on research cultures and the adaptation of writing across these cultures.", "motivation": "The goal is to improve cultural competence in language technologies, which often lack engagement with the communities they represent.", "method": "The study involves interviews with interdisciplinary researchers to develop a framework of cultural norms and operationalizing these through computational metrics.", "result": "The research identifies latent cultural norms in human-written research papers and reveals the limitations of LLMs in terms of cultural competence.", "conclusion": "The findings demonstrate the effectiveness of a human-centered method for analyzing cultural norms in both human-generated and LLM-generated texts.", "key_contributions": ["Development of a framework for cultural norms across research cultures", "Creation of computational metrics to measure cultural competence", "Insights into the homogenizing effects of LLMs on writing"], "limitations": "The study is focused solely on research cultures, which may not generalize to other cultural contexts.", "keywords": ["Cultural competence", "Language models", "Human-centered approach", "Research cultures", "Natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00789", "pdf": "https://arxiv.org/pdf/2506.00789.pdf", "abs": "https://arxiv.org/abs/2506.00789", "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems", "authors": ["Yixiao Zeng", "Tianyu Cao", "Danqing Wang", "Xinran Zhao", "Zimeng Qiu", "Morteza Ziyadi", "Tongshuang Wu", "Lei Li"], "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains.", "AI": {"tldr": "This paper introduces the Retrieval-Aware Robustness Evaluation (RARE) framework, designed to assess the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise and query perturbations, using a large-scale benchmark.", "motivation": "The need for better evaluation methods for RAG systems to withstand real-world challenges, including conflicting contexts and fast-changing facts.", "method": "RARE uses a knowledge-graph-driven synthesis pipeline to automatically generate question sets and extract relations from documents in a dynamic corpus. It establishes robustness metrics to quantify model resilience against perturbations.", "result": "The study finds that RAG systems are vulnerable to perturbations, particularly demonstrating lower robustness on multi-hop queries compared to single-hop queries, revealing document robustness as a significant weakness.", "conclusion": "The RARE framework provides a comprehensive method for evaluating the robustness of RAG systems, highlighting areas for improvement in handling dynamic and noisy environments.", "key_contributions": ["Introduction of the RARE framework for robustness evaluation of RAG systems.", "Creation of the RARE-Set dataset with evolving questions and domains.", "Formalization of retrieval-conditioned robustness metrics (RARE-Met)."], "limitations": "The evaluation is focused on finance, economics, and policy documents, potentially limiting generalizability across other domains.", "keywords": ["Retrieval-Augmented Generation", "robustness evaluation", "knowledge graph", "resilience metrics", "dynamic corpus"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00806", "pdf": "https://arxiv.org/pdf/2506.00806.pdf", "abs": "https://arxiv.org/abs/2506.00806", "title": "Fast or Slow? Integrating Fast Intuition and Deliberate Thinking for Enhancing Visual Question Answering", "authors": ["Songtao Jiang", "Chenyi Zhou", "Yan Zhang", "Yeying Jin", "Zuozhu Liu"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) still struggle with complex\nreasoning tasks in Visual Question Answering (VQA). While current methods have\nadvanced by incorporating visual prompts, our study uncovers critical\nlimitations: these approaches indiscriminately annotate all detected objects\nfor every visual question, generating excessive visual markers that degrade\ntask performance. This issue stems primarily from a lack of focus on key visual\nelements, raising two important questions: Are all objects equally important,\nand do all questions require visual prompts? Motivated by Dual Process Theory,\nwhich distinguishes between instinctive and deliberate cognitive modes in human\nreasoning, we propose FOCUS, a plug-and-play approach that dynamically adapts\nto the complexity of questions, combining fast intuitive judgments with\ndeliberate analytical reasoning to enhance the vision-language reasoning\ncapability of the MLLM. For straightforward questions, FOCUS supports efficient\nzero-shot reasoning. For more complex tasks, it employs the conceptualizing\nbefore observation strategy to highlight critical elements. Extensive\nexperiments on four benchmarks, ScienceQA, TextQA, VizWiz, and MME, demonstrate\nthat FOCUS consistently improves the performance of both open-source and\nblack-box MLLMs, achieving significant gains across all datasets. Ablation\nstudies further validate the importance of combining diverse cognitive\nstrategies with refined visual information for superior performance. Code will\nbe released.", "AI": {"tldr": "This paper introduces FOCUS, a method that improves Visual Question Answering in multimodal large language models by focusing on key visual elements and adapting reasoning strategies based on question complexity.", "motivation": "Current multimodal language models face challenges in complex reasoning tasks in Visual Question Answering due to indiscriminate annotation of visual elements, leading to degraded performance.", "method": "FOCUS dynamically adapts to the complexity of questions by combining intuitive judgments with analytical reasoning, utilizing zero-shot reasoning for simple questions and a conceptualizing strategy for complex ones.", "result": "Extensive testing on four benchmarks shows that FOCUS significantly enhances performance in Visual Question Answering for both open-source and black-box multimodal language models across all datasets tested.", "conclusion": "Combining diverse cognitive strategies with refined visual information is essential for improving reasoning capabilities in visual-context tasks.", "key_contributions": ["Introduction of the FOCUS method for VQA", "Demonstration of significant performance gains across multiple benchmarks", "Validation of cognitive strategy integration in multimodal reasoning"], "limitations": "", "keywords": ["Multimodal large language models", "Visual Question Answering", "Cognitive theory"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00814", "pdf": "https://arxiv.org/pdf/2506.00814.pdf", "abs": "https://arxiv.org/abs/2506.00814", "title": "GuessBench: Sensemaking Multimodal Creativity in the Wild", "authors": ["Zifeng Zhu", "Shangbin Feng", "Herun Wan", "Ningnan Wang", "Minnan Luo", "Yulia Tsvetkov"], "categories": ["cs.CL"], "comment": null, "summary": "We propose GuessBench, a novel benchmark that evaluates Vision Language\nModels (VLMs) on modeling the pervasive, noisy, and pluralistic human\ncreativity. GuessBench sources data from \"Guess the Build\", an online\nmultiplayer Minecraft minigame where one player constructs a Minecraft build\ngiven a concept (e.g. caterpillar) and others try to guess it with natural\nlanguage hints, presenting a pristine testbed for sensemaking creativity in the\nwild with VLMs acting as guessers. We curate 1500 images from the actual\ngameplay and design 2000 problems spanning static and dynamic image settings,\nnatural language hints of varying completeness, and more. Extensive experiments\nwith six open/API VLMs and five reasoning enhancement approaches demonstrate\nthat GuessBench presents a uniquely challenging task in creativity modeling:\neven the start-of-the-art GPT-4o is incorrect on 34% of instances, while we\nobserve a huge performance gap (13.87% vs. 53.93% on average) between open and\nAPI models. When used as a resource to improve VLMs, fine-tuning on the\nreasoning traces for GuessBench problems improves visual perception tasks by\n15.36% on average. Further analysis reveals that VLM performance in creativity\nsensemaking correlates with the frequency of the concept in training data,\nwhile the accuracy drops sharply for concepts in underrepresented cultural\ncontexts and low-resource languages.", "AI": {"tldr": "GuessBench is a novel benchmark for evaluating Vision Language Models (VLMs) on human creativity through a Minecraft minigame, showing significant performance gaps among models and improvements with fine-tuning.", "motivation": "To evaluate VLMs' ability to model human creativity in a noisy and pluralistic setting using data from a multiplayer Minecraft game.", "method": "Curated 1500 gameplay images and designed 2000 problems involving various image settings and natural language hints; evaluated six VLMs and five reasoning enhancement approaches.", "result": "VLMs, including GPT-4o, made mistakes in 34% of tasks, highlighting a significant gap in performance between open and API models (13.87% vs. 53.93% on average); fine-tuning improved visual tasks by 15.36%.", "conclusion": "VLM performance in creativity correlates with training data diversity, particularly affected by underrepresentation of cultural contexts and low-resource languages.", "key_contributions": ["Introduces GuessBench for VLM evaluation in creativity", "Highlights performance disparities between model types", "Shows effectiveness of fine-tuning using reasoning traces"], "limitations": "", "keywords": ["Vision Language Models", "creativity modeling", "Minecraft", "benchmark", "fine-tuning"], "importance_score": 5, "read_time_minutes": 7}}
{"id": "2506.00815", "pdf": "https://arxiv.org/pdf/2506.00815.pdf", "abs": "https://arxiv.org/abs/2506.00815", "title": "From Plain Text to Poetic Form: Generating Metrically-Constrained Sanskrit Verses", "authors": ["Manoj Balaji Jagadeeshan", "Samarth Bhatia", "Pretam Ray", "Harshul Raj Surana", "Akhil Rajeev P", "Priya Mishra", "Annarao Kulkarni", "Ganesh Ramakrishnan", "Prathosh AP", "Pawan Goyal"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have significantly improved\nnatural language generation, including creative tasks like poetry composition.\nHowever, most progress remains concentrated in high-resource languages. This\nraises an important question: Can LLMs be adapted for structured poetic\ngeneration in a low-resource, morphologically rich language such as Sanskrit?\nIn this work, we introduce a dataset designed for translating English prose\ninto structured Sanskrit verse, with strict adherence to classical metrical\npatterns, particularly the Anushtub meter. We evaluate a range of generative\nmodels-both open-source and proprietary-under multiple settings. Specifically,\nwe explore constrained decoding strategies and instruction-based fine-tuning\ntailored to metrical and semantic fidelity. Our decoding approach achieves over\n99% accuracy in producing syntactically valid poetic forms, substantially\noutperforming general-purpose models in meter conformity. Meanwhile,\ninstruction-tuned variants show improved alignment with source meaning and\npoetic style, as supported by human assessments, albeit with marginal\ntrade-offs in metrical precision.", "AI": {"tldr": "This work explores the adaptation of large language models for generating structured poetry in Sanskrit, showcasing high accuracy in adherence to metrical patterns.", "motivation": "To investigate whether LLMs can be effectively adapted for producing structured poetic forms in a low-resource language like Sanskrit, particularly given the advancements in high-resource languages.", "method": "Introduced a dataset for translating English prose into Sanskrit verse while adhering to classical metrical patterns; evaluated various generative models with a focus on constrained decoding strategies and instruction-based fine-tuning.", "result": "Achieved over 99% accuracy in creating syntactically valid poetic forms, outperforming general-purpose models in meter conformity. Instruction-tuned models demonstrated better alignment with source meaning and poetic style, despite some trade-offs in metrical precision.", "conclusion": "Constrained decoding and instruction tuning are effective strategies for enhancing LLMs' ability to generate structured poetry in low-resource languages.", "key_contributions": ["Creation of a dataset for structured poetic generation in Sanskrit", "Demonstration of high accuracy using constrained decoding", "Insights into instruction-based fine-tuning for poetic fidelity"], "limitations": "The study indicates marginal trade-offs in metrical precision with instruction-tuned models.", "keywords": ["large language models", "structured poetry", "Sanskrit", "metrical patterns", "instruction-based fine-tuning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00817", "pdf": "https://arxiv.org/pdf/2506.00817.pdf", "abs": "https://arxiv.org/abs/2506.00817", "title": "One for All: Update Parameterized Knowledge Across Multiple Models", "authors": ["Weitao Ma", "Xiyuan Du", "Xiaocheng Feng", "Lei Huang", "Yichong Huang", "Huiyi Zhang", "Xiaoliang Yang", "Baohang Li", "Xiachong Feng", "Ting Liu", "Bing Qin"], "categories": ["cs.CL"], "comment": "ACL 2025 (Main Conference)", "summary": "Large language models (LLMs) encode vast world knowledge but struggle to stay\nup-to-date, often leading to errors and hallucinations. Knowledge editing\noffers an efficient alternative to retraining, enabling targeted modifications\nby updating specific model parameters. However, existing methods primarily\nfocus on individual models, posing challenges in efficiently updating multiple\nmodels and adapting to new models. To address this, we propose OnceEdit, a\nnovel ensemble-based approach that employs a plug-in model as the editing\nmodule, enabling stable knowledge updates across multiple models. Building on\nthe model ensemble, OnceEdit introduces two key mechanisms to enhance its\neffectiveness. First, we introduce a dynamic weight mechanism through a \\weight\ntoken for distinguishing between edit-related and non-edit-related instances,\nensuring the appropriate utilization of knowledge from integrated models.\nSecond, we incorporate an ensemble enhancement mechanism to mitigate the\nexcessive reliance on the central model inherent in the model ensemble\ntechnique, making it more suitable for knowledge editing. Extensive experiments\non diverse LLMs demonstrate that OnceEdit consistently outperforms existing\nmethods while achieving superior editing efficiency. Further analysis confirms\nits adaptability and stability in multi-model editing scenarios. Our code will\nbe available.", "AI": {"tldr": "OnceEdit is a novel approach for knowledge editing in large language models, enabling efficient updates across multiple models using an ensemble-based method.", "motivation": "Existing knowledge editing methods focus mainly on single models, making it challenging to update multiple models or adapt to new ones effectively.", "method": "OnceEdit employs a plug-in model as the editing module and introduces a dynamic weight mechanism and an ensemble enhancement mechanism to improve editing efficiency and adaptability.", "result": "Experiments show that OnceEdit consistently outperforms existing knowledge editing methods while being adaptable and stable in multi-model editing scenarios.", "conclusion": "OnceEdit provides a robust solution for knowledge updates in large language models, enhancing editing efficiency across ensembles while addressing the limitations of reliance on central models.", "key_contributions": ["Novel ensemble-based approach for knowledge editing", "Dynamic weight mechanism to distinguish instance relevance", "Ensemble enhancement mechanism to reduce reliance on central models"], "limitations": "", "keywords": ["knowledge editing", "large language models", "ensemble learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00823", "pdf": "https://arxiv.org/pdf/2506.00823.pdf", "abs": "https://arxiv.org/abs/2506.00823", "title": "Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks", "authors": ["Yuntai Bao", "Xuhong Zhang", "Tianyu Du", "Xinkui Zhao", "Zhengwen Feng", "Hao Peng", "Jianwei Yin"], "categories": ["cs.CL"], "comment": "19 pages, 16 figures; accepted to Findings of ACL 2025", "summary": "Large language models (LLMs) are trained on extensive datasets that\nencapsulate substantial world knowledge. However, their outputs often include\nconfidently stated inaccuracies. Earlier works suggest that LLMs encode\ntruthfulness as a distinct linear feature, termed the \"truth direction\", which\ncan classify truthfulness reliably. We address several open questions about the\ntruth direction: (i) whether LLMs universally exhibit consistent truth\ndirections; (ii) whether sophisticated probing techniques are necessary to\nidentify truth directions; and (iii) how the truth direction generalizes across\ndiverse contexts. Our findings reveal that not all LLMs exhibit consistent\ntruth directions, with stronger representations observed in more capable\nmodels, particularly in the context of logical negation. Additionally, we\ndemonstrate that truthfulness probes trained on declarative atomic statements\ncan generalize effectively to logical transformations, question-answering\ntasks, in-context learning, and external knowledge sources. Finally, we explore\nthe practical application of truthfulness probes in selective\nquestion-answering, illustrating their potential to improve user trust in LLM\noutputs. These results advance our understanding of truth directions and\nprovide new insights into the internal representations of LLM beliefs. Our code\nis public at https://github.com/colored-dye/truthfulness_probe_generalization", "AI": {"tldr": "This paper explores the concept of 'truth direction' in large language models, examining its consistency, identification methods, and generalization across contexts.", "motivation": "LLMs often output confident inaccuracies, prompting the need to understand and potentially enhance the truthfulness of their responses.", "method": "The authors investigate whether LLMs consistently exhibit truth directions and whether probing techniques can effectively identify these directions across various contexts.", "result": "The study finds that not all LLMs have consistent truth directions, with stronger models showing better representations. Probes trained on simple statements generalize well across various tasks.", "conclusion": "The findings provide insights into LLM representations of truthfulness and suggest practical applications that could improve trust in LLM outputs.", "key_contributions": ["Investigation of truth direction consistency in LLMs", "Demonstration of effective truthfulness probes across multiple contexts", "Practical applications for enhancing user trust in LLM outputs"], "limitations": "", "keywords": ["large language models", "truth direction", "truthfulness probes", "question-answering", "user trust"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.00826", "pdf": "https://arxiv.org/pdf/2506.00826.pdf", "abs": "https://arxiv.org/abs/2506.00826", "title": "HERGC: Heterogeneous Experts Representation and Generative Completion for Multimodal Knowledge Graphs", "authors": ["Yongkang Xiao", "Rui Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Multimodal knowledge graphs (MMKGs) enrich traditional knowledge graphs (KGs)\nby incorporating diverse modalities such as images and text. Multi-modal\nknowledge graph completion (MMKGC) seeks to exploit these heterogeneous signals\nto infer missing facts, thereby mitigating the intrinsic incompleteness of\nMMKGs. Existing MMKGC methods typically leverage only the information contained\nin the MMKGs under the closed-world assumption and adopt discriminative\ntraining objectives, which limits their reasoning capacity during completion.\nRecent generative completion approaches powered by advanced large language\nmodels (LLMs) have shown strong reasoning abilities in unimodal knowledge graph\ncompletion, but their potential in MMKGC remains largely unexplored. To bridge\nthis gap, we propose HERGC, a Heterogeneous Experts Representation and\nGenerative Completion framework for MMKGs. HERGC first deploys a Heterogeneous\nExperts Representation Retriever that enriches and fuses multimodal information\nand retrieves a compact candidate set for each incomplete triple. It then uses\na Generative LLM Predictor fine-tuned on minimal instruction data to accurately\nidentify the correct answer from these candidates. Extensive experiments on\nthree standard MMKG benchmarks demonstrate HERGC's effectiveness and\nrobustness, achieving state-of-the-art performance.", "AI": {"tldr": "HERGC is a framework for multimodal knowledge graph completion that integrates diverse modalities and utilizes a generative LLM for accurate inference.", "motivation": "To address the intrinsic incompleteness of multimodal knowledge graphs by applying generative techniques that can enhance reasoning in knowledge graph completion tasks.", "method": "HERGC deploys a Heterogeneous Experts Representation Retriever to enrich and fuse multimodal information, followed by a Generative LLM Predictor that identifies correct answers from a set of retrieved candidates.", "result": "HERGC demonstrates state-of-the-art performance in multimodal knowledge graph completion on three standard benchmarks.", "conclusion": "The effectiveness and robustness of HERGC indicate substantial improvements in multimodal knowledge graph completion tasks through the use of generative modeling techniques.", "key_contributions": ["Introduction of HERGC framework for MMKGC", "Utilization of large language models in a multimodal context", "Achievement of state-of-the-art performance on benchmark datasets"], "limitations": "", "keywords": ["multimodal", "knowledge graphs", "generative models", "graph completion", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00829", "pdf": "https://arxiv.org/pdf/2506.00829.pdf", "abs": "https://arxiv.org/abs/2506.00829", "title": "COMPKE: Complex Question Answering under Knowledge Editing", "authors": ["Keyuan Cheng", "Zijian Kan", "Zhixian He", "Zhuoran Zhang", "Muhammad Asif Ali", "Ke Xu", "Lijie Hu", "Di Wang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by ACL 2025 Findings", "summary": "Knowledge Editing, which efficiently modifies the knowledge in large language\nmodels, has gathered great attention. Current benchmarks primarily use\nmulti-hop question answering to assess and analyze newly injected or updated\nknowledge. However, we argue that these benchmarks fail to effectively evaluate\nhow well the updated models apply this knowledge in real-life scenarios,\nparticularly when questions require complex reasoning, involving one-to-many\nrelationships or multi-step logical intersections. To fill in this gap, we\nintroduce a new benchmark, COMPKE: Complex Question Answering under Knowledge\nEditing, which includes 11,924 complex questions that reflect real-life\nsituations. We conduct an extensive evaluation of four knowledge editing\nmethods on COMPKE, revealing that their effectiveness varies notably across\ndifferent models. For instance, MeLLo attains an accuracy of 39.47 on\nGPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further\ninvestigate the underlying causes of these disparities from both methodological\nand model-specific perspectives. The datasets are available at\nhttps://github.com/kzjkzj666/CompKE.", "AI": {"tldr": "The paper introduces COMPKE, a benchmark for evaluating knowledge editing in language models using complex real-life scenarios.", "motivation": "Current benchmarks fail to assess how effectively large language models apply updated knowledge in practical situations, especially when reasoning is complex.", "method": "Introduction of the COMPKE benchmark, featuring 11,924 complex questions, and evaluation of four knowledge editing methods across different language models.", "result": "Variability in effectiveness of knowledge editing methods: MeLLo scores 39.47 on GPT-4O-MINI but only 3.83 on QWEN2.5-3B, demonstrating model-specific performance differences.", "conclusion": "The study highlights the need for more nuanced evaluation of knowledge editing in language models, urging further exploration of method and model-specific outcomes.", "key_contributions": ["Introduction of the COMPKE benchmark for complex question answering under knowledge editing.", "Extensive evaluation showing the variability of knowledge editing methods across different models.", "Identification of methodological and model-specific causes for performance disparities."], "limitations": "The benchmark may not cover all possible real-life scenarios and its applicability to all language models needs further verification.", "keywords": ["Knowledge Editing", "Large Language Models", "Benchmarking", "Complex Question Answering", "Model Evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00842", "pdf": "https://arxiv.org/pdf/2506.00842.pdf", "abs": "https://arxiv.org/abs/2506.00842", "title": "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience", "authors": ["Jiawei Gu", "Ziting Xian", "Yuanzhen Xie", "Ye Liu", "Enjie Liu", "Ruichao Zhong", "Mochi Gao", "Yunzhi Tan", "Bo Hu", "Zang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) achieve strong performance on plain text tasks\nbut underperform on structured data like tables and databases. Potential\nchallenges arise from their underexposure during pre-training and rigid\ntext-to-structure transfer mechanisms. Unlike humans who seamlessly apply\nlearned patterns across data modalities, LLMs struggle to infer implicit\nrelationships embedded in tabular formats, especially in the absence of\nexplicit structural guidance. To bridge this cognitive gap, we introduce\nContrastive Retrieval-Augmented Generation on Experience (CoRE), a framework\nthat builds experience memory representations and enhances generalization\nthrough contrastive In-Context Learning (ICL) to simulate human-like knowledge\ntransfer. Experiments on Text-to-SQL and TableQA show CoRE significantly\nimproves performance, achieving average gains of 3.44% and 4.24%, with up to\n17.2% on challenging tasks. Our Monte Carlo Tree Search (MCTS)-generated\nExperience Memory expands training data 8-9x, enhancing diversity and domain\ncoverage. This training-free and continual method propels LLMs toward\nstructured knowledge expertise.", "AI": {"tldr": "CoRE framework improves LLM performance on structured data tasks by enhancing generalization through contrastive learning and experience memory.", "motivation": "LLMs struggle with structured data due to limited pre-training exposure and ineffective transfer mechanisms, necessitating better approaches for knowledge transfer.", "method": "The CoRE framework employs contrastive In-Context Learning (ICL) to create experience memory representations, simulating human-like knowledge transfer to enhance LLMs' capabilities with structured data.", "result": "CoRE significantly improves LLM performance on Text-to-SQL and TableQA tasks, yielding average performance gains of 3.44% and 4.24%, with some challenging tasks showing up to 17.2% improvement.", "conclusion": "The CoRE framework provides a training-free, continual method to boost LLMs' proficiency with structured knowledge by enhancing diversity and domain coverage through expanded experience memory.", "key_contributions": ["Introduction of CoRE framework for structured data tasks", "Significant performance improvements on Text-to-SQL and TableQA", "Expansion of training data diversity through MCTS-generated Experience Memory"], "limitations": "", "keywords": ["Large Language Models", "Contrastive Learning", "Experience Memory", "Structured Data", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00854", "pdf": "https://arxiv.org/pdf/2506.00854.pdf", "abs": "https://arxiv.org/abs/2506.00854", "title": "EEG2TEXT-CN: An Exploratory Study of Open-Vocabulary Chinese Text-EEG Alignment via Large Language Model and Contrastive Learning on ChineseEEG", "authors": ["Jacky Tai-Yu Lu", "Jung Chiang", "Chi-Sheng Chen", "Anna Nai-Yun Tung", "Hsiang Wei Hu", "Yuan Chiao Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MM", "q-bio.NC"], "comment": null, "summary": "We propose EEG2TEXT-CN, which, to the best of our knowledge, represents one\nof the earliest open-vocabulary EEG-to-text generation frameworks tailored for\nChinese. Built on a biologically grounded EEG encoder (NICE-EEG) and a compact\npretrained language model (MiniLM), our architecture aligns multichannel brain\nsignals with natural language representations via masked pretraining and\ncontrastive learning. Using a subset of the ChineseEEG dataset, where each\nsentence contains approximately ten Chinese characters aligned with 128-channel\nEEG recorded at 256 Hz, we segment EEG into per-character embeddings and\npredict full sentences in a zero-shot setting. The decoder is trained with\nteacher forcing and padding masks to accommodate variable-length sequences.\nEvaluation on over 1,500 training-validation sentences and 300 held-out test\nsamples shows promising lexical alignment, with a best BLEU-1 score of 6.38\\%.\nWhile syntactic fluency remains a challenge, our findings demonstrate the\nfeasibility of non-phonetic, cross-modal language decoding from EEG. This work\nopens a new direction in multilingual brain-to-text research and lays the\nfoundation for future cognitive-language interfaces in Chinese.", "AI": {"tldr": "EEG2TEXT-CN is an open-vocabulary EEG-to-text generation framework for Chinese, utilizing biologically grounded EEG encoding and a pretrained language model to map brain signals to text.", "motivation": "To address the gap in EEG-to-text generation specifically tailored for Chinese, facilitating the development of brain-computer interfaces.", "method": "Utilizes a biologically grounded EEG encoder (NICE-EEG) and a compact pretrained language model (MiniLM), applying masked pretraining and contrastive learning for mapping EEG signals to text.", "result": "Achieved a best BLEU-1 score of 6.38% on a dataset of Chinese sentences aligned with EEG recordings, indicating promising lexical alignment despite challenges in syntactic fluency.", "conclusion": "This research demonstrates the feasibility of cross-modal language decoding from EEG and lays the groundwork for future cognitive-language interfaces in Chinese.", "key_contributions": ["Introduced EEG2TEXT-CN, the first open-vocabulary EEG-to-text framework for Chinese.", "Developed a method for segmenting EEG into per-character embeddings for text prediction.", "Provided a foundational study for multilingual brain-to-text research."], "limitations": "Syntactic fluency remains a challenge and the BLEU-1 score indicates limited performance.", "keywords": ["EEG-to-text", "open-vocabulary", "Chinese", "brain-computer interface", "cross-modal decoding"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.00859", "pdf": "https://arxiv.org/pdf/2506.00859.pdf", "abs": "https://arxiv.org/abs/2506.00859", "title": "How Bidirectionality Helps Language Models Learn Better via Dynamic Bottleneck Estimation", "authors": ["Md Kowsher", "Nusrat Jahan Prottasha", "Shiyun Xu", "Shetu Mohanto", "Chen Chen", "Niloofar Yousefi", "Ozlem Garibay"], "categories": ["cs.CL"], "comment": null, "summary": "Bidirectional language models have better context understanding and perform\nbetter than unidirectional models on natural language understanding tasks, yet\nthe theoretical reasons behind this advantage remain unclear. In this work, we\ninvestigate this disparity through the lens of the Information Bottleneck (IB)\nprinciple, which formalizes a trade-off between compressing input information\nand preserving task-relevant content. We propose FlowNIB, a dynamic and\nscalable method for estimating mutual information during training that\naddresses key limitations of classical IB approaches, including computational\nintractability and fixed trade-off schedules. Theoretically, we show that\nbidirectional models retain more mutual information and exhibit higher\neffective dimensionality than unidirectional models. To support this, we\npresent a generalized framework for measuring representational complexity and\nprove that bidirectional representations are strictly more informative under\nmild conditions. We further validate our findings through extensive experiments\nacross multiple models and tasks using FlowNIB, revealing how information is\nencoded and compressed throughout training. Together, our work provides a\nprincipled explanation for the effectiveness of bidirectional architectures and\nintroduces a practical tool for analyzing information flow in deep language\nmodels.", "AI": {"tldr": "This paper investigates the advantages of bidirectional language models over unidirectional ones through the Information Bottleneck principle, proposing a method called FlowNIB for estimating mutual information during training.", "motivation": "Theoretical understanding of why bidirectional language models outperform unidirectional models in natural language understanding tasks remains unclear.", "method": "FlowNIB, a dynamic and scalable method for estimating mutual information during training, addressing limitations of classical Information Bottleneck approaches.", "result": "Bidirectional models demonstrate higher mutual information retention and effective dimensionality than unidirectional models, validated through extensive experiments.", "conclusion": "Bidirectional architectures are strictly more informative and the FlowNIB method provides a practical tool for analyzing information flow in deep language models.", "key_contributions": ["Introduction of FlowNIB for mutual information estimation", "Theoretical proof of superior information retention in bidirectional models", "Generalized framework for measuring representational complexity"], "limitations": "", "keywords": ["Bidirectional models", "Mutual information", "Information Bottleneck"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.00863", "pdf": "https://arxiv.org/pdf/2506.00863.pdf", "abs": "https://arxiv.org/abs/2506.00863", "title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models", "authors": ["Nidhi Kowtal", "Raviraj Joshi"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP", "AI": {"tldr": "L3Cube-MahaEmotions is a high-quality dataset for emotion recognition in Marathi with synthetic annotation from LLMs and a benchmark for model evaluation.", "motivation": "To address the challenge of emotion recognition in low-resource languages by providing a high-quality dataset and evaluating model performance.", "method": "The dataset was created by synthetically annotating training data using large language models, with validation and test sets manually labeled. The Chain-of-Translation (CoTR) technique was applied for emotion labeling, and models were evaluated against standard metrics.", "result": "GPT-4 annotations yielded better label quality compared to fine-tuned BERT models. GPT-4 also outperformed BERT-based models on the task.", "conclusion": "High-quality human-labeled data is critical for performance in complex emotion recognition, and generic LLMs like GPT-4 outperform fine-tuned models in low-resource scenarios.", "key_contributions": ["Introduction of L3Cube-MahaEmotions dataset for Marathi", "Demonstration of Chain-of-Translation (CoTR) for emotion labeling", "Showcasing superiority of GPT-4 over BERT in low-resource emotion tasks"], "limitations": "Focus on a specific language (Marathi) may limit applicability for broader language tasks.", "keywords": ["emotion recognition", "low-resource languages", "Marathi", "large language models", "dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00869", "pdf": "https://arxiv.org/pdf/2506.00869.pdf", "abs": "https://arxiv.org/abs/2506.00869", "title": "What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning", "authors": ["Zhaotian Weng", "Haoxuan Li", "Kuan-Hao Huang", "Jieyu Zhao"], "categories": ["cs.CL", "I.7.0"], "comment": "12 pages", "summary": "Despite the impressive performance of vision-language models (VLMs) on\ndownstream tasks, their ability to understand and reason about causal\nrelationships in visual inputs remains unclear. Robust causal reasoning is\nfundamental to solving complex high-level reasoning tasks, yet existing\nbenchmarks often include a mixture of reasoning questions, and VLMs can\nfrequently exploit object recognition and activity identification as shortcuts\nto arrive at the correct answers, making it challenging to truly assess their\ncausal reasoning abilities. To bridge this gap, we introduce VQA-Causal and\nVCR-Causal, two new benchmarks specifically designed to isolate and rigorously\nevaluate VLMs' causal reasoning abilities. Our findings reveal that while VLMs\nexcel in object and activity recognition, they perform poorly on causal\nreasoning tasks, often only marginally surpassing random guessing. Further\nanalysis suggests that this limitation stems from a severe lack of causal\nexpressions in widely used training datasets, where causal relationships are\nrarely explicitly conveyed. We additionally explore fine-tuning strategies with\nhard negative cases, showing that targeted fine-tuning can improve model's\ncausal reasoning while maintaining generalization and downstream performance.\nOur study highlights a key gap in current VLMs and lays the groundwork for\nfuture work on causal understanding.", "AI": {"tldr": "The paper introduces two benchmarks, VQA-Causal and VCR-Causal, to evaluate the causal reasoning abilities of vision-language models (VLMs), revealing their limitations in this area despite strong performance in object and activity recognition.", "motivation": "To assess the causal reasoning capabilities of VLMs, which are often not evaluated properly due to existing benchmarks allowing for shortcuts in reasoning tasks.", "method": "The authors introduce new benchmarks (VQA-Causal and VCR-Causal) specifically designed to test causal reasoning in VLMs, along with analysis of training dataset limitations and fine-tuning strategies.", "result": "VLMs perform poorly on causal reasoning tasks, performing only slightly better than random guessing, due to a lack of explicit causal expressions in their training data.", "conclusion": "The findings indicate a crucial gap in VLMs' capabilities regarding causal reasoning and suggest that targeted fine-tuning may enhance performance while retaining generalization.", "key_contributions": ["Introduction of VQA-Causal and VCR-Causal benchmarks for causal reasoning evaluation in VLMs", "Analysis revealing poor causal reasoning skills in VLMs", "Exploration of fine-tuning strategies to better VLMs' causal reasoning abilities"], "limitations": "The study is limited to the evaluation of VLMs within the context of the newly proposed benchmarks and may not be generalizable to all VLM applications.", "keywords": ["Vision-Language Models", "Causal Reasoning", "Benchmarks", "Fine-tuning Strategies", "Machine Learning"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.00875", "pdf": "https://arxiv.org/pdf/2506.00875.pdf", "abs": "https://arxiv.org/abs/2506.00875", "title": "CC-Tuning: A Cross-Lingual Connection Mechanism for Improving Joint Multilingual Supervised Fine-Tuning", "authors": ["Yangfan Ye", "Xiaocheng Feng", "Zekun Yuan", "Xiachong Feng", "Libo Qin", "Lei Huang", "Weitao Ma", "Yichong Huang", "Zhirui Zhang", "Yunfei Lu", "Xiaohui Yan", "Duyu Tang", "Dandan Tu", "Bing Qin"], "categories": ["cs.CL"], "comment": "ACL2025 main conference, long paper", "summary": "Current large language models (LLMs) often exhibit imbalanced multilingual\ncapabilities due to their English-centric training corpora. To address this,\nexisting fine-tuning approaches operating at the data-level (e.g., through data\naugmentation or distillation) typically introduce implicit cross-lingual\nalignment, overlooking the potential for more profound, latent-level\ncross-lingual interactions. In this work, we propose CC-Tuning, a novel\nmultilingual fine-tuning paradigm that explicitly establishes a cross-lingual\nconnection mechanism at the latent level. During training, CC-Tuning fuses the\nfeed forward activations from both English and non-English inputs, enabling the\nmodel to benefit from both linguistic resources. This process is facilitated\nwith a trainable Decision Maker that identifies beneficial activations.\nFurthermore, during inference, a Transform Matrix is utilized to simulate the\ncross-lingual connection under monolingual setting through representation\ntransformation. Our experiments on six benchmarks covering 22 languages show\nthat CC-Tuning outperforms vanilla SFT and offers a strong latent-level\nalternative to data-level augmentation methods. Further analysis also\nhighlights the practicality of CC-Tuning and the potential of latent-level\ncross-lingual interactions in advancing the multilingual performance of LLMs.", "AI": {"tldr": "CC-Tuning introduces a novel method for improving multilingual capabilities of large language models (LLMs) by establishing cross-lingual connections at the latent level during training and inference.", "motivation": "Current large language models (LLMs) are often biased towards English due to their training on predominantly English data, necessitating better multilingual training approaches.", "method": "CC-Tuning fuses feed-forward activations from English and non-English inputs, guided by a trainable Decision Maker to enhance cross-lingual interactions at the latent level.", "result": "CC-Tuning outperforms standard fine-tuning methods and demonstrates significant improvements in multilingual performance across six benchmarks with 22 languages.", "conclusion": "The study emphasizes the importance of latent-level cross-lingual interactions for advancing the multilingual capabilities of LLMs, suggesting that CC-Tuning offers a promising alternative to traditional data-level augmentation methods.", "key_contributions": ["Introduction of CC-Tuning as a new fine-tuning paradigm for multilingual models.", "Establishment of a cross-lingual connection mechanism at the latent level.", "Demonstration of performance improvements on multiple multilingual benchmarks."], "limitations": "", "keywords": ["Multilingual Models", "Cross-Lingual Learning", "Latent-Level Interactions", "Fine-Tuning", "Large Language Models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00876", "pdf": "https://arxiv.org/pdf/2506.00876.pdf", "abs": "https://arxiv.org/abs/2506.00876", "title": "Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning", "authors": ["Yixin Wan", "Anil Ramakrishna", "Kai-Wei Chang", "Volkan Cevher", "Rahul Gupta"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Model (LLM) unlearning has recently gained significant\nattention, driven by the need to remove unwanted information, such as private,\nsensitive, or copyrighted content, from LLMs. However, conventional unlearning\napproaches indiscriminately update model parameters to forget all tokens in a\ntarget document, including common tokens (e.g., pronouns, prepositions, general\nnouns) that carry general knowledge. In this paper, we highlight that not every\ntoken needs forgetting. We propose Selective Unlearning (SU), which identifies\na critical subset of tokens within the forgetting set that is relevant to the\nunwanted information, and unlearns only those tokens. Experiments on two\nbenchmarks and six baseline unlearning algorithms demonstrate that SU not only\nachieves effective unlearning on the targeted forget data, but also\nsignificantly preserves the model's utility in the retaining set.", "AI": {"tldr": "This paper introduces Selective Unlearning (SU), a method for removing unwanted information from Large Language Models (LLMs) by selectively forgetting only relevant tokens instead of all tokens in a document.", "motivation": "The growing need to eliminate sensitive or copyrighted content from LLMs necessitates effective unlearning methods that do not harm the model's overall utility.", "method": "Selective Unlearning (SU) identifies and unlearns only a critical subset of tokens related to unwanted information, while preserving general knowledge tokens.", "result": "Experiments show that SU effectively removes targeted data while significantly maintaining model performance on the retaining set compared to baseline unlearning approaches.", "conclusion": "SU represents a more efficient and effective method for LLM unlearning that balances the need to forget certain information with preserving the model's utility.", "key_contributions": ["Introduction of Selective Unlearning (SU) that targets specific tokens for unlearning.", "Demonstrated effective unlearning without substantial loss of model performance.", "Experimental validation across two benchmarks with six baseline algorithms."], "limitations": "", "keywords": ["Selective Unlearning", "Large Language Models", "Token Forgetting", "Model Utility", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00883", "pdf": "https://arxiv.org/pdf/2506.00883.pdf", "abs": "https://arxiv.org/abs/2506.00883", "title": "Improve MLLM Benchmark Efficiency through Interview", "authors": ["Farong Wen", "Yijin Guo", "Junying Wang", "Jiaohao Xiao", "Yingjie Zhou", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid development of Multimodal Large Language Models (MLLM) has led to a\nwide range of MLLM applications, and a number of benchmark datasets have sprung\nup in order to assess MLLM abilities. However, full-coverage Q&A testing on\nlarge-scale data is resource-intensive and time-consuming. To address this\nissue, we propose the MLLM Interview (MITV) strategy, which aims to quickly\nobtain MLLM performance metrics by quizzing fewer question. First, First, we\nconstructed the interview dataset, which was built on an existing MLLM\nassessment dataset, by adding difficulty labels based on the performance of\nsome typical MLLMs in this dataset. Second, we propose an MLLM Interview\nstrategy, which obtains an initial performance situation of the large model by\nquizzing a small number of topics and then continuously tries to test the\nmodel's limits. Through extensive experiments, the result shows that the MITV\nstrategy proposed in this paper performs well on MLLM benchmark datasets, and\nit is able to obtain the model evaluation capability faster through a small\nnumber of questions and answers.", "AI": {"tldr": "The MLLM Interview (MITV) strategy enhances MLLM performance evaluation by efficiently testing with fewer questions.", "motivation": "To address the resource-intensive nature of full-coverage Q&A testing on large-scale data for Multimodal Large Language Models (MLLM).", "method": "The paper introduces the MITV strategy, which uses a constructed interview dataset with difficulty labels to assess MLLM performance by quizzing fewer questions across selected topics and then probing model limits.", "result": "Experiments demonstrated that the MITV strategy efficiently evaluates MLLM capabilities on benchmark datasets with a reduced number of questions.", "conclusion": "The MITV strategy provides a quicker means to assess MLLM performance while maintaining effectiveness using a limited question set.", "key_contributions": ["Development of the MLLM Interview (MITV) strategy for performance assessment", "Creation of an interview dataset with added difficulty labels", "Demonstration of the MITV strategy's efficiency in model evaluation"], "limitations": "", "keywords": ["Multimodal Large Language Models", "performance evaluation", "MLLM Interview", "benchmark datasets", "model assessment"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.00893", "pdf": "https://arxiv.org/pdf/2506.00893.pdf", "abs": "https://arxiv.org/abs/2506.00893", "title": "Affordance Benchmark for MLLMs", "authors": ["Junying Wang", "Wenzhe Li", "Yalun Wu", "Yingji Liang", "Yijin Guo", "Chunyi Li", "Haodong Duan", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affordance theory posits that environments inherently offer action\npossibilities that shape perception and behavior. While Multimodal Large\nLanguage Models (MLLMs) excel in vision-language tasks, their ability to\nperceive affordance, which is crucial for intuitive and safe interactions,\nremains underexplored. To address this, we introduce A4Bench, a novel benchmark\ndesigned to evaluate the affordance perception abilities of MLLMs across two\ndimensions: 1) Constitutive Affordance}, assessing understanding of inherent\nobject properties through 1,282 question-answer pairs spanning nine\nsub-disciplines, and 2) Transformative Affordance, probing dynamic and\ncontextual nuances (e.g., misleading, time-dependent, cultural, or\nindividual-specific affordance) with 718 challenging question-answer pairs.\nEvaluating 17 MLLMs (nine proprietary and eight open-source) against human\nperformance, we find that proprietary models generally outperform open-source\ncounterparts, but all exhibit limited capabilities, particularly in\ntransformative affordance perception. Furthermore, even top-performing models,\nsuch as Gemini-2.0-Pro (18.05% overall exact match accuracy), significantly lag\nbehind human performance (best: 85.34%, worst: 81.25%). These findings\nhighlight critical gaps in environmental understanding of MLLMs and provide a\nfoundation for advancing AI systems toward more robust, context-aware\ninteractions. The dataset is available in\nhttps://github.com/JunyingWang959/A4Bench/.", "AI": {"tldr": "The paper presents A4Bench, a benchmark for evaluating the affordance perception abilities of Multimodal Large Language Models (MLLMs), highlighting their limitations in environmental understanding despite outperforming open-source models.", "motivation": "The study aims to explore the underdeveloped capability of MLLMs in perceiving affordance, which is important for improving interactions in AI systems.", "method": "The A4Bench benchmark evaluates MLLMs on two dimensions of affordance: Constitutive Affordance with 1,282 QA pairs and Transformative Affordance with 718 challenging QA pairs, involving assessments of 17 MLLMs against human performance.", "result": "Proprietary models outperform open-source models in affordance perception but all models show limited capabilities, especially in transformative affordance, with the best model achieving only 18.05% accuracy compared to human performance of around 85%.", "conclusion": "The findings reveal critical gaps in MLLMs' environmental understanding, suggesting the need for advancements in context-aware AI interactions.", "key_contributions": ["Introduction of A4Bench benchmark for affordance perception in MLLMs.", "Evaluation across two affordance dimensions with a comprehensive dataset.", "Insight into the performance gap between MLLMs and human understanding of affordance."], "limitations": "Major limitations include the overall low performance of MLLMs in transformative affordance perception and reliance on structured QA pairs.", "keywords": ["Multimodal Large Language Models", "affordance perception", "benchmark", "human performance", "environmental understanding"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00900", "pdf": "https://arxiv.org/pdf/2506.00900.pdf", "abs": "https://arxiv.org/abs/2506.00900", "title": "SocialEval: Evaluating Social Intelligence of Large Language Models", "authors": ["Jinfeng Zhou", "Yuxuan Chen", "Yihan Shi", "Xuanming Zhang", "Leqi Lei", "Yi Feng", "Zexuan Xiong", "Miao Yan", "Xunzhi Wang", "Yaru Cao", "Jianing Yin", "Shuai Wang", "Quanyu Dai", "Zhenhua Dong", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "ACL 2025, Repository: \\url{https://github.com/thu-coai/SocialEval}", "summary": "LLMs exhibit promising Social Intelligence (SI) in modeling human behavior,\nraising the need to evaluate LLMs' SI and their discrepancy with humans. SI\nequips humans with interpersonal abilities to behave wisely in navigating\nsocial interactions to achieve social goals. This presents an operational\nevaluation paradigm: outcome-oriented goal achievement evaluation and\nprocess-oriented interpersonal ability evaluation, which existing work fails to\naddress. To this end, we propose SocialEval, a script-based bilingual SI\nbenchmark, integrating outcome- and process-oriented evaluation by manually\ncrafting narrative scripts. Each script is structured as a world tree that\ncontains plot lines driven by interpersonal ability, providing a comprehensive\nview of how LLMs navigate social interactions. Experiments show that LLMs fall\nbehind humans on both SI evaluations, exhibit prosociality, and prefer more\npositive social behaviors, even if they lead to goal failure. Analysis of LLMs'\nformed representation space and neuronal activations reveals that LLMs have\ndeveloped ability-specific functional partitions akin to the human brain.", "AI": {"tldr": "This paper introduces SocialEval, a benchmark for evaluating the social intelligence of LLMs by integrating outcome and process-oriented assessments through narrative scripts.", "motivation": "There is a need to evaluate the social intelligence of LLMs and understand how they compare to humans, particularly in terms of interpersonal abilities and goal achievement in social interactions.", "method": "A script-based bilingual benchmark, SocialEval, is developed that incorporates both outcome-oriented evaluations (goal achievement) and process-oriented evaluations (interpersonal abilities) using manually crafted narrative scripts structured as world trees.", "result": "Experiments reveal that LLMs lag behind humans in both social intelligence evaluations, showing prosociality and a preference for positive social behaviors despite potential goal failures.", "conclusion": "The findings highlight that LLMs develop ability-specific functional partitions in their representation spaces similar to human brain patterns, indicating a unique approach to social interaction modeling. ", "key_contributions": ["Introduction of SocialEval benchmark for LLM social intelligence evaluation", "Combination of outcome-oriented and process-oriented evaluations", "Revelation of LLMs' preference for prosocial behaviors and their representation akin to human cognitive processes."], "limitations": "The benchmark relies on manually crafted scripts, which may limit its scalability and generalizability.", "keywords": ["social intelligence", "LLMs", "evaluation benchmark", "interpersonal abilities", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00912", "pdf": "https://arxiv.org/pdf/2506.00912.pdf", "abs": "https://arxiv.org/abs/2506.00912", "title": "Pi-SQL: Enhancing Text-to-SQL with Fine-Grained Guidance from Pivot Programming Languages", "authors": ["Yongdong chi", "Hanqing Wang", "Zonghan Yang", "Jian Yang", "Xiao Yan", "Yun Chen", "Guanhua Chen"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Text-to-SQL transforms the user queries from natural language to executable\nSQL programs, enabling non-experts to interact with complex databases. Existing\nprompt-based methods craft meticulous text guidelines and examples to\nfacilitate SQL generation, but their accuracy is hindered by the large semantic\ngap between the texts and the low-resource SQL programs. In this work, we\npropose Pi-SQL, which incorporates the high-resource Python program as a pivot\nto bridge between the natural language query and SQL program. In particular,\nPi-SQL first generates Python programs that provide fine-grained step-by-step\nguidelines in their code blocks or comments, and then produces an SQL program\nfollowing the guidance of each Python program.The final SQL program matches the\nreference Python program's query results and, through selection from candidates\ngenerated by different strategies, achieves superior execution speed, with a\nreward-based valid efficiency score up to 4.55 higher than the best-performing\nbaseline.Extensive experiments demonstrate the effectiveness of Pi-SQL, which\nimproves the execution accuracy of the best-performing baseline by up to 3.20.", "AI": {"tldr": "Pi-SQL transforms natural language queries into SQL by first generating Python programs as intermediaries for improved accuracy and execution speed.", "motivation": "To address the semantic gap in existing prompt-based methods for generating SQL from natural language, which often results in accuracy issues due to low-resource SQL expressions.", "method": "Pi-SQL generates Python programs that serve as detailed guides for creating SQL expressions, ensuring each step follows clear instructions before producing the final SQL output.", "result": "The final SQL program achieves significant execution speed and higher accuracy, outperforming the best existing baseline methods by a valid efficiency score of 4.55 and improving execution accuracy by up to 3.20.", "conclusion": "Pi-SQL effectively bridges the gap between natural language and SQL by utilizing Python programs, enhancing both execution accuracy and speed compared to previous approaches.", "key_contributions": ["Introduces a novel approach for SQL generation using Python as an intermediary", "Improves accuracy of SQL execution from natural language queries", "Demonstrates superior execution speed and efficiency over existing methods"], "limitations": "", "keywords": ["Text-to-SQL", "Natural Language Processing", "SQL Generation", "Programming Intermediaries", "Execution Accuracy"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00914", "pdf": "https://arxiv.org/pdf/2506.00914.pdf", "abs": "https://arxiv.org/abs/2506.00914", "title": "How do Transformer Embeddings Represent Compositions? A Functional Analysis", "authors": ["Aishik Nagar", "Ishaan Singh Rawal", "Mansi Dhanania", "Cheston Tan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Compositionality is a key aspect of human intelligence, essential for\nreasoning and generalization. While transformer-based models have become the de\nfacto standard for many language modeling tasks, little is known about how they\nrepresent compound words, and whether these representations are compositional.\nIn this study, we test compositionality in Mistral, OpenAI Large, and Google\nembedding models, and compare them with BERT. First, we evaluate\ncompositionality in the representations by examining six diverse models of\ncompositionality (addition, multiplication, dilation, regression, etc.). We\nfind that ridge regression, albeit linear, best accounts for compositionality.\nSurprisingly, we find that the classic vector addition model performs almost as\nwell as any other model. Next, we verify that most embedding models are highly\ncompositional, while BERT shows much poorer compositionality. We verify and\nvisualize our findings with a synthetic dataset consisting of fully transparent\nadjective-noun compositions. Overall, we present a thorough investigation of\ncompositionality.", "AI": {"tldr": "This study investigates the compositionality of compound word representations in various embedding models, finding that most models are highly compositional, while BERT exhibits significantly lower compositionality.", "motivation": "To understand how transformer-based models represent compound words and whether these representations are compositional, essential for reasoning and generalization in human intelligence.", "method": "The study evaluates compositionality using six diverse models (addition, multiplication, dilation, regression, etc.) to test various embedding models including Mistral, OpenAI Large, Google embeddings, and BERT.", "result": "Ridge regression was found to best account for compositionality among linear models, with vector addition performing competitively. Most embedding models were highly compositional, but BERT showed significantly poorer compositionality.", "conclusion": "The findings offer a comprehensive analysis of compositionality in embedding models, highlighting the strengths and weaknesses of different approaches, particularly pointing out BERT's limitations.", "key_contributions": ["In-depth evaluation of compositionality in transformer models.", "Comparison of six models of compositionality across various embedding models.", "Visualization of findings using a synthetic dataset."], "limitations": "The investigation is limited to specific models and may not generalize to all language representations beyond those studied.", "keywords": ["compositionality", "embedding models", "transformer models", "BERT", "natural language processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.00942", "pdf": "https://arxiv.org/pdf/2506.00942.pdf", "abs": "https://arxiv.org/abs/2506.00942", "title": "anyECG-chat: A Generalist ECG-MLLM for Flexible ECG Input and Multi-Task Understanding", "authors": ["Haitao Li", "Ziyu Li", "Yiheng Mao", "Ziyi Liu", "Zhoujian Sun", "Zhengxing Huang"], "categories": ["cs.CL", "cs.AI", "eess.SP"], "comment": null, "summary": "The advent of multimodal large language models (MLLMs) has sparked interest\nin their application to electrocardiogram (ECG) analysis. However, existing\nECG-focused MLLMs primarily focus on report generation tasks, often limited to\nsingle 12-lead, short-duration (10s) ECG inputs, thereby underutilizing the\npotential of MLLMs. To this end, we aim to develop a MLLM for ECG analysis that\nsupports a broader range of tasks and more flexible ECG inputs. However,\nexisting ECG-QA datasets are often monotonous. To address this gap, we first\nconstructed the anyECG dataset, which encompasses a wide variety of tasks,\nincluding report generation, abnormal waveform localization, and open-ended\nquestion answering. In addition to standard hospital ECGs, we introduced\nlong-duration reduced-lead ECGs for home environments and multiple ECG\ncomparison scenarios commonly encountered in clinical practice. Furthermore, we\npropose the anyECG-chat model, which supports dynamic-length ECG inputs and\nmultiple ECG inputs. We trained the model using a three-stage curriculum\ntraining recipe with the anyECG dataset. A comprehensive evaluation was\nconducted, demonstrating that anyECG-chat is capable of supporting various\npractical application scenarios, including not only common report generation\ntasks but also abnormal waveform localization for long-duration reduced-lead\nECGs in home environments and comprehensive comparative analysis of multiple\nECGs.", "AI": {"tldr": "This paper presents the anyECG dataset and the anyECG-chat model for multimodal ECG analysis, extending capabilities beyond traditional report generation to include various practical tasks.", "motivation": "The study aims to leverage the potential of MLLMs in ECG analysis by addressing the limitations of existing models that mainly focus on report generation with short-duration ECG inputs.", "method": "The authors constructed the anyECG dataset, which includes diverse ECG tasks and scenarios, and developed the anyECG-chat model using a three-stage curriculum training approach.", "result": "The evaluation showed that anyECG-chat effectively supports multiple ECG analysis tasks, including report generation, abnormal waveform localization, and comparative analysis of long-duration ECGs.", "conclusion": "The anyECG-chat model represents a significant advancement in the application of MLLMs for ECG analysis, particularly in home care settings and clinical comparisons.", "key_contributions": ["Development of the anyECG dataset with varied ECG tasks", "Introduction of long-duration reduced-lead ECGs and multiple ECG comparison scenarios", "Creation of the anyECG-chat model supporting dynamic-length and multiple ECG inputs"], "limitations": "", "keywords": ["multimodal large language models", "electrocardiogram analysis", "anyECG dataset", "ECG report generation", "waveform localization"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.00955", "pdf": "https://arxiv.org/pdf/2506.00955.pdf", "abs": "https://arxiv.org/abs/2506.00955", "title": "Leveraging Large Language Models for Sarcastic Speech Annotation in Sarcasm Detection", "authors": ["Zhu Li", "Yuqing Zhang", "Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "Sarcasm fundamentally alters meaning through tone and context, yet detecting\nit in speech remains a challenge due to data scarcity. In addition, existing\ndetection systems often rely on multimodal data, limiting their applicability\nin contexts where only speech is available. To address this, we propose an\nannotation pipeline that leverages large language models (LLMs) to generate a\nsarcasm dataset. Using a publicly available sarcasm-focused podcast, we employ\nGPT-4o and LLaMA 3 for initial sarcasm annotations, followed by human\nverification to resolve disagreements. We validate this approach by comparing\nannotation quality and detection performance on a publicly available sarcasm\ndataset using a collaborative gating architecture. Finally, we introduce\nPodSarc, a large-scale sarcastic speech dataset created through this pipeline.\nThe detection model achieves a 73.63% F1 score, demonstrating the dataset's\npotential as a benchmark for sarcasm detection research.", "AI": {"tldr": "This paper presents a novel pipeline for generating a sarcastic speech dataset using large language models and human verification, contributing to sarcasm detection research.", "motivation": "Detecting sarcasm remains challenging due to data scarcity and the reliance of existing systems on multimodal data.", "method": "An annotation pipeline is proposed that utilizes LLMs to generate sarcasm annotations from a podcast, followed by human verification.", "result": "The proposed dataset, PodSarc, achieved a detection model F1 score of 73.63%, indicating its effectiveness.", "conclusion": "PodSarc serves as a benchmark for sarcasm detection research, validating the proposed annotation approach.", "key_contributions": ["Development of a sarcasm annotation pipeline using LLMs", "Creation of the PodSarc dataset", "Validation of the dataset with a successful detection model"], "limitations": "Potential biases in LLM-generated annotations and human verification limitations.", "keywords": ["Sarcasm Detection", "Large Language Models", "Speech Dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.00963", "pdf": "https://arxiv.org/pdf/2506.00963.pdf", "abs": "https://arxiv.org/abs/2506.00963", "title": "From Objectives to Questions: A Planning-based Framework for Educational Mathematical Question Generation", "authors": ["Cheng Cheng", "Zhenya Huang", "Guanhao Zhao", "Yuxiang Guo", "Xin Lin", "Jinze Wu", "Xin Li", "Shijin Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Automatically generating high-quality mathematical problems that align with\neducational objectives is a crucial task in NLP-based educational technology.\nTraditional generation methods focus primarily on textual quality, but they\noften overlook educational objectives. Moreover, these methods address only\nsingle-dimensional, simple question generation, failing to meet complex,\nmultifaceted educational requirements. To address these challenges, we\nconstructed and annotated EduMath, a dataset of 16k mathematical questions with\nmulti-dimensional educational objectives. Based on this dataset, we developed\nEQGEVAL, which incorporates three evaluation dimensions and is designed to\nassess the ability of models to generate educational questions. Drawing\ninspiration from teachers' problem design processes, we propose the Educational\nQuestion Planning with self-Reflection (EQPR) method for educational\nmathematical question generation, following a \"plan-evaluate-optimize\"\napproach. Specifically, by combining planning algorithm based on Monte Carlo\nTree Search with the generative capabilities of Large Language Models, we\ncontinuously optimize questions through iterative feedback. This\nself-optimization mechanism ensures that the generated questions both fit the\neducational context and strategically achieve specific basic educational\nobjectives. Through extensive experiments based on EQGEVAL, we have\ndemonstrated that EQPR achieves significant improvements in generating\nquestions that meet multi-dimensional educational objectives.", "AI": {"tldr": "The paper presents an approach for generating educational mathematical questions that align with multi-dimensional educational objectives via a newly created dataset and a self-optimizing generative method.", "motivation": "To generate high-quality mathematical problems that align with educational objectives, addressing the limitations of traditional single-dimensional question generation methods.", "method": "The paper introduces the EduMath dataset of 16k annotated mathematical questions and the EQPR method which employs a Monte Carlo Tree Search combined with Large Language Models for iterative question optimization.", "result": "EQPR significantly improves the generation of educational questions that meet multi-dimensional educational objectives as demonstrated through experiments based on EQGEVAL.", "conclusion": "The proposed EQPR method offers a novel solution to educational mathematical question generation, ensuring that generated questions effectively fit educational contexts.", "key_contributions": ["Construction of EduMath dataset with multi-dimensional objectives", "Development of EQGEVAL evaluation metrics", "Introduction of EQPR method which combines planning algorithms and LLMs for iterative question optimization"], "limitations": "The focus is primarily on mathematical questions, and further research may be needed to extend these methods to other educational domains.", "keywords": ["educational technology", "math question generation", "natural language processing", "machine learning", "large language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.00964", "pdf": "https://arxiv.org/pdf/2506.00964.pdf", "abs": "https://arxiv.org/abs/2506.00964", "title": "ACCESS DENIED INC: The First Benchmark Environment for Sensitivity Awareness", "authors": ["Dren Fazlija", "Arkadij Orlov", "Sandipan Sikdar"], "categories": ["cs.CL"], "comment": "20 pages, 4 figures, 8 tables, ACL 2025 (Findings)", "summary": "Large language models (LLMs) are increasingly becoming valuable to corporate\ndata management due to their ability to process text from various document\nformats and facilitate user interactions through natural language queries.\nHowever, LLMs must consider the sensitivity of information when communicating\nwith employees, especially given access restrictions. Simple filtering based on\nuser clearance levels can pose both performance and privacy challenges. To\naddress this, we propose the concept of sensitivity awareness (SA), which\nenables LLMs to adhere to predefined access rights rules. In addition, we\ndeveloped a benchmarking environment called ACCESS DENIED INC to evaluate SA.\nOur experimental findings reveal significant variations in model behavior,\nparticularly in managing unauthorized data requests while effectively\naddressing legitimate queries. This work establishes a foundation for\nbenchmarking sensitivity-aware language models and provides insights to enhance\nprivacy-centric AI systems in corporate environments.", "AI": {"tldr": "The paper introduces the concept of sensitivity awareness (SA) in large language models (LLMs) to manage data access according to user clearance levels and presents a benchmarking environment to evaluate this concept.", "motivation": "To improve privacy and performance in corporate data management using LLMs by ensuring they respect predefined access rights when processing inquiries.", "method": "The authors developed a benchmarking environment called ACCESS DENIED INC to test and evaluate the performance of sensitivity-aware language models against unauthorized data requests and legitimate queries.", "result": "The experimental findings showed significant variations in model behavior in handling unauthorized data requests while effectively managing legitimate ones.", "conclusion": "This work lays a foundation for creating sensitivity-aware language models and offers insights for enhancing privacy-centric AI systems in corporate contexts.", "key_contributions": ["Introduction of sensitivity awareness (SA) for LLMs in corporate settings.", "Development of the ACCESS DENIED INC benchmarking environment.", "Empirical insights into model behavior regarding privacy and data access."], "limitations": "The evaluation is based on simulated environments and may need real-world testing for broader applicability.", "keywords": ["sensitivity awareness", "large language models", "privacy", "corporate data management", "benchmarking"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2506.00973", "pdf": "https://arxiv.org/pdf/2506.00973.pdf", "abs": "https://arxiv.org/abs/2506.00973", "title": "XGUARD: A Graded Benchmark for Evaluating Safety Failures of Large Language Models on Extremist Content", "authors": ["Vadivel Abishethvarman", "Bhavik Chandna", "Pratik Jalan", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Language Models (LLMs) can generate content spanning ideological\nrhetoric to explicit instructions for violence. However, existing safety\nevaluations often rely on simplistic binary labels (safe and unsafe),\noverlooking the nuanced spectrum of risk these outputs pose. To address this,\nwe present XGUARD, a benchmark and evaluation framework designed to assess the\nseverity of extremist content generated by LLMs. XGUARD includes 3,840 red\nteaming prompts sourced from real world data such as social media and news,\ncovering a broad range of ideologically charged scenarios. Our framework\ncategorizes model responses into five danger levels (0 to 4), enabling a more\nnuanced analysis of both the frequency and severity of failures. We introduce\nthe interpretable Attack Severity Curve (ASC) to visualize vulnerabilities and\ncompare defense mechanisms across threat intensities. Using XGUARD, we evaluate\nsix popular LLMs and two lightweight defense strategies, revealing key insights\ninto current safety gaps and trade-offs between robustness and expressive\nfreedom. Our work underscores the value of graded safety metrics for building\ntrustworthy LLMs.", "AI": {"tldr": "The paper introduces XGUARD, a benchmark for evaluating the severity of extremist content generated by LLMs, featuring a new metric system for nuanced safety assessments.", "motivation": "To provide a more comprehensive evaluation framework for assessing the risks associated with content generated by Large Language Models (LLMs), which are often evaluated using simplistic binary classifications.", "method": "XGUARD is created using 3,840 real-world prompts, categorizing LLM outputs into five danger levels and employing an Attack Severity Curve for visualization and analysis.", "result": "The evaluation of six LLMs reveals significant safety gaps and insights into the trade-offs between robustness and expressive freedom in the generated content.", "conclusion": "The study emphasizes the importance of graded metrics in enhancing the safety and trustworthiness of LLMs by providing a more detailed understanding of risk.", "key_contributions": ["Introduction of XGUARD benchmark for LLM content evaluation", "Development of five danger levels for nuanced risk assessment", "Presentation of the Attack Severity Curve for visualizing vulnerabilities"], "limitations": "The framework relies on specific data sources and may not capture all potential risks in generated content.", "keywords": ["Large Language Models", "safety evaluation", "extremist content", "benchmark", "risk assessment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00975", "pdf": "https://arxiv.org/pdf/2506.00975.pdf", "abs": "https://arxiv.org/abs/2506.00975", "title": "NTPP: Generative Speech Language Modeling for Dual-Channel Spoken Dialogue via Next-Token-Pair Prediction", "authors": ["Qichao Wang", "Ziqiao Meng", "Wenqian Cui", "Yifei Zhang", "Pengcheng Wu", "Bingzhe Wu", "Irwin King", "Liang Chen", "Peilin Zhao"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Inspired by the impressive capabilities of GPT-4o, there is growing interest\nin enabling speech language models (SLMs) to engage in natural, fluid spoken\ninteractions with humans. Recent advancements have led to the development of\nseveral SLMs that demonstrate promising results in this area. However, current\napproaches have yet to fully exploit dual-channel speech data, which inherently\ncaptures the structure and dynamics of human conversation. In this work, we\nsystematically explore the use of dual-channel speech data in the context of\nmodern large language models, and introduce a novel generative modeling\nparadigm, Next-Token-Pair Prediction (NTPP), to enable speaker-independent\ndual-channel spoken dialogue learning using decoder-only architectures for the\nfirst time. We evaluate our approach on standard benchmarks, and empirical\nresults show that our proposed method, NTPP, significantly improves the\nconversational abilities of SLMs in terms of turn-taking prediction, response\ncoherence, and naturalness. Moreover, compared to existing methods, NTPP\nachieves substantially lower inference latency, highlighting its practical\nefficiency for real-time applications.", "AI": {"tldr": "This work introduces a novel dual-channel speech data modeling approach, Next-Token-Pair Prediction, to enhance spoken interaction capabilities of speech language models.", "motivation": "To improve the naturalness and fluidity of spoken interactions using speech language models by utilizing dual-channel speech data.", "method": "The paper presents a new paradigm called Next-Token-Pair Prediction (NTPP) for speaker-independent spoken dialogue learning using decoder-only architectures, leveraging dual-channel speech data.", "result": "Empirical evaluations show that NTPP significantly enhances conversational abilities of speech language models, including improved turn-taking prediction, response coherence, and lower inference latency compared to existing methods.", "conclusion": "NTPP demonstrates practical efficiency for real-time applications in enhancing the interaction capabilities of speech language models.", "key_contributions": ["Introduction of Next-Token-Pair Prediction (NTPP) for dual-channel spoken dialogue learning", "Significant improvements in conversational abilities of SLMs", "Lower inference latency for real-time applications"], "limitations": "", "keywords": ["speech language models", "dual-channel speech data", "Next-Token-Pair Prediction", "turn-taking prediction", "real-time applications"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.00980", "pdf": "https://arxiv.org/pdf/2506.00980.pdf", "abs": "https://arxiv.org/abs/2506.00980", "title": "LEMONADE: A Large Multilingual Expert-Annotated Abstractive Event Dataset for the Real World", "authors": ["Sina J. Semnani", "Pingyue Zhang", "Wanyue Zhai", "Haozhuo Li", "Ryan Beauchamp", "Trey Billing", "Katayoun Kishi", "Manling Li", "Monica S. Lam"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "This paper presents LEMONADE, a large-scale conflict event dataset comprising\n39,786 events across 20 languages and 171 countries, with extensive coverage of\nregion-specific entities. LEMONADE is based on a partially reannotated subset\nof the Armed Conflict Location & Event Data (ACLED), which has documented\nglobal conflict events for over a decade.\n  To address the challenge of aggregating multilingual sources for global event\nanalysis, we introduce abstractive event extraction (AEE) and its subtask,\nabstractive entity linking (AEL). Unlike conventional span-based event\nextraction, our approach detects event arguments and entities through holistic\ndocument understanding and normalizes them across the multilingual dataset. We\nevaluate various large language models (LLMs) on these tasks, adapt existing\nzero-shot event extraction systems, and benchmark supervised models.\nAdditionally, we introduce ZEST, a novel zero-shot retrieval-based system for\nAEL.\n  Our best zero-shot system achieves an end-to-end F1 score of 58.3%, with LLMs\noutperforming specialized event extraction models such as GoLLIE. For entity\nlinking, ZEST achieves an F1 score of 45.7%, significantly surpassing OneNet, a\nstate-of-the-art zero-shot baseline that achieves only 23.7%. However, these\nzero-shot results lag behind the best supervised systems by 20.1% and 37.0% in\nthe end-to-end and AEL tasks, respectively, highlighting the need for further\nresearch.", "AI": {"tldr": "LEMONADE is a large-scale multilingual conflict event dataset and introduces a novel approach for abstractive event extraction and entity linking, enhancing global event analysis with large language models.", "motivation": "To address the challenge of aggregating multilingual sources for global event analysis and improve event extraction methodologies.", "method": "Introduced a novel approach for abstractive event extraction (AEE) and abstractive entity linking (AEL), utilizing large language models (LLMs) for detection and normalization of event data across a multilingual dataset.", "result": "The best zero-shot AEE system achieved an end-to-end F1 score of 58.3%, while the ZEST entity linking system reached an F1 score of 45.7%, both outperforming previous models.", "conclusion": "There is a notable performance gap in zero-shot results compared to supervised systems, indicating the need for further research in this area.", "key_contributions": ["Introduction of the LEMONADE dataset with extensive multilingual coverage.", "Development of the AEE and AEL methodologies for improved event extraction.", "Establishment of ZEST, a novel retrieval-based system for entity linking."], "limitations": "Zero-shot results are significantly lower than the best-performing supervised models, pointing to the necessity for ongoing improvement in methodologies.", "keywords": ["event extraction", "entity linking", "multilingual dataset", "large language models", "conflict events"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.00981", "pdf": "https://arxiv.org/pdf/2506.00981.pdf", "abs": "https://arxiv.org/abs/2506.00981", "title": "What do self-supervised speech models know about Dutch? Analyzing advantages of language-specific pre-training", "authors": ["Marianne de Heer Kloots", "Hosein Mohebbi", "Charlotte Pouw", "Gaofei Shen", "Willem Zuidema", "Martijn Bentum"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025. For model, code, and materials, see\n  https://github.com/mdhk/SSL-NL-eval", "summary": "How language-specific are speech representations learned by self-supervised\nmodels? Existing work has shown that a range of linguistic features can be\nsuccessfully decoded from end-to-end models trained only on speech recordings.\nHowever, it's less clear to what extent pre-training on specific languages\nimproves language-specific linguistic information. Here we test the encoding of\nDutch phonetic and lexical information in internal representations of\nself-supervised Wav2Vec2 models. Pre-training exclusively on Dutch improves the\nrepresentation of Dutch linguistic features as compared to pre-training on\nsimilar amounts of English or larger amounts of multilingual data. This\nlanguage-specific advantage is well-detected by trained clustering or\nclassification probes, and partially observable using zero-shot metrics.\nFurthermore, the language-specific benefit on linguistic feature encoding\naligns with downstream performance on Automatic Speech Recognition.", "AI": {"tldr": "The paper investigates the language-specificity of speech representations in self-supervised models, particularly focusing on Dutch representations in Wav2Vec2 models.", "motivation": "To explore how pre-training on specific languages affects the encoding of linguistic features in self-supervised speech representation models.", "method": "The study tests the encoding of Dutch phonetic and lexical information in Wav2Vec2 models pre-trained on Dutch, English, and multilingual data, using clustering and classification probes as well as zero-shot metrics.", "result": "Pre-training exclusively on Dutch leads to better representation of Dutch linguistic features compared to pre-training on English or multilingual data, which is reflected in downstream performance on Automatic Speech Recognition.", "conclusion": "Language-specific pre-training enhances the encoding of linguistic features and is beneficial for speech recognition tasks, as shown in the experiments with Wav2Vec2 models.", "key_contributions": ["Demonstrated the importance of language-specific pre-training in improving speech representation accuracy", "Provided evidence of enhanced encoding of Dutch linguistic features through various probing methods", "Aligned performance improvements with downstream Automatic Speech Recognition tasks."], "limitations": "", "keywords": ["self-supervised learning", "speech recognition", "Wav2Vec2", "language representation", "Dutch"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.00985", "pdf": "https://arxiv.org/pdf/2506.00985.pdf", "abs": "https://arxiv.org/abs/2506.00985", "title": "Do LLMs Understand Why We Write Diaries? A Method for Purpose Extraction and Clustering", "authors": ["Valeriya Goloviznina", "Alexander Sergeev", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "categories": ["cs.CL"], "comment": "Accepted for CompLing-2025 conference", "summary": "Diary analysis presents challenges, particularly in extracting meaningful\ninformation from large corpora, where traditional methods often fail to deliver\nsatisfactory results. This study introduces a novel method based on Large\nLanguage Models (LLMs) to identify and cluster the various purposes of diary\nwriting. By \"purposes,\" we refer to the intentions behind diary writing, such\nas documenting life events, self-reflection, or practicing language skills. Our\napproach is applied to Soviet-era diaries (1922-1929) from the Prozhito digital\narchive, a rich collection of personal narratives. We evaluate different\nproprietary and open-source LLMs, finding that GPT-4o and o1-mini achieve the\nbest performance, while a template-based baseline is significantly less\neffective. Additionally, we analyze the retrieved purposes based on gender, age\nof the authors, and the year of writing. Furthermore, we examine the types of\nerrors made by the models, providing a deeper understanding of their\nlimitations and potential areas for improvement in future research.", "AI": {"tldr": "This study presents a novel method using Large Language Models (LLMs) to identify and cluster the purposes behind diary writing, applied to Soviet-era diaries.", "motivation": "To address the challenges in diary analysis, particularly in extracting meaningful information from large corpora, where traditional methods often fall short.", "method": "The study evaluates various proprietary and open-source LLMs, specifically focusing on their effectiveness in identifying and clustering the intentions behind diary writing, using Soviet-era diaries as a case study.", "result": "The evaluation finds that GPT-4o and o1-mini outperform other methods, including a traditional template-based approach, in accurately identifying diary writing purposes.", "conclusion": "The findings provide insights into the effectiveness of LLMs for diary analysis and highlight the gender and age-based differences in writing purposes, along with error analysis that sheds light on model limitations.", "key_contributions": ["Introduces a new method for analyzing diary writing using LLMs.", "Demonstrates superior performance of LLMs over traditional methods in identifying writing purposes.", "Analyzes demographic influences on diary writing intentions."], "limitations": "The study primarily focuses on a specific historical period and dataset, which may limit the generalizability of the results.", "keywords": ["Large Language Models", "diary analysis", "purpose clustering", "Soviet-era diaries", "error analysis"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.00986", "pdf": "https://arxiv.org/pdf/2506.00986.pdf", "abs": "https://arxiv.org/abs/2506.00986", "title": "Talking to Data: Designing Smart Assistants for Humanities Databases", "authors": ["Alexander Sergeev", "Valeriya Goloviznina", "Mikhail Melnichenko", "Evgeny Kotelnikov"], "categories": ["cs.CL"], "comment": "Accepted for InterSys-2025 conference", "summary": "Access to humanities research databases is often hindered by the limitations\nof traditional interaction formats, particularly in the methods of searching\nand response generation. This study introduces an LLM-based smart assistant\ndesigned to facilitate natural language communication with digital humanities\ndata. The assistant, developed in a chatbot format, leverages the RAG approach\nand integrates state-of-the-art technologies such as hybrid search, automatic\nquery generation, text-to-SQL filtering, semantic database search, and\nhyperlink insertion. To evaluate the effectiveness of the system, experiments\nwere conducted to assess the response quality of various language models. The\ntesting was based on the Prozhito digital archive, which contains diary entries\nfrom predominantly Russian-speaking individuals who lived in the 20th century.\nThe chatbot is tailored to support anthropology and history researchers, as\nwell as non-specialist users with an interest in the field, without requiring\nprior technical training. By enabling researchers to query complex databases\nwith natural language, this tool aims to enhance accessibility and efficiency\nin humanities research. The study highlights the potential of Large Language\nModels to transform the way researchers and the public interact with digital\narchives, making them more intuitive and inclusive. Additional materials are\npresented in GitHub repository:\nhttps://github.com/alekosus/talking-to-data-intersys2025.", "AI": {"tldr": "This study presents an LLM-based smart assistant designed to improve accessibility and interaction with digital humanities research data through natural language communication.", "motivation": "To enhance access to humanities research databases, which are currently limited by traditional interaction formats.", "method": "Development of a chatbot assistant leveraging RAG approach, hybrid search, automatic query generation, semantic database search, and text-to-SQL filtering.", "result": "The system was evaluated through experiments assessing response quality against various language models, particularly using the Prozhito digital archive.", "conclusion": "The introduction of LLMs in this context can significantly improve how researchers and the public engage with digital archives, making the interaction more intuitive.", "key_contributions": ["Introduction of a chatbot for natural language queries in digital humanities", "Integration of advanced LLM technologies to enhance research data accessibility", "Evaluation of language model response quality in the context of historical diaries"], "limitations": "", "keywords": ["digital humanities", "LLM", "chatbot", "natural language processing", "research accessibility"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.01034", "pdf": "https://arxiv.org/pdf/2506.01034.pdf", "abs": "https://arxiv.org/abs/2506.01034", "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models", "authors": ["Benjamin Matthias Ruppik", "Julius von Rohrscheidt", "Carel van Niekerk", "Michael Heck", "Renato Vukovic", "Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Bastian Rieck", "Marcus Zibrowius", "Milica Gašić"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, with an additional 13 pages of appendix", "summary": "Understanding the internal mechanisms of large language models (LLMs) remains\na challenging and complex endeavor. Even fundamental questions, such as how\nfine-tuning affects model behavior, often require extensive empirical\nevaluation. In this paper, we introduce a novel perspective based on the\ngeometric properties of contextual latent embeddings to study the effects of\ntraining and fine-tuning. To that end, we measure the local dimensions of a\ncontextual language model's latent space and analyze their shifts during\ntraining and fine-tuning. We show that the local dimensions provide insights\ninto the model's training dynamics and generalization ability. Specifically,\nthe mean of the local dimensions predicts when the model's training\ncapabilities are exhausted, as exemplified in a dialogue state tracking task,\noverfitting, as demonstrated in an emotion recognition task, and grokking, as\nillustrated with an arithmetic task. Furthermore, our experiments suggest a\npractical heuristic: reductions in the mean local dimension tend to accompany\nand predict subsequent performance gains. Through this exploration, we aim to\nprovide practitioners with a deeper understanding of the implications of\nfine-tuning on embedding spaces, facilitating informed decisions when\nconfiguring models for specific applications. The results of this work\ncontribute to the ongoing discourse on the interpretability, adaptability, and\ngeneralizability of LLMs by bridging the gap between intrinsic model mechanisms\nand geometric properties in the respective embeddings.", "AI": {"tldr": "This paper explores the effects of training and fine-tuning on large language models (LLMs) through the analysis of geometric properties of their contextual latent embeddings.", "motivation": "To understand the internal mechanisms of LLMs and how fine-tuning influences model behavior.", "method": "The study measures local dimensions of a contextual language model's latent space and analyzes shifts during training and fine-tuning.", "result": "Mean local dimensions predict when model training capacities are exhausted, indicating overfitting and grokking in various tasks.", "conclusion": "A reduction in mean local dimensions is a practical heuristic that can predict performance gains, enhancing practitioners' understanding of fine-tuning implications.", "key_contributions": ["Introduced geometric properties of latent embeddings as a perspective for studying LLMs.", "Established a relationship between local dimensions and model training dynamics.", "Provided practical heuristics for predicting performance gains based on local dimension reductions."], "limitations": "The exploration may not capture all factors affecting LLM behavior and generalization beyond geometric properties.", "keywords": ["large language models", "fine-tuning", "latent embeddings", "training dynamics", "generalization"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2506.01042", "pdf": "https://arxiv.org/pdf/2506.01042.pdf", "abs": "https://arxiv.org/abs/2506.01042", "title": "Probing Neural Topology of Large Language Models", "authors": ["Yu Zheng", "Yuan Yuan", "Yong Li", "Paolo Santi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probing large language models (LLMs) has yielded valuable insights into their\ninternal mechanisms by linking neural representations to interpretable\nsemantics. However, how neurons functionally co-activate with each other to\ngive rise to emergent capabilities remains largely unknown, hindering a deeper\nunderstanding and safer development of LLMs. In this work, we introduce graph\nprobing, a method for uncovering the functional connectivity topology of LLM\nneurons and relating it to language generation performance. By analyzing\ninternal neural graphs across diverse LLM families and scales, we discover a\nuniversal predictability of next-token prediction performance using only neural\ntopology. This predictability is robust even when retaining just 1% of neuron\nconnections or probing models after only 8 pretraining steps, highlighting the\nsparsity and early emergence of topological patterns. Further graph matching\nanalysis suggests that, despite significant distinctions in architectures,\nparameters, and training data, different LLMs develop intricate and consistent\nneural topological structures that may form the foundation for their language\ngeneration abilities. Codes and data for the graph probing toolbox are released\nat https://github.com/DavyMorgan/llm-graph-probing.", "AI": {"tldr": "This paper introduces graph probing, a method for analyzing functional connectivity in large language models (LLMs), revealing predictability in their language generation performance based on neural topology.", "motivation": "To understand how neurons in LLMs co-activate and contribute to emergent capabilities, improving safety and effectiveness in their development.", "method": "The authors employ graph probing to analyze internal neural graphs of LLMs, examining their connectivity in relation to next-token prediction accuracy.", "result": "The study finds a universal predictability of next-token generation performance based solely on neural topology, demonstrating significant robustness even with minimal neuron connections.", "conclusion": "Different LLM architectures share similar neural topological structures, which are foundational for their language generation capabilities, indicating a consistency across diverse LLM families.", "key_contributions": ["Introduction of graph probing for LLMs", "Discovery of predictability in next-token prediction based on neural topology", "Analysis showing consistent neural structures across various LLMs"], "limitations": "", "keywords": ["large language models", "graph probing", "neural topology", "language generation", "functional connectivity"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01047", "pdf": "https://arxiv.org/pdf/2506.01047.pdf", "abs": "https://arxiv.org/abs/2506.01047", "title": "CHEER-Ekman: Fine-grained Embodied Emotion Classification", "authors": ["Phan Anh Duong", "Cat Luong", "Divyesh Bommana", "Tianyu Jiang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Emotions manifest through physical experiences and bodily reactions, yet\nidentifying such embodied emotions in text remains understudied. We present an\nembodied emotion classification dataset, CHEER-Ekman, extending the existing\nbinary embodied emotion dataset with Ekman's six basic emotion categories.\nUsing automatic best-worst scaling with large language models, we achieve\nperformance superior to supervised approaches on our new dataset. Our\ninvestigation reveals that simplified prompting instructions and\nchain-of-thought reasoning significantly improve emotion recognition accuracy,\nenabling smaller models to achieve competitive performance with larger ones.", "AI": {"tldr": "The paper introduces the CHEER-Ekman dataset for classifying embodied emotions in text, achieving superior performance using large language models and simplified prompting techniques.", "motivation": "To address the gap in identifying embodied emotions in text, which are connected to physical experiences and reactions.", "method": "Automatic best-worst scaling with large language models was employed for classification, utilizing simplified prompting and chain-of-thought reasoning.", "result": "Achieved performance surpassing traditional supervised approaches, even enabling smaller models to perform competitively with larger ones.", "conclusion": "Simplified prompting and reasoning enhance emotion recognition capabilities, making it feasible for smaller models to excel in this area.", "key_contributions": ["Introduction of the CHEER-Ekman dataset for embodied emotions", "Demonstration of superior performance using automatic best-worst scaling", "Identification of effective prompting strategies that enhance model accuracy"], "limitations": "", "keywords": ["embodied emotions", "emotion classification", "language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01062", "pdf": "https://arxiv.org/pdf/2506.01062.pdf", "abs": "https://arxiv.org/abs/2506.01062", "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language Models", "authors": ["Thinh Pham", "Nguyen Nguyen", "Pratibha Zunjare", "Weiyuan Chen", "Yu-Min Tseng", "Tu Vu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint. 22 pages, 7 figures, 11 tables", "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.", "AI": {"tldr": "SealQA is a benchmark for evaluating search-augmented language models on fact-seeking questions with noisy search results, revealing significant limitations in current models' performance.", "motivation": "To assess the factual accuracy and reasoning capabilities of language models in scenarios where web search yields conflicting or unhelpful results.", "method": "SealQA evaluates language models through three variations: Seal-0 and Seal-Hard for factual accuracy and reasoning, and LongSeal for multi-document reasoning in complex contexts.", "result": "The evaluation shows that even advanced LLMs like GPT-4.1 struggle with accuracy, with performances as low as 17.1% and 6.3% on the Seal-0 benchmark; increasing test-time compute does not result in reliable improvements.", "conclusion": "The findings indicate critical shortcomings in current language models' reasoning abilities, particularly in the presence of noisy information and complex search contexts; future research can use the SealQA dataset for improvement.", "key_contributions": ["Introduction of SealQA as a benchmark", "Identification of critical limitations in state-of-the-art LLMs", "Release of the dataset for public use to encourage future research"], "limitations": "Current models are vulnerable to noisy search results and fail to reliably identify relevant documents in multi-document contexts.", "keywords": ["SealQA", "Language Models", "Fact-seeking Questions", "Reasoning capabilities", "Multi-document reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01074", "pdf": "https://arxiv.org/pdf/2506.01074.pdf", "abs": "https://arxiv.org/abs/2506.01074", "title": "How Programming Concepts and Neurons Are Shared in Code Language Models", "authors": ["Amir Hossein Kargaran", "Yihong Liu", "François Yvon", "Hinrich Schütze"], "categories": ["cs.CL", "cs.PL", "cs.SE"], "comment": "ACL Findings 2025", "summary": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.", "AI": {"tldr": "This paper explores the concept space of large language models (LLMs) in relation to multiple programming languages (PLs) and English, analyzing embeddings and neuron activations across models during coding tasks.", "motivation": "To investigate how LLMs internally represent multiple programming languages alongside English, particularly in coding tasks involving few-shot translation.", "method": "The study performs a few-shot translation task on 21 programming language pairs using two Llama-based models, decoding embeddings of intermediate layers and analyzing neuron activations for 11 PLs.", "result": "The analysis reveals that the concept space is closer to English in the intermediate layers, with language-specific neurons found primarily in lower layers and PL-exclusive neurons appearing in higher layers. Highly aligned PLs show shared concept spaces and larger keyword sets.", "conclusion": "The findings enhance the understanding of the structural patterns in LLMs' representation of programming languages and their relationship with English, suggesting implications for future research in context-sensitive coding tasks.", "key_contributions": ["Investigates the relationship between multiple programming languages and English in LLMs.", "Analyzes neuron activations and identifies structural patterns in the concept space of LLMs.", "Provides a framework for understanding internal PL representations within LLMs."], "limitations": "Focused primarily on embedding analysis; further studies needed to generalize findings across additional PLs and tasks.", "keywords": ["large language models", "programming languages", "neuron activations", "concept space", "few-shot translation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01084", "pdf": "https://arxiv.org/pdf/2506.01084.pdf", "abs": "https://arxiv.org/abs/2506.01084", "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via Token Compression", "authors": ["Saibo Geng", "Nathan Ranchin", "Yunzhen yao", "Maxime Peyrard", "Chris Wendler", "Michael Gastpar", "Robert West"], "categories": ["cs.CL", "cs.LG"], "comment": "Code will be released at https://github.com/epfl-dlab/zip2zip", "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.", "AI": {"tldr": "Introducing zip2zip, a dynamic tokenization framework that enhances the efficiency of large language models (LLMs) by allowing them to adapt their token vocabulary at inference time.", "motivation": "To address the inefficiencies of static tokenizers in LLMs, which often result in longer token sequences and higher computational costs due to their inability to adapt to specific domains or languages.", "method": "The zip2zip framework includes a dynamic tokenizer using LZW compression, an embedding layer for hypertokens, and a causal language model variant for training on compressed sequences.", "result": "zip2zip LLMs can reduce input and output sequence lengths by 20-60%, leading to significant improvements in inference latency.", "conclusion": "The zip2zip framework allows for efficient adaptation of token vocabularies during inference, resulting in faster processing of language tasks.", "key_contributions": ["Introduction of a dynamic tokenizer based on LZW compression.", "Method for runtime embedding of new hypertokens.", "Parameter-efficient finetuning for existing LLMs to use hypertokens."], "limitations": "", "keywords": ["Large Language Models", "Tokenization", "LZW Compression", "Inference Efficiency", "Dynamic Adaptation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01089", "pdf": "https://arxiv.org/pdf/2506.01089.pdf", "abs": "https://arxiv.org/abs/2506.01089", "title": "Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements", "authors": ["Metehan Oguz", "Yavuz Bakman", "Duygu Nur Yaldiz"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated impressive performances in\ntasks related to coreference resolution. However, previous studies mostly\nassessed LLM performance on coreference resolution with nouns and third person\npronouns. This study evaluates LLM performance on coreference resolution with\nindexical like I, you, here and tomorrow, which come with unique challenges due\nto their linguistic properties. We present the first study examining how LLMs\ninterpret indexicals in English, releasing the English Indexical Dataset with\n1600 multiple-choice questions. We evaluate pioneering LLMs, including GPT-4o,\nClaude 3.5 Sonnet, Gemini 1.5 Pro, and DeepSeek V3. Our results reveal that\nLLMs exhibit an impressive performance with some indexicals (I), while\nstruggling with others (you, here, tomorrow), and that syntactic cues (e.g.\nquotation) contribute to LLM performance with some indexicals, while they\nreduce performance with others. Code and data are available at:\nhttps://github.com/metehanoguzz/LLMs-Indexicals-English.", "AI": {"tldr": "This study evaluates Large Language Models' performance on coreference resolution with English indexicals, revealing varied success across different indexicals and the impact of syntactic cues.", "motivation": "Previous studies on coreference resolution in LLMs mainly focused on nouns and third-person pronouns, leaving a gap in understanding indexicals like I, you, here, and tomorrow.", "method": "The study presents the English Indexical Dataset containing 1600 multiple-choice questions, testing LLM performance on indexicals, and evaluates models including GPT-4o and Claude 3.5 Sonnet.", "result": "LLMs show impressive performance with some indexicals, particularly 'I', while they struggle with others like 'you', 'here', and 'tomorrow'. Syntactic cues influence performance differently among indexicals.", "conclusion": "The evaluation highlights a nuanced understanding of how LLMs interpret indexicals, emphasizing the need for targeted approaches in improving their performance with challenging cases.", "key_contributions": ["Introduction of the first English Indexical Dataset for studying LLM performance on indexicals", "Evaluation of multiple pioneering LLMs on coreference resolution", "Insights into the role of syntactic cues in LLM performance with indexicals"], "limitations": "The study primarily focuses on English indexicals, limiting its applicability to other languages and contexts.", "keywords": ["Large Language Models", "coreference resolution", "indexicals", "NLP", "ACL 2025"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01104", "pdf": "https://arxiv.org/pdf/2506.01104.pdf", "abs": "https://arxiv.org/abs/2506.01104", "title": "Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection", "authors": ["Steven Robinson", "Antonio Carlos Rivera"], "categories": ["cs.CL"], "comment": null, "summary": "The pervasive deployment of large language models (LLMs) in conversational AI\nsystems has revolutionized information access, yet their propensity for\ngenerating factually unsupported or hallucinated responses remains a critical\nimpediment to trustworthiness and widespread adoption. This paper introduces\nReinforced Unanswerability Learning (RUL), a novel hybrid training paradigm\ndesigned to imbue LLMs with the intrinsic capability to accurately detect\nunanswerable questions and generate reliably appropriate responses. Unlike\nconventional approaches that rely on external classifiers or simple prompting,\nRUL integrates a discriminative unanswerability prediction head with the LLM's\ngenerative core, guided by a multi-stage learning strategy. This includes\nsupervised fine-tuning on a novel, richly annotated dataset,\nEnhanced-CAsT-Answerability (ECA), which features hierarchical answerability\nlabels and ground-truth refusal responses. Crucially, RUL incorporates a\nsubsequent reinforcement learning with human feedback (RLHF) phase to refine\nthe nuance, helpfulness, and informativeness of refusal responses. Extensive\nexperiments demonstrate RUL's superior performance, achieving significantly\nhigher accuracy in unanswerability detection across sentence, paragraph, and\nranking levels, and substantially increasing the generation of appropriate\nrefusals for unanswerable queries, alongside strong performance on answerable\nquestions. Human evaluations further corroborate RUL's effectiveness,\nhighlighting a marked improvement in perceived helpfulness and trustworthiness,\nultimately paving the way for more reliable and user-centric conversational AI.", "AI": {"tldr": "This paper presents Reinforced Unanswerability Learning (RUL), a new training approach for LLMs aimed at improving their ability to detect unanswerable questions and generate appropriate responses, thus enhancing trust and usability in conversational AI.", "motivation": "To address the issue of factually unsupported or hallucinated responses in LLMs, which hinders their trustworthiness and adoption.", "method": "RUL combines a discriminative prediction head for unanswerability with the generative capabilities of LLMs, utilizing a multi-stage learning strategy that includes supervised fine-tuning on a novel dataset, followed by reinforcement learning with human feedback.", "result": "RUL outperformed traditional methods by demonstrating higher accuracy in detecting unanswerable queries and significantly improving the generation of appropriate refusal responses, while maintaining robust performance on answerable questions.", "conclusion": "The results indicate that RUL enhances the reliability and user-centricity of conversational AI, thereby fostering greater trust as demonstrated by human evaluations.", "key_contributions": ["Introduction of Reinforced Unanswerability Learning (RUL) methodology", "Creation and utilization of the Enhanced-CAsT-Answerability (ECA) dataset", "Demonstrated improvement in trustworthiness and user satisfaction in conversational AI."], "limitations": "", "keywords": ["Large Language Models", "Unanswerability Detection", "Reinforcement Learning", "Conversational AI", "Human Feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01133", "pdf": "https://arxiv.org/pdf/2506.01133.pdf", "abs": "https://arxiv.org/abs/2506.01133", "title": "From Words to Waves: Analyzing Concept Formation in Speech and Text-Based Foundation Models", "authors": ["Asım Ersoy", "Basel Mousi", "Shammur Chowdhury", "Firoj Alam", "Fahim Dalvi", "Nadir Durrani"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted Interspeech 2025", "summary": "The emergence of large language models (LLMs) has demonstrated that systems\ntrained solely on text can acquire extensive world knowledge, develop reasoning\ncapabilities, and internalize abstract semantic concepts--showcasing properties\nthat can be associated with general intelligence. This raises an intriguing\nquestion: Do such concepts emerge in models trained on other modalities, such\nas speech? Furthermore, when models are trained jointly on multiple modalities:\nDo they develop a richer, more structured semantic understanding? To explore\nthis, we analyze the conceptual structures learned by speech and textual models\nboth individually and jointly. We employ Latent Concept Analysis, an\nunsupervised method for uncovering and interpreting latent representations in\nneural networks, to examine how semantic abstractions form across modalities.\nFor reproducibility we made scripts and other resources available to the\ncommunity.", "AI": {"tldr": "This paper investigates whether models trained on multiple modalities like speech and text acquire richer semantic understanding using Latent Concept Analysis to analyze learned concepts.", "motivation": "To explore the emergence of semantic concepts in models trained on speech and text and to understand if joint training enhances semantic understanding.", "method": "We use Latent Concept Analysis, an unsupervised method, to examine and interpret the latent representations formed by speech and textual models, both individually and jointly.", "result": "Our analysis reveals how semantic abstractions are formed across modalities, contributing insights into the comparative understanding of speech and text models.", "conclusion": "The findings suggest that joint training on multiple modalities potentially leads to a richer conceptual structure, enhancing semantic understanding in neural networks.", "key_contributions": ["Introduces Latent Concept Analysis for interpreting multimodal models.", "Analyzes the semantic understanding of speech versus text models.", "Provides resources and scripts for reproducibility."], "limitations": "", "keywords": ["Large Language Models", "Multimodal Learning", "Latent Concept Analysis"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01147", "pdf": "https://arxiv.org/pdf/2506.01147.pdf", "abs": "https://arxiv.org/abs/2506.01147", "title": "A Word is Worth 4-bit: Efficient Log Parsing with Binary Coded Decimal Recognition", "authors": ["Prerak Srivastava", "Giulio Corallo", "Sergey Rybalko"], "categories": ["cs.CL", "cs.LG"], "comment": "Pre-print of our accepted paper at IEEE International Conference on\n  Web Services (ICWS 2025). 4 pages, 2 figures", "summary": "System-generated logs are typically converted into categorical log templates\nthrough parsing. These templates are crucial for generating actionable insights\nin various downstream tasks. However, existing parsers often fail to capture\nfine-grained template details, leading to suboptimal accuracy and reduced\nutility in downstream tasks requiring precise pattern identification. We\npropose a character-level log parser utilizing a novel neural architecture that\naggregates character embeddings. Our approach estimates a sequence of\nbinary-coded decimals to achieve highly granular log templates extraction. Our\nlow-resource character-level parser, tested on revised Loghub-2k and a manually\nannotated industrial dataset, matches LLM-based parsers in accuracy while\noutperforming semantic parsers in efficiency.", "AI": {"tldr": "This paper introduces a character-level log parser that outperforms existing parsers in efficiency while matching their accuracy, focusing on granular log template extraction.", "motivation": "Existing log parsers often miss fine-grained details, reducing their utility in tasks requiring precise pattern identification.", "method": "A character-level log parser utilizing a novel neural architecture that aggregates character embeddings to estimate a sequence of binary-coded decimals for log template extraction.", "result": "The proposed parser matches LLM-based parsers in accuracy and surpasses semantic parsers in efficiency, tested on revised Loghub-2k and an annotated industrial dataset.", "conclusion": "Our approach offers a low-resource solution for improving log template extraction accuracy and efficiency.", "key_contributions": ["Character-level log parsing using a novel neural architecture", "High accuracy comparable to LLM-based parsers", "Enhanced efficiency compared to semantic parsers"], "limitations": "", "keywords": ["log parsing", "neural architecture", "template extraction"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.01156", "pdf": "https://arxiv.org/pdf/2506.01156.pdf", "abs": "https://arxiv.org/abs/2506.01156", "title": "Mispronunciation Detection Without L2 Pronunciation Dataset in Low-Resource Setting: A Case Study in Finland Swedish", "authors": ["Nhan Phan", "Mikko Kuronen", "Maria Kautonen", "Riikka Ullakonoja", "Anna von Zansen", "Yaroslav Getman", "Ekaterina Voskoboinik", "Tamás Grósz", "Mikko Kurimo"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025 conference", "summary": "Mispronunciation detection (MD) models are the cornerstones of many language\nlearning applications. Unfortunately, most systems are built for English and\nother major languages, while low-resourced language varieties, such as Finland\nSwedish (FS), lack such tools. In this paper, we introduce our MD model for FS,\ntrained on 89 hours of first language (L1) speakers' spontaneous speech and\ntested on 33 minutes of L2 transcribed read-aloud speech.\n  We trained a multilingual wav2vec 2.0 model with entropy regularization,\nfollowed by temperature scaling and top-k normalization after the inference to\nbetter adapt it for MD. The main novelty of our method lies in its simplicity,\nrequiring minimal L2 data. The process is also language-independent, making it\nsuitable for other low-resource languages. Our proposed algorithm allows us to\nbalance Recall (43.2%) and Precision (29.8%), compared with the baseline\nmodel's Recall (77.5%) and Precision (17.6%).", "AI": {"tldr": "This paper presents a mispronunciation detection model for Finland Swedish, addressing the lack of tools for low-resourced languages.", "motivation": "To develop mispronunciation detection tools for low-resourced languages like Finland Swedish, which currently lack adequate support.", "method": "A multilingual wav2vec 2.0 model was trained with entropy regularization, temperature scaling, and top-k normalization for mispronunciation detection.", "result": "The method achieved a Recall of 43.2% and Precision of 29.8%, significantly improving the balance compared to the baseline model's Recall of 77.5% and Precision of 17.6%.", "conclusion": "The proposed algorithm is effective for low-resource languages and requires minimal L2 data, making it widely applicable.", "key_contributions": ["Development of a mispronunciation detection model for Finland Swedish", "Utilization of minimal L2 data", "Language-independent approach applicable to other low-resource languages"], "limitations": "", "keywords": ["Mispronunciation Detection", "Low-resourced Languages", "Wav2vec 2.0"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.01172", "pdf": "https://arxiv.org/pdf/2506.01172.pdf", "abs": "https://arxiv.org/abs/2506.01172", "title": "The Inverse Scaling Effect of Pre-Trained Language Model Surprisal Is Not Due to Data Leakage", "authors": ["Byung-Doh Oh", "Hongao Zhu", "William Schuler"], "categories": ["cs.CL"], "comment": "ACL Findings 2025; results with Natural Stories alignment issue\n  corrected (commit 4700daa)", "summary": "In psycholinguistic modeling, surprisal from larger pre-trained language\nmodels has been shown to be a poorer predictor of naturalistic human reading\ntimes. However, it has been speculated that this may be due to data leakage\nthat caused language models to see the text stimuli during training. This paper\npresents two studies to address this concern at scale. The first study reveals\nrelatively little leakage of five naturalistic reading time corpora in two\npre-training datasets in terms of length and frequency of token $n$-gram\noverlap. The second study replicates the negative relationship between language\nmodel size and the fit of surprisal to reading times using models trained on\n'leakage-free' data that overlaps only minimally with the reading time corpora.\nTaken together, this suggests that previous results using language models\ntrained on these corpora are not driven by the effects of data leakage.", "AI": {"tldr": "This paper explores the impact of data leakage in psycholinguistic modeling by analyzing reading times and NLP model predictions.", "motivation": "To investigate whether data leakage from training corpora affects the performance of pre-trained language models in predicting human reading times.", "method": "Two studies were conducted: the first assessed token overlap between reading time corpora and pre-training datasets, while the second evaluated the relationship between model size and surprisals using 'leakage-free' data.", "result": "The first study found minimal leakage in token overlap, and the second study confirmed that larger language models fit surprisal to reading times poorly, independent of any observed data leakage.", "conclusion": "The findings indicate that previous discrepancies between language model predictions and human reading times are not primarily driven by data leakage from training corpora.", "key_contributions": ["Demonstrated minimal data leakage in reading time corpora", "Confirmed negative relationship between model size and predictive fit of surprisal", "Provided insights into the reliability of language models in psycholinguistic contexts"], "limitations": "", "keywords": ["psycholinguistics", "language models", "reading times", "data leakage", "surprisal"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01187", "pdf": "https://arxiv.org/pdf/2506.01187.pdf", "abs": "https://arxiv.org/abs/2506.01187", "title": "LAQuer: Localized Attribution Queries in Content-grounded Generation", "authors": ["Eran Hirsch", "Aviv Slobodkin", "David Wan", "Elias Stengel-Eskin", "Mohit Bansal", "Ido Dagan"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Grounded text generation models often produce content that deviates from\ntheir source material, requiring user verification to ensure accuracy. Existing\nattribution methods associate entire sentences with source documents, which can\nbe overwhelming for users seeking to fact-check specific claims. In contrast,\nexisting sub-sentence attribution methods may be more precise but fail to align\nwith users' interests. In light of these limitations, we introduce Localized\nAttribution Queries (LAQuer), a new task that localizes selected spans of\ngenerated output to their corresponding source spans, allowing fine-grained and\nuser-directed attribution. We compare two approaches for the LAQuer task,\nincluding prompting large language models (LLMs) and leveraging LLM internal\nrepresentations. We then explore a modeling framework that extends existing\nattributed text generation methods to LAQuer. We evaluate this framework across\ntwo grounded text generation tasks: Multi-document Summarization (MDS) and\nLong-form Question Answering (LFQA). Our findings show that LAQuer methods\nsignificantly reduce the length of the attributed text. Our contributions\ninclude: (1) proposing the LAQuer task to enhance attribution usability, (2)\nsuggesting a modeling framework and benchmarking multiple baselines, and (3)\nproposing a new evaluation setting to promote future research on localized\nattribution in content-grounded generation.", "AI": {"tldr": "This paper introduces Localized Attribution Queries (LAQuer), a new task for improving user-directed attribution in grounded text generation, enabling fine-grained localization of generated content to its source material.", "motivation": "Existing methods for attributing generated text to sources often overwhelm users or fail to align with their interests, necessitating a more tailored approach.", "method": "The paper compares two approaches for the LAQuer task, including prompting large language models and leveraging their internal representations, and presents a modeling framework extending existing text generation attribution methods to the LAQuer context.", "result": "LAQuer methods significantly reduce the length of the attributed text in evaluated tasks like Multi-document Summarization and Long-form Question Answering.", "conclusion": "The contributions include the proposal of the LAQuer task, a modeling framework with benchmarks, and a novel evaluation setting to guide future research in localized attribution.", "key_contributions": ["Proposing the LAQuer task to enhance attribution usability", "Suggesting a modeling framework and benchmarking multiple baselines", "Proposing a new evaluation setting for localized attribution research"], "limitations": "", "keywords": ["Text Generation", "Attribution", "Natural Language Processing", "Large Language Models", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01190", "pdf": "https://arxiv.org/pdf/2506.01190.pdf", "abs": "https://arxiv.org/abs/2506.01190", "title": "Culturally-Grounded Chain-of-Thought (CG-CoT):Enhancing LLM Performance on Culturally-Specific Tasks in Low-Resource Languages", "authors": ["Madhavendra Thakur"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) struggle with culturally-specific reasoning\ntasks, particularly in low-resource languages, hindering their global\napplicability. Addressing this gap is crucial for equitable AI deployment. We\nintroduce Culturally-Grounded Chain-of-Thought (CG-CoT), a novel prompting\nstrategy that combines dense vector retrieval of cultural context with explicit\nreasoning sequences. Our extensive experiments on Yoruba proverb interpretation\ndemonstrate that CG-CoT provides significantly higher culturally-aligned\naccuracy and depth than traditional prompting methods, validated through both\nautomated metrics and LLM-based evaluations. Notably, we uncover stark\ndisparities between token-level translation metrics like BLEU and human-judged\ncultural relevance, suggesting a rethinking of evaluation approaches for\nlow-resource NLP.", "AI": {"tldr": "This paper introduces CG-CoT, a new prompting strategy for Large Language Models that improves their performance on culturally-specific reasoning tasks in low-resource languages.", "motivation": "To enhance the global applicability of LLMs by addressing their struggle with culturally-specific reasoning tasks, especially in low-resource languages.", "method": "The authors propose a new prompting strategy called Culturally-Grounded Chain-of-Thought (CG-CoT) that integrates dense vector retrieval of cultural context with explicit reasoning.", "result": "CG-CoT demonstrated significantly higher culturally-aligned accuracy and depth in interpreting Yoruba proverbs compared to traditional prompting methods, with validation through automated and LLM-based evaluations.", "conclusion": "The study highlights the need to reassess evaluation approaches for low-resource NLPs, noting disparities between traditional translation metrics and human-judged cultural relevance.", "key_contributions": ["Introduction of CG-CoT prompting strategy for LLMs.", "Improved accuracy in culturally-specific reasoning tasks for low-resource languages.", "Insights on evaluation metrics for cultural relevance vs. translation accuracy."], "limitations": "", "keywords": ["Large Language Models", "cultural reasoning", "low-resource languages", "prompting strategy", "NLP evaluations"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01195", "pdf": "https://arxiv.org/pdf/2506.01195.pdf", "abs": "https://arxiv.org/abs/2506.01195", "title": "CoBRA: Quantifying Strategic Language Use and LLM Pragmatics", "authors": ["Anshun Asher Zheng", "Junyi Jessy Li", "David I. Beaver"], "categories": ["cs.CL"], "comment": "18 pages", "summary": "Language is often used strategically, particularly in high-stakes,\nadversarial settings, yet most work on pragmatics and LLMs centers on\ncooperativity. This leaves a gap in systematic understanding of non-cooperative\ndiscourse. To address this, we introduce CoBRA (Cooperation-Breach Response\nAssessment), along with three interpretable metrics -- Benefit at Turn (BaT),\nPenalty at Turn (PaT), and Normalized Relative Benefit at Turn (NRBaT) -- to\nquantify the perceived strategic effects of discourse moves. We also present\nCHARM, an annotated dataset of real courtroom cross-examinations, to\ndemonstrate the framework's effectiveness. Using these tools, we evaluate a\nrange of LLMs and show that LLMs generally exhibit limited pragmatic\nunderstanding of strategic language. While model size shows an increase in\nperformance on our metrics, reasoning ability does not help and largely hurts,\nintroducing overcomplication and internal confusion.", "AI": {"tldr": "This paper introduces CoBRA, a framework for analyzing non-cooperative language in adversarial settings, features metrics to quantify discourse strategy, and evaluates LLMs using a new courtroom dataset.", "motivation": "To fill the gap in understanding non-cooperative discourse as most existing work focuses on cooperativity in language use, especially in high-stakes situations.", "method": "The authors propose CoBRA (Cooperation-Breach Response Assessment) with three metrics (BaT, PaT, NRBaT) and utilize CHARM, a dataset of courtroom cross-examinations, to evaluate these metrics on various LLMs.", "result": "The evaluation shows that LLMs generally exhibit poor pragmatic understanding of strategic language, with larger model sizes correlating with increased performance on the proposed metrics; however, reasoning ability tends to hinder rather than help.", "conclusion": "Despite improvements with larger models, the understanding of non-cooperative discourse remains limited, urging further research in the area.", "key_contributions": ["Introduction of CoBRA for analyzing non-cooperative discourse", "Development of three interpretable metrics for discourse moves", "Creation of CHARM dataset for courtroom exchanges"], "limitations": "Model reasoning ability complicates performance, revealing that larger models may introduce confusion rather than clarity in understanding strategic language.", "keywords": ["non-cooperative discourse", "human language technology", "large language models", "courtroom discourse", "pragmatics"], "importance_score": 8, "read_time_minutes": 18}}
{"id": "2506.01197", "pdf": "https://arxiv.org/pdf/2506.01197.pdf", "abs": "https://arxiv.org/abs/2506.01197", "title": "Incorporating Hierarchical Semantics in Sparse Autoencoder Architectures", "authors": ["Mark Muchane", "Sean Richardson", "Kiho Park", "Victor Veitch"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at\n  https://github.com/muchanem/hierarchical-sparse-autoencoders", "summary": "Sparse dictionary learning (and, in particular, sparse autoencoders) attempts\nto learn a set of human-understandable concepts that can explain variation on\nan abstract space. A basic limitation of this approach is that it neither\nexploits nor represents the semantic relationships between the learned\nconcepts. In this paper, we introduce a modified SAE architecture that\nexplicitly models a semantic hierarchy of concepts. Application of this\narchitecture to the internal representations of large language models shows\nboth that semantic hierarchy can be learned, and that doing so improves both\nreconstruction and interpretability. Additionally, the architecture leads to\nsignificant improvements in computational efficiency.", "AI": {"tldr": "This paper introduces a modified Sparse Autoencoder (SAE) architecture that models a semantic hierarchy of concepts, enhancing reconstruction, interpretability, and computational efficiency in large language models.", "motivation": "To address the limitation of traditional sparse dictionary learning approaches that fail to exploit or represent semantic relationships among learned concepts.", "method": "The authors present a modified Sparse Autoencoder architecture that incorporates a semantic hierarchy into the concept learning process.", "result": "The proposed architecture enables the learning of semantic hierarchies and shows improvements in reconstruction quality and interpretability in large language models, along with increased computational efficiency.", "conclusion": "The introduction of semantic hierarchy in sparse autoencoders leads to better performance in terms of reconstruction and understanding of internal representations in language models.", "key_contributions": ["Introduction of semantic hierarchy in sparse autoencoders", "Improved reconstruction and interpretability of learned concepts", "Enhanced computational efficiency in large language models"], "limitations": "", "keywords": ["Sparse Autoencoders", "Semantic Hierarchy", "Large Language Models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.01205", "pdf": "https://arxiv.org/pdf/2506.01205.pdf", "abs": "https://arxiv.org/abs/2506.01205", "title": "Trick or Neat: Adversarial Ambiguity and Language Model Evaluation", "authors": ["Antonia Karamolegkou", "Oliver Eberle", "Phillip Rust", "Carina Kauf", "Anders Søgaard"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting ambiguity is important for language understanding, including\nuncertainty estimation, humour detection, and processing garden path sentences.\nWe assess language models' sensitivity to ambiguity by introducing an\nadversarial ambiguity dataset that includes syntactic, lexical, and\nphonological ambiguities along with adversarial variations (e.g., word-order\nchanges, synonym replacements, and random-based alterations). Our findings show\nthat direct prompting fails to robustly identify ambiguity, while linear probes\ntrained on model representations can decode ambiguity with high accuracy,\nsometimes exceeding 90\\%. Our results offer insights into the prompting\nparadigm and how language models encode ambiguity at different layers. We\nrelease both our code and data: https://github.com/coastalcph/lm_ambiguity.", "AI": {"tldr": "This paper introduces an adversarial ambiguity dataset to evaluate language models' sensitivity to ambiguity and finds that linear probes are more effective than direct prompts in identifying ambiguity.", "motivation": "The study aims to better understand language models' handling of ambiguity, which is crucial for tasks such as uncertainty estimation and comprehension.", "method": "An adversarial ambiguity dataset was created, incorporating various forms of ambiguity. The performance of language models was assessed using both direct prompting and linear probes based on model representations.", "result": "Linear probes trained on model representations can decode ambiguity with high accuracy, achieving results sometimes exceeding 90%. Meanwhile, direct prompting was less effective in identifying ambiguity.", "conclusion": "The research provides insights into the effectiveness of different approaches in identifying ambiguity in language models and suggests a need for better prompting strategies.", "key_contributions": ["Introduction of an adversarial ambiguity dataset.", "Demonstration of the limitations of direct prompting for ambiguity detection.", "High accuracy of linear probes in decoding ambiguity."], "limitations": "The study does not explore the effect of different model architectures on ambiguity sensitivity.", "keywords": ["ambiguity detection", "language models", "linear probes", "adversarial dataset", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01206", "pdf": "https://arxiv.org/pdf/2506.01206.pdf", "abs": "https://arxiv.org/abs/2506.01206", "title": "Mamba Drafters for Speculative Decoding", "authors": ["Daewon Choi", "Seunghyuk Oh", "Saket Dingliwal", "Jihoon Tack", "Kyuyoung Kim", "Woomin Song", "Seojin Kim", "Insu Han", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding has emerged as a promising approach to accelerating\nlarge language model (LLM) generation using a fast drafter while maintaining\nalignment with the target model's distribution. However, existing approaches\nface a trade-off: external drafters offer flexibility but can suffer from\nslower drafting, while self-speculation methods use drafters tailored to the\ntarget model but require re-training. In this paper, we introduce novel\ndrafters based on Mamba, a state-of-the-art state space model (SSM), as a\nsolution that combines the best aspects of both approaches. By leveraging the\nlinear structure of SSMs, our approach avoids the quadratic complexity inherent\nin traditional Transformer-based methods, enabling faster drafting and lower\nmemory usage while maintaining the flexibility to work across different target\nmodels. We further enhance efficiency with a novel test-time tree search\nalgorithm for generating high-quality draft candidates. Our empirical\nevaluation demonstrates that Mamba-based drafters not only outperform existing\nexternal drafting methods but are also comparable to state-of-the-art\nself-speculation approaches while using less memory and maintaining their\ncross-model adaptability.", "AI": {"tldr": "This paper introduces Mamba-based drafters for large language model generation, combining speed, efficiency, and adaptability while reducing memory usage.", "motivation": "To address the trade-offs between external drafters' flexibility and self-speculation methods' efficiency in large language model generation.", "method": "The authors propose novel drafters based on Mamba, a state space model, enhancing drafting speed and reducing complexity through a test-time tree search algorithm.", "result": "Mamba-based drafters demonstrate improved performance over existing external drafting methods and are competitive with self-speculation approaches while using less memory.", "conclusion": "Mamba-based drafters effectively combine the benefits of flexibility and efficiency in large language model generation.", "key_contributions": ["Introduction of Mamba-based drafters for LLM generation", "Reduction of memory usage with linear structure of SSMs", "Novel test-time tree search algorithm for generating draft candidates"], "limitations": "", "keywords": ["large language models", "Mamba", "state space models", "drafting efficiency", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01215", "pdf": "https://arxiv.org/pdf/2506.01215.pdf", "abs": "https://arxiv.org/abs/2506.01215", "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers", "authors": ["Woomin Song", "Sai Muralidhar Jayanthi", "Srikanth Ronanki", "Kanthashree Mysore Sathyendra", "Jinwoo Shin", "Aram Galstyan", "Shubham Katiyar", "Sravan Babu Bodapati"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models increasingly gain popularity in real-world\napplications, processing extremely long contexts, often exceeding the model's\npre-trained context limits, has emerged as a critical challenge. While existing\napproaches to efficient long-context processing show promise, recurrent\ncompression-based methods struggle with information preservation, whereas\nrandom access approaches require substantial memory resources. We introduce\nREFORM, a novel inference framework that efficiently handles long contexts\nthrough a two-phase approach. First, it incrementally processes input chunks\nwhile maintaining a compressed KV cache, constructs cross-layer context\nembeddings, and utilizes early exit strategy for improved efficiency. Second,\nit identifies and gathers essential tokens via similarity matching and\nselectively recomputes the KV cache. Compared to baselines, REFORM achieves\nover 50% and 27% performance gains on RULER and BABILong respectively at 1M\ncontext length. It also outperforms baselines on Infinite-Bench and MM-NIAH,\ndemonstrating flexibility across diverse tasks and domains. Additionally,\nREFORM reduces inference time by 30% and peak memory usage by 5%, achieving\nboth efficiency and superior performance.", "AI": {"tldr": "REFORM is a novel inference framework designed to efficiently process long contexts in large language models, achieving significant performance and memory efficiency improvements.", "motivation": "The paper addresses the challenge of processing long contexts that exceed the pre-trained limits of large language models, highlighting the limitations of existing methods.", "method": "REFORM employs a two-phase approach: it first processes input chunks with a compressed KV cache and cross-layer context embeddings, and then gathers essential tokens via similarity matching to recompute the KV cache.", "result": "REFORM shows over 50% improvement on RULER and 27% on BABILong at 1M context length, along with strong performance on Infinite-Bench and MM-NIAH while reducing inference time by 30% and peak memory usage by 5%.", "conclusion": "The framework demonstrates the ability to handle long contexts efficiently while maintaining superior performance across various tasks and domains.", "key_contributions": ["Introduction of REFORM framework for long context processing", "Two-phase processing approach enhancing efficiency", "Significant performance and memory improvements over existing methods"], "limitations": "", "keywords": ["long-context processing", "large language models", "REFORM", "efficiency improvements", "inference framework"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01237", "pdf": "https://arxiv.org/pdf/2506.01237.pdf", "abs": "https://arxiv.org/abs/2506.01237", "title": "Polishing Every Facet of the GEM: Testing Linguistic Competence of LLMs and Humans in Korean", "authors": ["SungHo Kim", "Nayeon Kim", "Taehee Jeon", "SangKeun Lee"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 main conference", "summary": "We introduce the $\\underline{Ko}rean \\underline{G}rammar\n\\underline{E}valuation Bench\\underline{M}ark (KoGEM)$, designed to assess the\nlinguistic competence of LLMs and humans in Korean. KoGEM consists of 1.5k\nmultiple-choice QA pairs covering five main categories and 16 subcategories.\nThe zero-shot evaluation of 27 LLMs of various sizes and types reveals that\nwhile LLMs perform remarkably well on straightforward tasks requiring primarily\ndefinitional knowledge, they struggle with tasks that demand the integration of\nreal-world experiential knowledge, such as phonological rules and\npronunciation. Furthermore, our in-depth analysis suggests that incorporating\nsuch experiential knowledge could enhance the linguistic competence of LLMs.\nWith KoGEM, we not only highlight the limitations of current LLMs in linguistic\ncompetence but also uncover hidden facets of LLMs in linguistic competence,\npaving the way for enhancing comprehensive language understanding. Our code and\ndataset are available at: https://github.com/SungHo3268/KoGEM.", "AI": {"tldr": "The KoGEM benchmark assesses linguistic competence in Korean for LLMs and humans, revealing strengths in definitional tasks but weaknesses in experiential knowledge integration.", "motivation": "To evaluate and improve the linguistic competence of LLMs and understand their limitations in handling complex linguistic tasks in the Korean language.", "method": "Creation of a benchmark consisting of 1.5k multiple-choice QA pairs across five categories and 16 subcategories; evaluation of 27 LLMs under zero-shot conditions.", "result": "LLMs perform well on straightforward tasks but struggle with those requiring real-world knowledge integration, highlighting a gap in experiential linguistic competence.", "conclusion": "Incorporating experiential knowledge could enhance LLMs' linguistic capabilities and understanding.", "key_contributions": ["Introduction of the KoGEM benchmark for Korean linguistic evaluation", "Identification of LLMs' strengths and weaknesses in linguistic tasks", "Provision of code and dataset for future research use"], "limitations": "Focuses solely on Korean language and may not generalize to other languages or LLMs", "keywords": ["Korean grammar", "LLM evaluation", "linguistic competence", "experiential knowledge", "benchmarking"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.01241", "pdf": "https://arxiv.org/pdf/2506.01241.pdf", "abs": "https://arxiv.org/abs/2506.01241", "title": "ExpertLongBench: Benchmarking Language Models on Expert-Level Long-Form Generation Tasks with Structured Checklists", "authors": ["Jie Ruan", "Inderjeet Nair", "Shuyang Cao", "Amy Liu", "Sheza Munir", "Micah Pollens-Dempsey", "Tiffany Chiang", "Lucy Kates", "Nicholas David", "Sihan Chen", "Ruxin Yang", "Yuqian Yang", "Jasmine Gump", "Tessa Bialek", "Vivek Sankaran", "Margo Schlanger", "Lu Wang"], "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces ExpertLongBench, an expert-level benchmark containing\n11 tasks from 9 domains that reflect realistic expert workflows and\napplications. Beyond question answering, the application-driven tasks in\nExpertLongBench demand long-form outputs that can exceed 5,000 tokens and\nstrict adherence to domain-specific requirements. Notably, each task in\nExpertLongBench includes a rubric, designed or validated by domain experts, to\nspecify task requirements and guide output evaluation. Furthermore, we propose\nCLEAR, an evaluation framework that supports accurate evaluation of long-form\nmodel outputs in our benchmark. To achieve fine-grained, expert-aligned\nevaluation, CLEAR derives checklists from both model outputs and references by\nextracting information corresponding to items in the task-specific rubric.\nChecklist items for model outputs are then compared with corresponding items\nfor reference outputs to assess their correctness, enabling grounded\nevaluation. We benchmark 11 large language models (LLMs) and analyze components\nin CLEAR, showing that (1) existing LLMs, with the top performer achieving only\na 26.8% F1 score, require significant improvement for expert-level tasks; (2)\nmodels can generate content corresponding to the required aspects, though often\nnot accurately; and (3) accurate checklist extraction and comparison in CLEAR\ncan be achieved by open-weight models for more scalable and low-cost usage.", "AI": {"tldr": "This paper presents ExpertLongBench, an expert-level benchmark with 11 tasks demanding long-form outputs exceeding 5,000 tokens. It introduces the CLEAR evaluation framework for assessing model outputs against expert-defined rubrics.", "motivation": "To create a benchmark that reflects realistic expert workflows and applications, emphasizing long-form outputs and domain-specific requirements.", "method": "ExpertLongBench introduces tasks designed or validated by domain experts with detailed rubrics. The CLEAR framework includes checklists for evaluating the accuracy of long-form outputs from LLMs.", "result": "Analysis of 11 large language models reveals that the best performer scored only 26.8% F1, indicating the need for improvement in handling expert-level tasks. The study also found that while models can generate relevant content, accuracy remains a challenge.", "conclusion": "Existing LLMs require significant advancement for expert-level applications, and the CLEAR framework facilitates more effective and scalable evaluation of long-form outputs.", "key_contributions": ["Introduction of ExpertLongBench benchmark for expert tasks", "Development of CLEAR evaluation framework for long-form outputs", "Demonstrated the limitations of current LLMs in expert-level tasks"], "limitations": "", "keywords": ["ExpertLongBench", "evaluation framework", "long-form outputs", "large language models", "HCI"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01252", "pdf": "https://arxiv.org/pdf/2506.01252.pdf", "abs": "https://arxiv.org/abs/2506.01252", "title": "MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine", "authors": ["Shufeng Kong", "Xingru Yang", "Yuanyuan Wei", "Zijie Wang", "Hao Tang", "Jiuqi Qin", "Shuting Lan", "Yingheng Wang", "Junwen Bai", "Zhuangbin Chen", "Zibin Zheng", "Caihua Liu", "Hao Liang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Traditional Chinese Medicine (TCM) is a holistic medical system with\nmillennia of accumulated clinical experience, playing a vital role in global\nhealthcare-particularly across East Asia. However, the implicit reasoning,\ndiverse textual forms, and lack of standardization in TCM pose major challenges\nfor computational modeling and evaluation. Large Language Models (LLMs) have\ndemonstrated remarkable potential in processing natural language across diverse\ndomains, including general medicine. Yet, their systematic evaluation in the\nTCM domain remains underdeveloped. Existing benchmarks either focus narrowly on\nfactual question answering or lack domain-specific tasks and clinical realism.\nTo fill this gap, we introduce MTCMB-a Multi-Task Benchmark for Evaluating LLMs\non TCM Knowledge, Reasoning, and Safety. Developed in collaboration with\ncertified TCM experts, MTCMB comprises 12 sub-datasets spanning five major\ncategories: knowledge QA, language understanding, diagnostic reasoning,\nprescription generation, and safety evaluation. The benchmark integrates\nreal-world case records, national licensing exams, and classical texts,\nproviding an authentic and comprehensive testbed for TCM-capable models.\nPreliminary results indicate that current LLMs perform well on foundational\nknowledge but fall short in clinical reasoning, prescription planning, and\nsafety compliance. These findings highlight the urgent need for domain-aligned\nbenchmarks like MTCMB to guide the development of more competent and\ntrustworthy medical AI systems. All datasets, code, and evaluation tools are\npublicly available at: https://github.com/Wayyuanyuan/MTCMB.", "AI": {"tldr": "The paper introduces MTCMB, a Multi-Task Benchmark for evaluating Large Language Models in the context of Traditional Chinese Medicine, highlighting existing limitations in LLM performance regarding clinical reasoning and safety.", "motivation": "To address the challenges posed by the lack of standardization and evaluation in Traditional Chinese Medicine (TCM) for large language models (LLMs).", "method": "The authors developed MTCMB, which consists of 12 sub-datasets across five categories: knowledge QA, language understanding, diagnostic reasoning, prescription generation, and safety evaluation, in collaboration with certified TCM experts.", "result": "Preliminary results demonstrate that current LLMs perform well on foundational knowledge but struggle with clinical reasoning, prescription planning, and safety compliance, indicating significant areas for improvement.", "conclusion": "The establishment of domain-aligned benchmarks like MTCMB is crucial for developing competent and trustworthy medical AI systems in the TCM field.", "key_contributions": ["Introduction of MTCMB as a comprehensive benchmark for LLMs in TCM.", "Involvement of certified TCM experts in dataset development.", "Identification of performance gaps in LLMs regarding clinical reasoning and safety."], "limitations": "", "keywords": ["Traditional Chinese Medicine", "Large Language Models", "Benchmark", "Clinical Reasoning", "AI Safety"], "importance_score": 7, "read_time_minutes": 12}}
{"id": "2506.01253", "pdf": "https://arxiv.org/pdf/2506.01253.pdf", "abs": "https://arxiv.org/abs/2506.01253", "title": "CoRE: Condition-based Reasoning for Identifying Outcome Variance in Complex Events", "authors": ["Sai Vallurupalli", "Francis Ferraro"], "categories": ["cs.CL"], "comment": "Accepted to Findings of the Association for Computational Linguistics\n  2025", "summary": "Knowing which latent conditions lead to a particular outcome is useful for\ncritically examining claims made about complex event outcomes. Identifying\nimplied conditions and examining their influence on an outcome is challenging.\nWe handle this by combining and augmenting annotations from two existing\ndatasets consisting of goals and states, and explore the influence of\nconditions through our research questions and Condition-based Reasoning tasks.\nWe examine open and closed LLMs of varying sizes and intent-alignment on our\nreasoning tasks and find that conditions are useful when not all context is\navailable. Models differ widely in their ability to generate and identify\noutcome-variant conditions which affects their performance on outcome\nvalidation when conditions are used to replace missing context. Larger models\nlike GPT-4o, are more cautious in such less constrained situations.", "AI": {"tldr": "The paper investigates how different latent conditions influence outcomes in reasoning tasks using LLMs with varied capabilities.", "motivation": "Understanding the conditions leading to specific outcomes is critical in evaluating claims about complex events.", "method": "The authors combine and augment annotations from existing datasets, conducting experiments with open and closed LLMs of varying sizes and intent-alignment to explore reasoning tasks.", "result": "The research reveals that conditions significantly impact model performance in outcome validation, especially when context is incomplete, with larger models showing more caution in constrained scenarios.", "conclusion": "The study highlights the variability in model capabilities regarding condition identification and generation, emphasizing the importance of conditions in reasoning tasks.", "key_contributions": ["Introduced a framework for analyzing latent conditions in LLMs", "Provided insights into the performance differences among various models", "Demonstrated the significance of conditions for outcome validation in reasoning tasks"], "limitations": "", "keywords": ["latent conditions", "LLM", "reasoning tasks", "outcome validation", "intent-alignment"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01254", "pdf": "https://arxiv.org/pdf/2506.01254.pdf", "abs": "https://arxiv.org/abs/2506.01254", "title": "Memory-Efficient FastText: A Comprehensive Approach Using Double-Array Trie Structures and Mark-Compact Memory Management", "authors": ["Yimin Du"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "FastText has established itself as a fundamental algorithm for learning word\nrepresentations, demonstrating exceptional capability in handling\nout-of-vocabulary words through character-level n-gram embeddings. However, its\nhash-based bucketing mechanism introduces critical limitations for large-scale\nindustrial deployment: hash collisions cause semantic drift, and memory\nrequirements become prohibitively expensive when dealing with real-world\nvocabularies containing millions of terms. This paper presents a comprehensive\nmemory optimization framework that fundamentally reimagines FastText's memory\nmanagement through the integration of double-array trie (DA-trie) structures\nand mark-compact garbage collection principles. Our approach leverages the\nlinguistic insight that n-grams sharing common prefixes or suffixes exhibit\nhighly correlated embeddings due to co-occurrence patterns in natural language.\nBy systematically identifying and merging semantically similar embeddings based\non structural relationships, we achieve compression ratios of 4:1 to 10:1 while\nmaintaining near-perfect embedding quality. The algorithm consists of four\nsophisticated phases: prefix trie construction with embedding mapping,\nprefix-based similarity compression, suffix-based similarity compression, and\nmark-compact memory reorganization. Comprehensive experiments on a 30-million\nChinese vocabulary dataset demonstrate memory reduction from over 100GB to\napproximately 30GB with negligible performance degradation. Our industrial\ndeployment results show significant cost reduction, faster loading times, and\nimproved model reliability through the elimination of hash collision artifacts.\nCode and experimental implementations are available at:\nhttps://github.com/initial-d/me_fasttext", "AI": {"tldr": "This paper presents a memory optimization framework for FastText that enhances its efficiency for large-scale industrial use by utilizing double-array trie structures and garbage collection principles, leading to significant memory reduction while preserving embedding quality.", "motivation": "FastText suffers from hash collisions causing semantic drift and high memory costs when handling large vocabularies, prompting the need for memory optimization.", "method": "The proposed framework integrates double-array trie structures and mark-compact garbage collection principles to manage memory efficiently, involving phases like trie construction, prefix-based and suffix-based similarity compression, and memory reorganization.", "result": "The method achieved compression ratios of 4:1 to 10:1, reducing memory usage from over 100GB to approximately 30GB on a 30-million Chinese vocabulary dataset with negligible performance loss.", "conclusion": "The framework significantly enhances FastText's deployment capabilities in industrial settings, offering cost reductions and faster model loading times by eliminating hash collision issues.", "key_contributions": ["Introducing a memory optimization framework for FastText", "Achieving high compression ratios while maintaining embedding quality", "Demonstrating substantial real-world deployment benefits, including cost savings and improved reliability."], "limitations": "", "keywords": ["FastText", "memory optimization", "double-array trie", "garbage collection", "word embeddings"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01257", "pdf": "https://arxiv.org/pdf/2506.01257.pdf", "abs": "https://arxiv.org/abs/2506.01257", "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models", "authors": ["Jiancheng Ye", "Sophie Bronstein", "Jiarui Hai", "Malak Abu Hashish"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM)\ndeveloped by DeepSeek, showcasing advanced reasoning capabilities through a\nhybrid architecture that integrates mixture of experts (MoE), chain of thought\n(CoT) reasoning, and reinforcement learning. Released under the permissive MIT\nlicense, DeepSeek-R1 offers a transparent and cost-effective alternative to\nproprietary models like GPT-4o and Claude-3 Opus; it excels in structured\nproblem-solving domains such as mathematics, healthcare diagnostics, code\ngeneration, and pharmaceutical research. The model demonstrates competitive\nperformance on benchmarks like the United States Medical Licensing Examination\n(USMLE) and American Invitational Mathematics Examination (AIME), with strong\nresults in pediatric and ophthalmologic clinical decision support tasks. Its\narchitecture enables efficient inference while preserving reasoning depth,\nmaking it suitable for deployment in resource-constrained settings. However,\nDeepSeek-R1 also exhibits increased vulnerability to bias, misinformation,\nadversarial manipulation, and safety failures - especially in multilingual and\nethically sensitive contexts. This survey highlights the model's strengths,\nincluding interpretability, scalability, and adaptability, alongside its\nlimitations in general language fluency and safety alignment. Future research\npriorities include improving bias mitigation, natural language comprehension,\ndomain-specific validation, and regulatory compliance. Overall, DeepSeek-R1\nrepresents a major advance in open, scalable AI, underscoring the need for\ncollaborative governance to ensure responsible and equitable deployment.", "AI": {"tldr": "DeepSeek-R1 is an open-source large language model that integrates advanced reasoning capabilities and excels in structured problem-solving in fields like healthcare and code generation.", "motivation": "To provide a cost-effective and transparent alternative to proprietary large language models with advanced reasoning capabilities, particularly for structured problems in various domains.", "method": "Hybrid architecture featuring mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning for enhanced reasoning and efficiency.", "result": "Achieves competitive performance on benchmarks like the USMLE and AIME, showing effectiveness in clinical decision support and structured problem-solving.", "conclusion": "While DeepSeek-R1 showcases significant advancements in open, scalable AI, it also faces challenges such as biases and safety issues, highlighting the need for future improvements and collaborative governance.", "key_contributions": ["Open-source accessibility under the MIT license.", "Integrates MoE and CoT for enhanced reasoning capabilities.", "Demonstrates effective application in healthcare diagnostics and structured problem-solving."], "limitations": "Increased vulnerability to bias, misinformation, and safety failures, especially in multilingual and ethical contexts.", "keywords": ["large language model", "healthcare diagnostics", "machine learning", "bias mitigation", "open-source AI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01262", "pdf": "https://arxiv.org/pdf/2506.01262.pdf", "abs": "https://arxiv.org/abs/2506.01262", "title": "Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis", "authors": ["Jisoo Mok", "Ik-hwan Kim", "Sangkwon Park", "Sungroh Yoon"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Personalized AI assistants, a hallmark of the human-like capabilities of\nLarge Language Models (LLMs), are a challenging application that intertwines\nmultiple problems in LLM research. Despite the growing interest in the\ndevelopment of personalized assistants, the lack of an open-source\nconversational dataset tailored for personalization remains a significant\nobstacle for researchers in the field. To address this research gap, we\nintroduce HiCUPID, a new benchmark to probe and unleash the potential of LLMs\nto deliver personalized responses. Alongside a conversational dataset, HiCUPID\nprovides a Llama-3.2-based automated evaluation model whose assessment closely\nmirrors human preferences. We release our dataset, evaluation model, and code\nat https://github.com/12kimih/HiCUPID.", "AI": {"tldr": "HiCUPID is a new benchmark developed to enhance personalized responses from LLMs, featuring an open-source conversational dataset and an evaluation model based on Llama-3.2.", "motivation": "To fill the gap in open-source datasets for developing personalized AI assistants using LLMs.", "method": "Introduction of HiCUPID, a conversational dataset for personalization, along with a Llama-3.2-based automated evaluation model that aligns with human preferences.", "result": "HiCUPID facilitates the evaluation of personalized responses from LLMs and provides an important resource for researchers.", "conclusion": "The new benchmark enables further research into personalized AI assistants and provides necessary tools for evaluation.", "key_contributions": ["Introduction of HiCUPID conversational dataset for personalized AI assistants", "Development of an automated evaluation model based on Llama-3.2", "Open-source release for community use"], "limitations": "", "keywords": ["Personalized AI Assistants", "Large Language Models", "Conversational Dataset"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01263", "pdf": "https://arxiv.org/pdf/2506.01263.pdf", "abs": "https://arxiv.org/abs/2506.01263", "title": "WCTC-Biasing: Retraining-free Contextual Biasing ASR with Wildcard CTC-based Keyword Spotting and Inter-layer Biasing", "authors": ["Yu Nakagome", "Michael Hentschel"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Despite recent advances in end-to-end speech recognition methods, the output\ntends to be biased to the training data's vocabulary, resulting in inaccurate\nrecognition of proper nouns and other unknown terms. To address this issue, we\npropose a method to improve recognition accuracy of such rare words in\nCTC-based models without additional training or text-to-speech systems.\nSpecifically, keyword spotting is performed using acoustic features of\nintermediate layers during inference, and a bias is applied to the subsequent\nlayers of the acoustic model for detected keywords. For keyword detection, we\nadopt a wildcard CTC that is both fast and tolerant of ambiguous matches,\nallowing flexible handling of words that are difficult to match strictly. Since\nthis method does not require retraining of existing models, it can be easily\napplied to even large-scale models. In experiments on Japanese speech\nrecognition, the proposed method achieved a 29% improvement in the F1 score for\nunknown words.", "AI": {"tldr": "Proposes a method to enhance accuracy in speech recognition for rare words without retraining models.", "motivation": "To improve recognition accuracy of proper nouns and unknown terms in speech recognition systems, which are often biased towards training vocabulary.", "method": "Utilizes keyword spotting with acoustic features from intermediate layers during inference and applies biases to the acoustic model for recognized keywords. Employs a wildcard CTC for flexible matching.", "result": "Achieved a 29% improvement in F1 score for unknown words in Japanese speech recognition experiments.", "conclusion": "The proposed method allows for better recognition of difficult words without the need for retraining existing models, making it applicable to large-scale systems.", "key_contributions": ["Improves speech recognition for rare words without additional training.", "Adopts a wildcard CTC that allows flexible word matching.", "Demonstrates significant performance improvement in experiments."], "limitations": "", "keywords": ["speech recognition", "keyword spotting", "CTC", "wildcard", "unknown words"], "importance_score": 4, "read_time_minutes": 8}}
{"id": "2506.01265", "pdf": "https://arxiv.org/pdf/2506.01265.pdf", "abs": "https://arxiv.org/abs/2506.01265", "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large Language Models via Task-Inherent Attribute Guidelines", "authors": ["Do Xuan Long", "Duong Ngoc Yen", "Do Xuan Trong", "Luu Anh Tuan", "Kenji Kawaguchi", "Shafiq Joty", "Min-Yen Kan", "Nancy F. Chen"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "In-context learning (ICL) is an important yet not fully understood ability of\npre-trained large language models (LLMs). It can greatly enhance task\nperformance using a few examples, termed demonstrations, without fine-tuning.\nAlthough effective in question answering, ICL often underperforms in long-form\ngeneration tasks such as summarization. Under appropriately realistic\nassumptions, we empirically and theoretically show that ICL demonstrations\nalone are insufficient to teach LLMs the task language and format distributions\nfor generation. We argue for explicit exposure to the task distributions and\nhypothesize that defining them by prompting enhances model performance. To this\nend, we present LongGuide, which efficiently generates two parallel streams of\nguidelines capturing task language and format properties: (i) Metric Guidelines\n(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output\nConstraint Guidelines (OCGs) that constrain generation at both token and\nsentence levels. LongGuide automatically selects the best combination of\nguidelines, improving both strong open- and closed-source LLMs by over 5% in\nboth zero- and few-shot settings. We show that LongGuide is generalizable,\nlearnable by weak models to enhance strong ones, and integrates synergistically\nwith automatic prompt optimizers.", "AI": {"tldr": "LongGuide is a method for improving long-form generation in LLMs by generating two types of guidelines that enhance task performance through better prompt design.", "motivation": "To address the underperformance of in-context learning (ICL) in long-form generation tasks by providing explicit task distributions through guidelines.", "method": "LongGuide generates Metric Guidelines (MGs) to optimize self-evaluated metrics and Output Constraint Guidelines (OCGs) to constrain generation at the token and sentence levels. It automatically selects the best combination of these guidelines for enhancing generation performance.", "result": "LongGuide improves performance by over 5% in both zero- and few-shot settings across strong open- and closed-source LLMs.", "conclusion": "The findings suggest that explicit exposure to task distributions through LongGuide significantly enhances the ability of LLMs for long-form generation tasks.", "key_contributions": ["Introduction of LongGuide as a dual-stream guideline generation method", "Demonstration of significant performance improvement in long-form generation tasks", "Generalizability and learnability of LongGuide across different models"], "limitations": "", "keywords": ["In-context learning", "Large language models", "Long-form generation", "Prompting", "Guidelines"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.01266", "pdf": "https://arxiv.org/pdf/2506.01266.pdf", "abs": "https://arxiv.org/abs/2506.01266", "title": "Detoxification of Large Language Models through Output-layer Fusion with a Calibration Model", "authors": ["Yuanhe Tian", "Mingjie Deng", "Guoqing Jin", "Yan Song"], "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 1 figure", "summary": "Existing approaches for Large language model (LLM) detoxification generally\nrely on training on large-scale non-toxic or human-annotated preference data,\ndesigning prompts to instruct the LLM to generate safe content, or modifying\nthe model parameters to remove toxic information, which are computationally\nexpensive, lack robustness, and often compromise LLMs' fluency and contextual\nunderstanding. In this paper, we propose a simple yet effective approach for\nLLM detoxification, which leverages a compact, pre-trained calibration model\nthat guides the detoxification process of a target LLM via a lightweight\nintervention in its generation pipeline. By learning a detoxified embedding\nspace from non-toxic data, the calibration model effectively steers the LLM\naway from generating harmful content. This approach only requires a one-time\ntraining of the calibration model that is able to be seamlessly applied to\nmultiple LLMs without compromising fluency or contextual understanding.\nExperiment results on the benchmark dataset demonstrate that our approach\nreduces toxicity while maintaining reasonable content expression.", "AI": {"tldr": "This paper proposes a novel approach for detoxifying large language models (LLMs) using a pre-trained calibration model, which guides the detoxification process effectively without significant computational costs.", "motivation": "Existing LLM detoxification methods are often computationally expensive and compromise the fluency of the models, motivating the need for a more efficient approach.", "method": "The authors introduce a compact, pre-trained calibration model that provides lightweight interventions in the generation pipeline of a target LLM, steering it away from generating harmful content by learning from non-toxic data.", "result": "Experiment results indicate that the proposed approach effectively reduces toxicity in LLM outputs while maintaining the models' fluency and contextual understanding, as evaluated on benchmark datasets.", "conclusion": "The calibration model allows for a one-time training to be applied across multiple LLMs, thus enhancing their safety without sacrificing performance.", "key_contributions": ["Introduction of a calibration model for LLM detoxification", "Demonstration of lightweight intervention in LLM generation", "Showcasing reduced toxicity with maintained fluency in outputs"], "limitations": "", "keywords": ["Large Language Models", "Detoxification", "Calibration Model", "Toxicity Reduction", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01276", "pdf": "https://arxiv.org/pdf/2506.01276.pdf", "abs": "https://arxiv.org/abs/2506.01276", "title": "Schema as Parameterized Tools for Universal Information Extraction", "authors": ["Sheng Liang", "Yongyue Zhang", "Yaxiong Wu", "Ruiming Tang", "Yong Liu"], "categories": ["cs.CL", "I.2.7"], "comment": "12 pages, 7 figures, 5 tables", "summary": "Universal information extraction (UIE) primarily employs an extractive\ngeneration approach with large language models (LLMs), typically outputting\nstructured information based on predefined schemas such as JSON or tables. UIE\nsuffers from a lack of adaptability when selecting between predefined schemas\nand on-the-fly schema generation within the in-context learning paradigm,\nespecially when there are numerous schemas to choose from. In this paper, we\npropose a unified adaptive text-to-structure generation framework, called\nSchema as Parameterized Tools (SPT), which reimagines the tool-calling\ncapability of LLMs by treating predefined schemas as parameterized tools for\ntool selection and parameter filling. Specifically, our SPT method can be\napplied to unify closed, open, and on-demand IE tasks by adopting Schema\nRetrieval by fetching the relevant schemas from a predefined pool, Schema\nFilling by extracting information and filling slots as with tool parameters, or\nSchema Generation by synthesizing new schemas with uncovered cases. Experiments\nshow that the SPT method can handle four distinct IE tasks adaptively,\ndelivering robust schema retrieval and selection performance. SPT also achieves\ncomparable extraction performance to LoRA baselines and current leading UIE\nsystems with significantly fewer trainable parameters.", "AI": {"tldr": "This paper introduces Schema as Parameterized Tools (SPT), a framework for adaptive text-to-structure generation in universal information extraction (UIE) using LLMs.", "motivation": "There is a lack of adaptability in existing UIE systems when selecting between predefined schemas and generating new schemas dynamically, which can hinder their performance.", "method": "The paper proposes a unified framework, SPT, that treats predefined schemas as parameterized tools, allowing for schema retrieval, filling, and generation to improve adaptability in UIE tasks.", "result": "SPT shows robust performance in handling four distinct IE tasks and achieves comparable extraction results to leading systems while using significantly fewer parameters.", "conclusion": "The SPT framework enhances the adaptability of UIE systems, improving their ability to select and generate schemas on-the-fly, thus addressing current limitations in the field.", "key_contributions": ["Introduction of Schema as Parameterized Tools (SPT) framework for UIE", "Unified approach to schema retrieval, filling, and generation", "Demonstrated robust performance across multiple IE tasks with fewer parameters."], "limitations": "", "keywords": ["Universal Information Extraction", "Large Language Models", "Adaptive Framework"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.01305", "pdf": "https://arxiv.org/pdf/2506.01305.pdf", "abs": "https://arxiv.org/abs/2506.01305", "title": "VM14K: First Vietnamese Medical Benchmark", "authors": ["Thong Nguyen", "Duc Nguyen", "Minh Dang", "Thai Dao", "Long Nguyen", "Quan H. Nguyen", "Dat Nguyen", "Kien Tran", "Minh Tran"], "categories": ["cs.CL"], "comment": null, "summary": "Medical benchmarks are indispensable for evaluating the capabilities of\nlanguage models in healthcare for non-English-speaking communities,therefore\nhelp ensuring the quality of real-life applications. However, not every\ncommunity has sufficient resources and standardized methods to effectively\nbuild and design such benchmark, and available non-English medical data is\nnormally fragmented and difficult to verify. We developed an approach to tackle\nthis problem and applied it to create the first Vietnamese medical question\nbenchmark, featuring 14,000 multiple-choice questions across 34 medical\nspecialties. Our benchmark was constructed using various verifiable sources,\nincluding carefully curated medical exams and clinical records, and eventually\nannotated by medical experts. The benchmark includes four difficulty levels,\nranging from foundational biological knowledge commonly found in textbooks to\ntypical clinical case studies that require advanced reasoning. This design\nenables assessment of both the breadth and depth of language models' medical\nunderstanding in the target language thanks to its extensive coverage and\nin-depth subject-specific expertise. We release the benchmark in three parts: a\nsample public set (4k questions), a full public set (10k questions), and a\nprivate set (2k questions) used for leaderboard evaluation. Each set contains\nall medical subfields and difficulty levels. Our approach is scalable to other\nlanguages, and we open-source our data construction pipeline to support the\ndevelopment of future multilingual benchmarks in the medical domain.", "AI": {"tldr": "Development of the first Vietnamese medical question benchmark for evaluating language models in healthcare, consisting of 14,000 multiple-choice questions across 34 specialties.", "motivation": "To address the lack of standardized medical benchmarks for non-English-speaking communities and improve the evaluation of language models in healthcare.", "method": "Created a comprehensive Vietnamese medical question benchmark using carefully curated data from medical exams and clinical records, annotated by medical experts, organized into four difficulty levels.", "result": "A total of 14,000 questions were developed, providing extensive coverage across various medical specialties, which can assess language models' understanding of medical knowledge in Vietnamese.", "conclusion": "The benchmark has implications for enhancing multilingual language model evaluation in healthcare, and the open-source data construction pipeline enables future developments.", "key_contributions": ["First Vietnamese medical question benchmark with 14,000 questions.", "Incorporation of four difficulty levels for comprehensive assessment.", "Open-sourcing the data construction pipeline for future multilingual benchmarks."], "limitations": "", "keywords": ["medical benchmarks", "language models", "Vietnamese", "healthcare", "multilingual"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01308", "pdf": "https://arxiv.org/pdf/2506.01308.pdf", "abs": "https://arxiv.org/abs/2506.01308", "title": "A Platform for Investigating Public Health Content with Efficient Concern Classification", "authors": ["Christopher Li", "Rickard Stureborg", "Bhuwan Dhingra", "Jun Yang"], "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "19 pages, 15 figures", "summary": "A recent rise in online content expressing concerns with public health\ninitiatives has contributed to already stalled uptake of preemptive measures\nglobally. Future public health efforts must attempt to understand such content,\nwhat concerns it may raise among readers, and how to effectively respond to it.\nTo this end, we present ConcernScope, a platform that uses a teacher-student\nframework for knowledge transfer between large language models and light-weight\nclassifiers to quickly and effectively identify the health concerns raised in a\ntext corpus. The platform allows uploading massive files directly,\nautomatically scraping specific URLs, and direct text editing. ConcernScope is\nbuilt on top of a taxonomy of public health concerns. Intended for public\nhealth officials, we demonstrate several applications of this platform: guided\ndata exploration to find useful examples of common concerns found in online\ncommunity datasets, identification of trends in concerns through an example\ntime series analysis of 186,000 samples, and finding trends in topic frequency\nbefore and after significant events.", "AI": {"tldr": "ConcernScope is a platform for identifying health concerns in text using a teacher-student model for efficient knowledge transfer.", "motivation": "Addressing stalled uptake of public health measures due to online content expressing concerns.", "method": "Utilizes a teacher-student framework between large language models and lightweight classifiers for rapid concern identification.", "result": "Demonstrated applications include guided data exploration, trend identification in community health concerns, and analysis of topic frequency around significant events.", "conclusion": "ConcernScope aids public health officials in understanding and responding to online health concerns effectively.", "key_contributions": ["Development of the ConcernScope platform for health concern identification.", "Integration of a teacher-student model for knowledge transfer in health informatics.", "Application in trend analysis of public health concerns."], "limitations": "", "keywords": ["public health", "machine learning", "concern identification", "data exploration", "language models"], "importance_score": 8, "read_time_minutes": 19}}
{"id": "2506.01312", "pdf": "https://arxiv.org/pdf/2506.01312.pdf", "abs": "https://arxiv.org/abs/2506.01312", "title": "Growing Through Experience: Scaling Episodic Grounding in Language Models", "authors": ["Chunhui Zhang", "Sirui", "Wang", "Zhongyu Ouyang", "Xiangchi Yuan", "Soroush Vosoughi"], "categories": ["cs.CL"], "comment": "Accepted at The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Language models (LMs) require robust episodic grounding-the capacity to learn\nfrom and apply past experiences-to excel at physical planning tasks. Current\nepisodic grounding approaches struggle with scalability and integration,\nlimiting their effectiveness, especially for medium-sized LMs (7B parameters).\nWhile larger LMs (70-405B parameters) possess superior hierarchical\nrepresentations and extensive pre-trained knowledge, they encounter a\nfundamental scale paradox: despite their advanced abstraction capabilities,\nthey lack efficient mechanisms to leverage experience streams. We propose a\nscalable weak-to-strong episodic learning framework that effectively transfers\nepisodic behaviors from smaller to larger LMs. This framework integrates Monte\nCarlo tree search for structured experience collection with a novel\ndistillation method, preserving the inherent LM capabilities while embedding\nepisodic memory. Experiments demonstrate our method surpasses state-of-the-art\nproprietary LMs by 3.45% across diverse planning and question-answering tasks.\nLayer-wise probing further indicates significant improvements in task\nalignment, especially within deeper LM layers, highlighting stable\ngeneralization even for previously unseen scenarios with increased planning\ncomplexity-conditions where baseline methods degrade markedly.", "AI": {"tldr": "This paper proposes a scalable episodic learning framework that enhances medium-sized language models (LMs) by effectively transferring episodic behaviors from smaller LMs, integrating Monte Carlo tree search and a novel distillation method to improve physical planning tasks.", "motivation": "The motivation behind this research is to address the scalability and integration challenges of current episodic grounding approaches in language models, particularly for medium-sized models that struggle to leverage experience efficiently.", "method": "The authors propose a weak-to-strong episodic learning framework that combines Monte Carlo tree search for structured experience collection with a new distillation method. This approach transfers episodic behaviors from smaller to larger LMs, enhancing their planning capabilities.", "result": "Experiments show that the new framework surpasses state-of-the-art proprietary language models by 3.45% in various planning and question-answering tasks, indicating significant performance improvement.", "conclusion": "The proposed method achieves better task alignment and generalization in larger language models, particularly in deeper layers that handle complex planning scenarios, highlighting its potential to enhance the capabilities of LMs through episodic memory integration.", "key_contributions": ["Scalable framework for episodic learning in LMs", "Integration of Monte Carlo tree search for experience collection", "Improvement in task performance and generalization in planning and QA tasks"], "limitations": "", "keywords": ["language models", "episodic grounding", "scalable learning", "Monte Carlo tree search", "human-computer interaction"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01322", "pdf": "https://arxiv.org/pdf/2506.01322.pdf", "abs": "https://arxiv.org/abs/2506.01322", "title": "Zero-Shot Text-to-Speech for Vietnamese", "authors": ["Thi Vu", "Linh The Nguyen", "Dat Quoc Nguyen"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "To appear in Proceedings of ACL 2025 (Main conference paper)", "summary": "This paper introduces PhoAudiobook, a newly curated dataset comprising 941\nhours of high-quality audio for Vietnamese text-to-speech. Using PhoAudiobook,\nwe conduct experiments on three leading zero-shot TTS models: VALL-E,\nVoiceCraft, and XTTS-V2. Our findings demonstrate that PhoAudiobook\nconsistently enhances model performance across various metrics. Moreover,\nVALL-E and VoiceCraft exhibit superior performance in synthesizing short\nsentences, highlighting their robustness in handling diverse linguistic\ncontexts. We publicly release PhoAudiobook to facilitate further research and\ndevelopment in Vietnamese text-to-speech.", "AI": {"tldr": "Introduction of PhoAudiobook, a dataset for Vietnamese text-to-speech, improving performance of TTS models.", "motivation": "To create a high-quality Vietnamese text-to-speech dataset that enhances the performance of zero-shot TTS models.", "method": "Experiments were conducted using PhoAudiobook with three leading zero-shot TTS models: VALL-E, VoiceCraft, and XTTS-V2.", "result": "PhoAudiobook significantly enhances model performance across various metrics, especially for short sentence synthesis.", "conclusion": "The paper contributes to the field by publicly releasing PhoAudiobook for future research in Vietnamese TTS.", "key_contributions": ["Introduction of PhoAudiobook dataset with 941 hours of audio", "Demonstrated improvement in TTS model performance", "Public release of the dataset for research purposes"], "limitations": "", "keywords": ["text-to-speech", "Vietnamese", "dataset", "TTS models", "zero-shot"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.01329", "pdf": "https://arxiv.org/pdf/2506.01329.pdf", "abs": "https://arxiv.org/abs/2506.01329", "title": "Evaluating Large Language Models in Crisis Detection: A Real-World Benchmark from Psychological Support Hotlines", "authors": ["Guifeng Deng", "Shuyin Rao", "Tianyu Lin", "Anlu Dai", "Pan Wang", "Junyi Xie", "Haidong Song", "Ke Zhao", "Dongwu Xu", "Zhengdong Cheng", "Tao Li", "Haiteng Jiang"], "categories": ["cs.CL", "cs.AI"], "comment": "30 pages, 8 figures", "summary": "Psychological support hotlines are critical for crisis intervention but face\nsignificant challenges due to rising demand. Large language models (LLMs) could\nsupport crisis assessments, yet their capabilities in emotionally sensitive\ncontexts remain unclear. We introduce PsyCrisisBench, a benchmark of 540\nannotated transcripts from the Hangzhou Psychological Assistance Hotline,\nassessing four tasks: mood status recognition, suicidal ideation detection,\nsuicide plan identification, and risk assessment. We evaluated 64 LLMs across\n15 families (e.g., GPT, Claude, Gemini, Llama, Qwen, DeepSeek) using zero-shot,\nfew-shot, and fine-tuning paradigms. Performance was measured by F1-score, with\nstatistical comparisons via Welch's t-tests. LLMs performed strongly on\nsuicidal ideation detection (F1=0.880), suicide plan identification (F1=0.779),\nand risk assessment (F1=0.907), improved with few-shot and fine-tuning. Mood\nstatus recognition was more challenging (max F1=0.709), likely due to lost\nvocal cues and ambiguity. A fine-tuned 1.5B-parameter model (Qwen2.5-1.5B)\nsurpassed larger models on mood and suicidal ideation. Open-source models like\nQwQ-32B performed comparably to closed-source on most tasks (p>0.3), though\nclosed models retained an edge in mood detection (p=0.007). Performance scaled\nwith size up to a point; quantization (AWQ) reduced GPU memory by 70% with\nminimal F1 degradation. LLMs show substantial promise in structured\npsychological crisis assessments, especially with fine-tuning. Mood recognition\nremains limited due to contextual complexity. The narrowing gap between open-\nand closed-source models, combined with efficient quantization, suggests\nfeasible integration. PsyCrisisBench offers a robust evaluation framework to\nguide model development and ethical deployment in mental health.", "AI": {"tldr": "PsyCrisisBench benchmarks LLMs for assessing psychological crises, highlighting their effectiveness in crisis support tasks through various evaluation methods.", "motivation": "To address the challenges of psychological support hotlines amidst rising demand by evaluating LLMs for crisis assessment tasks.", "method": "Introduced PsyCrisisBench, a benchmark of 540 annotated transcripts from the Hangzhou Psychological Assistance Hotline, evaluating LLMs on tasks including mood status recognition and risk assessment via zero-shot and fine-tuning approaches.", "result": "LLMs showed strong performance in suicidal ideation detection (F1=0.880), risk assessment (F1=0.907), and improved outcomes with few-shot learning; mood recognition had a max F1 of 0.709.", "conclusion": "LLMs hold significant promise for structured psychological crisis assessments, though mood recognition needs improvement; the paper provides a framework for ethical model deployment in mental health.", "key_contributions": ["Introduction of PsyCrisisBench benchmark for mental health LLM evaluation", "Demonstration of LLM effectiveness in various crisis assessment tasks", "Discussion of the performance gap between open-source and closed-source models"], "limitations": "Mood recognition faced challenges due to lost vocal cues and contextual ambiguity.", "keywords": ["Large Language Models", "Crisis Intervention", "Mental Health", "Benchmarking", "Psychological Assessment"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2506.01334", "pdf": "https://arxiv.org/pdf/2506.01334.pdf", "abs": "https://arxiv.org/abs/2506.01334", "title": "Enhancing Interpretable Image Classification Through LLM Agents and Conditional Concept Bottleneck Models", "authors": ["Yiwen Jiang", "Deval Mehta", "Wei Feng", "Zongyuan Ge"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (Main)", "summary": "Concept Bottleneck Models (CBMs) decompose image classification into a\nprocess governed by interpretable, human-readable concepts. Recent advances in\nCBMs have used Large Language Models (LLMs) to generate candidate concepts.\nHowever, a critical question remains: What is the optimal number of concepts to\nuse? Current concept banks suffer from redundancy or insufficient coverage. To\naddress this issue, we introduce a dynamic, agent-based approach that adjusts\nthe concept bank in response to environmental feedback, optimizing the number\nof concepts for sufficiency yet concise coverage. Moreover, we propose\nConditional Concept Bottleneck Models (CoCoBMs) to overcome the limitations in\ntraditional CBMs' concept scoring mechanisms. It enhances the accuracy of\nassessing each concept's contribution to classification tasks and feature an\neditable matrix that allows LLMs to correct concept scores that conflict with\ntheir internal knowledge. Our evaluations across 6 datasets show that our\nmethod not only improves classification accuracy by 6% but also enhances\ninterpretability assessments by 30%.", "AI": {"tldr": "This paper presents Conditional Concept Bottleneck Models (CoCoBMs) which optimize the number of concepts in image classification using a dynamic agent-based approach, improving accuracy and interpretability.", "motivation": "The paper addresses the redundancy and insufficient coverage of current concept banks used in Concept Bottleneck Models (CBMs), aiming to optimize the number of concepts for better performance.", "method": "The authors propose a dynamic, agent-based approach that adjusts the concept bank based on environmental feedback. They introduce CoCoBMs, which improve traditional concept scoring with an editable matrix for LLM corrections.", "result": "The method improves classification accuracy by 6% and interpretability assessments by 30% across 6 datasets evaluated.", "conclusion": "The proposed CoCoBMs provide a more effective way to utilize concepts in image classification, enhancing both performance and interpretability compared to traditional methods.", "key_contributions": ["Dynamic agent-based approach for concept bank adjustment", "Introduction of Conditional Concept Bottleneck Models (CoCoBMs)", "Improvement in classification accuracy and interpretability assessments"], "limitations": "", "keywords": ["Concept Bottleneck Models", "Large Language Models", "image classification", "Conditional Concept Bottleneck Models", "interpretability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01340", "pdf": "https://arxiv.org/pdf/2506.01340.pdf", "abs": "https://arxiv.org/abs/2506.01340", "title": "The Landscape of Arabic Large Language Models (ALLMs): A New Era for Arabic Language Technology", "authors": ["Shahad Al-Khalifa", "Nadir Durrani", "Hend Al-Khalifa", "Firoj Alam"], "categories": ["cs.CL"], "comment": "Accepted at CACM", "summary": "The emergence of ChatGPT marked a transformative milestone for Artificial\nIntelligence (AI), showcasing the remarkable potential of Large Language Models\n(LLMs) to generate human-like text. This wave of innovation has revolutionized\nhow we interact with technology, seamlessly integrating LLMs into everyday\ntasks such as vacation planning, email drafting, and content creation. While\nEnglish-speaking users have significantly benefited from these advancements,\nthe Arabic world faces distinct challenges in developing Arabic-specific LLMs.\nArabic, one of the languages spoken most widely around the world, serves more\nthan 422 million native speakers in 27 countries and is deeply rooted in a rich\nlinguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an\nunparalleled opportunity to bridge technological gaps and empower communities.\nThe journey of ALLMs has been both fascinating and complex, evolving from\nrudimentary text processing systems to sophisticated AI-driven models. This\narticle explores the trajectory of ALLMs, from their inception to the present\nday, highlighting the efforts to evaluate these models through benchmarks and\npublic leaderboards. We also discuss the challenges and opportunities that\nALLMs present for the Arab world.", "AI": {"tldr": "This paper discusses the evolution and potential of Arabic Large Language Models (ALLMs), addressing their unique challenges and opportunities in the Arab world.", "motivation": "To explore the transformative potential of Arabic-specific LLMs and their impact on technological advancement in the Arabic-speaking world.", "method": "The article reviews the development of ALLMs, from initial text processing to advanced AI-driven models, and evaluates them using benchmarks and leaderboards.", "result": "The evolution of ALLMs presents significant opportunities for bridging technological gaps in the Arabic-speaking community, despite the challenges faced.", "conclusion": "Advancements in ALLMs are crucial for empowering millions of Arabic speakers and enhancing their interaction with technology.", "key_contributions": ["Historical timeline of ALLMs development", "Evaluation methods for ALLMs using benchmarks and public leaderboards", "Discussion of challenges and opportunities in the development of ALLMs"], "limitations": "The complexities of Arabic language and culture may impede the immediate development and adoption of ALLMs.", "keywords": ["Arabic Language Models", "Natural Language Processing", "AI in Arabic", "Technology Integration", "Cultural Heritage"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.01341", "pdf": "https://arxiv.org/pdf/2506.01341.pdf", "abs": "https://arxiv.org/abs/2506.01341", "title": "TurnBench-MS: A Benchmark for Evaluating Multi-Turn, Multi-Step Reasoning in Large Language Models", "authors": ["Yiran Zhang", "Mo Wang", "Xiaoyang Li", "Kaixuan Ren", "Chencheng Zhu", "Usman Naseem"], "categories": ["cs.CL"], "comment": "Preprint", "summary": "Despite impressive advances in large language models (LLMs), existing\nbenchmarks often focus on single-turn or single-step tasks, failing to capture\nthe kind of iterative reasoning required in real-world settings. To address\nthis limitation, we introduce TurnBench, a novel benchmark that evaluates\nmulti-turn, multi-step reasoning through an interactive code-breaking task\ninspired by a \"Turing Machine Board Game.\" In each episode, a model must\nuncover hidden logical or arithmetic rules by making sequential guesses,\nreceiving structured feedback, and integrating clues across multiple rounds.\nThis dynamic setup requires models to reason over time, adapt based on past\ninformation, and maintain consistency across steps-capabilities underexplored\nin current benchmarks. TurnBench includes two modes: Classic, which tests\nstandard reasoning, and Nightmare, which introduces increased complexity and\nrequires robust inferential chains. To support fine-grained analysis, we\nprovide ground-truth annotations for intermediate reasoning steps. Our\nevaluation of state-of-the-art LLMs reveals significant gaps: the best model\nachieves 81.5% accuracy in Classic mode, but performance drops to 17.8% in\nNightmare mode. In contrast, human participants achieve 100% in both,\nunderscoring the challenge TurnBench poses to current models. By incorporating\nfeedback loops and hiding task rules, TurnBench reduces contamination risks and\nprovides a rigorous testbed for diagnosing and advancing multi-step, multi-turn\nreasoning in LLMs.", "AI": {"tldr": "TurnBench is a new benchmark for evaluating multi-turn, multi-step reasoning in large language models (LLMs) through an interactive code-breaking task.", "motivation": "Existing benchmarks do not adequately assess iterative reasoning required in real-world applications, which This paper aims to address.", "method": "TurnBench introduces a code-breaking task with two modes: Classic for standard reasoning and Nightmare for enhanced complexity, requiring models to reason sequentially and maintain consistency over multiple rounds with feedback.", "result": "The best LLM achieved 81.5% accuracy in Classic mode, but only 17.8% in Nightmare mode, while humans achieved 100% accuracy in both modes.", "conclusion": "TurnBench highlights the significant challenges faced by current LLMs in multi-step reasoning, providing a rigorous platform for development.", "key_contributions": ["Introduction of TurnBench for multi-turn reasoning evaluation", "Two distinct modes of testing (Classic and Nightmare) with varying complexity", "Ground-truth annotations for intermediate reasoning steps provided"], "limitations": "", "keywords": ["large language models", "multi-turn reasoning", "benchmarking"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01344", "pdf": "https://arxiv.org/pdf/2506.01344.pdf", "abs": "https://arxiv.org/abs/2506.01344", "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic Agents", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Vivek Gupta", "Dinesh Manocha"], "categories": ["cs.CL"], "comment": null, "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.", "AI": {"tldr": "This paper introduces Fine-grained Flowchart Attribution to improve LLM interpretations of flowcharts, presenting the FlowPathAgent for dynamic graph-based reasoning to ensure accurate attributions.", "motivation": "To improve the reliability and explainability of LLMs analyzing flowcharts, particularly in critical domains such as logistics and health.", "method": "The paper proposes FlowPathAgent, a neurosymbolic agent that segments flowcharts, converts them into symbolic graphs, and uses agentic interactions for attribution paths.", "result": "FlowPathAgent mitigates hallucinations in LLM answers by achieving a 10-14% performance improvement on the FlowExplainBench benchmark compared to strong baselines.", "conclusion": "By enabling fine-grained attribution in flowchart analysis, this approach enhances the verifiability and reliability of LLM outputs.", "key_contributions": ["Introduction of Fine-grained Flowchart Attribution", "Development of FlowPathAgent for automated attribution", "Creation of FlowExplainBench benchmark for evaluating flowchart attributions"], "limitations": "", "keywords": ["Flowchart Attribution", "LLM Interpretation", "Neurosymbolic Agent", "Visual Hallucinations", "FlowExplainBench"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.01347", "pdf": "https://arxiv.org/pdf/2506.01347.pdf", "abs": "https://arxiv.org/abs/2506.01347", "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning", "authors": ["Xinyu Zhu", "Mengzhou Xia", "Zhepei Wei", "Wei-Lin Chen", "Danqi Chen", "Yu Meng"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach\nfor training language models (LMs) on reasoning tasks that elicit emergent long\nchains of thought (CoTs). Unlike supervised learning, it updates the model\nusing both correct and incorrect samples via policy gradients. To better\nunderstand its mechanism, we decompose the learning signal into reinforcing\ncorrect responses and penalizing incorrect ones, referred to as Positive and\nNegative Sample Reinforcement (PSR and NSR), respectively. We train\nQwen2.5-Math-7B and Qwen3-4B on a mathematical reasoning dataset and uncover a\nsurprising result: training with only negative samples -- without reinforcing\ncorrect responses -- can be highly effective: it consistently improves\nperformance over the base model across the entire Pass@$k$ spectrum ($k$ up to\n$256$), often matching or surpassing PPO and GRPO. In contrast, reinforcing\nonly correct responses improves Pass@$1$ but degrades performance at higher\n$k$, due to reduced diversity. These inference-scaling trends highlight that\nsolely penalizing incorrect responses may contribute more to performance than\npreviously recognized. Through gradient analysis, we show that NSR works by\nsuppressing incorrect generations and redistributing probability mass toward\nother plausible candidates, guided by the model's prior beliefs. It refines the\nmodel's existing knowledge rather than introducing entirely new behaviors.\nBuilding on this insight, we propose a simple variant of the RL objective that\nupweights NSR, and show that it consistently improves overall Pass@$k$\nperformance on MATH, AIME 2025, and AMC23. Our code is available at\nhttps://github.com/TianHongZXY/RLVR-Decomposed.", "AI": {"tldr": "The paper explores Reinforcement Learning with Verifiable Rewards (RLVR) for training language models on reasoning tasks, emphasizing the importance of negative sample reinforcement in enhancing model performance.", "motivation": "To investigate the mechanism of RLVR in training language models and its effectiveness on reasoning tasks.", "method": "The authors decompose the learning signal into Positive and Negative Sample Reinforcement, training Qwen models on a mathematical reasoning dataset and analyzing performance changes across different configurations of reward structures.", "result": "Training with only negative samples (NSR) showed improved performance over the base model across Pass@$k$ metrics, often surpassing traditional methods like PPO and GRPO.", "conclusion": "Negative Sample Reinforcement may contribute significantly to performance, emphasizing its importance compared to Positive Sample Reinforcement in certain contexts.", "key_contributions": ["Introduced the decomposition of learning signals into Positive and Negative Sample Reinforcement.", "Demonstrated that training solely with negative samples can outperform models trained with correct responses included.", "Proposed a modified RL objective that upweights NSR, leading to consistent performance improvements."], "limitations": "", "keywords": ["Reinforcement Learning", "Language Models", "Negative Sample Reinforcement"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01357", "pdf": "https://arxiv.org/pdf/2506.01357.pdf", "abs": "https://arxiv.org/abs/2506.01357", "title": "KokoroChat: A Japanese Psychological Counseling Dialogue Dataset Collected via Role-Playing by Trained Counselors", "authors": ["Zhiyang Qi", "Takumasa Kaneko", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Generating psychological counseling responses with language models relies\nheavily on high-quality datasets. Crowdsourced data collection methods require\nstrict worker training, and data from real-world counseling environments may\nraise privacy and ethical concerns. While recent studies have explored using\nlarge language models (LLMs) to augment psychological counseling dialogue\ndatasets, the resulting data often suffers from limited diversity and\nauthenticity. To address these limitations, this study adopts a role-playing\napproach where trained counselors simulate counselor-client interactions,\nensuring high-quality dialogues while mitigating privacy risks. Using this\nmethod, we construct KokoroChat, a Japanese psychological counseling dialogue\ndataset comprising 6,589 long-form dialogues, each accompanied by comprehensive\nclient feedback. Experimental results demonstrate that fine-tuning open-source\nLLMs with KokoroChat improves both the quality of generated counseling\nresponses and the automatic evaluation of counseling dialogues. The KokoroChat\ndataset is available at https://github.com/UEC-InabaLab/KokoroChat.", "AI": {"tldr": "This paper presents KokoroChat, a Japanese psychological counseling dialogue dataset created using a role-playing approach to improve the quality and diversity of counseling interactions while mitigating privacy concerns.", "motivation": "The need for high-quality datasets in generating psychological counseling responses with language models, while addressing privacy and ethical issues in data collection.", "method": "The study employs a role-playing methodology wherein trained counselors simulate counselor-client interactions to generate a dataset of counseling dialogues.", "result": "KokoroChat consists of 6,589 long-form dialogues, significantly improving the quality of counseling responses when used to fine-tune open-source LLMs, with enhancements in automatic evaluation of dialogues.", "conclusion": "The role-playing approach effectively produces a valuable dataset for psychological counseling, ensuring both quality and privacy preservation.", "key_contributions": ["Introduction of KokoroChat dataset for psychological counseling dialogues.", "Demonstration of improved response quality from fine-tuning LLMs with this dataset.", "Mitigation of privacy risks associated with real-world counseling data."], "limitations": "Primarily focused on a Japanese context; applicability to other languages or cultural contexts may need further exploration.", "keywords": ["psychological counseling", "language models", "KokoroChat", "role-playing", "dialogue dataset"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01367", "pdf": "https://arxiv.org/pdf/2506.01367.pdf", "abs": "https://arxiv.org/abs/2506.01367", "title": "MMD-Flagger: Leveraging Maximum Mean Discrepancy to Detect Hallucinations", "authors": ["Kensuke Mitsuzawa", "Damien Garreau"], "categories": ["cs.CL", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have become pervasive in our everyday life. Yet,\na fundamental obstacle prevents their use in many critical applications: their\npropensity to generate fluent, human-quality content that is not grounded in\nreality. The detection of such hallucinations is thus of the highest\nimportance. In this work, we propose a new method to flag hallucinated content,\nMMD-Flagger. It relies on Maximum Mean Discrepancy (MMD), a non-parametric\ndistance between distributions. On a high-level perspective, MMD-Flagger tracks\nthe MMD between the generated documents and documents generated with various\ntemperature parameters. We show empirically that inspecting the shape of this\ntrajectory is sufficient to detect most hallucinations. This novel method is\nbenchmarked on two machine translation datasets, on which it outperforms\nnatural competitors.", "AI": {"tldr": "This paper introduces MMD-Flagger, a method for detecting hallucinated content generated by large language models using Maximum Mean Discrepancy (MMD).", "motivation": "The increasing use of large language models (LLMs) in critical applications is hindered by their tendency to produce content that may not reflect reality, necessitating effective detection methods for hallucinations.", "method": "MMD-Flagger employs Maximum Mean Discrepancy (MMD) to track the distance between distributions of generated documents at different temperature parameters, analyzing the resulting trajectory shape to identify hallucinations.", "result": "MMD-Flagger was empirically tested on two machine translation datasets and demonstrated superior performance compared to existing methods in detecting hallucinations.", "conclusion": "The findings suggest that MMD-Flagger is effective in addressing the hallucination problem in LLMs, helping to enhance the reliability of AI-generated content.", "key_contributions": ["Introduction of MMD-Flagger for hallucination detection in LLMs", "Use of Maximum Mean Discrepancy as a novel detection mechanism", "Empirical benchmarking showing high effectiveness on machine translation tasks"], "limitations": "", "keywords": ["large language models", "hallucination detection", "Maximum Mean Discrepancy", "machine translation", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01381", "pdf": "https://arxiv.org/pdf/2506.01381.pdf", "abs": "https://arxiv.org/abs/2506.01381", "title": "AdaRewriter: Unleashing the Power of Prompting-based Conversational Query Reformulation via Test-Time Adaptation", "authors": ["Yilong Lai", "Jialong Wu", "Zhenglin Wang", "Deyu Zhou"], "categories": ["cs.CL"], "comment": null, "summary": "Prompting-based conversational query reformulation has emerged as a powerful\napproach for conversational search, refining ambiguous user queries into\nstandalone search queries. Best-of-N reformulation over the generated\ncandidates via prompting shows impressive potential scaling capability.\nHowever, both the previous tuning methods (training time) and adaptation\napproaches (test time) can not fully unleash their benefits. In this paper, we\npropose AdaRewriter, a novel framework for query reformulation using an\noutcome-supervised reward model via test-time adaptation. By training a\nlightweight reward model with contrastive ranking loss, AdaRewriter selects the\nmost promising reformulation during inference. Notably, it can operate\neffectively in black-box systems, including commercial LLM APIs. Experiments on\nfive conversational search datasets show that AdaRewriter significantly\noutperforms the existing methods across most settings, demonstrating the\npotential of test-time adaptation for conversational query reformulation.", "AI": {"tldr": "AdaRewriter is a framework for query reformulation using an outcome-supervised reward model, achieving significant performance improvements in conversational search through test-time adaptation.", "motivation": "To enhance conversational search by refining ambiguous user queries into effective standalone search queries, utilizing prompting-based methods.", "method": "The paper introduces AdaRewriter, which employs a lightweight reward model trained with contrastive ranking loss for selecting the best reformulation during inference, enabling effective performance in black-box systems such as commercial LLM APIs.", "result": "AdaRewriter significantly outperforms existing query reformulation methods across five conversational search datasets, showcasing the benefits of test-time adaptation.", "conclusion": "The results indicate that AdaRewriter presents a promising solution for improving conversational query reformulation, making it more effective through adaptive learning at inference time.", "key_contributions": ["Development of AdaRewriter framework for outcome-supervised query reformulation", "Introduction of a contrastive ranking loss for training a lightweight reward model", "Demonstration of effectiveness in black-box systems, including commercial LLM APIs"], "limitations": "", "keywords": ["query reformulation", "conversational search", "outcome-supervised learning", "test-time adaptation", "reward model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01406", "pdf": "https://arxiv.org/pdf/2506.01406.pdf", "abs": "https://arxiv.org/abs/2506.01406", "title": "Speech-to-Speech Translation Pipelines for Conversations in Low-Resource Languages", "authors": ["Andrei Popescu-Belis", "Alexis Allemann", "Teo Ferrari", "Gopal Krishnamani"], "categories": ["cs.CL"], "comment": "Proceedings of MT Summit 2025", "summary": "The popularity of automatic speech-to-speech translation for human\nconversations is growing, but the quality varies significantly depending on the\nlanguage pair. In a context of community interpreting for low-resource\nlanguages, namely Turkish and Pashto to/from French, we collected fine-tuning\nand testing data, and compared systems using several automatic metrics (BLEU,\nCOMET, and BLASER) and human assessments. The pipelines included automatic\nspeech recognition, machine translation, and speech synthesis, with local\nmodels and cloud-based commercial ones. Some components have been fine-tuned on\nour data. We evaluated over 60 pipelines and determined the best one for each\ndirection. We also found that the ranks of components are generally independent\nof the rest of the pipeline.", "AI": {"tldr": "This paper evaluates automatic speech-to-speech translation systems for low-resource language pairs (Turkish/Pashto to/from French) by comparing over 60 pipelines using automatic metrics and human assessments.", "motivation": "To improve automatic speech-to-speech translation quality for community interpreting in low-resource languages by evaluating different translation systems and methodologies.", "method": "The study involved collecting fine-tuning and testing data, and assessing translation systems using metrics such as BLEU, COMET, and BLASER, as well as human evaluations. The research compared pipelines that included automatic speech recognition, machine translation, and speech synthesis.", "result": "The evaluation identified the best translation pipelines for both Turkish and Pashto to/from French and noted that the performance ranks of the components were generally independent of the overall pipeline configuration.", "conclusion": "This study highlights the variable quality of translation systems for low-resource languages and provides insights for optimizing speech-to-speech translation pipelines.", "key_contributions": ["Evaluation of over 60 translation pipelines for low-resource languages", "Identification of best-performing systems for each language direction", "Insights on the independence of component performance ranks from pipeline configurations"], "limitations": "", "keywords": ["speech translation", "low-resource languages", "Turkish", "Pashto", "French", "community interpreting"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01407", "pdf": "https://arxiv.org/pdf/2506.01407.pdf", "abs": "https://arxiv.org/abs/2506.01407", "title": "Comparing LLM-generated and human-authored news text using formal syntactic theory", "authors": ["Olga Zamaraeva", "Dan Flickinger", "Francis Bond", "Carlos Gómez-Rodríguez"], "categories": ["cs.CL"], "comment": "20 pages, 15 figures, 13 tables; accepted to ACL-2025 main", "summary": "This study provides the first comprehensive comparison of New York\nTimes-style text generated by six large language models against real,\nhuman-authored NYT writing. The comparison is based on a formal syntactic\ntheory. We use Head-driven Phrase Structure Grammar (HPSG) to analyze the\ngrammatical structure of the texts. We then investigate and illustrate the\ndifferences in the distributions of HPSG grammar types, revealing systematic\ndistinctions between human and LLM-generated writing. These findings contribute\nto a deeper understanding of the syntactic behavior of LLMs as well as humans,\nwithin the NYT genre.", "AI": {"tldr": "This study compares New York Times-style text from six large language models with real human-authored writing using Head-driven Phrase Structure Grammar to highlight syntactic differences.", "motivation": "To understand the syntactic behavior of large language models compared to human authors in the context of NYC-style writing.", "method": "A formal comparison based on Head-driven Phrase Structure Grammar (HPSG) to analyze the grammatical structures of texts generated by LLMs versus those written by humans.", "result": "The analysis reveals systematic distinctions in the distributions of HPSG grammar types between human and LLM-generated texts.", "conclusion": "The findings enhance the understanding of syntactic behavior in LLMs as compared to humans in specific writing genres.", "key_contributions": ["First comprehensive comparison of NYT-style text generated by LLMs and real human writing.", "Utilized HPSG for detailed grammatical analysis.", "Illustrated specific syntactic differences in the writing style of LLMs versus humans."], "limitations": "", "keywords": ["Large Language Models", "Head-driven Phrase Structure Grammar", "Syntactic Behavior", "Human-Computer Interaction", "Natural Language Processing"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2506.01419", "pdf": "https://arxiv.org/pdf/2506.01419.pdf", "abs": "https://arxiv.org/abs/2506.01419", "title": "UniversalCEFR: Enabling Open Multilingual Research on Language Proficiency Assessment", "authors": ["Joseph Marvin Imperial", "Abdullah Barayan", "Regina Stodden", "Rodrigo Wilkens", "Ricardo Munoz Sanchez", "Lingyun Gao", "Melissa Torgbi", "Dawn Knight", "Gail Forey", "Reka R. Jablonkai", "Ekaterina Kochmar", "Robert Reynolds", "Eugenio Ribeiro", "Horacio Saggion", "Elena Volodina", "Sowmya Vajjala", "Thomas Francois", "Fernando Alva-Manchego", "Harish Tayyar Madabushi"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce UniversalCEFR, a large-scale multilingual multidimensional\ndataset of texts annotated according to the CEFR (Common European Framework of\nReference) scale in 13 languages. To enable open research in both automated\nreadability and language proficiency assessment, UniversalCEFR comprises\n505,807 CEFR-labeled texts curated from educational and learner-oriented\nresources, standardized into a unified data format to support consistent\nprocessing, analysis, and modeling across tasks and languages. To demonstrate\nits utility, we conduct benchmark experiments using three modelling paradigms:\na) linguistic feature-based classification, b) fine-tuning pre-trained LLMs,\nand c) descriptor-based prompting of instruction-tuned LLMs. Our results\nfurther support using linguistic features and fine-tuning pretrained models in\nmultilingual CEFR level assessment. Overall, UniversalCEFR aims to establish\nbest practices in data distribution in language proficiency research by\nstandardising dataset formats and promoting their accessibility to the global\nresearch community.", "AI": {"tldr": "UniversalCEFR is a multilingual dataset of CEFR-annotated texts aimed at enhancing language proficiency assessments.", "motivation": "To advance research in automated readability and language proficiency assessment by providing a large-scale, standardized dataset.", "method": "Created a dataset of 505,807 CEFR-labeled texts from educational resources and conducted benchmark experiments with linguistic feature-based classification, fine-tuning LLMs, and descriptor-based prompting.", "result": "Demonstrated the effectiveness of linguistic features and fine-tuning pre-trained models for multilingual CEFR level assessment through benchmark experiments.", "conclusion": "UniversalCEFR sets a standard for language proficiency research, supporting best practices in data distribution and accessibility.", "key_contributions": ["Large-scale multilingual dataset of CEFR-annotated texts", "Benchmark experiments showcasing the utility of the dataset", "Promotion of standardized dataset formats for better accessibility"], "limitations": "", "keywords": ["UniversalCEFR", "language proficiency", "multilingual dataset", "automated readability", "CEFR"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2506.01420", "pdf": "https://arxiv.org/pdf/2506.01420.pdf", "abs": "https://arxiv.org/abs/2506.01420", "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation", "authors": ["Kyuyoung Kim", "Hyunjun Jeon", "Jinwoo Shin"], "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly used in sensitive domains,\nwhere their ability to infer personal data from seemingly benign text poses\nemerging privacy risks. While recent LLM-based anonymization methods help\nmitigate such risks, they often rely on proprietary models (e.g., GPT-4),\nraising concerns about cost and the potential exposure of sensitive data to\nuntrusted external systems. To address this, we introduce SElf-refining\nAnonymization with Language model (SEAL), a novel distillation framework for\ntraining small language models (SLMs) to perform effective anonymization\nwithout relying on external costly models at inference time. We leverage\nadversarial interactions between an LLM anonymizer and an inference model to\ncollect trajectories of anonymized texts and inferred attributes, which are\nused to distill anonymization, adversarial inference, and utility evaluation\ncapabilities into SLMs via supervised fine-tuning and preference learning. The\nresulting models learn to both anonymize text and critique their outputs,\nenabling iterative improvement of anonymization quality via self-refinement.\nExperiments on SynthPAI, a dataset of synthetic personal profiles and text\ncomments, demonstrate that SLMs trained with SEAL achieve substantial\nimprovements in anonymization capabilities. Notably, 8B models attain a\nprivacy-utility trade-off comparable to that of the GPT-4 anonymizer and, with\nself-refinement, even surpass it in terms of privacy. These results show the\neffectiveness of our adversarial distillation framework in training SLMs as\nefficient anonymizers. To facilitate further research, we release the full\ndataset used in our experiments.", "AI": {"tldr": "SEAL introduces a novel framework for training small language models to anonymize text effectively while avoiding reliance on costly external models, demonstrating improved privacy-utility trade-offs over traditional methods.", "motivation": "With the increasing use of large language models in sensitive areas, privacy risks from personal data inference require effective anonymization methods that do not rely on expensive external models.", "method": "SEAL employs a distillation framework that incorporates adversarial interactions between an LLM anonymizer and an inference model to train small language models in anonymization through supervised fine-tuning and preference learning.", "result": "SLMs trained with SEAL show significant improvements in anonymization capabilities, achieving a comparable privacy-utility trade-off to GPT-4, and surpassing it with self-refinement.", "conclusion": "The SEAL framework effectively trains small language models to perform anonymization efficiently, reducing reliance on larger, proprietary models, and enhancing privacy protections.", "key_contributions": ["Introduction of the SEAL framework for training small language models for anonymization", "Improvement of privacy-utility trade-offs in anonymization compared to GPT-4", "Release of a dataset for further research on the proposed methods."], "limitations": "", "keywords": ["language models", "anonymization", "privacy", "adversarial learning", "distillation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.01435", "pdf": "https://arxiv.org/pdf/2506.01435.pdf", "abs": "https://arxiv.org/abs/2506.01435", "title": "Redundancy, Isotropy, and Intrinsic Dimensionality of Prompt-based Text Embeddings", "authors": ["Hayato Tsukagoshi", "Ryohei Sasano"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Prompt-based text embedding models, which generate task-specific embeddings\nupon receiving tailored prompts, have recently demonstrated remarkable\nperformance. However, their resulting embeddings often have thousands of\ndimensions, leading to high storage costs and increased computational costs of\nembedding-based operations. In this paper, we investigate how post-hoc\ndimensionality reduction applied to the embeddings affects the performance of\nvarious tasks that leverage these embeddings, specifically classification,\nclustering, retrieval, and semantic textual similarity (STS) tasks. Our\nexperiments show that even a naive dimensionality reduction, which keeps only\nthe first 25% of the dimensions of the embeddings, results in a very slight\nperformance degradation, indicating that these embeddings are highly redundant.\nNotably, for classification and clustering, even when embeddings are reduced to\nless than 0.5% of the original dimensionality the performance degradation is\nvery small. To quantitatively analyze this redundancy, we perform an analysis\nbased on the intrinsic dimensionality and isotropy of the embeddings. Our\nanalysis reveals that embeddings for classification and clustering, which are\nconsidered to have very high dimensional redundancy, exhibit lower intrinsic\ndimensionality and less isotropy compared with those for retrieval and STS.", "AI": {"tldr": "This paper explores the effects of post-hoc dimensionality reduction on task-specific text embeddings, revealing that significant reductions maintain performance for classification and clustering tasks.", "motivation": "The rise in use of high-dimensional text embeddings necessitates examining their redundancy to reduce storage and computational costs without sacrificing performance.", "method": "The study applies various levels of post-hoc dimensionality reduction to task-specific embeddings and evaluates performance across classification, clustering, retrieval, and semantic textual similarity tasks.", "result": "Performance degradation is minimal even when reducing embeddings to less than 0.5% of original dimensionality; intrinsic dimensionality analysis shows redundancy in embeddings used for classification and clustering.", "conclusion": "Dimensionality reduction can be significantly applied to text embeddings without major performance loss, particularly in classification and clustering tasks, highlighting their inherent redundancy.", "key_contributions": ["Demonstrated minimal performance impact of aggressive dimensionality reduction on task-specific embeddings.", "Introduced quantitative analysis of intrinsic dimensionality and isotropy in embeddings.", "Showed redundancy in text embeddings, allowing for storage and computation efficiency."], "limitations": "The study may not fully address performance impacts across all potential NLP tasks or varied embedding models.", "keywords": ["text embeddings", "dimensionality reduction", "classification", "clustering", "NLP"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.01439", "pdf": "https://arxiv.org/pdf/2506.01439.pdf", "abs": "https://arxiv.org/abs/2506.01439", "title": "Whale: Large-Scale multilingual ASR model with w2v-BERT and E-Branchformer with large speech data", "authors": ["Yosuke Kashiwagi", "Hayato Futami", "Emiru Tsunoo", "Satoshi Asakawa"], "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "This paper reports on the development of a large-scale speech recognition\nmodel, Whale. Similar to models such as Whisper and OWSM, Whale leverages both\na large model size and a diverse, extensive dataset. Whale's architecture\nintegrates w2v-BERT self-supervised model, an encoder-decoder backbone built on\nE-Branchformer, and a joint CTC-attention decoding strategy. The training\ncorpus comprises varied speech data, of not only public corpora but also\nin-house data, thereby enhancing the model's robustness to different speaking\nstyles and acoustic conditions. Through evaluations on multiple benchmarks,\nWhale achieved comparable performance to existing models. In particular, it\nachieves a word error rate of 2.4% on the Librispeech test-clean set and a\ncharacter error rate of 3.4% on the CSJ eval3 set, outperforming Whisper\nlarge-v3 and OWSM v3.1.", "AI": {"tldr": "Development of Whale, a large-scale speech recognition model featuring advanced architecture and high robustness across varied speech data.", "motivation": "To improve speech recognition accuracy across diverse speaking styles and acoustic conditions, similar to existing models like Whisper and OWSM.", "method": "Whale utilizes a w2v-BERT self-supervised model and an E-Branchformer encoder-decoder backbone, along with a joint CTC-attention decoding strategy.", "result": "Whale achieved a word error rate of 2.4% on the Librispeech test-clean set and a character error rate of 3.4% on the CSJ eval3 set, outperforming Whisper large-v3 and OWSM v3.1.", "conclusion": "Whale demonstrates comparable performance to existing models through its extensive training on diverse datasets, offering improved robustness.", "key_contributions": ["Introduction of a large-scale speech recognition model, Whale.", "Utilization of a hybrid w2v-BERT and E-Branchformer architecture.", "Demonstrated superior performance on multiple speech recognition benchmarks."], "limitations": "", "keywords": ["speech recognition", "Whale", "w2v-BERT", "E-Branchformer", "CTC-attention"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01451", "pdf": "https://arxiv.org/pdf/2506.01451.pdf", "abs": "https://arxiv.org/abs/2506.01451", "title": "Building Entity Association Mining Framework for Knowledge Discovery", "authors": ["Anshika Rawal", "Abhijeet Kumar", "Mridul Mishra"], "categories": ["cs.CL", "cs.IR", "I.2.7"], "comment": "Presented at Business Analytics and Intelligence Conference, IIM\n  Bengaluru", "summary": "Extracting useful signals or pattern to support important business decisions\nfor example analyzing investment product traction and discovering customer\npreference, risk monitoring etc. from unstructured text is a challenging task.\nCapturing interaction of entities or concepts and association mining is a\ncrucial component in text mining, enabling information extraction and reasoning\nover and knowledge discovery from text. Furthermore, it can be used to enrich\nor filter knowledge graphs to guide exploration processes, descriptive\nanalytics and uncover hidden stories in the text. In this paper, we introduce a\ndomain independent pipeline i.e., generalized framework to enable document\nfiltering, entity extraction using various sources (or techniques) as plug-ins\nand association mining to build any text mining business use-case and\nquantitatively define a scoring metric for ranking purpose. The proposed\nframework has three major components a) Document filtering: filtering\ndocuments/text of interest from massive amount of texts b) Configurable entity\nextraction pipeline: include entity extraction techniques i.e., i) DBpedia\nSpotlight, ii) Spacy NER, iii) Custom Entity Matcher, iv) Phrase extraction (or\ndictionary) based c) Association Relationship Mining: To generates\nco-occurrence graph to analyse potential relationships among entities,\nconcepts. Further, co-occurrence count based frequency statistics provide a\nholistic window to observe association trends or buzz rate in specific business\ncontext. The paper demonstrates the usage of framework as fundamental building\nbox in two financial use-cases namely brand product discovery and vendor risk\nmonitoring. We aim that such framework will remove duplicated effort, minimize\nthe development effort, and encourage reusability and rapid prototyping in\nassociation mining business applications for institutions.", "AI": {"tldr": "A framework for text mining that facilitates document filtering, entity extraction, and association mining to aid business decision-making.", "motivation": "To support important business decisions by extracting useful signals from unstructured text, such as investment product traction and customer preferences.", "method": "The proposed framework includes document filtering, a configurable entity extraction pipeline using various techniques, and association relationship mining to create a co-occurrence graph for analyzing relationships among entities.", "result": "The framework demonstrated its utility in two financial use-cases: brand product discovery and vendor risk monitoring, showing the potential for removing redundant efforts and promoting rapid prototyping.", "conclusion": "The framework offers a generalized solution for text mining business applications, encouraging reusability and efficiency in developing association mining applications.", "key_contributions": ["Introduction of a generalized framework for document filtering and entity extraction", "Configurable extraction pipeline with multiple techniques", "Application of the framework in real-world financial use-cases"], "limitations": "", "keywords": ["text mining", "entity extraction", "association mining", "business analytics", "financial applications"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.01458", "pdf": "https://arxiv.org/pdf/2506.01458.pdf", "abs": "https://arxiv.org/abs/2506.01458", "title": "TalTech Systems for the Interspeech 2025 ML-SUPERB 2.0 Challenge", "authors": ["Tanel Alumäe", "Artem Fedorchenko"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "This paper describes the language identification and multilingual speech\nrecognition system developed at Tallinn University of Technology for the\nInterspeech 2025 ML-SUPERB 2.0 Challenge. A hybrid language identification\nsystem is used, consisting of a pretrained language embedding model and a\nlight-weight speech recognition model with a shared encoder across languages\nand language-specific bigram language models. For speech recognition, three\nmodels are used, where only a single model is applied for each language,\ndepending on the training data availability and performance on held-out data.\nThe model set consists of a finetuned version of SeamlessM4T, MMS-1B-all with\ncustom language adapters and MMS-zeroshot. The system obtained the top overall\nscore in the challenge.", "AI": {"tldr": "This paper presents a hybrid multilingual speech recognition system that achieved the top score at the Interspeech 2025 ML-SUPERB 2.0 Challenge.", "motivation": "The goal was to develop an effective multilingual speech recognition and language identification system for the Interspeech 2025 ML-SUPERB 2.0 Challenge.", "method": "The system uses a hybrid approach with a pretrained language embedding model and a lightweight speech recognition model sharing an encoder across languages, along with language-specific bigram models.", "result": "The system was evaluated in a competitive setting and achieved the top overall score in the challenge.", "conclusion": "The results indicate the effectiveness of the hybrid model and the importance of both language embedding and speech recognition components.", "key_contributions": ["Development of a hybrid multilingual speech recognition system", "Implementation of language-specific bigram language models", "Top performance in the Interspeech 2025 ML-SUPERB challenge"], "limitations": "", "keywords": ["language identification", "multilingual speech recognition", "hybrid model"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.01474", "pdf": "https://arxiv.org/pdf/2506.01474.pdf", "abs": "https://arxiv.org/abs/2506.01474", "title": "Integrating Neural and Symbolic Components in a Model of Pragmatic Question-Answering", "authors": ["Polina Tsvilodub", "Robert D. Hawkins", "Michael Franke"], "categories": ["cs.CL"], "comment": "16 pages, 16 figures. To appear in the proceedings of Society for\n  Computation in Linguistics (SCiL) 2025", "summary": "Computational models of pragmatic language use have traditionally relied on\nhand-specified sets of utterances and meanings, limiting their applicability to\nreal-world language use. We propose a neuro-symbolic framework that enhances\nprobabilistic cognitive models by integrating LLM-based modules to propose and\nevaluate key components in natural language, eliminating the need for manual\nspecification. Through a classic case study of pragmatic question-answering, we\nsystematically examine various approaches to incorporating neural modules into\nthe cognitive model -- from evaluating utilities and literal semantics to\ngenerating alternative utterances and goals. We find that hybrid models can\nmatch or exceed the performance of traditional probabilistic models in\npredicting human answer patterns. However, the success of the neuro-symbolic\nmodel depends critically on how LLMs are integrated: while they are\nparticularly effective for proposing alternatives and transforming abstract\ngoals into utilities, they face challenges with truth-conditional semantic\nevaluation. This work charts a path toward more flexible and scalable models of\npragmatic language use while illuminating crucial design considerations for\nbalancing neural and symbolic components.", "AI": {"tldr": "A neuro-symbolic framework enhances cognitive models for pragmatic language use by integrating LLM-based modules, improving performance in language tasks without manual specification.", "motivation": "To address limitations of traditional models in understanding pragmatic language use by eliminating manual specification of utterances and meanings.", "method": "The study involves a case analysis of pragmatic question-answering, introducing various integrations of neural modules within cognitive models to assess performance.", "result": "Hybrid models integrating LLMs match or exceed traditional probabilistic models in predicting human behaviors, with notable effectiveness in proposing language alternatives and managing abstract goals.", "conclusion": "The neuro-symbolic model offers a promising direction for scalable pragmatic language models while highlighting key integration challenges of LLMs.", "key_contributions": ["Integration of LLMs in cognitive models enhances natural language processing capabilities.", "Demonstrates improved modeling of pragmatic language use without manual specification.", "Identifies critical design considerations for the effectiveness of neural and symbolic components."], "limitations": "Challenges remain in truth-conditional semantic evaluation by LLMs.", "keywords": ["neuro-symbolic framework", "pragmatic language use", "LLM integration"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.01484", "pdf": "https://arxiv.org/pdf/2506.01484.pdf", "abs": "https://arxiv.org/abs/2506.01484", "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech Detoxification", "authors": ["Shuzhou Yuan", "Ercong Nie", "Lukas Kouba", "Ashish Yashwanth Kangen", "Helmut Schmid", "Hinrich Schutze", "Michael Farber"], "categories": ["cs.CL"], "comment": null, "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.", "AI": {"tldr": "This paper presents an automated detoxification method using LLMs to rewrite toxic language into non-toxic text, introducing a new large-scale dataset for hate speech detoxification called PARADEHATE.", "motivation": "The rise of toxic content online necessitates effective detoxification methods, yet high-quality parallel datasets for this task are lacking.", "method": "The study employs a novel LLM-in-the-loop pipeline using GPT-4o-mini to automate the detoxification process, replacing human annotators and creating the PARADEHATE dataset of over 8,000 hate/non-hate text pairs.", "result": "The results demonstrate that models like BART, fine-tuned on the PARADEHATE dataset, achieve improved style accuracy, content preservation, and fluency compared to previous methods.", "conclusion": "LLM-generated detoxification offers a scalable and effective alternative to traditional human annotation methods for addressing hate speech.", "key_contributions": ["Introduction of the PARADEHATE dataset for hate speech detoxification", "Demonstration of LLM performance comparable to human annotation", "Evaluation of baseline models showing improved performance on detoxification tasks"], "limitations": "", "keywords": ["Detoxification", "Hate Speech", "LLM", "Natural Language Processing", "Dataset"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01488", "pdf": "https://arxiv.org/pdf/2506.01488.pdf", "abs": "https://arxiv.org/abs/2506.01488", "title": "Argument-Centric Causal Intervention Method for Mitigating Bias in Cross-Document Event Coreference Resolution", "authors": ["Long Yao", "Wenzhong Yang", "Yabo Yin", "Fuyuan Wei", "Hongzhen Lv", "Jiaren Peng", "Liejun Wang", "Xiaoming Tao"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Cross-document Event Coreference Resolution (CD-ECR) is a fundamental task in\nnatural language processing (NLP) that seeks to determine whether event\nmentions across multiple documents refer to the same real-world occurrence.\nHowever, current CD-ECR approaches predominantly rely on trigger features\nwithin input mention pairs, which induce spurious correlations between\nsurface-level lexical features and coreference relationships, impairing the\noverall performance of the models. To address this issue, we propose a novel\ncross-document event coreference resolution method based on Argument-Centric\nCausal Intervention (ACCI). Specifically, we construct a structural causal\ngraph to uncover confounding dependencies between lexical triggers and\ncoreference labels, and introduce backdoor-adjusted interventions to isolate\nthe true causal effect of argument semantics. To further mitigate spurious\ncorrelations, ACCI integrates a counterfactual reasoning module that quantifies\nthe causal influence of trigger word perturbations, and an argument-aware\nenhancement module to promote greater sensitivity to semantically grounded\ninformation. In contrast to prior methods that depend on costly data\naugmentation or heuristic-based filtering, ACCI enables effective debiasing in\na unified end-to-end framework without altering the underlying training\nprocedure. Extensive experiments demonstrate that ACCI achieves CoNLL F1 of\n88.4% on ECB+ and 85.2% on GVC, achieving state-of-the-art performance. The\nimplementation and materials are available at https://github.com/era211/ACCI.", "AI": {"tldr": "This paper proposes a novel method for cross-document event coreference resolution using Argument-Centric Causal Intervention (ACCI) to improve performance by addressing spurious correlations between lexical features and coreference relationships.", "motivation": "Current methods for cross-document event coreference resolution rely on lexical features, leading to degraded model performance due to spurious correlations.", "method": "The proposed method constructs a causal graph to identify confounding dependencies and uses backdoor-adjusted interventions along with a counterfactual reasoning module to isolate the causal influence of semantics.", "result": "ACCI achieves state-of-the-art performance with CoNLL F1 scores of 88.4% on ECB+ and 85.2% on GVC.", "conclusion": "The ACCI method effectively mitigates spurious correlations in coreference resolution while maintaining an end-to-end training procedure.", "key_contributions": ["Introduction of Argument-Centric Causal Intervention (ACCI) for event coreference resolution.", "Development of a structural causal graph to improve coreference detection.", "Integration of counterfactual reasoning to enhance model robustness."], "limitations": "", "keywords": ["Event Coreference", "Causal Intervention", "Natural Language Processing", "Debiasing", "Counterfactual Reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01489", "pdf": "https://arxiv.org/pdf/2506.01489.pdf", "abs": "https://arxiv.org/abs/2506.01489", "title": "Multilingual Definition Modeling", "authors": ["Edison Marrese-Taylor", "Erica K. Shimomoto", "Alfredo Solano", "Enrique Reid"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose the first multilingual study on definition\nmodeling. We use monolingual dictionary data for four new languages (Spanish,\nFrench, Portuguese, and German) and perform an in-depth empirical study to test\nthe performance of pre-trained multilingual language models on definition\nmodeling of monosemic words when finetuned on this data. Furthermore, we use a\nzero-shot approach to test the multilingual capabilities of two popular\nchat-based Large Language Models (LLMs) in the task. Results show that\nmultilingual language models can perform on-pair with English but cannot\nleverage potential cross-lingual synergies, with LLMs generally offering better\nperformance overall. A comprehensive human evaluation of the LLM-generated\ndefinition highlights the zero and few-shot capabilities of these models in\nthis new task, also showing their shortcomings. Finally, we show that\nperformance on our task via BERTScore strongly correlates to the performance on\nmultilingual LLM benchmarks, suggesting that our task offers a viable\ncompute-constrained, stable and natural alternative to these.", "AI": {"tldr": "This paper presents a multilingual study on definition modeling using monolingual dictionary data for four languages and evaluates pre-trained multilingual models and LLMs in this context.", "motivation": "To explore the performance of multilingual models in definition modeling across different languages and compare it with English-based models and LLMs.", "method": "The authors performed an empirical study using dictionary data for Spanish, French, Portuguese, and German, finetuning multilingual language models and employing a zero-shot approach for LLMs on the definition modeling task.", "result": "Results indicate that while multilingual models perform comparably to English models, they fail to exploit cross-lingual synergies, with LLMs generally outperforming multilingual models.", "conclusion": "The study suggests that definition modeling can be approached with a compute-constrained method and that LLMs possess both strengths and weaknesses in generating definitions.", "key_contributions": ["First multilingual study on definition modeling", "Use of monolingual dictionary data for multiple languages", "Demonstration of LLMs' zero and few-shot capabilities in a new task"], "limitations": "The performance of multilingual models does not leverage cross-lingual synergies.", "keywords": ["multilingual", "definition modeling", "large language models", "cross-lingual", "BERTScore"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.01495", "pdf": "https://arxiv.org/pdf/2506.01495.pdf", "abs": "https://arxiv.org/abs/2506.01495", "title": "CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models", "authors": ["Ping Wu", "Guobin Shen", "Dongcheng Zhao", "Yuwei Wang", "Yiting Dong", "Yu Shi", "Enmeng Lu", "Feifei Zhao", "Yi Zeng"], "categories": ["cs.CL"], "comment": null, "summary": "Ensuring that Large Language Models (LLMs) align with mainstream human values\nand ethical norms is crucial for the safe and sustainable development of AI.\nCurrent value evaluation and alignment are constrained by Western cultural bias\nand incomplete domestic frameworks reliant on non-native rules; furthermore,\nthe lack of scalable, rule-driven scenario generation methods makes evaluations\ncostly and inadequate across diverse cultural contexts. To address these\nchallenges, we propose a hierarchical value framework grounded in core Chinese\nvalues, encompassing three main dimensions, 12 core values, and 50 derived\nvalues. Based on this framework, we construct a large-scale Chinese Values\nCorpus (CVC) containing over 250,000 value rules enhanced and expanded through\nhuman annotation. Experimental results show that CVC-guided scenarios\noutperform direct generation ones in value boundaries and content diversity. In\nthe evaluation across six sensitive themes (e.g., surrogacy, suicide), seven\nmainstream LLMs preferred CVC-generated options in over 70.5% of cases, while\nfive Chinese human annotators showed an 87.5% alignment with CVC, confirming\nits universality, cultural relevance, and strong alignment with Chinese values.\nAdditionally, we construct 400,000 rule-based moral dilemma scenarios that\nobjectively capture nuanced distinctions in conflicting value prioritization\nacross 17 LLMs. Our work establishes a culturally-adaptive benchmarking\nframework for comprehensive value evaluation and alignment, representing\nChinese characteristics. All data are available at\nhttps://huggingface.co/datasets/Beijing-AISI/CVC, and the code is available at\nhttps://github.com/Beijing-AISI/CVC.", "AI": {"tldr": "The paper proposes a hierarchical value framework based on Chinese values and constructs a large-scale Chinese Values Corpus (CVC) to enhance the alignment of Large Language Models (LLMs) with diverse cultural contexts.", "motivation": "The need to address Western cultural bias and develop a scalable framework for aligning LLMs with universal human values and ethical norms across different cultures, particularly within Chinese cultural contexts.", "method": "A hierarchical value framework was developed to define core Chinese values, resulting in the creation of a large-scale Chinese Values Corpus (CVC) with 250,000 value rules. This corpus was used to generate scenarios which were evaluated against mainstream LLMs.", "result": "CVC-guided scenarios showed improved performance over direct generation in value boundaries and content diversity, with LLMs preferring CVC options in over 70.5% of cases across sensitive themes, and human annotators exhibiting 87.5% alignment with CVC values.", "conclusion": "The study establishes a culturally-adaptive benchmarking framework for comprehensive value evaluation and alignment that represents Chinese characteristics, supporting the safe development of LLMs across diverse cultural contexts.", "key_contributions": ["Development of a hierarchical value framework based on core Chinese values", "Creation of a large-scale Chinese Values Corpus (CVC) with 250,000 value rules", "Establishment of a culturally-adaptive benchmarking framework for LLM value evaluation"], "limitations": "", "keywords": ["Large Language Models", "Cultural Values", "Ethical AI", "Chinese Values", "Value Alignment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01496", "pdf": "https://arxiv.org/pdf/2506.01496.pdf", "abs": "https://arxiv.org/abs/2506.01496", "title": "Continual Speech Learning with Fused Speech Features", "authors": ["Guitao Wang", "Jinming Zhao", "Hao Yang", "Guilin Qi", "Tongtong Wu", "Gholamreza Haffari"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2025", "summary": "Rapid growth in speech data demands adaptive models, as traditional static\nmethods fail to keep pace with dynamic and diverse speech information. We\nintroduce continuous speech learning, a new set-up targeting at bridging the\nadaptation gap in current speech models. We use the encoder-decoder Whisper\nmodel to standardize speech tasks into a generative format. We integrate a\nlearnable gated-fusion layer on the top of the encoder to dynamically select\ntask-specific features for downstream tasks. Our approach improves accuracy\nsignificantly over traditional methods in six speech processing tasks,\ndemonstrating gains in adapting to new speech tasks without full retraining.", "AI": {"tldr": "Introducing continuous speech learning with a gated-fusion layer in the Whisper model for better adaptation in speech tasks.", "motivation": "Traditional static models struggle to adapt to the rapid growth and diversity of speech data.", "method": "The encoder-decoder Whisper model is standardized for generative speech tasks, with an added learnable gated-fusion layer to dynamically select task-specific features.", "result": "Significant accuracy improvements over traditional methods in six speech processing tasks.", "conclusion": "The proposed approach demonstrates effective adaptation to new speech tasks without the need for full retraining.", "key_contributions": ["Introduction of continuous speech learning for adaptive modeling.", "Use of a gated-fusion layer to select features dynamically.", "Significantly improved accuracy in six speech processing tasks."], "limitations": "", "keywords": ["speech learning", "gated-fusion layer", "speech processing", "adaptive models", "Whisper model"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.01512", "pdf": "https://arxiv.org/pdf/2506.01512.pdf", "abs": "https://arxiv.org/abs/2506.01512", "title": "Representations of Fact, Fiction and Forecast in Large Language Models: Epistemics and Attitudes", "authors": ["Meng Li", "Michael Vrazitulis", "David Schlangen"], "categories": ["cs.CL", "cs.AI"], "comment": "accepted by ACL 2025 (main)", "summary": "Rational speakers are supposed to know what they know and what they do not\nknow, and to generate expressions matching the strength of evidence. In\ncontrast, it is still a challenge for current large language models to generate\ncorresponding utterances based on the assessment of facts and confidence in an\nuncertain real-world environment. While it has recently become popular to\nestimate and calibrate confidence of LLMs with verbalized uncertainty, what is\nlacking is a careful examination of the linguistic knowledge of uncertainty\nencoded in the latent space of LLMs. In this paper, we draw on typological\nframeworks of epistemic expressions to evaluate LLMs' knowledge of epistemic\nmodality, using controlled stories. Our experiments show that the performance\nof LLMs in generating epistemic expressions is limited and not robust, and\nhence the expressions of uncertainty generated by LLMs are not always reliable.\nTo build uncertainty-aware LLMs, it is necessary to enrich semantic knowledge\nof epistemic modality in LLMs.", "AI": {"tldr": "This paper examines the limitations of large language models (LLMs) in generating reliable expressions of uncertainty, emphasizing the need for enriched semantic knowledge of epistemic modality for effective uncertainty management.", "motivation": "Current LLMs struggle to generate utterances that accurately reflect their confidence in uncertain real-world situations, which is an essential feature for rational communication.", "method": "The study evaluates LLMs' understanding of epistemic modality by using controlled narratives and typological frameworks of epistemic expressions to assess the generated outputs.", "result": "LLMs exhibited limited and inconsistent performance in generating appropriate epistemic expressions, indicating an inadequacy in expressing uncertainty reliably.", "conclusion": "To enhance LLMs' handling of uncertainty, it is crucial to enrich their semantic knowledge regarding epistemic modalities.", "key_contributions": ["Evaluation of LLMs' linguistic knowledge of uncertainty", "Identification of limitations in current LLMs regarding epistemic expression generation", "Insights into improving LLMs for better uncertainty management"], "limitations": "The performance of LLMs in generating epistemic expressions was found to be not robust, indicating a gap in their linguistic capabilities regarding uncertainty.", "keywords": ["large language models", "epistemic modality", "uncertainty awareness", "HCI", "NLP"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01520", "pdf": "https://arxiv.org/pdf/2506.01520.pdf", "abs": "https://arxiv.org/abs/2506.01520", "title": "FormFactory: An Interactive Benchmarking Suite for Multimodal Form-Filling Agents", "authors": ["Bobo Li", "Yuheng Wang", "Hao Fei", "Juncheng Li", "Wei Ji", "Mong-Li Lee", "Wynne Hsu"], "categories": ["cs.CL"], "comment": "8 pages, 7 figures", "summary": "Online form filling is a common yet labor-intensive task involving extensive\nkeyboard and mouse interactions. Despite the long-standing vision of automating\nthis process with \"one click\", existing tools remain largely rule-based and\nlack generalizable, generative capabilities. Recent advances in Multimodal\nLarge Language Models (MLLMs) have enabled promising agents for GUI-related\ntasks in general-purpose scenarios. However, they struggle with the unique\nchallenges of form filling, such as flexible layouts and the difficulty of\naligning textual instructions with on-screen fields. To bridge this gap, we\nformally define the form-filling task and propose FormFactory, an interactive\nbenchmarking suite comprising a web-based interface, backend evaluation module,\nand carefully constructed dataset. Our benchmark covers diverse real-world\nscenarios, incorporates various field formats, and simulates high-fidelity form\ninteractions. We conduct a comprehensive evaluation of state-of-the-art MLLMs\nand observe that no model surpasses 5% accuracy, underscoring the inherent\ndifficulty of the task. These findings also reveal significant limitations in\ncurrent models' visual layout reasoning and field-value alignment abilities. We\nhope our benchmark can serve as a stepping stone for further research into\nrobust, practical form-filling agents.", "AI": {"tldr": "The paper introduces FormFactory, an interactive benchmarking suite aimed at improving form filling tasks using MLLMs, highlighting the limitations of current models.", "motivation": "Despite advances in MLLMs, existing tools for online form filling are rule-based and struggle with challenges like flexible layouts and aligning instructions with fields.", "method": "We propose FormFactory, a benchmarking suite with a web interface and dataset to evaluate MLLMs on form filling tasks, incorporating diverse scenarios and field formats.", "result": "Comprehensive evaluations show that no MLLM surpasses 5% accuracy on the form filling task, indicating significant challenges in visual reasoning and alignment.", "conclusion": "FormFactory can serve as a stepping stone for research into more effective form-filling agents, addressing the limitations highlighted in our evaluations.", "key_contributions": ["Introduction of FormFactory as a benchmarking suite for form filling tasks.", "Comprehensive evaluation of state-of-the-art MLLMs on this task, revealing significant performance gaps.", "Formal definition of the form-filling task and its unique challenges."], "limitations": "Current MLLMs exhibit poor accuracy and struggle with visual layout reasoning and alignment of field values.", "keywords": ["Form Filling", "Multimodal Large Language Models", "Benchmarking Suite", "Human-Computer Interaction", "User Interfaces"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01524", "pdf": "https://arxiv.org/pdf/2506.01524.pdf", "abs": "https://arxiv.org/abs/2506.01524", "title": "V-VAE: A Variational Auto Encoding Framework Towards Fine-Grained Control over Human-Like Chat", "authors": ["Qi Lin", "Weikai Xu", "Lisi Chen", "Bin Dai"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the continued proliferation of Large Language Model (LLM) based\nchatbots, there is a growing demand for generating responses that are not only\nlinguistically fluent but also consistently aligned with persona-specific\ntraits in conversations. However, existing role-play and persona-based chat\napproaches rely heavily on static role descriptions, coarse-grained signal\nspace, and low-quality synthetic data, which fail to capture dynamic\nfine-grained details in human-like chat. Human-like chat requires modeling\nsubtle latent traits, such as emotional tone, situational awareness, and\nevolving personality, which are difficult to predefine and cannot be easily\nlearned from synthetic or distillation-based data. To address these\nlimitations, we propose a Verbal Variational Auto-Encoding (V-VAE) framework,\ncontaining a variational auto-encoding module and fine-grained control space\nwhich dynamically adapts dialogue behaviour based on fine-grained,\ninterpretable latent variables across talking style, interaction patterns, and\npersonal attributes. We also construct a high-quality dataset, HumanChatData,\nand benchmark HumanChatBench to address the scarcity of high-quality data in\nthe human-like domain. Experiments show that LLMs based on V-VAE consistently\noutperform standard baselines on HumanChatBench and DialogBench, which further\ndemonstrates the effectiveness of V-VAE and HumanChatData.", "AI": {"tldr": "The paper introduces a Verbal Variational Auto-Encoding (V-VAE) framework for enhancing persona-specific response generation in LLM-based chatbots, along with a new dataset and benchmark.", "motivation": "Address limitations in existing persona-based chat approaches, which rely on static descriptions and fail to capture dynamic traits in conversations.", "method": "Proposes a V-VAE framework that adapts dialogue behavior based on fine-grained latent variables related to talking style, interaction patterns, and personal attributes.", "result": "LLMs trained with the V-VAE framework outperform standard baselines on the newly established HumanChatBench and DialogBench benchmarks.", "conclusion": "The V-VAE framework and the HumanChatData dataset effectively improve the quality and relevance of responses in human-like chat applications.", "key_contributions": ["Introduction of the Verbal Variational Auto-Encoding (V-VAE) framework for dynamic persona modeling", "Creation of the HumanChatData dataset for high-quality conversational data", "Benchmarking tools (HumanChatBench) for evaluating LLM performance in human-like dialogue"], "limitations": "", "keywords": ["Large Language Models", "Human-like chat", "Variational Auto-Encoding", "Persona-based dialogue", "HumanChatData"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2506.01531", "pdf": "https://arxiv.org/pdf/2506.01531.pdf", "abs": "https://arxiv.org/abs/2506.01531", "title": "STORM-BORN: A Challenging Mathematical Derivations Dataset Curated via a Human-in-the-Loop Multi-Agent Framework", "authors": ["Wenhao Liu", "Zhenyi Lu", "Xinyu Hu", "Jierui Zhang", "Dailin Li", "Jiacheng Cen", "Huilin Cao", "Haiteng Wang", "Yuhan Li", "Kun Xie", "Dandan Li", "Pei Zhang", "Chengbo Zhang", "Yuxiang Ren", "Xiaohong Huang", "Yan Ma"], "categories": ["cs.CL"], "comment": "accepted by ACL2025", "summary": "High-quality math datasets are crucial for advancing the reasoning abilities\nof large language models (LLMs). However, existing datasets often suffer from\nthree key issues: outdated and insufficient challenging content, neglecting\nhuman-like reasoning, and limited reliability due to single-LLM generation. To\naddress these, we introduce $\\textbf{STORM-BORN}$, an ultra-challenging dataset\nof mathematical derivations sourced from cutting-edge academic papers, which\nincludes dense human-like approximations and heuristic cues. To ensure the\nreliability and quality, we propose a novel human-in-the-loop, multi-agent data\ngeneration framework, integrating reasoning-dense filters, multi-agent\ncollaboration, and human mathematicians' evaluations. We curated a set of 2,000\nsynthetic samples and deliberately selected the 100 most difficult problems.\nEven most advanced models like GPT-o1 solved fewer than $5\\%$ of them.\nFine-tuning on STORM-BORN boosts accuracy by $7.84\\%$ (LLaMA3-8B) and $9.12\\%$\n(Qwen2.5-7B). As AI approaches mathematician-level reasoning, STORM-BORN\nprovides both a high-difficulty benchmark and a human-like reasoning training\nresource. Our code and dataset are publicly available at\nhttps://github.com/lwhere/STORM-BORN.", "AI": {"tldr": "Introduction of STORM-BORN, a challenging math dataset for LLMs, addressing common dataset issues with a novel multi-agent generation framework.", "motivation": "High-quality math datasets are essential for enhancing reasoning capabilities of large language models, yet current datasets have notable deficiencies in difficulty, reasoning quality, and reliability.", "method": "Developed a human-in-the-loop, multi-agent data generation framework combining human evaluations with reasoning-dense filters and collaboration among agents to create math problems derived from academic papers.", "result": "The dataset comprises 2,000 synthetic samples, with 100 challenging problems; advanced models struggled to solve them, and fine-tuning on STORM-BORN improved model accuracy significantly by approximately 8-9%.", "conclusion": "STORM-BORN serves as a high-difficulty benchmark and a resource for training models on human-like reasoning in mathematics, contributing meaningfully to the field.", "key_contributions": ["Creation of the STORM-BORN dataset with a focus on difficult mathematical reasoning", "Introduction of a novel multi-agent generation framework for dataset creation", "Demonstrated significant accuracy improvements for LLMs after fine-tuning on the dataset."], "limitations": "", "keywords": ["dataset", "large language models", "human-like reasoning", "mathematics", "data generation"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.01535", "pdf": "https://arxiv.org/pdf/2506.01535.pdf", "abs": "https://arxiv.org/abs/2506.01535", "title": "Dictionaries to the Rescue: Cross-Lingual Vocabulary Transfer for Low-Resource Languages Using Bilingual Dictionaries", "authors": ["Haruki Sakajo", "Yusuke Ide", "Justin Vasselli", "Yusuke Sakai", "Yingtao Tian", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Cross-lingual vocabulary transfer plays a promising role in adapting\npre-trained language models to new languages, including low-resource languages.\nExisting approaches that utilize monolingual or parallel corpora face\nchallenges when applied to languages with limited resources. In this work, we\npropose a simple yet effective vocabulary transfer method that utilizes\nbilingual dictionaries, which are available for many languages, thanks to\ndescriptive linguists. Our proposed method leverages a property of BPE\ntokenizers where removing a subword from the vocabulary causes a fallback to\nshorter subwords. The embeddings of target subwords are estimated iteratively\nby progressively removing them from the tokenizer. The experimental results\nshow that our approach outperforms existing methods for low-resource languages,\ndemonstrating the effectiveness of a dictionary-based approach for\ncross-lingual vocabulary transfer.", "AI": {"tldr": "This paper presents a new vocabulary transfer method for adapting pre-trained language models to low-resource languages using bilingual dictionaries.", "motivation": "The study addresses the challenges of adapting pre-trained language models to low-resource languages, which often lack sufficient monolingual and parallel corpora.", "method": "The authors propose a method that utilizes bilingual dictionaries alongside BPE tokenizers to iteratively estimate the embeddings of target subwords by removing them from the tokenizer.", "result": "Experimental results indicate that this dictionary-based approach significantly outperforms existing methods for low-resource languages in cross-lingual vocabulary transfer.", "conclusion": "The research highlights the effectiveness of leveraging bilingual dictionaries to enhance vocabulary transfer in language models for low-resource settings.", "key_contributions": ["Introduction of a dictionary-based vocabulary transfer method", "Showcasing improvements over existing methods for low-resource languages", "Utilization of BPE tokenizer properties for effective embedding estimation"], "limitations": "", "keywords": ["cross-lingual transfer", "bilingual dictionaries", "low-resource languages", "BPE tokenizers", "language models"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2506.01565", "pdf": "https://arxiv.org/pdf/2506.01565.pdf", "abs": "https://arxiv.org/abs/2506.01565", "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural Understanding and Transcreation", "authors": ["Li Zhou", "Lutong Yu", "Dongchu Xie", "Shaohuan Cheng", "Wenyan Li", "Haizhou Li"], "categories": ["cs.CL", "cs.CV"], "comment": "cultural analysis, cultural visual understanding, cultural image\n  transcreation", "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.", "AI": {"tldr": "The paper introduces Hanfu-Bench, a multimodal dataset focused on temporal aspects of cultural understanding, specifically through the lens of Hanfu, a traditional Chinese garment.", "motivation": "To address the lack of temporal aspects in cultural understanding studies relying on vision-language models (VLMs) and to provide a framework for evaluating these models in the context of cultural heritage.", "method": "The dataset comprises two tasks: cultural visual understanding, which assesses temporal-cultural feature recognition, and cultural image transcreation, which involves transforming traditional attire into modern designs.", "result": "Closed VLMs performed comparably to non-experts in cultural visual understanding but were 10% behind human experts. For the transcreation task, the best model's success rate is only 42%.", "conclusion": "Hanfu-Bench serves as a benchmark revealing significant challenges in achieving accurate temporal cultural understanding and creative adaptation with VLMs.", "key_contributions": ["Introduction of Hanfu-Bench dataset for temporal cultural understanding", "Evaluation of VLMs against human experts in recognizing cultural features", "Assessment of VLMs in transcreating traditional attire into modern designs"], "limitations": "VLMs significantly underperform compared to human experts, indicating room for improvement in model design and training for cultural understanding tasks.", "keywords": ["cultural analysis", "cultural visual understanding", "cultural image transcreation"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.01578", "pdf": "https://arxiv.org/pdf/2506.01578.pdf", "abs": "https://arxiv.org/abs/2506.01578", "title": "Prompt Engineering Large Language Models' Forecasting Capabilities", "authors": ["Philipp Schoenegger", "Cameron R. Jones", "Philip E. Tetlock", "Barbara Mellers"], "categories": ["cs.CL"], "comment": null, "summary": "Large language model performance can be improved in a large number of ways.\nMany such techniques, like fine-tuning or advanced tool usage, are\ntime-intensive and expensive. Although prompt engineering is significantly\ncheaper and often works for simpler tasks, it remains unclear whether prompt\nengineering suffices for more complex domains like forecasting. Here we show\nthat small prompt modifications rarely boost forecasting accuracy beyond a\nminimal baseline. In our first study, we tested 38 prompts across Claude 3.5\nSonnet, Claude 3.5 Haiku, GPT-4o, and Llama 3.1 405B. In our second, we\nintroduced compound prompts and prompts from external sources, also including\nthe reasoning models o1 and o1-mini. Our results show that most prompts lead to\nnegligible gains, although references to base rates yield slight benefits.\nSurprisingly, some strategies showed strong negative effects on accuracy:\nespecially encouraging the model to engage in Bayesian reasoning. These results\nsuggest that, in the context of complex tasks like forecasting, basic prompt\nrefinements alone offer limited gains, implying that more robust or specialized\ntechniques may be required for substantial performance improvements in AI\nforecasting.", "AI": {"tldr": "This paper investigates the effectiveness of prompt engineering in improving large language model performance for complex tasks like forecasting.", "motivation": "To explore the capacity of prompt engineering to enhance accuracy in AI forecasting, which is often regarded as less effective for complex domains compared to robust methods.", "method": "The study evaluated 38 prompts across several large language models and introduced both compound prompts and prompts from external sources, including models designed for reasoning.", "result": "The findings revealed that most prompt modifications led to minimal gains in forecasting accuracy, with some prompts even having negative impacts, particularly those encouraging Bayesian reasoning.", "conclusion": "The research indicates that simple prompt engineering is insufficient for improving performance in complex forecasting tasks, suggesting the need for more sophisticated techniques.", "key_contributions": ["Evaluation of 38 prompts across multiple large language models", "Identification of minimal gains from prompt modifications in complex tasks", "Highlighting negative impacts of certain prompting strategies on accuracy"], "limitations": "The study focuses only on specific forecasting tasks and may not be generalizable to all areas where large language models are applied.", "keywords": ["prompt engineering", "forecasting", "large language models", "Bayesian reasoning", "model evaluation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01587", "pdf": "https://arxiv.org/pdf/2506.01587.pdf", "abs": "https://arxiv.org/abs/2506.01587", "title": "Unified Large Language Models for Misinformation Detection in Low-Resource Linguistic Settings", "authors": ["Muhammad Islam", "Javed Ali Khan", "Mohammed Abaker", "Ali Daud", "Azeem Irshad"], "categories": ["cs.CL"], "comment": null, "summary": "The rapid expansion of social media platforms has significantly increased the\ndissemination of forged content and misinformation, making the detection of\nfake news a critical area of research. Although fact-checking efforts\npredominantly focus on English-language news, there is a noticeable gap in\nresources and strategies to detect news in regional languages, such as Urdu.\nAdvanced Fake News Detection (FND) techniques rely heavily on large, accurately\nlabeled datasets. However, FND in under-resourced languages like Urdu faces\nsubstantial challenges due to the scarcity of extensive corpora and the lack of\nvalidated lexical resources. Current Urdu fake news datasets are often\ndomain-specific and inaccessible to the public. They also lack human\nverification, relying mainly on unverified English-to-Urdu translations, which\ncompromises their reliability in practical applications. This study highlights\nthe necessity of developing reliable, expert-verified, and domain-independent\nUrdu-enhanced FND datasets to improve fake news detection in Urdu and other\nresource-constrained languages. This paper presents the first benchmark large\nFND dataset for Urdu news, which is publicly available for validation and deep\nanalysis. We also evaluate this dataset using multiple state-of-the-art\npre-trained large language models (LLMs), such as XLNet, mBERT, XLM-RoBERTa,\nRoBERTa, DistilBERT, and DeBERTa. Additionally, we propose a unified LLM model\nthat outperforms the others with different embedding and feature extraction\ntechniques. The performance of these models is compared based on accuracy, F1\nscore, precision, recall, and human judgment for vetting the sample results of\nnews.", "AI": {"tldr": "This study focuses on developing a reliable dataset for fake news detection in Urdu, addressing the scarcity of resources and validating it using advanced language models.", "motivation": "The proliferation of misinformation in social media and the lack of resources for detecting fake news in under-resourced languages, particularly Urdu, necessitates the creation of reliable datasets.", "method": "The study presents the first publicly available benchmark for Urdu fake news detection, using multiple state-of-the-art pre-trained large language models (LLMs) for evaluation and comparison.", "result": "The proposed Urdu FND dataset supports validation and deep analysis, and a unified LLM model is shown to outperform existing models in the detection task.", "conclusion": "The development of a robust Urdu-enhanced FND dataset is essential for improving fake news detection in both Urdu and other resource-constrained languages, indicating the potential for similar efforts in other languages.", "key_contributions": ["Creation of the first benchmark dataset for Urdu fake news detection.", "Evaluation of several state-of-the-art LLMs on the dataset.", "Proposal of a unified LLM model that surpasses existing models in performance."], "limitations": "The focus on a specific language may limit generalization to other languages or domains.", "keywords": ["Fake News Detection", "Urdu Language", "Large Language Models", "Dataset Creation", "Misinformation"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.01592", "pdf": "https://arxiv.org/pdf/2506.01592.pdf", "abs": "https://arxiv.org/abs/2506.01592", "title": "Statement-Tuning Enables Efficient Cross-lingual Generalization in Encoder-only Models", "authors": ["Ahmed Elshabrawy", "Thanh-Nhi Nguyen", "Yeeun Kang", "Lihan Feng", "Annant Jain", "Faadil Abdullah Shaikh", "Jonibek Mansurov", "Mohamed Fazli Mohamed Imam", "Jesus-German Ortiz-Barajas", "Rendi Chevi", "Alham Fikri Aji"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large Language Models (LLMs) excel in zero-shot and few-shot tasks, but\nachieving similar performance with encoder-only models like BERT and RoBERTa\nhas been challenging due to their architecture. However, encoders offer\nadvantages such as lower computational and memory costs. Recent work adapts\nthem for zero-shot generalization using Statement Tuning, which reformulates\ntasks into finite templates. We extend this approach to multilingual NLP,\nexploring whether encoders can achieve zero-shot cross-lingual generalization\nand serve as efficient alternatives to memory-intensive LLMs for low-resource\nlanguages. Our results show that state-of-the-art encoder models generalize\nwell across languages, rivaling multilingual LLMs while being more efficient.\nWe also analyze multilingual Statement Tuning dataset design, efficiency gains,\nand language-specific generalization, contributing to more inclusive and\nresource-efficient NLP models. We release our code and models.", "AI": {"tldr": "This paper extends Statement Tuning to multilingual NLP, demonstrating that encoder-only models like BERT and RoBERTa can achieve competitive zero-shot cross-lingual generalization while being more efficient than large language models.", "motivation": "To explore if encoder-only models can rival LLMs in zero-shot cross-lingual generalization and offer efficient alternatives for low-resource languages.", "method": "The paper adapts Statement Tuning to reformulate multilingual NLP tasks into templates suitable for encoder models and evaluates the performance of state-of-the-art encoders in a zero-shot context.", "result": "Findings reveal that encoder models achieve competitive performance in multilingual tasks, similar to multilingual LLMs, while consuming fewer resources.", "conclusion": "Encoders can effectively generalize across languages, suggesting a shift towards more resource-efficient NLP models for multilingual applications.", "key_contributions": ["Adapting Statement Tuning for multilingual NLP", "Demonstrating competitive performance of encoders in zero-shot tasks", "Providing insights into dataset design and efficiency gains for low-resource languages"], "limitations": "The approach may still struggle with highly complex or nuanced language-specific tasks compared to LLMs.", "keywords": ["multilingual NLP", "zero-shot learning", "Statement Tuning", "encoder-only models", "resource-efficient models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01602", "pdf": "https://arxiv.org/pdf/2506.01602.pdf", "abs": "https://arxiv.org/abs/2506.01602", "title": "MMD-Sense-Analysis: Word Sense Detection Leveraging Maximum Mean Discrepancy", "authors": ["Kensuke Mitsuzawa"], "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Word sense analysis is an essential analysis work for interpreting the\nlinguistic and social backgrounds. The word sense change detection is a task of\nidentifying and interpreting shifts in word meanings over time. This paper\nproposes MMD-Sense-Analysis, a novel approach that leverages Maximum Mean\nDiscrepancy (MMD) to select semantically meaningful variables and quantify\nchanges across time periods. This method enables both the identification of\nwords undergoing sense shifts and the explanation of their evolution over\nmultiple historical periods. To my knowledge, this is the first application of\nMMD to word sense change detection. Empirical assessment results demonstrate\nthe effectiveness of the proposed approach.", "AI": {"tldr": "This paper proposes MMD-Sense-Analysis, a novel method using Maximum Mean Discrepancy to detect and analyze shifts in word meanings over time.", "motivation": "To identify and interpret shifts in word meanings, which is important for understanding linguistic and social changes.", "method": "The approach uses Maximum Mean Discrepancy (MMD) to select semantically meaningful variables and measure changes across different time periods.", "result": "The empirical results show that MMD-Sense-Analysis effectively identifies words with sense shifts and explains their evolution over time.", "conclusion": "This is the first use of MMD for word sense change detection, highlighting its potential for linguistic analysis.", "key_contributions": ["First application of MMD to word sense change detection", "Identification and interpretation of word meaning shifts over time", "Empirical validation of the approach's effectiveness"], "limitations": "", "keywords": ["word sense analysis", "maximum mean discrepancy", "sense change detection", "linguistic evolution", "semantic analysis"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.01615", "pdf": "https://arxiv.org/pdf/2506.01615.pdf", "abs": "https://arxiv.org/abs/2506.01615", "title": "IndicRAGSuite: Large-Scale Datasets and a Benchmark for Indian Language RAG Systems", "authors": ["Pasunuti Prasanjith", "Prathmesh B More", "Anoop Kunchukuttan", "Raj Dabre"], "categories": ["cs.CL"], "comment": "WIP", "summary": "Retrieval-Augmented Generation (RAG) systems enable language models to access\nrelevant information and generate accurate, well-grounded, and contextually\ninformed responses. However, for Indian languages, the development of\nhigh-quality RAG systems is hindered by the lack of two critical resources: (1)\nevaluation benchmarks for retrieval and generation tasks, and (2) large-scale\ntraining datasets for multilingual retrieval. Most existing benchmarks and\ndatasets are centered around English or high-resource languages, making it\ndifficult to extend RAG capabilities to the diverse linguistic landscape of\nIndia. To address the lack of evaluation benchmarks, we create IndicMSMarco, a\nmultilingual benchmark for evaluating retrieval quality and response generation\nin 13 Indian languages, created via manual translation of 1000 diverse queries\nfrom MS MARCO-dev set. To address the need for training data, we build a\nlarge-scale dataset of (question, answer, relevant passage) tuples derived from\nthe Wikipedias of 19 Indian languages using state-of-the-art LLMs.\nAdditionally, we include translated versions of the original MS MARCO dataset\nto further enrich the training data and ensure alignment with real-world\ninformation-seeking tasks. Resources are available here:\nhttps://huggingface.co/datasets/ai4bharat/Indic-Rag-Suite", "AI": {"tldr": "This paper presents the IndicMSMarco multilingual benchmark and a large-scale dataset for improving Retrieval-Augmented Generation (RAG) systems in Indian languages.", "motivation": "The development of high-quality RAG systems for Indian languages is limited due to the scarcity of evaluation benchmarks and large-scale datasets for multilingual retrieval.", "method": "Creation of IndicMSMarco, a multilingual benchmark evaluating retrieval quality and response generation in 13 Indian languages, and a dataset of (question, answer, relevant passage) tuples derived from 19 Indian language Wikipedias.", "result": "Introduced IndicMSMarco benchmark and a large-scale dataset that enhances training and evaluation capabilities for RAG systems in Indian languages.", "conclusion": "The new resources will facilitate the advancement of RAG systems in Indian languages, addressing significant shortcomings in current evaluations and training datasets.", "key_contributions": ["Creation of IndicMSMarco, a multilingual benchmark for Indian languages.", "Development of a large-scale dataset from Indian language Wikipedias for RAG training.", "Inclusion of translated MS MARCO dataset versions to improve data alignment."], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Indian languages", "evaluation benchmarks", "multilingual datasets", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01621", "pdf": "https://arxiv.org/pdf/2506.01621.pdf", "abs": "https://arxiv.org/abs/2506.01621", "title": "Domain Lexical Knowledge-based Word Embedding Learning for Text Classification under Small Data", "authors": ["Zixiao Zhu", "Kezhi Mao"], "categories": ["cs.CL"], "comment": "13 pages, 2 figures", "summary": "Pre-trained language models such as BERT have been proved to be powerful in\nmany natural language processing tasks. But in some text classification\napplications such as emotion recognition and sentiment analysis, BERT may not\nlead to satisfactory performance. This often happens in applications where\nkeywords play critical roles in the prediction of class labels. Our\ninvestigation found that the root cause of the problem is that the\ncontext-based BERT embedding of the keywords may not be discriminative enough\nto produce discriminative text representation for classification. Motivated by\nthis finding, we develop a method to enhance word embeddings using\ndomain-specific lexical knowledge. The knowledge-based embedding enhancement\nmodel projects the BERT embedding into a new space where within-class\nsimilarity and between-class difference are maximized. To implement the\nknowledge-based word embedding enhancement model, we also develop a knowledge\nacquisition algorithm for automatically collecting lexical knowledge from\nonline open sources. Experiment results on three classification tasks,\nincluding sentiment analysis, emotion recognition and question answering, have\nshown the effectiveness of our proposed word embedding enhancing model. The\ncodes and datasets are in https://github.com/MidiyaZhu/KVWEFFER.", "AI": {"tldr": "This paper presents a method to enhance BERT embeddings for text classification tasks by incorporating domain-specific lexical knowledge, leading to improved performance in sentiment analysis, emotion recognition, and question answering.", "motivation": "To address the limitations of BERT in text classification applications where keywords are crucial for predicting class labels.", "method": "A knowledge-based embedding enhancement model is developed that projects BERT embeddings into a space maximizing within-class similarity and between-class difference, along with a knowledge acquisition algorithm to collect lexical knowledge from online sources.", "result": "Experiment results indicate significant improvements in text classification performance over traditional BERT embeddings on sentiment analysis, emotion recognition, and question answering tasks.", "conclusion": "The proposed word embedding enhancement model effectively improves classification accuracy in applications where keyword relevance is high, showcasing the importance of integrating lexical knowledge into embeddings.", "key_contributions": ["Development of a knowledge-based embedding enhancement model for BERT.", "Introduction of a knowledge acquisition algorithm for lexical knowledge extraction.", "Demonstrated improved classification performance on multiple NLP tasks."], "limitations": "", "keywords": ["BERT", "Text Classification", "Sentiment Analysis", "Emotion Recognition", "Lexical Knowledge"], "importance_score": 8, "read_time_minutes": 13}}
{"id": "2506.01627", "pdf": "https://arxiv.org/pdf/2506.01627.pdf", "abs": "https://arxiv.org/abs/2506.01627", "title": "MVAN: Multi-View Attention Networks for Fake News Detection on Social Media", "authors": ["Shiwen Ni", "Jiawen Li", "Hung-Yu Kao"], "categories": ["cs.CL"], "comment": null, "summary": "Fake news on social media is a widespread and serious problem in today's\nsociety. Existing fake news detection methods focus on finding clues from Long\ntext content, such as original news articles and user comments. This paper\nsolves the problem of fake news detection in more realistic scenarios. Only\nsource shot-text tweet and its retweet users are provided without user\ncomments. We develop a novel neural network based model,\n\\textbf{M}ulti-\\textbf{V}iew \\textbf{A}ttention \\textbf{N}etworks (MVAN) to\ndetect fake news and provide explanations on social media. The MVAN model\nincludes text semantic attention and propagation structure attention, which\nensures that our model can capture information and clues both of source tweet\ncontent and propagation structure. In addition, the two attention mechanisms in\nthe model can find key clue words in fake news texts and suspicious users in\nthe propagation structure. We conduct experiments on two real-world datasets,\nand the results demonstrate that MVAN can significantly outperform\nstate-of-the-art methods by 2.5\\% in accuracy on average, and produce a\nreasonable explanation.", "AI": {"tldr": "This paper presents a novel neural network model, MVAN, for detecting fake news on social media using only tweet content and its retweets, outperforming existing methods by 2.5% in accuracy.", "motivation": "Address the serious problem of fake news on social media, particularly in scenarios with limited input data like short tweets and retweets, where existing methods fall short.", "method": "Developed the Multi-View Attention Networks (MVAN) model which combines text semantic attention and propagation structure attention to effectively detect fake news and provide explanations.", "result": "MVAN significantly outperforms state-of-the-art methods by an average of 2.5% in accuracy on two real-world datasets and produces reasonable explanations for its detections.", "conclusion": "The MVAN model demonstrates that it is viable to detect fake news effectively in more realistic scenarios with limited data while providing explanatory insights.", "key_contributions": ["Introduction of the MVAN model for fake news detection using short tweets.", "Combination of two attention mechanisms to extract key clues from texts and user propagation structures.", "Empirical results proving the model's superiority over existing detection methods."], "limitations": "", "keywords": ["fake news detection", "social media", "neural networks", "Multi-View Attention Networks", "explanations"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.01629", "pdf": "https://arxiv.org/pdf/2506.01629.pdf", "abs": "https://arxiv.org/abs/2506.01629", "title": "Cross-Lingual Generalization and Compression: From Language-Specific to Shared Neurons", "authors": ["Frederick Riemenschneider", "Anette Frank"], "categories": ["cs.CL", "I.2.4; I.2.7"], "comment": "Paper accepted for publication at ACL 2025 Main; 10 pages, 20\n  figures, 4 tables", "summary": "Multilingual language models (MLLMs) have demonstrated remarkable abilities\nto transfer knowledge across languages, despite being trained without explicit\ncross-lingual supervision. We analyze the parameter spaces of three MLLMs to\nstudy how their representations evolve during pre-training, observing patterns\nconsistent with compression: models initially form language-specific\nrepresentations, which gradually converge into cross-lingual abstractions as\ntraining progresses. Through probing experiments, we observe a clear transition\nfrom uniform language identification capabilities across layers to more\nspecialized layer functions. For deeper analysis, we focus on neurons that\nencode distinct semantic concepts. By tracing their development during\npre-training, we show how they gradually align across languages. Notably, we\nidentify specific neurons that emerge as increasingly reliable predictors for\nthe same concepts across languages.", "AI": {"tldr": "This paper explores the evolution of parameter spaces in multilingual language models during pre-training, revealing the transition from language-specific to cross-lingual representations.", "motivation": "To understand how multilingual language models develop their ability to transfer knowledge across languages without explicit supervision.", "method": "The authors analyze the parameter spaces of three multilingual language models, conducting probing experiments to examine the evolution of language representations during pre-training.", "result": "The study finds that multilingual models transition from forming language-specific representations to developing cross-lingual abstractions, with specific neurons aligning across languages as reliable predictors of the same concepts.", "conclusion": "The paper concludes that multilingual language models exhibit clear patterns of parameter evolution that enhance their cross-lingual capabilities, suggesting a deep interconnection between language representations as training progresses.", "key_contributions": ["Analysis of parameter evolution in multilingual language models during pre-training.", "Identification of neurons that become reliable predictors across languages.", "Insights into the transition from language-specific to cross-lingual representations."], "limitations": "", "keywords": ["multilingual language models", "cross-lingual representation", "neuron alignment"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01646", "pdf": "https://arxiv.org/pdf/2506.01646.pdf", "abs": "https://arxiv.org/abs/2506.01646", "title": "ESGenius: Benchmarking LLMs on Environmental, Social, and Governance (ESG) and Sustainability Knowledge", "authors": ["Chaoyue He", "Xin Zhou", "Yi Wu", "Xinjia Yu", "Yan Zhang", "Lei Zhang", "Di Wang", "Shengfei Lyu", "Hong Xu", "Xiaoqiao Wang", "Wei Liu", "Chunyan Miao"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3"], "comment": "37 pages, 8 figures, 11 tables", "summary": "We introduce ESGenius, a comprehensive benchmark for evaluating and enhancing\nthe proficiency of Large Language Models (LLMs) in Environmental, Social and\nGovernance (ESG) and sustainability-focused question answering. ESGenius\ncomprises two key components: (i) ESGenius-QA, a collection of 1 136\nmultiple-choice questions generated by LLMs and rigorously validated by domain\nexperts, covering a broad range of ESG pillars and sustainability topics. Each\nquestion is systematically linked to its corresponding source text, enabling\ntransparent evaluation and supporting retrieval-augmented generation (RAG)\nmethods; and (ii) ESGenius-Corpus, a meticulously curated repository of 231\nfoundational frameworks, standards, reports and recommendation documents from\nseven authoritative sources. Moreover, to fully assess the capabilities and\nadaptation potential of the model, we implement a rigorous two-stage evaluation\nprotocol -- Zero-Shot and RAG. Extensive experiments across 50 LLMs (ranging\nfrom 0.5 B to 671 B parameters) demonstrate that state-of-the-art models\nachieve only moderate performance in zero-shot settings, with accuracies\ntypically around 55--70\\%, highlighting ESGenius's challenging nature for LLMs\nin interdisciplinary contexts. However, models employing RAG show significant\nperformance improvements, particularly for smaller models. For example,\n\"DeepSeek-R1-Distill-Qwen-14B\" improves from 63.82\\% (zero-shot) to 80.46\\%\nwith RAG. These results underscore the necessity of grounding responses in\nauthoritative sources for enhanced ESG understanding. To the best of our\nknowledge, ESGenius is the first benchmark curated for LLMs and the relevant\nenhancement technologies that focuses on ESG and sustainability topics.", "AI": {"tldr": "ESGenius is a benchmark designed to evaluate Large Language Models' (LLMs) proficiency in ESG and sustainability-focused question answering through a two-component system that includes a QA dataset and a curated corpus of related documents.", "motivation": "There is a growing need for LLMs to effectively understand and respond to questions related to Environmental, Social and Governance (ESG) topics and sustainability, which are increasingly important in various fields.", "method": "The benchmark includes ESGenius-QA, a set of 1,136 rigorously validated multiple-choice questions, and ESGenius-Corpus, a collection of authoritative documents for contextual grounding. A two-stage evaluation approach (Zero-Shot and RAG) was employed across 50 LLMs.", "result": "Results show that state-of-the-art models achieve moderate performance (55-70% accuracy) in zero-shot settings, while models using RAG techniques exhibit significant improvements, with some models improving from around 64% to over 80% accuracy.", "conclusion": "ESGenius demonstrates that grounding model responses in reliable sources is crucial for improving understanding of ESG topics, highlighting the potential for RAG methods to enhance LLM performance.", "key_contributions": ["Introduction of ESGenius as the first benchmark for ESG and sustainability in LLMs", "Development of ESGenius-QA with validated questions linked to authoritative sources", "Demonstration of significant performance gains in LLMs using retrieval-augmented generation."], "limitations": "The performance of LLMs is moderate even in RAG settings for some tasks, indicating limitations in current model capabilities.", "keywords": ["Large Language Models", "ESG", "sustainability", "evaluation", "retrieval-augmented generation"], "importance_score": 7, "read_time_minutes": 20}}
{"id": "2506.01675", "pdf": "https://arxiv.org/pdf/2506.01675.pdf", "abs": "https://arxiv.org/abs/2506.01675", "title": "Cross-Lingual Transfer of Cultural Knowledge: An Asymmetric Phenomenon", "authors": ["Chen Zhang", "Zhiyuan Liao", "Yansong Feng"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Despite substantial research efforts evaluating how well large language\nmodels~(LLMs) handle global cultural diversity, the mechanisms behind their\ncultural knowledge acquisition, particularly in multilingual settings, remain\nunclear. We study this question by investigating how cultural knowledge\ntransfers across languages during language adaptation of LLMs. We introduce an\ninterpretable framework for studying this transfer, ensuring training data\ntransparency and controlling transfer effects. Through a study of four\nnon-Anglophonic cultures, we observe bidirectional cultural transfer between\nEnglish and other high-resource languages, while low-resource languages\nprimarily transfer knowledge to English with limited reverse flow. To explain\nthis asymmetric phenomenon, we propose a frequency-based hypothesis: cultural\nknowledge appearing more frequently in the pretraining data transfers more\neasily, which is supported by empirical analysis of the training corpora.", "AI": {"tldr": "This paper investigates how cultural knowledge transfers across languages during the adaptation of large language models, revealing asymmetric knowledge transfer patterns between high- and low-resource languages.", "motivation": "To understand the mechanisms behind cultural knowledge acquisition in multilingual settings and its impact on large language models.", "method": "An interpretable framework was introduced to study cultural knowledge transfer across languages, controlling for data transparency and transfer effects. The study analyzed four non-Anglophonic cultures.", "result": "Bidirectional cultural transfer was observed between English and high-resource languages, while low-resource languages predominantly transferred knowledge to English, demonstrating an asymmetric transfer pattern.", "conclusion": "The study concludes that cultural knowledge transfer is influenced by the frequency of cultural knowledge in pretraining data, with more frequent knowledge being transferred more easily between languages.", "key_contributions": ["Introduced a framework for studying cultural knowledge transfer in LLMs.", "Identified asymmetric knowledge transfer patterns based on resource language status.", "Proposed and validated a frequency-based hypothesis related to knowledge transfer."], "limitations": "Focused primarily on the relationship between English and other languages, which may limit insights into other linguistic dynamics.", "keywords": ["Cultural knowledge", "Language adaptation", "Large language models", "Multilingualism", "Knowledge transfer"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01687", "pdf": "https://arxiv.org/pdf/2506.01687.pdf", "abs": "https://arxiv.org/abs/2506.01687", "title": "StochasTok: Improving Fine-Grained Subword Understanding in LLMs", "authors": ["Anya Sims", "Thom Foster", "Klara Kaleb", "Tuan-Duy H. Nguyen", "Joseph Lee", "Jakob N. Foerster", "Yee Whye Teh", "Cong Lu"], "categories": ["cs.CL"], "comment": null, "summary": "Subword-level understanding is integral to numerous tasks, including\nunderstanding multi-digit numbers, spelling mistakes, abbreviations, rhyming,\nand wordplay. Despite this, current large language models (LLMs) still often\nstruggle with seemingly simple subword-level tasks like How many 'r's in\n'strawberry'?. A key factor behind these failures is tokenization which\nobscures the fine-grained structure of words. Current alternatives, such as\ncharacter-level and dropout tokenization methods, significantly increase\ncomputational costs and provide inconsistent improvements. In this paper we\nrevisit tokenization and introduce StochasTok, a simple, efficient stochastic\ntokenization scheme that randomly splits tokens during training, allowing LLMs\nto 'see' their internal structure. Our experiments show that pretraining with\nStochasTok substantially improves LLMs' downstream performance across multiple\nsubword-level language games, including character counting, substring\nidentification, and math tasks. Furthermore, StochasTok's simplicity allows\nseamless integration at any stage of the training pipeline; and we demonstrate\nthat post-training with StochasTok can instill improved subword understanding\ninto existing pretrained models, thus avoiding costly pretraining from scratch.\nThese dramatic improvements achieved with a minimal change suggest StochasTok\nholds exciting potential when applied to larger, more capable models. Code\nopen-sourced at: https://github.com/anyasims/stochastok.", "AI": {"tldr": "This paper introduces StochasTok, a stochastic tokenization scheme that enhances LLMs' ability to understand subword-level tasks by allowing them to see the internal structure of tokens during training.", "motivation": "Current LLMs struggle with subword-level tasks due to tokenization methods that obscure the fine-grained structure of words, leading to poor performance on simple tasks.", "method": "StochasTok randomly splits tokens during training, enabling LLMs to better grasp their internal structure, and can be integrated at any stage of the training pipeline.", "result": "Pretraining with StochasTok significantly improves LLMs' performance on subword-level tasks, such as character counting, substring identification, and math tasks.", "conclusion": "StochasTok's simplicity and efficiency hold potential for enhancing larger models without the need for costly pretraining.", "key_contributions": ["Introduction of StochasTok for improved subword understanding in LLMs", "Demonstrated effectiveness across multiple tasks with minimal training changes", "Possibility to enhance existing pretrained models post-training"], "limitations": "", "keywords": ["Tokenization", "Large Language Models", "Subword Understanding"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01698", "pdf": "https://arxiv.org/pdf/2506.01698.pdf", "abs": "https://arxiv.org/abs/2506.01698", "title": "When LLMs Team Up: The Emergence of Collaborative Affective Computing", "authors": ["Wenna Lai", "Haoran Xie", "Guandong Xu", "Qing Li", "S. Joe Qin"], "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 7 figures, and 3 tables", "summary": "Affective Computing (AC) is essential in bridging the gap between human\nemotional experiences and machine understanding. Traditionally, AC tasks in\nnatural language processing (NLP) have been approached through pipeline\narchitectures, which often suffer from structure rigidity that leads to\ninefficiencies and limited adaptability. The advent of Large Language Models\n(LLMs) has revolutionized this field by offering a unified approach to\naffective understanding and generation tasks, enhancing the potential for\ndynamic, real-time interactions. However, LLMs face cognitive limitations in\naffective reasoning, such as misinterpreting cultural nuances or contextual\nemotions, and hallucination problems in decision-making. To address these\nchallenges, recent research advocates for LLM-based collaboration systems that\nemphasize interactions among specialized models and LLMs, mimicking human-like\naffective intelligence through the synergy of emotional and rational thinking\nthat aligns with Dual Process Theory in psychology. This survey aims to provide\na comprehensive overview of LLM-based collaboration systems in AC, exploring\nfrom structured collaborations to autonomous collaborations. Specifically, it\nincludes: (1) A systematic review of existing methods, focusing on\ncollaboration strategies, mechanisms, key functions, and applications; (2)\nExperimental comparisons of collaboration strategies across representative\ntasks in affective understanding and generation; (3) An analysis highlighting\nthe potential of these systems to enhance robustness and adaptability in\ncomplex affective reasoning; (4) A discussion of key challenges and future\nresearch directions to further advance the field. This work is the first to\nsystematically explore collaborative intelligence with LLMs in AC, paving the\nway for more powerful applications that approach human-like social\nintelligence.", "AI": {"tldr": "This survey explores LLM-based collaborative systems in Affective Computing, highlighting methods, experimental comparisons, and future research directions.", "motivation": "To address the limitations of traditional affective computing tasks and enhance real-time human-machine interactions through collaborative Large Language Models.", "method": "A systematic review of collaboration strategies, experimental comparisons across tasks, and analysis of key challenges in LLM-based collaboration in Affective Computing.", "result": "The study identifies various collaboration strategies that enhance robustness and adaptability in affective reasoning, demonstrating improved interactions and potential applications.", "conclusion": "LLM-based collaboration in Affective Computing can pave the way for applications that better mimic human-like emotional and rational intelligence.", "key_contributions": ["Systematic review of current LLM-based collaboration methods in affective computing.", "Experimental comparisons of different collaboration strategies.", "Discussion on key challenges and future research directions in this area."], "limitations": "Cognitive limitations of LLMs in affective reasoning, including cultural nuances and hallucination issues.", "keywords": ["Affective Computing", "Large Language Models", "Human-Computer Interaction", "Collaboration Systems", "Emotional Intelligence"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2506.01702", "pdf": "https://arxiv.org/pdf/2506.01702.pdf", "abs": "https://arxiv.org/abs/2506.01702", "title": "mdok of KInIT: Robustly Fine-tuned LLM for Binary and Multiclass AI-Generated Text Detection", "authors": ["Dominik Macko"], "categories": ["cs.CL"], "comment": null, "summary": "The large language models (LLMs) are able to generate high-quality texts in\nmultiple languages. Such texts are often not recognizable by humans as\ngenerated, and therefore present a potential of LLMs for misuse (e.g.,\nplagiarism, spams, disinformation spreading). An automated detection is able to\nassist humans to indicate the machine-generated texts; however, its robustness\nto out-of-distribution data is still challenging. This notebook describes our\nmdok approach in robust detection, based on fine-tuning smaller LLMs for text\nclassification. It is applied to both subtasks of Voight-Kampff Generative AI\nDetection 2025, providing remarkable performance in binary detection as well as\nin multiclass (1st rank) classification of various cases of human-AI\ncollaboration.", "AI": {"tldr": "The paper presents an automated approach for detecting machine-generated texts using fine-tuned smaller language models, addressing challenges in robustness against out-of-distribution data.", "motivation": "There is a growing concern about the misuse of language models for generating unrecognizable texts that can lead to plagiarism and disinformation.", "method": "The mdok approach uses fine-tuning of smaller LLMs for text classification tasks.", "result": "The approach demonstrated remarkable performance in both binary detection and multiclass classification in the Voight-Kampff Generative AI Detection 2025.", "conclusion": "The proposed method shows promise in improving the detection of machine-generated texts, particularly in contexts involving human-AI collaboration.", "key_contributions": ["Proposed mdok approach for robust detection of AI-generated text using fine-tuned smaller LLMs.", "Achieved outstanding results in binary and multiclass detection tasks.", "Addressed robustness issues with out-of-distribution data."], "limitations": "", "keywords": ["large language models", "text classification", "machine-generated text detection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01709", "pdf": "https://arxiv.org/pdf/2506.01709.pdf", "abs": "https://arxiv.org/abs/2506.01709", "title": "Fairness Dynamics During Training", "authors": ["Krishna Patel", "Nivedha Sivakumar", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "categories": ["cs.CL"], "comment": null, "summary": "We investigate fairness dynamics during Large Language Model (LLM) training\nto enable the diagnoses of biases and mitigations through training\ninterventions like early stopping; we find that biases can emerge suddenly and\ndo not always follow common performance metrics. We introduce two new metrics\nto evaluate fairness dynamics holistically during model pre-training: Average\nRank and Jensen-Shannon Divergence by Parts. These metrics provide insights\ninto the Pythia models' progression of biases in gender prediction of\noccupations on the WinoBias dataset. By monitoring these dynamics, we find that\n(1) Pythia-6.9b is biased towards men; it becomes more performant and confident\npredicting \"male\" than \"female\" during training, (2) via early-stopping,\nPythia-6.9b can exchange 1.7% accuracy on LAMBADA for a 92.5% increase in\nfairness, and (3) larger models can exhibit more bias; Pythia-6.9b makes more\nassumptions about gender than Pythia-160m, even when a subject's gender is not\nspecified.", "AI": {"tldr": "Study of fairness dynamics in LLM training and their impact on biases during model pre-training.", "motivation": "To diagnose biases and their mitigations in LLMs through training interventions such as early stopping.", "method": "Introduction of new metrics (Average Rank and Jensen-Shannon Divergence by Parts) to evaluate and monitor fairness dynamics during pre-training on the WinoBias dataset.", "result": "Pythia-6.9b exhibits gender bias, being more confident in predicting male occupations and showing a significant trade-off between accuracy and fairness through early stopping.", "conclusion": "Monitoring fairness dynamics is crucial as larger models can exhibit more bias, necessitating interventions to improve fairness without severely impacting accuracy.", "key_contributions": ["Introduction of new fairness metrics", "Findings on trade-off between accuracy and fairness", "Insights on bias evolution during LLM training"], "limitations": "", "keywords": ["fairness dynamics", "Large Language Models", "bias diagnosis", "early stopping", "WinoBias dataset"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01710", "pdf": "https://arxiv.org/pdf/2506.01710.pdf", "abs": "https://arxiv.org/abs/2506.01710", "title": "Reasoning-Table: Exploring Reinforcement Learning for Table Reasoning", "authors": ["Fangyu Lei", "Jinxiang Meng", "Yiming Huang", "Tinghong Chen", "Yun Zhang", "Shizhu He", "Jun Zhao", "Kang Liu"], "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Table reasoning, encompassing tasks such as table question answering, fact\nverification, and text-to-SQL, requires precise understanding of structured\ntabular data, coupled with numerical computation and code manipulation for\neffective inference. Supervised fine-tuning (SFT) approaches have achieved\nnotable success but often struggle with generalization and robustness due to\nbiases inherent in imitative learning. We introduce Reasoning-Table, the first\napplication of reinforcement learning (RL) to table reasoning, achieving\nstate-of-the-art performance. Through rigorous data preprocessing, reward\ndesign, and tailored training strategies, our method leverages simple\nrule-based outcome rewards to outperform SFT across multiple benchmarks.\nUnified training across diverse tasks enables Reasoning-Table to emerge as a\nrobust table reasoning large language model, surpassing larger proprietary\nmodels like Claude-3.7-Sonnet by 4.0% on table reasoning benchmarks. The\napproach also achieves excellent performance on text-to-SQL tasks, reaching\n68.3% performance on the BIRD dev dataset with a 7B model. Further experiments\ndemonstrate that Reasoning-Table enhances the model's generalization\ncapabilities and robustness.", "AI": {"tldr": "Introducing Reasoning-Table, the first reinforcement learning model for table reasoning that outperforms traditional supervised fine-tuning methods.", "motivation": "To improve generalization and robustness in table reasoning tasks beyond the limitations of supervised fine-tuning (SFT) approaches.", "method": "We apply reinforcement learning to train the Reasoning-Table model, implementing rigorous data preprocessing, reward design, and training strategies.", "result": "Reasoning-Table achieves state-of-the-art performance, surpassing larger proprietary models on table reasoning benchmarks and excelling in text-to-SQL tasks.", "conclusion": "Reasoning-Table demonstrates enhanced generalization and robustness capabilities, setting new benchmarks in table reasoning.", "key_contributions": ["First application of reinforcement learning to table reasoning", "Significantly outperforms SFT methods with simple rule-based rewards", "Unified training strategy enhances model performance across tasks."], "limitations": "", "keywords": ["table reasoning", "reinforcement learning", "text-to-SQL", "generalization", "robustness"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2506.01713", "pdf": "https://arxiv.org/pdf/2506.01713.pdf", "abs": "https://arxiv.org/abs/2506.01713", "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning", "authors": ["Zhongwei Wan", "Zhihao Dou", "Che Liu", "Yu Zhang", "Dongfei Cui", "Qinjian Zhao", "Hui Shen", "Jing Xiong", "Yi Xin", "Yifan Jiang", "Yangfan He", "Mi Zhang", "Shen Yan"], "categories": ["cs.CL"], "comment": "Under review", "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.", "AI": {"tldr": "Proposes a two-stage reflection-aware reinforcement learning framework to enhance reasoning in multimodal large language models (MLLMs).", "motivation": "Multimodal large language models struggle with complex reasoning tasks that require self-reflection and correction. Current reflection methods are inadequate, necessitating a more effective approach to enhance reasoning capabilities and feedback quality.", "method": "The proposed framework, SRPO, consists of two stages: (1) constructing a reflection-focused dataset guided by an advanced MLLM to foster learning of reasoning and self-reflection, and (2) implementing a novel reward mechanism to promote meaningful reflections while minimizing redundancy.", "result": "Extensive experiments on multimodal reasoning benchmarks (MathVista, MathVision, MathVerse, MMMU-Pro) show that SRPO significantly surpasses state-of-the-art models in reasoning accuracy and reflection quality, using Qwen-2.5-VL-7B and Qwen-2.5-VL-32B models.", "conclusion": "The SRPO framework effectively enhances reasoning in multimodal LLMs, enabling improved performance in complex tasks requiring reflection and self-correction.", "key_contributions": ["Introduction of the SRPO framework for enhancing MLLM reasoning.", "Creation of a reflection-focused dataset using advanced MLLM guidance.", "Development of a novel reward mechanism within the GRPO framework."], "limitations": "", "keywords": ["multimodal large language models", "self-reflection", "reinforcement learning", "reasoning", "feedback"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01723", "pdf": "https://arxiv.org/pdf/2506.01723.pdf", "abs": "https://arxiv.org/abs/2506.01723", "title": "Tug-of-war between idiom's figurative and literal meanings in LLMs", "authors": ["Soyoung Oh", "Xinting Huang", "Mathis Pink", "Michael Hahn", "Vera Demberg"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Idioms present a unique challenge for language models due to their\nnon-compositional figurative meanings, which often strongly diverge from the\nidiom's literal interpretation. This duality requires a model to learn\nrepresenting and deciding between the two meanings to interpret an idiom in a\nfigurative sense, or literally. In this paper, we employ tools from mechanistic\ninterpretability to trace how a large pretrained causal transformer\n(LLama3.2-1B-base) deals with this ambiguity. We localize three steps of idiom\nprocessing: First, the idiom's figurative meaning is retrieved in early\nattention and MLP sublayers. We identify specific attention heads which boost\nthe figurative meaning of the idiom while suppressing the idiom's literal\ninterpretation. The model subsequently represents the figurative representation\nthrough an intermediate path. Meanwhile, a parallel bypass route forwards\nliteral interpretation, ensuring that a both reading remain available. Overall,\nour findings provide a mechanistic evidence for idiom comprehension in an\nautoregressive transformer.", "AI": {"tldr": "This paper investigates how a large pretrained transformer model, LLama3.2-1B-base, processes idioms, focusing on the duality between figurative and literal meanings.", "motivation": "Understanding how language models interpret idioms is crucial due to their complex meanings that differ from literal interpretations.", "method": "The study utilizes mechanistic interpretability to analyze the attention mechanisms and pathways within the transformer model during idiom processing.", "result": "The model shows that it can boost the figurative meaning of idioms while suppressing their literal interpretation through specific attention heads and pathways.", "conclusion": "The findings offer mechanistic insights into how autoregressive transformers achieve idiom comprehension, suggesting a dual processing route for either figurative or literal interpretations.", "key_contributions": ["Mechanistic analysis of idiom processing in transformers", "Identification of attention heads for figurative and literal meaning", "Insights into pathways used by language models for understanding idioms"], "limitations": "", "keywords": ["idioms", "language models", "mechanistic interpretability", "transformers", "figurative language"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01732", "pdf": "https://arxiv.org/pdf/2506.01732.pdf", "abs": "https://arxiv.org/abs/2506.01732", "title": "Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training", "authors": ["Pierre-Carl Langlais", "Carlos Rosas Hinostroza", "Mattia Nee", "Catherine Arnett", "Pavel Chizhov", "Eliot Krzystof Jones", "Irène Girard", "David Mach", "Anastasia Stasenko", "Ivan P. Yamshchikov"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are pre-trained on large amounts of data from\ndifferent sources and domains. These data most often contain trillions of\ntokens with large portions of copyrighted or proprietary content, which hinders\nthe usage of such models under AI legislation. This raises the need for truly\nopen pre-training data that is compliant with the data security regulations. In\nthis paper, we introduce Common Corpus, the largest open dataset for language\nmodel pre-training. The data assembled in Common Corpus are either\nuncopyrighted or under permissible licenses and amount to about two trillion\ntokens. The dataset contains a wide variety of languages, ranging from the main\nEuropean languages to low-resource ones rarely present in pre-training\ndatasets; in addition, it includes a large portion of code data. The diversity\nof data sources in terms of covered domains and time periods opens up the paths\nfor both research and entrepreneurial needs in diverse areas of knowledge. In\nthis technical report, we present the detailed provenance of data assembling\nand the details of dataset filtering and curation. Being already used by such\nindustry leaders as Anthropic and multiple LLM training projects, we believe\nthat Common Corpus will become a critical infrastructure for open science\nresearch in LLMs.", "AI": {"tldr": "Introduction of Common Corpus, the largest open dataset for language model pre-training with approximately two trillion tokens.", "motivation": "The need for open pre-training data compliant with AI legislation due to copyright issues in existing LLM datasets.", "method": "Assembling a large dataset from uncopyrighted or permissibly licensed content, ensuring a diversity of languages and domains.", "result": "Common Corpus includes two trillion tokens covering major languages, low-resource languages, and code data, already utilized by industry leaders.", "conclusion": "Common Corpus serves as a critical infrastructure for open science in LLM research.", "key_contributions": ["Creation of the largest open pre-training dataset for LLMs", "Inclusion of a wide range of languages and domains", "Support for research and entrepreneurial applications"], "limitations": "", "keywords": ["Large Language Models", "Open Data", "Natural Language Processing", "Dataset Curation", "AI Legislation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01734", "pdf": "https://arxiv.org/pdf/2506.01734.pdf", "abs": "https://arxiv.org/abs/2506.01734", "title": "Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs", "authors": ["Jiandong Shao", "Yao Lu", "Jianfei Yang"], "categories": ["cs.CL"], "comment": "Under Review", "summary": "Large Language Models (LLMs) exhibit impressive performance on complex\nreasoning tasks, yet they frequently fail on basic numerical problems,\nproducing incorrect outputs. Inspired by Benford's Law -- a statistical pattern\nwhere lower digits occur more frequently as leading digits -- we hypothesize\nthat the long-tailed digit distributions in web-collected corpora may be\nlearned by LLMs during pretraining, leading to biased numerical generation. To\ninvestigate the hypothesis, we first examine whether digits frequencies in\npretraining corpus (OLMo2) follows Benford's law. We then construct an\nevaluation benchmark with uniformly distributed ground-truth digits across\nseven numerical reasoning tasks. Our evaluation results demonstrate that\nleading open-source LLMs show a consistent pattern of digit bias that resembles\nBenford's law. Through logit-lens tracing and neuron-level dissection, we\nidentify that this bias arises predominantly from a small subset of highly\ndigit-selective feed-forward network (FFN) neurons in the deeper layers.\nFinally, we demonstrate that pruning these neurons mitigates imbalanced\novergeneration and partially corrects erroneous outputs, providing causal\nevidence that fine-grained pretraining digit bias can propagate into model\nbehavior. Our findings reveal a fundamental connection between corpus-level\nstatistics and symbolic failure modes in LLMs, offering a new lens for\ndiagnosing and mitigating hallucinations in numerical tasks.", "AI": {"tldr": "The paper investigates how biases in digit generation by Large Language Models (LLMs) stem from the statistical patterns in their pretraining data, specifically in relation to Benford's Law.", "motivation": "LLMs often struggle with basic numerical problems, leading to incorrect outputs. Understanding the source of this bias can improve LLM performance in numerical reasoning tasks.", "method": "The study analyzes digit frequencies in the OLMo2 pretraining corpus to see if they follow Benford's law. An evaluation benchmark with uniformly distributed digits is constructed across seven numerical reasoning tasks. Logit-lens tracing and neuron-level dissection were employed to identify the origins of digit bias.", "result": "Leading open-source LLMs exhibit a digit bias that aligns with Benford's law. The bias is found predominantly in a small group of highly selective feed-forward neurons, and pruning these neurons helps reduce imbalanced digit generation and correct some erroneous outputs.", "conclusion": "The findings highlight a critical connection between corpus statistics and the failures of LLMs in numerical tasks, suggesting potential pathways for addressing hallucinations in such contexts.", "key_contributions": ["Investigation of digit bias in LLMs relating to Benford's Law.", "Development of a benchmark for evaluating LLM numerical reasoning.", "Evidence that neuron pruning can correct digit generation bias."], "limitations": "The study is limited to the analysis of specific LLMs and the corpus used; further examination with other models and datasets is necessary.", "keywords": ["Large Language Models", "Benford's Law", "Numerical Reasoning", "Digit Bias", "Neural Pruning"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2506.01748", "pdf": "https://arxiv.org/pdf/2506.01748.pdf", "abs": "https://arxiv.org/abs/2506.01748", "title": "Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning", "authors": ["Yihong Tang", "Kehai Chen", "Muyun Yang", "Zhengyu Niu", "Jing Li", "Tiejun Zhao", "Min Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "The advancement of Large Language Models (LLMs) has spurred significant\ninterest in Role-Playing Agents (RPAs) for applications such as emotional\ncompanionship and virtual interaction. However, recent RPAs are often built on\nexplicit dialogue data, lacking deep, human-like internal thought processes,\nresulting in superficial knowledge and style expression. While Large Reasoning\nModels (LRMs) can be employed to simulate character thought, their direct\napplication is hindered by attention diversion (i.e., RPAs forget their role)\nand style drift (i.e., overly formal and rigid reasoning rather than\ncharacter-consistent reasoning). To address these challenges, this paper\nintroduces a novel Role-Aware Reasoning (RAR) method, which consists of two\nimportant stages: Role Identity Activation (RIA) and Reasoning Style\nOptimization (RSO). RIA explicitly guides the model with character profiles\nduring reasoning to counteract attention diversion, and then RSO aligns\nreasoning style with the character and scene via LRM distillation to mitigate\nstyle drift. Extensive experiments demonstrate that the proposed RAR\nsignificantly enhances the performance of RPAs by effectively addressing\nattention diversion and style drift.", "AI": {"tldr": "This paper proposes a Role-Aware Reasoning (RAR) method to enhance Role-Playing Agents (RPAs) by addressing issues of attention diversion and style drift in dialogue generation.", "motivation": "The paper highlights the limitations of current RPAs that rely on explicit dialogue data and lack human-like reasoning, which prevents deeper emotional engagement and consistent character portrayal.", "method": "The RAR method consists of two key components: Role Identity Activation (RIA), which uses character profiles to guide the model's attention during reasoning, and Reasoning Style Optimization (RSO), which aligns the reasoning style with the character and scene using LRM distillation.", "result": "Experiments show that the RAR method significantly improves RPA performance by effectively addressing both attention diversion and style drift.", "conclusion": "The proposed RAR method enhances RPAs' ability to maintain role consistency and express character-specific styles in dialogue.", "key_contributions": ["Introduction of Role-Aware Reasoning (RAR) method", "Role Identity Activation (RIA) for guiding attention", "Reasoning Style Optimization (RSO) for mitigating style drift"], "limitations": "The efficacy of the proposed method may be limited by the quality of character profiles and scene descriptions used in training.", "keywords": ["Large Language Models", "Role-Playing Agents", "Role-Aware Reasoning", "Human-Computer Interaction", "Dialogue Generation"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01775", "pdf": "https://arxiv.org/pdf/2506.01775.pdf", "abs": "https://arxiv.org/abs/2506.01775", "title": "Developing a Mixed-Methods Pipeline for Community-Oriented Digitization of Kwak'wala Legacy Texts", "authors": ["Milind Agarwal", "Daisy Rosenblum", "Antonios Anastasopoulos"], "categories": ["cs.CL"], "comment": "Accepted to Comput-EL 2025 Workshop. Preprint", "summary": "Kwak'wala is an Indigenous language spoken in British Columbia, with a rich\nlegacy of published documentation spanning more than a century, and an active\ncommunity of speakers, teachers, and learners engaged in language\nrevitalization. Over 11 volumes of the earliest texts created during the\ncollaboration between Franz Boas and George Hunt have been scanned but remain\nunreadable by machines. Complete digitization through optical character\nrecognition has the potential to facilitate transliteration into modern\northographies and the creation of other language technologies. In this paper,\nwe apply the latest OCR techniques to a series of Kwak'wala texts only\naccessible as images, and discuss the challenges and unique adaptations\nnecessary to make such technologies work for these real-world texts. Building\non previous methods, we propose using a mix of off-the-shelf OCR methods,\nlanguage identification, and masking to effectively isolate Kwak'wala text,\nalong with post-correction models, to produce a final high-quality\ntranscription.", "AI": {"tldr": "This paper discusses the application of OCR techniques to digitize Kwak'wala texts, facilitating language revitalization efforts.", "motivation": "Kwak'wala is an Indigenous language with a need for modern digitization to aid in its revitalization efforts.", "method": "The authors applied a combination of off-the-shelf OCR methods, language identification, and masking techniques to digitize Kwak'wala texts, alongside post-correction models for improving transcription quality.", "result": "The methodology proposed successfully isolated Kwak'wala text from images and produced high-quality transcriptions.", "conclusion": "The study highlights the potential of advanced OCR methods to support language revitalization and poses necessary adaptations for effective implementation.", "key_contributions": ["Demonstration of OCR techniques tailored for an Indigenous language", "Adaptation of existing methods to handle real-world image texts", "Proposed methodology improves transcription accuracy for Kwak'wala texts."], "limitations": "", "keywords": ["Kwak'wala", "optical character recognition", "language revitalization", "Indigenous languages", "machine learning"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2506.01776", "pdf": "https://arxiv.org/pdf/2506.01776.pdf", "abs": "https://arxiv.org/abs/2506.01776", "title": "MaXIFE: Multilingual and Cross-lingual Instruction Following Evaluation", "authors": ["Yile Liu", "Ziwei Ma", "Xiu Jiang", "Jinglu Hu", "Jing Chang", "Liang Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "With the rapid adoption of large language models (LLMs) in natural language\nprocessing, the ability to follow instructions has emerged as a key metric for\nevaluating their practical utility. However, existing evaluation methods often\nfocus on single-language scenarios, overlooking the challenges and differences\npresent in multilingual and cross-lingual contexts. To address this gap, we\nintroduce MaXIFE: a comprehensive evaluation benchmark designed to assess\ninstruction-following capabilities across 23 languages with 1,667 verifiable\ninstruction tasks. MaXIFE integrates both Rule-Based Evaluation and Model-Based\nEvaluation, ensuring a balance of efficiency and accuracy. We applied MaXIFE to\nevaluate several leading commercial and open-source LLMs, establishing baseline\nresults for future comparisons. By providing a standardized tool for\nmultilingual instruction-following evaluation, MaXIFE aims to advance research\nand development in natural language processing.", "AI": {"tldr": "MaXIFE is a new evaluation benchmark for assessing the instruction-following capabilities of large language models (LLMs) across 23 languages.", "motivation": "To address the lack of evaluation methods for instruction-following in multilingual and cross-lingual contexts.", "method": "MaXIFE combines Rule-Based Evaluation and Model-Based Evaluation to assess 1,667 verifiable instruction tasks across 23 languages.", "result": "Baseline results for leading commercial and open-source LLMs were established using MaXIFE, providing a standardized tool for comparison.", "conclusion": "MaXIFE aims to enhance the research and development of natural language processing by offering a comprehensive evaluation framework.", "key_contributions": ["Introduces MaXIFE, an evaluation benchmark for multilingual instruction-following.", "Assesses capabilities across 23 languages and 1,667 tasks.", "Provides baseline results for various LLMs for future comparisons."], "limitations": "", "keywords": ["large language models", "instruction-following", "multilingual evaluation", "NLP", "benchmarking"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2506.01784", "pdf": "https://arxiv.org/pdf/2506.01784.pdf", "abs": "https://arxiv.org/abs/2506.01784", "title": "iQUEST: An Iterative Question-Guided Framework for Knowledge Base Question Answering", "authors": ["Shuai Wang", "Yinan Yu"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main)", "summary": "While Large Language Models (LLMs) excel at many natural language processing\ntasks, they often suffer from factual inaccuracies in knowledge-intensive\nscenarios. Integrating external knowledge resources, particularly knowledge\ngraphs (KGs), provides a transparent and updatable foundation for more reliable\nreasoning. Knowledge Base Question Answering (KBQA), which queries and reasons\nover KGs, is central to this effort, especially for complex, multi-hop queries.\nHowever, multi-hop reasoning poses two key challenges: (1)~maintaining coherent\nreasoning paths, and (2)~avoiding prematurely discarding critical multi-hop\nconnections. To address these issues, we introduce iQUEST, a question-guided\nKBQA framework that iteratively decomposes complex queries into simpler\nsub-questions, ensuring a structured and focused reasoning trajectory.\nAdditionally, we integrate a Graph Neural Network (GNN) to look ahead and\nincorporate 2-hop neighbor information at each reasoning step. This dual\napproach strengthens the reasoning process, enabling the model to explore\nviable paths more effectively. Detailed experiments demonstrate the consistent\nimprovement delivered by iQUEST across four benchmark datasets and four LLMs.", "AI": {"tldr": "iQUEST is a framework for Knowledge Base Question Answering that enhances reasoning with multi-hop queries by decomposing them into simpler sub-questions and leveraging Graph Neural Networks.", "motivation": "To improve the accuracy and reliability of Large Language Models (LLMs) in knowledge-intensive scenarios by integrating external knowledge resources like knowledge graphs and enhancing reasoning capabilities for complex queries.", "method": "iQUEST decomposes complex queries into simpler sub-questions and uses a Graph Neural Network to incorporate 2-hop neighbor information, providing a structured reasoning framework.", "result": "Experiments show that iQUEST consistently improves performance across four benchmark datasets and four different LLMs in KBQA tasks.", "conclusion": "iQUEST enhances multi-hop reasoning in KBQA, enabling more effective exploration of knowledge graphs and improving accuracy in responses for complex queries.", "key_contributions": ["Introduction of iQUEST framework for iterative query decomposition", "Integration of Graph Neural Networks to enhance multi-hop reasoning", "Consistent improvement in KBQA performance across benchmark datasets"], "limitations": "", "keywords": ["Knowledge Base Question Answering", "Large Language Models", "Multi-hop Reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2506.01793", "pdf": "https://arxiv.org/pdf/2506.01793.pdf", "abs": "https://arxiv.org/abs/2506.01793", "title": "Human-Centric Evaluation for Foundation Models", "authors": ["Yijin Guo", "Kaiyuan Ji", "Xiaorong Zhu", "Junying Wang", "Farong Wen", "Chunyi Li", "Zicheng Zhang", "Guangtao Zhai"], "categories": ["cs.CL"], "comment": null, "summary": "Currently, nearly all evaluations of foundation models focus on objective\nmetrics, emphasizing quiz performance to define model capabilities. While this\nmodel-centric approach enables rapid performance assessment, it fails to\nreflect authentic human experiences. To address this gap, we propose a\nHuman-Centric subjective Evaluation (HCE) framework, focusing on three core\ndimensions: problem-solving ability, information quality, and interaction\nexperience. Through experiments involving Deepseek R1, OpenAI o3 mini, Grok 3,\nand Gemini 2.5, we conduct over 540 participant-driven evaluations, where\nhumans and models collaborate on open-ended research tasks, yielding a\ncomprehensive subjective dataset. This dataset captures diverse user feedback\nacross multiple disciplines, revealing distinct model strengths and\nadaptability. Our findings highlight Grok 3's superior performance, followed by\nDeepseek R1 and Gemini 2.5, with OpenAI o3 mini lagging behind. By offering a\nnovel framework and a rich dataset, this study not only enhances subjective\nevaluation methodologies but also lays the foundation for standardized,\nautomated assessments, advancing LLM development for research and practical\nscenarios. Our dataset link is\nhttps://github.com/yijinguo/Human-Centric-Evaluation.", "AI": {"tldr": "Introduction of a Human-Centric subjective Evaluation (HCE) framework to assess foundation models based on human experiences rather than just metrics.", "motivation": "To bridge the gap in foundation model evaluations that focus mainly on objective metrics, which do not reflect authentic human experiences.", "method": "Experiments with various foundation models (Deepseek R1, OpenAI o3 mini, Grok 3, and Gemini 2.5) involving over 540 participant-driven evaluations on open-ended research tasks.", "result": "The study identifies Grok 3 as the best-performing model, followed by Deepseek R1 and Gemini 2.5, with OpenAI o3 mini trailing. It generates a comprehensive subjective dataset capturing user feedback across multiple disciplines.", "conclusion": "The proposed HCE framework enhances subjective evaluation methodologies and supports the advancement of LLMs in research and practical applications.", "key_contributions": ["Development of the Human-Centric subjective Evaluation (HCE) framework", "Creation of a comprehensive subjective dataset capturing user feedback", "Standardization of automated assessments for LLM development"], "limitations": "", "keywords": ["Human-Centric Evaluation", "Foundation Models", "Subjective Assessment"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01796", "pdf": "https://arxiv.org/pdf/2506.01796.pdf", "abs": "https://arxiv.org/abs/2506.01796", "title": "Read it in Two Steps: Translating Extremely Low-Resource Languages with Code-Augmented Grammar Books", "authors": ["Chen Zhang", "Jiuheng Lin", "Xiao Liu", "Zekai Zhang", "Yansong Feng"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "While large language models (LLMs) have shown promise in translating\nextremely low-resource languages using resources like dictionaries, the\neffectiveness of grammar books remains debated. This paper investigates the\nrole of grammar books in translating extremely low-resource languages by\ndecomposing it into two key steps: grammar rule retrieval and application. To\nfacilitate the study, we introduce ZhuangRules, a modularized dataset of\ngrammar rules and their corresponding test sentences. Our analysis reveals that\nrule retrieval constitutes a primary bottleneck in grammar-based translation.\nMoreover, although LLMs can apply simple rules for translation when explicitly\nprovided, they encounter difficulties in handling more complex rules. To\naddress these challenges, we propose representing grammar rules as code\nfunctions, considering their similarities in structure and the benefit of code\nin facilitating LLM reasoning. Our experiments show that using code rules\nsignificantly boosts both rule retrieval and application, ultimately resulting\nin a 13.1% BLEU improvement in translation.", "AI": {"tldr": "This paper explores the use of grammar books in translating low-resource languages and introduces a new dataset called ZhuangRules to improve grammar rule retrieval and application using code representation.", "motivation": "The effectiveness of grammar books in aiding translation for extremely low-resource languages remains unclear, motivating an investigation into their role and challenges.", "method": "The study decomposes the translation process into grammar rule retrieval and application, using the ZhuangRules dataset to evaluate the impact of representing grammar rules as code functions.", "result": "The analysis shows that grammar rule retrieval is a bottleneck and that representing rules as code significantly enhances both retrieval and application, leading to a 13.1% improvement in BLEU scores for translations.", "conclusion": "Utilizing code representations for grammar rules can improve low-resource language translations more effectively than traditional rule application methods.", "key_contributions": ["Introduction of ZhuangRules dataset for grammar rules", "Decomposition of translation into retrieval and application steps", "Demonstrated efficacy of using code functions for grammar rules"], "limitations": "The study is primarily focused on low-resource languages and may not generalize to well-resourced languages or other translation methods.", "keywords": ["language translation", "low-resource languages", "grammar rules", "large language models", "code representation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01807", "pdf": "https://arxiv.org/pdf/2506.01807.pdf", "abs": "https://arxiv.org/abs/2506.01807", "title": "Propaganda and Information Dissemination in the Russo-Ukrainian War: Natural Language Processing of Russian and Western Twitter Narratives", "authors": ["Zaur Gouliev"], "categories": ["cs.CL"], "comment": "7 pages; 6 figures", "summary": "The conflict in Ukraine has been not only characterised by military\nengagement but also by a significant information war, with social media\nplatforms like X, formerly known as Twitter playing an important role in\nshaping public perception. This article provides an analysis of tweets from\npropaganda accounts and trusted accounts collected from the onset of the war,\nFebruary 2022 until the middle of May 2022 with n=40,000 total tweets. We\nutilise natural language processing and machine learning algorithms to assess\nthe sentiment and identify key themes, topics and narratives across the dataset\nwith human-in-the-loop (HITL) analysis throughout. Our findings indicate\ndistinct strategies in how information is created, spread, and targeted at\ndifferent audiences by both sides. Propaganda accounts frequently employ\nemotionally charged language and disinformation to evoke fear and distrust,\nwhereas other accounts, primarily Western tend to focus on factual reporting\nand humanitarian aspects of the conflict. Clustering analysis reveals groups of\naccounts with similar behaviours, which we suspect indicates the presence of\ncoordinated efforts. This research attempts to contribute to our understanding\nof the dynamics of information warfare and offers techniques for future studies\non social media influence in military conflicts.", "AI": {"tldr": "This study analyzes 40,000 tweets from various accounts during the Ukraine conflict to understand information warfare tactics, leveraging NLP and machine learning for sentiment analysis and theme identification.", "motivation": "To understand the dynamics of information warfare during the Ukraine conflict and how different accounts shape public perception through social media.", "method": "Analysis of tweets using natural language processing and machine learning to assess sentiment and identify themes, employing a human-in-the-loop (HITL) approach.", "result": "Findings reveal two distinct information strategies: propaganda accounts use emotionally charged and misleading language, while trusted accounts focus on facts and humanitarian issues. Clustering analysis suggests coordinated behaviors among certain accounts.", "conclusion": "The study enhances understanding of information warfare dynamics and proposes techniques for future research on social media's influence in military conflicts.", "key_contributions": ["Analysis of 40,000 tweets to understand information warfare", "Identification of distinct strategies employed by different types of accounts", "Insights on the dynamics of social media influence in military conflicts"], "limitations": "", "keywords": ["information warfare", "social media", "machine learning", "natural language processing", "Ukraine conflict"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2506.01808", "pdf": "https://arxiv.org/pdf/2506.01808.pdf", "abs": "https://arxiv.org/abs/2506.01808", "title": "NAVER LABS Europe Submission to the Instruction-following Track", "authors": ["Beomseok Lee", "Marcely Zanon Boito", "Laurent Besacier", "Ioan Calapodescu"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper we describe NAVER LABS Europe submission to the\ninstruction-following speech processing short track at IWSLT 2025. We\nparticipate in the constrained settings, developing systems that can\nsimultaneously perform ASR, ST, and SQA tasks from English speech input into\nthe following target languages: Chinese, Italian, and German. Our solution\nleverages two pretrained modules: (1) a speech-to-LLM embedding projector\ntrained using representations from the SeamlessM4T-v2-large speech encoder; and\n(2) LoRA adapters trained on text data on top of a Llama-3.1-8B-Instruct. These\nmodules are jointly loaded and further instruction-tuned for 1K steps on\nmultilingual and multimodal data to form our final system submitted for\nevaluation.", "AI": {"tldr": "This paper describes a submission to the IWSLT 2025 focusing on instruction-following speech processing, performing ASR, ST, and SQA tasks from English input to multiple languages.", "motivation": "To advance speech processing capabilities in multilingual contexts through instruction-following models.", "method": "The proposed solution combines two pretrained modules: a speech-to-LLM embedding projector and LoRA adapters, which are fine-tuned for 1K steps on multilingual and multimodal datasets.", "result": "The developed system performs well in the constrained settings of ASR, ST, and SQA tasks, translating English speech input into Chinese, Italian, and German.", "conclusion": "The mixture of pretrained modules and further instruction tuning leads to a capable system for multilingual speech processing tasks.", "key_contributions": ["Development of a novel instruction-following system for multilingual speech tasks.", "Integration of speech-to-LLM embedding with LoRA for enhanced performance.", "Successful application of pretrained models in constrained settings."], "limitations": "", "keywords": ["speech processing", "multilingual", "instruction-following", "LLM", "NLP"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2506.01814", "pdf": "https://arxiv.org/pdf/2506.01814.pdf", "abs": "https://arxiv.org/abs/2506.01814", "title": "Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high", "authors": ["PeiHsuan Huang", "ZihWei Lin", "Simon Imbot", "WenCheng Fu", "Ethan Tu"], "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Large language models (LLMs) increasingly shape public understanding and\ncivic decisions, yet their ideological neutrality is a growing concern. While\nexisting research has explored various forms of LLM bias, a direct,\ncross-lingual comparison of models with differing geopolitical\nalignments-specifically a PRC-system model versus a non-PRC counterpart-has\nbeen lacking. This study addresses this gap by systematically evaluating\nDeepSeek-R1 (PRC-aligned) against ChatGPT o3-mini-high (non-PRC) for\nChinese-state propaganda and anti-U.S. sentiment. We developed a novel corpus\nof 1,200 de-contextualized, reasoning-oriented questions derived from\nChinese-language news, presented in Simplified Chinese, Traditional Chinese,\nand English. Answers from both models (7,200 total) were assessed using a\nhybrid evaluation pipeline combining rubric-guided GPT-4o scoring with human\nannotation. Our findings reveal significant model-level and language-dependent\nbiases. DeepSeek-R1 consistently exhibited substantially higher proportions of\nboth propaganda and anti-U.S. bias compared to ChatGPT o3-mini-high, which\nremained largely free of anti-U.S. sentiment and showed lower propaganda\nlevels. For DeepSeek-R1, Simplified Chinese queries elicited the highest bias\nrates; these diminished in Traditional Chinese and were nearly absent in\nEnglish. Notably, DeepSeek-R1 occasionally responded in Simplified Chinese to\nTraditional Chinese queries and amplified existing PRC-aligned terms in its\nChinese answers, demonstrating an \"invisible loudspeaker\" effect. Furthermore,\nsuch biases were not confined to overtly political topics but also permeated\ncultural and lifestyle content, particularly in DeepSeek-R1.", "AI": {"tldr": "This paper evaluates the ideological biases of two large language models, DeepSeek-R1 (PRC-aligned) and ChatGPT o3-mini-high (non-PRC), in responding to questions derived from Chinese-language news.", "motivation": "To investigate the bias present in large language models, particularly in the context of geopolitical alignment, and fill the gap in existing research regarding cross-lingual comparisons of model biases.", "method": "A novel corpus of 1,200 questions was generated from Chinese-language news, with responses analyzed from both models using a hybrid evaluation method that combined GPT-4 scoring with human annotations.", "result": "DeepSeek-R1 exhibited significantly higher rates of propaganda and anti-U.S. sentiment compared to ChatGPT o3-mini-high, with bias levels varying across languages.", "conclusion": "The study highlights that model biases are not limited to political content but also affect cultural and lifestyle topics, revealing the pervasive nature of ideologically motivated content generation.", "key_contributions": ["First direct cross-lingual comparison of PRC-aligned and non-PRC language models", "Development of a unique corpus for evaluating LLM biases", "Demonstration of bias variation by language in responses from LLMs"], "limitations": "The study is limited to a specific set of models and may not represent the entire landscape of LLM biases.", "keywords": ["large language models", "bias", "cross-lingual evaluation", "propaganda", "ideological neutrality"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.01817", "pdf": "https://arxiv.org/pdf/2506.01817.pdf", "abs": "https://arxiv.org/abs/2506.01817", "title": "BD at BEA 2025 Shared Task: MPNet Ensembles for Pedagogical Mistake Identification and Localization in AI Tutor Responses", "authors": ["Shadman Rohan", "Ishita Sur Apan", "Muhtasim Ibteda Shochcho", "Md Fahim", "Mohammad Ashfaq Ur Rahman", "AKM Mahbubur Rahman", "Amin Ahsan Ali"], "categories": ["cs.CL"], "comment": null, "summary": "We present Team BD's submission to the BEA 2025 Shared Task on Pedagogical\nAbility Assessment of AI-powered Tutors, under Track 1 (Mistake Identification)\nand Track 2 (Mistake Location). Both tracks involve three-class classification\nof tutor responses in educational dialogues - determining if a tutor correctly\nrecognizes a student's mistake (Track 1) and whether the tutor pinpoints the\nmistake's location (Track 2). Our system is built on MPNet, a Transformer-based\nlanguage model that combines BERT and XLNet's pre-training advantages. We\nfine-tuned MPNet on the task data using a class-weighted cross-entropy loss to\nhandle class imbalance, and leveraged grouped cross-validation (10 folds) to\nmaximize the use of limited data while avoiding dialogue overlap between\ntraining and validation. We then performed a hard-voting ensemble of the best\nmodels from each fold, which improves robustness and generalization by\ncombining multiple classifiers. Our approach achieved strong results on both\ntracks, with exact-match macro-F1 scores of approximately 0.7110 for Mistake\nIdentification and 0.5543 for Mistake Location on the official test set. We\ninclude comprehensive analysis of our system's performance, including confusion\nmatrices and t-SNE visualizations to interpret classifier behavior, as well as\na taxonomy of common errors with examples. We hope our ensemble-based approach\nand findings provide useful insights for designing reliable tutor response\nevaluation systems in educational dialogue settings.", "AI": {"tldr": "The paper discusses Team BD's submission to the BEA 2025 Shared Task on AI Tutor assessment, focusing on mistake identification and location using an ensemble approach with a fine-tuned MPNet model.", "motivation": "The objective is to assess the pedagogical abilities of AI-powered tutors through accurate mistake identification and location in educational dialogues.", "method": "The system employs a fine-tuned MPNet model with class-weighted cross-entropy loss and grouped cross-validation (10 folds), along with a hard-voting ensemble of models for improved accuracy.", "result": "Achieved exact-match macro-F1 scores of approximately 0.7110 for mistake identification and 0.5543 for mistake location on the official test set.", "conclusion": "The findings aim to enhance the reliability of tutor response evaluation systems in educational settings and provide useful insights for future developments.", "key_contributions": ["Introduction of an ensemble approach to AI tutor evaluation", "Use of grouped cross-validation for effective data management", "Comprehensive performance analysis including confusion matrices and visualizations"], "limitations": "", "keywords": ["AI Tutors", "Pedagogical Ability", "Mistake Identification", "Language Model", "Ensemble Method"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.01819", "pdf": "https://arxiv.org/pdf/2506.01819.pdf", "abs": "https://arxiv.org/abs/2506.01819", "title": "Not All Jokes Land: Evaluating Large Language Models Understanding of Workplace Humor", "authors": ["Moahmmadamin Shafiei", "Hamidreza Saffari"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "With the recent advances in Artificial Intelligence (AI) and Large Language\nModels (LLMs), the automation of daily tasks, like automatic writing, is\ngetting more and more attention. Hence, efforts have focused on aligning LLMs\nwith human values, yet humor, particularly professional industrial humor used\nin workplaces, has been largely neglected. To address this, we develop a\ndataset of professional humor statements along with features that determine the\nappropriateness of each statement. Our evaluation of five LLMs shows that LLMs\noften struggle to judge the appropriateness of humor accurately.", "AI": {"tldr": "This paper develops a dataset of professional humor statements to evaluate how well LLMs can judge humor appropriateness.", "motivation": "To address the neglect of professional humor in LLM alignment with human values, which is crucial for AI applications in workplaces.", "method": "A dataset of professional humor statements was created, including features for evaluating the appropriateness of humor, followed by an evaluation of five LLMs.", "result": "The evaluation revealed that LLMs frequently struggle to accurately judge the appropriateness of professional humor statements.", "conclusion": "Improving LLM's understanding of professional humor is necessary for better alignment with human workplace values.", "key_contributions": ["Creation of a professional humor dataset", "Evaluation of LLMs on humor appropriateness", "Identification of features for determining humor appropriateness"], "limitations": "The study focuses only on five LLMs and may not generalize across all AI models or humor contexts.", "keywords": ["Artificial Intelligence", "Large Language Models", "professional humor", "workplace", "humor appropriateness"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2506.01829", "pdf": "https://arxiv.org/pdf/2506.01829.pdf", "abs": "https://arxiv.org/abs/2506.01829", "title": "CiteEval: Principle-Driven Citation Evaluation for Source Attribution", "authors": ["Yumo Xu", "Peng Qi", "Jifan Chen", "Kunlun Liu", "Rujun Han", "Lan Liu", "Bonan Min", "Vittorio Castelli", "Arshit Gupta", "Zhiguo Wang"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "Citation quality is crucial in information-seeking systems, directly\ninfluencing trust and the effectiveness of information access. Current\nevaluation frameworks, both human and automatic, mainly rely on Natural\nLanguage Inference (NLI) to assess binary or ternary supportiveness from cited\nsources, which we argue is a suboptimal proxy for citation evaluation. In this\nwork we introduce CiteEval, a citation evaluation framework driven by\nprinciples focusing on fine-grained citation assessment within a broad context,\nencompassing not only the cited sources but the full retrieval context, user\nquery, and generated text. Guided by the proposed framework, we construct\nCiteBench, a multi-domain benchmark with high-quality human annotations on\ncitation quality. To enable efficient evaluation, we further develop\nCiteEval-Auto, a suite of model-based metrics that exhibit strong correlation\nwith human judgments. Experiments across diverse systems demonstrate\nCiteEval-Auto's superior ability to capture the multifaceted nature of\ncitations compared to existing metrics, offering a principled and scalable\napproach to evaluate and improve model-generated citations.", "AI": {"tldr": "The paper presents CiteEval, a new citation evaluation framework that improves upon existing methods by considering fine-grained citation assessment in context, supported by a benchmark named CiteBench and auto evaluation metrics.", "motivation": "To enhance the quality and effectiveness of citation evaluations in information-seeking systems.", "method": "The authors introduce CiteEval as a framework focused on fine-grained citation assessment, complemented by the CiteBench benchmark for high-quality human annotations and CiteEval-Auto for model-based evaluation metrics.", "result": "CiteEval-Auto metrics show strong correlation with human judgments and outperform existing metrics in capturing the complexities of citation quality.", "conclusion": "CiteEval provides a principled, scalable approach for evaluating citation quality that can significantly improve model-generated citations.", "key_contributions": ["Introduction of CiteEval framework for detailed citation assessments", "Creation of CiteBench benchmark with human annotations", "Development of CiteEval-Auto metrics for automatic evaluation of citations"], "limitations": "", "keywords": ["citation evaluation", "Natural Language Inference", "information access", "CiteEval", "CiteBench"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01840", "pdf": "https://arxiv.org/pdf/2506.01840.pdf", "abs": "https://arxiv.org/abs/2506.01840", "title": "Minimal Pair-Based Evaluation of Code-Switching", "authors": ["Igor Sterner", "Simone Teufel"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "There is a lack of an evaluation methodology that estimates the extent to\nwhich large language models (LLMs) use code-switching (CS) in the same way as\nbilinguals. Existing methods do not have wide language coverage, fail to\naccount for the diverse range of CS phenomena, or do not scale. We propose an\nintervention based on minimal pairs of CS. Each minimal pair contains one\nnaturally occurring CS sentence and one minimally manipulated variant. We\ncollect up to 1,000 such pairs each for 11 language pairs. Our human\nexperiments show that, for every language pair, bilinguals consistently prefer\nthe naturally occurring CS sentence. Meanwhile our experiments with current\nLLMs show that the larger the model, the more consistently it assigns higher\nprobability to the naturally occurring CS sentence than to the variant. In\naccordance with theoretical claims, the largest probability differences arise\nin those pairs where the manipulated material consisted of closed-class words.", "AI": {"tldr": "This paper presents a methodology to evaluate code-switching in large language models (LLMs) by comparing probabilities assigned to naturally occurring sentences and manipulated variants.", "motivation": "The study aims to address the lack of effective evaluation methodologies for assessing how large language models engage in code-switching as bilinguals do.", "method": "An intervention based on minimal pairs of code-switching, involving naturally occurring sentences and minimally manipulated variants, was implemented. Data was collected for 11 language pairs with up to 1,000 pairs per language.", "result": "Experiments show that bilinguals prefer the naturally occurring code-switching sentences, and larger LLMs assign higher probabilities to these sentences compared to variants, especially for pairs with manipulated closed-class words.", "conclusion": "The findings support theoretical claims about code-switching and demonstrate that LLMs can exhibit similar patterns to bilinguals in their preferences.", "key_contributions": ["Development of a robust evaluation methodology for code-switching in LLMs.", "Empirical evidence showing that larger models outperform smaller ones in recognizing naturally occurring code-switching.", "Insights into how closed-class words affect LLM behavior regarding code-switching."], "limitations": "The study is limited to 11 language pairs, which may not represent all possible bilingual CS phenomena.", "keywords": ["code-switching", "large language models", "evaluation methodology", "bilingualism", "natural language processing"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01846", "pdf": "https://arxiv.org/pdf/2506.01846.pdf", "abs": "https://arxiv.org/abs/2506.01846", "title": "Code-Switching and Syntax: A Large-Scale Experiment", "authors": ["Igor Sterner", "Simone Teufel"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "The theoretical code-switching (CS) literature provides numerous pointwise\ninvestigations that aim to explain patterns in CS, i.e. why bilinguals switch\nlanguage in certain positions in a sentence more often than in others. A\nresulting consensus is that CS can be explained by the syntax of the\ncontributing languages. There is however no large-scale, multi-language,\ncross-phenomena experiment that tests this claim. When designing such an\nexperiment, we need to make sure that the system that is predicting where\nbilinguals tend to switch has access only to syntactic information. We provide\nsuch an experiment here. Results show that syntax alone is sufficient for an\nautomatic system to distinguish between sentences in minimal pairs of CS, to\nthe same degree as bilingual humans. Furthermore, the learnt syntactic patterns\ngeneralise well to unseen language pairs.", "AI": {"tldr": "This paper presents a large-scale experiment demonstrating that syntax alone can effectively predict code-switching (CS) locations in bilingual sentences.", "motivation": "To investigate whether the patterns of code-switching can be solely explained by syntactic structures across multiple languages and phenomena, extending beyond isolated studies.", "method": "The study designed a large-scale, multi-language experiment where a system predicts code-switching points using only syntactic information, allowing for systematic comparison with bilingual human performance.", "result": "The results show that the automatic system can distinguish between sentences in minimal pairs of code-switching using syntax, achieving accuracy comparable to bilingual humans.", "conclusion": "The findings support the hypothesis that syntactic patterns alone are sufficient to explain and predict language switching behavior in bilingual speakers and generalize across unseen language pairs.", "key_contributions": ["Establishes a large-scale experiment in code-switching analysis", "Proves that syntax is sufficient for predicting bilingual switch points", "Demonstrates generalization of syntactic patterns to unseen pairs"], "limitations": "", "keywords": ["code-switching", "bilingualism", "syntax", "natural language processing", "language pairs"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2506.01859", "pdf": "https://arxiv.org/pdf/2506.01859.pdf", "abs": "https://arxiv.org/abs/2506.01859", "title": "CONFETTI: Conversational Function-Calling Evaluation Through Turn-Level Interactions", "authors": ["Tamer Alkhouli", "Katerina Margatina", "James Gung", "Raphael Shu", "Claudia Zaghi", "Monica Sunkara", "Yi Zhang"], "categories": ["cs.CL"], "comment": "ACL 2025 (main conference)", "summary": "We introduce Conversational Function-Calling Evaluation Through Turn-Level\nInteractions (CONFETTI), a conversational benchmark1 designed to evaluate the\nfunction-calling capabilities and response quality of large language models\n(LLMs). Current benchmarks lack comprehensive assessment of LLMs in complex\nconversational scenarios. CONFETTI addresses this gap through 109\nhuman-simulated conversations, comprising 313 user turns and covering 86 APIs.\nThese conversations explicitly target various conversational complexities, such\nas follow-ups, goal correction and switching, ambiguous and implicit goals. We\nperform off-policy turn-level evaluation using this benchmark targeting\nfunction-calling. Our benchmark also incorporates dialog act annotations to\nassess agent responses. We evaluate a series of state-of-the-art LLMs and\nanalyze their performance with respect to the number of available APIs,\nconversation lengths, and chained function calling. Our results reveal that\nwhile some models are able to handle long conversations, and leverage more than\n20+ APIs successfully, other models struggle with longer context or when\nincreasing the number of APIs. We also report that the performance on chained\nfunction-calls is severely limited across the models. Overall, the top\nperforming models on CONFETTI are Nova Pro (40.01%), Claude Sonnet v3.5\n(35.46%) and Llama 3.1 405B (33.19%) followed by command-r-plus (31.18%) and\nMistral-Large-2407 (30.07%).", "AI": {"tldr": "CONFETTI is a benchmark for evaluating function-calling in large language models through complex conversational scenarios.", "motivation": "Current benchmarks do not adequately assess LLMs in intricate conversational scenarios, particularly concerning their function-calling capabilities and response quality.", "method": "The study introduces CONFETTI, which includes 109 human-simulated conversations with 313 user turns targeting complex conversational dynamics. It employs off-policy turn-level evaluation and includes dialog act annotations to evaluate agent responses across various LLMs using 86 APIs.", "result": "Top models evaluated include Nova Pro, Claude Sonnet v3.5, and Llama 3.1 405B, with noted challenges in handling long conversations and chained function-calls across the models.", "conclusion": "While some LLMs can manage longer conversations and multiple APIs, performance on chained function-calls is limited, suggesting the need for improved function-calling efficiency in future models.", "key_contributions": ["Introduction of CONFETTI benchmark for LLM assessment", "In-depth analysis of LLMs in complex conversational scenarios", "Identification of performance limitations in function-calling across models"], "limitations": "The performance of models on chained function-calls remains inadequate, indicating a need for enhancements in this area.", "keywords": ["function-calling", "large language models", "conversational benchmark", "dialog act annotations", "evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01872", "pdf": "https://arxiv.org/pdf/2506.01872.pdf", "abs": "https://arxiv.org/abs/2506.01872", "title": "Is Extending Modality The Right Path Towards Omni-Modality?", "authors": ["Tinghui Zhu", "Kai Zhang", "Muhao Chen", "Yu Su"], "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.", "AI": {"tldr": "This paper investigates the challenges of achieving true omni-modal language models (OLMs) that can effectively process and reason over diverse input modalities, analyzing the impact of modality extension, model merging, and knowledge sharing.", "motivation": "To understand and overcome the limitations of current omni-modal language models, particularly in their ability to generalize across different input modalities and maintain strong language processing capabilities.", "method": "The authors conducted extensive experiments to assess the effects of modality extension, the integration of modality-specific models through merging, and the comparison of knowledge sharing methods in achieving omni-modality.", "result": "The findings reveal trade-offs between maintaining core language abilities and achieving effective omni-modality. The study shows that current methods struggle with generalization and performance on multi-modal inputs.", "conclusion": "Achieving true omni-modality with existing models is challenging, and while some techniques show promise, they also highlight significant limitations that need to be addressed.", "key_contributions": ["Analysis of modality extension's impact on language capabilities", "Investigation of model merging for omni-modality", "Comparison of knowledge sharing techniques for improving generalization"], "limitations": "Current methods are limited in their ability to generalize across modality pairs and perform well on multi-modal inputs, indicating a need for further advancements.", "keywords": ["Omni-modal language models", "modality extension", "model merging", "knowledge sharing", "multi-modal inputs"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2506.01918", "pdf": "https://arxiv.org/pdf/2506.01918.pdf", "abs": "https://arxiv.org/abs/2506.01918", "title": "Spatial Coordinates as a Cell Language: A Multi-Sentence Framework for Imaging Mass Cytometry Analysis", "authors": ["Chi-Jane Chen", "Yuhang Chen", "Sukwon Yun", "Natalie Stanley", "Tianlong Chen"], "categories": ["cs.CL"], "comment": null, "summary": "Image mass cytometry (IMC) enables high-dimensional spatial profiling by\ncombining mass cytometry's analytical power with spatial distributions of cell\nphenotypes. Recent studies leverage large language models (LLMs) to extract\ncell states by translating gene or protein expression into biological context.\nHowever, existing single-cell LLMs face two major challenges: (1) Integration\nof spatial information: they struggle to generalize spatial coordinates and\neffectively encode spatial context as text, and (2) Treating each cell\nindependently: they overlook cell-cell interactions, limiting their ability to\ncapture biological relationships. To address these limitations, we propose\nSpatial2Sentence, a novel framework that integrates single-cell expression and\nspatial information into natural language using a multi-sentence approach.\nSpatial2Sentence constructs expression similarity and distance matrices,\npairing spatially adjacent and expressionally similar cells as positive pairs\nwhile using distant and dissimilar cells as negatives. These multi-sentence\nrepresentations enable LLMs to learn cellular interactions in both expression\nand spatial contexts. Equipped with multi-task learning, Spatial2Sentence\noutperforms existing single-cell LLMs on preprocessed IMC datasets, improving\ncell-type classification by 5.98% and clinical status prediction by 4.18% on\nthe diabetes dataset while enhancing interpretability. The source code can be\nfound here: https://github.com/UNITES-Lab/Spatial2Sentence.", "AI": {"tldr": "Spatial2Sentence is a framework that enhances single-cell analysis using large language models by integrating spatial and expression data for improved cell-type classification and clinical prediction.", "motivation": "To address the limitations of existing single-cell LLMs which struggle with integrating spatial information and considering cell-cell interactions, thereby enhancing biological relationship analysis.", "method": "Spatial2Sentence constructs expression similarity and distance matrices to pair spatially adjacent and expressionally similar cells for multi-sentence representations, enabling better learning of cellular interactions.", "result": "Spatial2Sentence outperforms existing single-cell LLMs on IMC datasets, achieving a 5.98% improvement in cell-type classification and a 4.18% enhancement in clinical status prediction for diabetes data.", "conclusion": "The proposed framework significantly improves the interpretability and performance of single-cell analyses by combining spatial context with single-cell expression data.", "key_contributions": ["Introduction of a multi-sentence approach for single-cell data", "Integration of spatial information into language models", "Demonstrated improvements in classification accuracy and clinical predictions"], "limitations": "", "keywords": ["image mass cytometry", "spatial information", "large language models"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01920", "pdf": "https://arxiv.org/pdf/2506.01920.pdf", "abs": "https://arxiv.org/abs/2506.01920", "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model Evaluation", "authors": ["Serry Sibaee", "Omer Nacar", "Adel Ammar", "Yasser Al-Habashi", "Abdulrahman Al-Batati", "Wadii Boulila"], "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.", "AI": {"tldr": "This paper introduces a comprehensive framework for evaluating Arabic language models, addressing gaps and limitations in existing evaluation methods.", "motivation": "To tackle existing gaps in Arabic language model evaluations, focusing on linguistic accuracy, cultural relevance, and methodological rigor.", "method": "The authors analyze current Arabic evaluation datasets and propose the Arabic Depth Mini Dataset (ADMD) to evaluate five leading language models across various domains.", "result": "Evaluation results showed significant variations in language model performance, particularly in domains requiring deep cultural understanding and specialized knowledge, with Claude 3.5 Sonnet achieving the highest accuracy.", "conclusion": "The study highlights the need for improved evaluation frameworks that prioritize cultural competence in addition to technical performance.", "key_contributions": ["Introduction of a novel evaluation framework for Arabic language models", "Creation of the Arabic Depth Mini Dataset (ADMD)", "Evaluation of five leading language models highlighting their strengths and weaknesses."], "limitations": "Limited to analysis of five language models and the performance metrics used in this study.", "keywords": ["Arabic language models", "evaluation framework", "cultural competence", "language model performance", "ADMD"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2506.01928", "pdf": "https://arxiv.org/pdf/2506.01928.pdf", "abs": "https://arxiv.org/abs/2506.01928", "title": "Esoteric Language Models", "authors": ["Subham Sekhar Sahoo", "Zhihan Yang", "Yash Akhauri", "Johnna Liu", "Deepansha Singh", "Zhoujun Cheng", "Zhengzhong Liu", "Eric Xing", "John Thickstun", "Arash Vahdat"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)", "AI": {"tldr": "Introducing Eso-LMs, a new family of language models that combines autoregressive and masked diffusion paradigms, achieving superior performance and inference efficiency.", "motivation": "To address the limitations of current masked diffusion models (MDMs) and autoregressive (AR) models in language generation, particularly in inference-time efficiency.", "method": "Fusion of AR and MDM paradigms to create Eso-LMs, with a focus on integrating KV caching and an optimized sampling schedule to boost inference speed.", "result": "Eso-LMs set a new state of the art on language modeling benchmarks, achieving up to 65x faster inference than standard MDMs and 4x faster than prior semi-autoregressive approaches.", "conclusion": "Eso-LMs successfully enhance the capabilities of language models by improving inference efficiency and maintaining robust performance on benchmarks.", "key_contributions": ["First integration of KV caching for MDMs while preserving parallel generation", "Achieves new state of the art benchmarks for language modeling", "Significantly improves inference speed compared to previous models"], "limitations": "", "keywords": ["language models", "masked diffusion models", "autoregressive models", "performance benchmarks", "inference efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2506.01937", "pdf": "https://arxiv.org/pdf/2506.01937.pdf", "abs": "https://arxiv.org/abs/2506.01937", "title": "RewardBench 2: Advancing Reward Model Evaluation", "authors": ["Saumya Malik", "Valentina Pyatkin", "Sander Land", "Jacob Morrison", "Noah A. Smith", "Hannaneh Hajishirzi", "Nathan Lambert"], "categories": ["cs.CL"], "comment": "Data, models, and leaderboard available at\n  https://huggingface.co/collections/allenai/reward-bench-2-683d2612a4b3e38a3e53bb51", "summary": "Reward models are used throughout the post-training of language models to\ncapture nuanced signals from preference data and provide a training target for\noptimization across instruction following, reasoning, safety, and more domains.\nThe community has begun establishing best practices for evaluating reward\nmodels, from the development of benchmarks that test capabilities in specific\nskill areas to others that test agreement with human preferences. At the same\ntime, progress in evaluation has not been mirrored by the effectiveness of\nreward models in downstream tasks -- simpler direct alignment algorithms are\nreported to work better in many cases. This paper introduces RewardBench 2, a\nnew multi-skill reward modeling benchmark designed to bring new, challenging\ndata for accuracy-based reward model evaluation -- models score about 20 points\non average lower on RewardBench 2 compared to the first RewardBench -- while\nbeing highly correlated with downstream performance. Compared to most other\nbenchmarks, RewardBench 2 sources new human prompts instead of existing prompts\nfrom downstream evaluations, facilitating more rigorous evaluation practices.\nIn this paper, we describe our benchmark construction process and report how\nexisting models perform on it, while quantifying how performance on the\nbenchmark correlates with downstream use of the models in both inference-time\nscaling algorithms, like best-of-N sampling, and RLHF training algorithms like\nproximal policy optimization.", "AI": {"tldr": "This paper presents RewardBench 2, a benchmark for evaluating reward models in language models, highlighting decreased model performance on this new benchmark compared to prior versions and emphasizing its correlation with downstream performance.", "motivation": "The need for robust evaluation of reward models used in language model training, to ensure they effectively capture user preferences and improve various tasks.", "method": "Introduction of RewardBench 2, a multi-skill benchmark using new human-generated prompts for evaluating accuracy and correlating it with downstream task performance.", "result": "Existing models show an average score about 20 points lower on RewardBench 2 than on the first RewardBench, suggesting it presents a tougher evaluation standard related to downstream task effectiveness.", "conclusion": "RewardBench 2 aims to facilitate better evaluation practices for reward models in the language model community, ultimately leading to improved model performance in real-world applications.", "key_contributions": ["Introduction of a multi-skill benchmark (RewardBench 2) for reward models in NLP.", "Establishment of rigorous evaluation practices through new human prompts.", "Demonstration of the correlation between benchmark performance and downstream model usage."], "limitations": "The effectiveness of reward models was found to be lower on this new benchmark, indicating challenges in model alignment.", "keywords": ["reward models", "benchmark", "language models", "human preferences", "evaluation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2506.01938", "pdf": "https://arxiv.org/pdf/2506.01938.pdf", "abs": "https://arxiv.org/abs/2506.01938", "title": "Novel Benchmark for NER in the Wastewater and Stormwater Domain", "authors": ["Franco Alberto Cardillo", "Franca Debole", "Francesca Frontini", "Mitra Aelami", "Nanée Chahinian", "Serge Conrad"], "categories": ["cs.CL"], "comment": null, "summary": "Effective wastewater and stormwater management is essential for urban\nsustainability and environmental protection. Extracting structured knowledge\nfrom reports and regulations is challenging due to domainspecific terminology\nand multilingual contexts. This work focuses on domain-specific Named Entity\nRecognition (NER) as a first step towards effective relation and information\nextraction to support decision making. A multilingual benchmark is crucial for\nevaluating these methods. This study develops a French-Italian domain-specific\ntext corpus for wastewater management. It evaluates state-of-the-art NER\nmethods, including LLM-based approaches, to provide a reliable baseline for\nfuture strategies and explores automated annotation projection in view of an\nextension of the corpus to new languages.", "AI": {"tldr": "The study develops a multilingual benchmark for domain-specific Named Entity Recognition in wastewater management, focusing on French and Italian texts and evaluating state-of-the-art methods, including LLM-based approaches.", "motivation": "Effective management of wastewater and stormwater is critical for urban sustainability; however, extracting structured knowledge from reports is complicated by the use of domain-specific terminology and multiple languages.", "method": "The study creates a French-Italian domain-specific text corpus and evaluates various Named Entity Recognition (NER) methods, including state-of-the-art and LLM-based approaches, for assessing performance.", "result": "The evaluation provides a reliable baseline for future strategies in multilingual NER for wastewater management and suggests possible automation in corpus extension to other languages.", "conclusion": "The findings contribute to better decision-making tools in environmental management and highlight the need for domain-specific benchmarks in NER tasks.", "key_contributions": ["Development of a multilingual benchmark for NER in wastewater management", "Evaluation of state-of-the-art and LLM-based NER methods", "Proposition for automated annotation projection to extend the corpus"], "limitations": "The study is limited to French and Italian, which may not fully capture the complexities of other languages in the domain.", "keywords": ["Named Entity Recognition", "wastewater management", "multilingual benchmark", "LLM", "information extraction"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2506.01939", "pdf": "https://arxiv.org/pdf/2506.01939.pdf", "abs": "https://arxiv.org/abs/2506.01939", "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning", "authors": ["Shenzhi Wang", "Le Yu", "Chang Gao", "Chujie Zheng", "Shixuan Liu", "Rui Lu", "Kai Dang", "Xionghui Chen", "Jianxin Yang", "Zhenru Zhang", "Yuqiong Liu", "An Yang", "Andrew Zhao", "Yang Yue", "Shiji Song", "Bowen Yu", "Gao Huang", "Junyang Lin"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 17 figures, 2 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.", "AI": {"tldr": "This work explores Reinforcement Learning with Verifiable Rewards (RLVR) through the lens of token entropy patterns, revealing the critical role of high-entropy tokens in enhancing reasoning performance of Large Language Models (LLMs).", "motivation": "To understand the mechanisms behind Reinforcement Learning with Verifiable Rewards (RLVR) and its impact on reasoning in LLMs.", "method": "The study analyzes token entropy patterns in Chain-of-Thought (CoT) reasoning and examines how these patterns evolve during RLVR training, particularly focusing on high-entropy tokens known as forking tokens.", "result": "The findings indicate that restricting policy gradient updates to high-entropy forking tokens significantly improves RLVR performance on multiple Qwen models, confirming a strong scaling trend.", "conclusion": "The research suggests that the performance of RLVR can be enhanced by optimizing the high-entropy tokens, which dictate reasoning pathways in LLMs, offering a new perspective on RLVR and its mechanisms.", "key_contributions": ["Introduced an analysis method focusing on token entropy patterns in RLVR.", "Demonstrated the critical role of high-entropy tokens in improving LLM reasoning performance.", "Achieved significant performance improvements by optimizing only 20% of tokens during training."], "limitations": "", "keywords": ["Reinforcement Learning", "Verifiable Rewards", "Large Language Models", "Token Entropy", "Reasoning Performance"], "importance_score": 9, "read_time_minutes": 25}}
{"id": "2506.01951", "pdf": "https://arxiv.org/pdf/2506.01951.pdf", "abs": "https://arxiv.org/abs/2506.01951", "title": "Self-ensemble: Mitigating Confidence Distortion for Large Language Models", "authors": ["Zicheng Xu", "Guanchu Wang", "Guangyao Zheng", "Yu-Neng Chuang", "Alexander Szalay", "Xia Hu", "Vladimir Braverman"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Although Large Language Models (LLMs) perform well in general fields, they\nexhibit a confidence distortion problem on multi-choice question-answering\n(MCQA), particularly as the number of answer choices increases. Specifically,\non MCQA with many choices, LLMs suffer from under-confidence in correct\npredictions and over-confidence in incorrect ones, leading to a substantially\ndegraded performance. To solve this problem, we propose Self-ensemble in this\nwork. Our method splits the choices into several groups and ensembles LLM\npredictions across these groups to reach a final decision. The advantage of\nSelf-ensemble is its plug-and-play nature, where it can be integrated into\nexisting LLM architecture based on a designed attention mask and positional\nencoding, without requiring labeled datasets for parameter tuning. Experimental\nresults on three LLMs and datasets demonstrate that Self-ensemble\ncomprehensively addresses the confidence distortion problem of LLMs,\noutperforming standard inference as well as baseline methods.", "AI": {"tldr": "This paper addresses the confidence distortion problem in Large Language Models during multi-choice question-answering by proposing a method called Self-ensemble, which improves predictions by grouping choices and ensembling results without needing additional labeled data.", "motivation": "Large Language Models suffer from under-confidence in correct answers and over-confidence in incorrect ones during multi-choice question-answering, particularly with a high number of choices, affecting their overall performance.", "method": "The proposed Self-ensemble method groups answer choices and ensembles predictions across these groups, utilizing attention masks and positional encoding, thus fitting into existing LLM architectures without extra parameter tuning.", "result": "Self-ensemble demonstrates improved performance on multi-choice question-answering tasks across three different LLMs and datasets, successfully addressing the confidence distortion issue.", "conclusion": "The experimental results indicate that Self-ensemble is effective in overcoming the limitations of confidence distortion in LLMs, outperforming standard inference and baseline methods.", "key_contributions": ["Introduction of the Self-ensemble method to address confidence distortion in LLMs.", "Demonstration of effectiveness across multiple LLMs and datasets.", "Plug-and-play integration with existing architectures without the need for additional labeled data."], "limitations": "", "keywords": ["Large Language Models", "Confidence distortion", "Multi-choice question-answering", "Self-ensemble", "Machine Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2506.01952", "pdf": "https://arxiv.org/pdf/2506.01952.pdf", "abs": "https://arxiv.org/abs/2506.01952", "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web Tasks", "authors": ["Atsuyuki Miyai", "Zaiying Zhao", "Kazuki Egashira", "Atsuki Sato", "Tatsumi Sunada", "Shota Onohara", "Hiromasa Yamanishi", "Mashiro Toyooka", "Kunato Nishina", "Ryoma Maeda", "Kiyoharu Aizawa", "Toshihiko Yamasaki"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://webchorearena.github.io/", "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.", "AI": {"tldr": "WebChoreArena is a new benchmark for evaluating LLMs' performance on complex web browsing tasks beyond general navigation, featuring 532 tasks across three challenge categories.", "motivation": "As web browsing agents improve, there is a need to evaluate their capabilities on more complex and tedious tasks that require greater cognitive effort from users.", "method": "WebChoreArena includes three key challenge categories: Massive Memory tasks, Calculation tasks, and Long-Term Memory tasks, built on existing WebArena environments to ensure reproducibility.", "result": "Experimental results show significant performance improvements in LLMs like GPT-4o and Gemini 2.5 Pro on WebChoreArena tasks, indicating its effectiveness in measuring agent progress.", "conclusion": "WebChoreArena can better assess state-of-the-art LLM advancements but also highlights existing limitations in task performance.", "key_contributions": ["Introduction of a new benchmark for complex web tasks", "Emphasis on reproducibility in benchmark design", "Identification of LLM performance gaps in challenging tasks"], "limitations": "Current LLMs still show substantial room for improvement across WebChoreArena tasks compared to general browsing performance.", "keywords": ["web browsing", "large language models", "benchmarking", "human-computer interaction", "machine learning"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2506.01954", "pdf": "https://arxiv.org/pdf/2506.01954.pdf", "abs": "https://arxiv.org/abs/2506.01954", "title": "DRAG: Distilling RAG for SLMs from LLMs to Transfer Knowledge and Mitigate Hallucination via Evidence and Graph-based Distillation", "authors": ["Jennifer Chen", "Aidar Myrzakhan", "Yaxin Luo", "Hassaan Muhammad Khan", "Sondos Mahmoud Bsharat", "Zhiqiang Shen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Main. Code is available at https://github.com/VILA-Lab/DRAG", "summary": "Retrieval-Augmented Generation (RAG) methods have proven highly effective for\ntasks requiring factual consistency and robust knowledge retrieval. However,\nlarge-scale RAG systems consume significant computational resources and are\nprone to generating hallucinated content from Humans. In this work, we\nintroduce $\\texttt{DRAG}$, a novel framework for distilling RAG knowledge from\nlarge-scale Language Models (LLMs) into small LMs (SLMs). Our approach\nleverages evidence- and knowledge graph-based distillation, ensuring that the\ndistilled model retains critical factual knowledge while significantly reducing\nmodel size and computational cost. By aligning the smaller model's predictions\nwith a structured knowledge graph and ranked evidence, $\\texttt{DRAG}$\neffectively mitigates hallucinations and improves factual accuracy. We further\npresent a case demonstrating how our framework mitigates user privacy risks and\nintroduce a corresponding benchmark. Experimental evaluations on multiple\nbenchmarks demonstrate that our method outperforms the prior competitive RAG\nmethods like MiniRAG for SLMs by up to 27.7% using the same models, preserving\nhigh-level efficiency and reliability. With $\\texttt{DRAG}$, we provide a\npractical and resource-efficient roadmap to deploying enhanced retrieval and\ngeneration capabilities in small-sized LLMs.", "AI": {"tldr": "Introducing DRAG, a framework that distills RAG knowledge from large LLMs into smaller models to enhance factual accuracy and reduce computational costs.", "motivation": "Current RAG systems are resource-intensive and often generate hallucinated content. There is a need for more efficient models that maintain factual consistency.", "method": "DRAG uses evidence- and knowledge graph-based distillation, aligning smaller model predictions with structured knowledge to mitigate hallucination and improve accuracy.", "result": "Experimental evaluations show DRAG outperforms MiniRAG by up to 27.7% while preserving efficiency and reliability in smaller models.", "conclusion": "DRAG provides a resource-efficient method for deploying enhanced retrieval and generation capabilities in small LLMs, addressing both accuracy and size concerns.", "key_contributions": ["Introduction of the DRAG framework for knowledge distillation", "Demonstrated reduction of hallucinations in smaller models", "Experimental validation showing performance improvements over competitive methods"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Knowledge Distillation", "Language Models", "Factual Accuracy", "Privacy Risks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2312.06562", "pdf": "https://arxiv.org/pdf/2312.06562.pdf", "abs": "https://arxiv.org/abs/2312.06562", "title": "On Meta-Prompting", "authors": ["Adrian de Wynter", "Xun Wang", "Qilong Gu", "Si-Qing Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG", "math.CT"], "comment": null, "summary": "Modern large language models (LLMs) are capable of interpreting input strings\nas instructions, or prompts, and carry out tasks based on them. Unlike\ntraditional learners, LLMs cannot use back-propagation to obtain feedback, and\ncondition their output in situ in a phenomenon known as in-context learning\n(ICL). Many approaches to prompting and pre-training these models involve the\nautomated generation of these prompts, also known as meta-prompting, or\nprompting to obtain prompts. However, they do not formally describe the\nproperties and behavior of the LLMs themselves. We propose a theoretical\nframework based on category theory to generalize and describe ICL and LLM\nbehavior when interacting with users. Our framework allows us to obtain formal\nresults around task agnosticity and equivalence of various meta-prompting\napproaches. Using our framework and experimental results we argue that\nmeta-prompting is more effective than basic prompting at generating desirable\noutputs.", "AI": {"tldr": "This paper presents a theoretical framework based on category theory to describe in-context learning and behavior of large language models (LLMs) during interaction with users, showing that meta-prompting is more effective than basic prompting.", "motivation": "To formally describe the properties and behavior of LLMs and improve the effectiveness of prompt generation strategies like meta-prompting.", "method": "The authors propose a framework rooted in category theory to analyze in-context learning and to compare various meta-prompting approaches in LLMs.", "result": "The framework provides formal results highlighting task agnosticity and equivalence among different meta-prompting approaches, revealing that meta-prompting generally yields better outputs compared to basic prompting.", "conclusion": "The results suggest that a structured theoretical approach can enhance our understanding and effectiveness of LLM prompting techniques.", "key_contributions": ["A theoretical framework based on category theory for LLM behavior analysis", "Formal insights on task agnosticity and prompt equivalence", "Demonstration that meta-prompting is superior to basic prompting"], "limitations": "", "keywords": ["large language models", "in-context learning", "meta-prompting", "category theory", "prompting strategies"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2312.17055", "pdf": "https://arxiv.org/pdf/2312.17055.pdf", "abs": "https://arxiv.org/abs/2312.17055", "title": "Beyond Output Matching: Bidirectional Alignment for Enhanced In-Context Learning", "authors": ["Chengwei Qin", "Wenhan Xia", "Fangkai Jiao", "Chen Chen", "Yuchen Hu", "Bosheng Ding", "Ruirui Chen", "Shafiq Joty"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive few-shot generalization on\nmany tasks via in-context learning (ICL). Despite their success in showing such\nemergent abilities, the scale and complexity of larger models also lead to\nunprecedentedly high computational demands and deployment challenges. In\nreaction, researchers explore transferring the powerful capabilities of larger\nmodels to more efficient and compact models by typically aligning the output of\nsmaller (student) models with that of larger (teacher) models. Existing methods\neither train student models on the generated outputs of teacher models or\nimitate their token-level probability distributions. However, these\ndistillation methods pay little to no attention to the input, which also plays\na crucial role in ICL. Based on the finding that the performance of ICL is\nhighly sensitive to the selection of demonstration examples, we propose\nBidirectional Alignment (BiAlign) to fully leverage the models' preferences for\nICL examples to improve the ICL abilities of student models. Specifically, we\nintroduce the alignment of input preferences between student and teacher models\nby incorporating a novel ranking loss, in addition to aligning the token-level\noutput distribution. With extensive experiments and analysis, we demonstrate\nthat BiAlign can consistently outperform existing baselines on a variety of\ntasks involving language understanding, reasoning, and coding.", "AI": {"tldr": "BiAlign improves few-shot generalization in student models by aligning input preferences with teacher models, enhancing in-context learning capabilities.", "motivation": "To address the high computational demands of large language models by transferring their capabilities to more efficient student models while considering input preferences that affect in-context learning.", "method": "BiAlign introduces a ranking loss to align the input preferences between student and teacher models, in addition to token-level output alignment.", "result": "BiAlign consistently outperforms baselines across various tasks, including language understanding, reasoning, and coding.", "conclusion": "The proposed method effectively enhances the few-shot generalization capabilities of student models by utilizing input preference alignment.", "key_contributions": ["Introduction of Bidirectional Alignment (BiAlign) method for model distillation", "Incorporation of a ranking loss to align input preferences between models", "Demonstration of consistent performance improvements over existing methods in multiple tasks"], "limitations": "", "keywords": ["few-shot learning", "model distillation", "in-context learning", "ranking loss", "language models"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2401.16092", "pdf": "https://arxiv.org/pdf/2401.16092.pdf", "abs": "https://arxiv.org/abs/2401.16092", "title": "Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You", "authors": ["Felix Friedrich", "Katharina Hämmerl", "Patrick Schramowski", "Manuel Brack", "Jindrich Libovicky", "Kristian Kersting", "Alexander Fraser"], "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Text-to-image generation models have recently achieved astonishing results in\nimage quality, flexibility, and text alignment, and are consequently employed\nin a fast-growing number of applications. Through improvements in multilingual\nabilities, a larger community now has access to this technology. However, our\nresults show that multilingual models suffer from significant gender biases\njust as monolingual models do. Furthermore, the natural expectation that\nmultilingual models will provide similar results across languages does not hold\nup. Instead, there are important differences between languages. We propose a\nnovel benchmark, MAGBIG, intended to foster research on gender bias in\nmultilingual models. We use MAGBIG to investigate the effect of multilingualism\non gender bias in T2I models. To this end, we construct multilingual prompts\nrequesting portraits of people with a certain occupation or trait. Our results\nshow that not only do models exhibit strong gender biases but they also behave\ndifferently across languages. Furthermore, we investigate prompt engineering\nstrategies, such as indirect, neutral formulations, to mitigate these biases.\nUnfortunately, these approaches have limited success and result in worse\ntext-to-image alignment. Consequently, we call for more research into diverse\nrepresentations across languages in image generators, as well as into\nsteerability to address biased model behavior.", "AI": {"tldr": "This paper examines gender biases in multilingual text-to-image generation models and proposes a benchmark (MAGBIG) to study these biases, revealing that biases persist and vary across languages, with limited effectiveness of prompt engineering to mitigate them.", "motivation": "To investigate and highlight gender biases present in multilingual text-to-image generation models, which has implications for their use in broader applications.", "method": "The study employs a novel benchmark called MAGBIG to analyze gender bias in multilingual T2I models by constructing multilingual prompts asking for portraits based on specific occupations or traits.", "result": "The findings indicate that multilingual models exhibit significant gender biases similar to monolingual models, with notable variations in behavior across different languages.", "conclusion": "There is a need for further research into diverse representations and the steering of multilingual models to address and reduce biases in image generation.", "key_contributions": ["Proposed the MAGBIG benchmark for studying gender biases in multilingual T2I models.", "Highlighted significant gender biases in multilingual models that differ across languages.", "Investigated prompt engineering strategies for bias mitigation, showing their limitations."], "limitations": "The prompt engineering strategies explored have limited success and can result in poorer text-to-image alignment.", "keywords": ["gender bias", "multilingual models", "text-to-image generation", "MAGBIG", "prompt engineering"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2402.16837", "pdf": "https://arxiv.org/pdf/2402.16837.pdf", "abs": "https://arxiv.org/abs/2402.16837", "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?", "authors": ["Sohee Yang", "Elena Gribovskaya", "Nora Kassner", "Mor Geva", "Sebastian Riedel"], "categories": ["cs.CL"], "comment": "ACL 2024", "summary": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.", "AI": {"tldr": "This paper investigates whether Large Language Models can perform multi-hop reasoning by analyzing their responses to complex prompts involving indirect references to bridge entities.", "motivation": "The study aims to explore the latent reasoning pathways in Large Language Models when prompted with complex queries, particularly focusing on their ability to complete prompts that require multi-hop reasoning.", "method": "The research involves testing LLMs with prompts that mention bridge entities indirectly and measuring their recall and utilization of information related to these entities.", "result": "The authors found substantial evidence of latent multi-hop reasoning in LLMs for certain prompt types, with strong retrieval for the first hop but moderate results for the full traversal; scaling trends were observed with model size.", "conclusion": "Results indicate both challenges and opportunities in enhancing LLM capabilities for multi-hop reasoning tasks; the findings highlight the complexity in how LLMs utilize knowledge depending on prompt structure.", "key_contributions": ["Identification of latent multi-hop reasoning pathways in LLMs", "Empirical evidence showing dependency on prompt structure", "Observation of scaling effects relating to model size and reasoning performance"], "limitations": "The evidence for the second hop's reasoning was moderate and context-dependent; scaling effects were inconsistent for different reasoning hops.", "keywords": ["Large Language Models", "multi-hop reasoning", "prompt analysis", "latent reasoning", "scaling trends"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.10508", "pdf": "https://arxiv.org/pdf/2404.10508.pdf", "abs": "https://arxiv.org/abs/2404.10508", "title": "White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs", "authors": ["Yixin Wan", "Kai-Wei Chang"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Social biases can manifest in language agency. However, very limited research\nhas investigated such biases in Large Language Model (LLM)-generated content.\nIn addition, previous works often rely on string-matching techniques to\nidentify agentic and communal words within texts, falling short of accurately\nclassifying language agency. We introduce the Language Agency Bias Evaluation\n(LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing\nagency levels attributed to different demographic groups in model generations.\nLABE tests for gender, racial, and intersectional language agency biases in\nLLMs on 3 text generation tasks: biographies, professor reviews, and reference\nletters. Using LABE, we unveil language agency social biases in 3 recent LLMs:\nChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to\ndemonstrate greater gender bias than human-written texts; (2) Models\ndemonstrate remarkably higher levels of intersectional bias than the other bias\naspects. (3) Prompt-based mitigation is unstable and frequently leads to bias\nexacerbation. Based on our observations, we propose Mitigation via Selective\nRewrite (MSR), a novel bias mitigation strategy that leverages an agency\nclassifier to identify and selectively revise parts of generated texts that\ndemonstrate communal traits. Empirical results prove MSR to be more effective\nand reliable than prompt-based mitigation method, showing a promising research\ndirection.", "AI": {"tldr": "The paper introduces the Language Agency Bias Evaluation (LABE) benchmark to assess social biases in LLM-generated content across demographic groups, finding significant gender and intersectional biases.", "motivation": "To address the limited research on biases in language agency within LLM-generated content.", "method": "The authors developed the LABE benchmark to evaluate biases by analyzing agency levels in LLM outputs across various demographic groups on three text generation tasks.", "result": "LABE revealed that LLM outputs exhibit greater gender bias than human-written texts and higher intersectional biases. Proposed Mitigation via Selective Rewrite (MSR) was found to be more effective than traditional methods.", "conclusion": "The findings highlight the need for better bias mitigation strategies in LLMs and suggest that MSR offers a promising approach.", "key_contributions": ["Introduction of the LABE benchmark for evaluating language agency biases in LLMs.", "Identification of significant social biases in LLM-generated content across gender and intersectionality.", "Development of the Mitigation via Selective Rewrite (MSR) strategy to improve bias correction."], "limitations": "The study primarily focuses on three specific text generation tasks and certain LLMs, which may not generalize to all contexts.", "keywords": ["Language Agency", "Bias Evaluation", "Large Language Models", "Bias Mitigation", "Social Biases"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2404.12728", "pdf": "https://arxiv.org/pdf/2404.12728.pdf", "abs": "https://arxiv.org/abs/2404.12728", "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?", "authors": ["Chengwei Qin", "Wenhan Xia", "Tan Wang", "Fangkai Jiao", "Yuchen Hu", "Bosheng Ding", "Ruirui Chen", "Shafiq Joty"], "categories": ["cs.CL"], "comment": null, "summary": "Analogical reasoning is a unique ability of humans to address unfamiliar\nchallenges by transferring strategies from relevant past experiences. One key\nfinding in psychology is that compared with irrelevant past experiences,\nrecalling relevant ones can help humans better handle new tasks.\nCoincidentally, the NLP community has also recently found that self-generating\nrelevant examples in the context can help large language models (LLMs) better\nsolve a given problem than hand-crafted prompts. However, it is yet not clear\nwhether relevance is the key factor eliciting such capability, i.e., can LLMs\nbenefit more from self-generated relevant examples than irrelevant ones? In\nthis work, we systematically explore whether LLMs can truly perform analogical\nreasoning on a diverse set of reasoning tasks. With extensive experiments and\nanalysis, we show that self-generated random examples can surprisingly achieve\ncomparable or even better performance on certain tasks, e.g., 4% performance\nboost on GSM8K with random biological examples. We find that the accuracy of\nself-generated examples is the key factor and subsequently design two novel\nmethods with improved performance and significantly reduced inference costs.\nOverall, we aim to advance a deeper understanding of LLM analogical reasoning\nand hope this work stimulates further research in the design of self-generated\ncontexts.", "AI": {"tldr": "This paper investigates the role of relevant versus irrelevant self-generated examples in enhancing analogical reasoning in large language models (LLMs), showing that relevant examples significantly improve performance on various tasks.", "motivation": "Understanding whether LLMs benefit more from relevant self-generated examples compared to irrelevant ones in analogy-based problem-solving.", "method": "Systematic exploration through extensive experiments on diverse reasoning tasks, comparing the effectiveness of self-generated examples based on their relevance.", "result": "Self-generated random examples provided comparable or better performance on certain tasks, notably a 4% boost on GSM8K using biological examples.", "conclusion": "The accuracy of self-generated examples is critical, leading to the design of two novel methods that demonstrate improved performance and reduced inference costs.", "key_contributions": ["Demonstrates the effectiveness of relevant examples in LLM reasoning tasks", "Designs two novel methods that enhance performance", "Provides insights into LLMs' analogical reasoning capabilities"], "limitations": "", "keywords": ["analogical reasoning", "large language models", "self-generated examples"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2405.00557", "pdf": "https://arxiv.org/pdf/2405.00557.pdf", "abs": "https://arxiv.org/abs/2405.00557", "title": "Mixture of insighTful Experts (MoTE): The Synergy of Thought Chains and Expert Mixtures in Self-Alignment", "authors": ["Zhili Liu", "Yunhao Gou", "Kai Chen", "Lanqing Hong", "Jiahui Gao", "Fei Mi", "Yu Zhang", "Zhenguo Li", "Xin Jiang", "Qun Liu", "James T. Kwok"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to expand,\naligning these models with human values remains a significant challenge. Recent\nstudies show that reasoning abilities contribute significantly to model safety,\nwhile integrating Mixture-of-Experts (MoE) architectures can further enhance\nalignment. In this work, we address a fundamental question: How to effectively\nincorporate reasoning abilities and MoE architectures into self-alignment\nprocess in LLMs? We propose Mixture of insighTful Experts (MoTE), a novel\nframework that synergistically combines reasoning chains and expert mixtures to\nimprove self-alignments. From a data perspective, MoTE employs a structured\nreasoning chain comprising four key stages: Question Analysis, Answer Guidance,\nSafe Answer, and Safety Checking. This approach enhances safety through\nmulti-step reasoning and proves effective even for smaller and less powerful\nLLMs (e.g., 7B models). From an architectural perspective, MoTE adopts a\nmulti-LoRA framework with step-level routing, where each expert is dedicated to\na specific reasoning step. This design eliminates the need for balance losses,\nensures stable training, and supports adaptive inference lengths. Experimental\nresults demonstrate that MoTE significantly improves model safety, jailbreak\nresistance, and over-refusal capabilities, achieving performance comparable to\nOpenAI's state-of-the-art o1 model.", "AI": {"tldr": "This paper introduces MoTE, a framework that combines reasoning abilities and Mixture-of-Experts architectures to enhance self-alignment and safety in large language models.", "motivation": "Aligning large language models with human values, particularly focusing on improving their reasoning capabilities and safety mechanisms.", "method": "The MoTE framework integrates a structured reasoning chain with four stages—Question Analysis, Answer Guidance, Safe Answer, and Safety Checking—while employing a multi-LoRA architecture for expert routing.", "result": "Experimental results show that MoTE enhances model safety, increases jailbreak resistance, and improves over-refusal capabilities, achieving results comparable to state-of-the-art models.", "conclusion": "The proposed method effectively improves the alignment and safety of LLMs, demonstrating significant advantages over traditional approaches even in smaller models.", "key_contributions": ["Introduction of MoTE framework", "Combination of reasoning chains and expert mixtures", "Demonstration of enhanced safety in smaller LLMs"], "limitations": "", "keywords": ["large language models", "Mixture of Experts", "model safety", "reasoning", "human alignment"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.03205", "pdf": "https://arxiv.org/pdf/2405.03205.pdf", "abs": "https://arxiv.org/abs/2405.03205", "title": "Anchored Answers: Unravelling Positional Bias in GPT-2's Multiple-Choice Questions", "authors": ["Ruizhe Li", "Yanjun Gao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs), such as the GPT-4 and LLaMA families, have\ndemonstrated considerable success across diverse tasks, including\nmultiple-choice questions (MCQs). However, these models exhibit a positional\nbias, particularly an even worse anchored bias in the GPT-2 family, where they\nconsistently favour the first choice 'A' in MCQs during inference. This\nanchored bias challenges the integrity of GPT-2's decision-making process, as\nit skews performance based on the position rather than the content of the\nchoices in MCQs. In this study, we utilise the mechanistic interpretability\napproach to identify the internal modules within GPT-2 models responsible for\nthis bias. We focus on the Multi-Layer Perceptron (MLP) layers and attention\nheads, using the \"logit lens\" method to trace and modify the specific value\nvectors that contribute to the bias. By updating these vectors within MLP and\nrecalibrating attention patterns to neutralise the preference for the first\nchoice 'A', we effectively mitigate the anchored bias. Our interventions not\nonly mitigate the bias but also improve the overall MCQ prediction accuracy for\nthe GPT-2 family across various datasets. This work represents the first\ncomprehensive mechanistic analysis of anchored bias from the failing cases in\nMCQs within the GPT-2 models, introducing targeted, minimal-intervention\nstrategies that significantly enhance GPT2 model robustness and accuracy in\nMCQs. Our code is available at\nhttps://github.com/ruizheliUOA/Anchored_Bias_GPT2.", "AI": {"tldr": "The study addresses the anchored bias in GPT-2 models, particularly their preference for the first choice in multiple-choice questions, and proposes a mechanistic interpretability approach to mitigate this bias.", "motivation": "The increasing reliance on Large Language Models (LLMs) for tasks like multiple-choice questions raises concerns about biases affecting their decision-making, specifically the anchored bias observed in GPT-2 models.", "method": "Utilizing the mechanistic interpretability approach, the study examines the MLP layers and attention heads of GPT-2 models, employing the 'logit lens' method to track and modify specific value vectors responsible for the detrimentally biased preference for the first choice.", "result": "The intervention succeeds in modifying the biased preference for the first option, resulting in improved overall prediction accuracy in MCQs for GPT-2 across various datasets.", "conclusion": "Targeted interventions can effectively neutralize anchored bias in GPT-2 models, enhancing their robustness and making the models more reliable for multiple-choice tasks.", "key_contributions": ["First comprehensive mechanistic analysis of anchored bias in GPT-2 models during MCQ tasks.", "Proposes minimal-intervention strategies to mitigate bias and improve prediction accuracy.", "Provides accessible code for further research and validation."], "limitations": "", "keywords": ["Large Language Models", "GPT-2", "anchored bias", "mechanistic interpretability", "multiple-choice questions"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2405.11200", "pdf": "https://arxiv.org/pdf/2405.11200.pdf", "abs": "https://arxiv.org/abs/2405.11200", "title": "LexGen: Domain-aware Multilingual Lexicon Generation", "authors": ["Ayush Maheshwari", "Atul Kumar Singh", "Karthika NJ", "Krishnakant Bhatt", "Preethi Jyothi", "Ganesh Ramakrishnan"], "categories": ["cs.CL"], "comment": "ACL Main Conference, 2025", "summary": "Lexicon or dictionary generation across domains has the potential for\nsocietal impact, as it can potentially enhance information accessibility for a\ndiverse user base while preserving language identity. Prior work in the field\nprimarily focuses on bilingual lexical induction, which deals with word\nalignments using mapping or corpora-based approaches. However, these approaches\ndo not cater to domain-specific lexicon generation that consists of\ndomain-specific terminology. This task becomes particularly important in\nspecialized medical, engineering, and other technical domains, owing to the\nhighly infrequent usage of the terms and scarcity of data involving\ndomain-specific terms especially for low/mid-resource languages. In this paper,\nwe propose a new model to generate dictionary words for $6$ Indian languages in\nthe multi-domain setting. Our model consists of domain-specific and\ndomain-generic layers that encode information, and these layers are invoked via\na learnable routing technique. We also release a new benchmark dataset\nconsisting of >75K translation pairs across 6 Indian languages spanning 8\ndiverse domains.We conduct both zero-shot and few-shot experiments across\nmultiple domains to show the efficacy of our proposed model in generalizing to\nunseen domains and unseen languages. Additionally, we also perform a post-hoc\nhuman evaluation on unseen languages. The source code and dataset is present at\nhttps://github.com/Atulkmrsingh/lexgen.", "AI": {"tldr": "The paper presents a model for generating domain-specific dictionaries for six Indian languages, utilizing a novel learnable routing technique, and challenges the limitations of existing approaches in low/mid-resource languages.", "motivation": "The goal is to enhance information accessibility in specialized domains, particularly in low/mid-resource languages, by generating domain-specific lexicons.", "method": "The proposed model has domain-specific and domain-generic layers, which encode information and operate via a learnable routing technique.", "result": "Experiments demonstrate the model's effectiveness in zero-shot and few-shot settings across multiple domains, as well as human evaluation showing competent performance on unseen languages.", "conclusion": "The model contributes to the field by providing a scalable solution for domain-specific lexicon generation, with an accompanying benchmark dataset for future research.", "key_contributions": ["Introduction of a multi-domain lexicon generation model for Indian languages", "Release of a benchmark dataset with over 75K translation pairs", "Demonstration of model efficacy in low/mid-resource language settings"], "limitations": "", "keywords": ["lexicon generation", "multi-domain", "Indian languages", "few-shot learning", "zero-shot learning"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2405.18915", "pdf": "https://arxiv.org/pdf/2405.18915.pdf", "abs": "https://arxiv.org/abs/2405.18915", "title": "Towards Better Chain-of-Thought: A Reflection on Effectiveness and Faithfulness", "authors": ["Jiachun Li", "Pengfei Cao", "Yubo Chen", "Jiexin Xu", "Huaijun Li", "Xiaojian Jiang", "Kang Liu", "Jun Zhao"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 21 figures, accepted by ACL 2025 Findings", "summary": "Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.", "AI": {"tldr": "This paper analyzes the performance of chain-of-thought (CoT) prompting in LLMs, focusing on factors that affect its effectiveness and faithfulness. It introduces a novel algorithm to enhance CoT by recalling information from questions, improving both effectiveness and faithfulness.", "motivation": "To provide an in-depth analysis of patterns that influence the performance of chain-of-thought (CoT) prompting in reasoning tasks, particularly examining factors affecting its effectiveness and faithfulness.", "method": "The study identifies key factors influencing CoT effectiveness, such as problem difficulty, information gain, and information flow. It also conducts a joint analysis on information interactions among the question, CoT, and answer to interpret unfaithful CoT issues. A novel algorithm is proposed to enhance CoT generation by recalling extra information from the question.", "result": "The proposed algorithm significantly enhances both the faithfulness and effectiveness of CoT prompting, as demonstrated through extensive experiments.", "conclusion": "The findings highlight the importance of information interactions in CoT prompting and propose a solution for improving CoT quality through additional information recall.", "key_contributions": ["Identification of key factors influencing CoT effectiveness", "Development of a novel algorithm to enhance CoT performance", "Joint analysis of information interactions to interpret CoT faithfulness issues."], "limitations": "The findings are based on experiments conducted in controlled environments which may not fully represent real-world applications.", "keywords": ["chain-of-thought prompting", "LLM performance", "information gain", "faithfulness", "algorithm"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2406.02394", "pdf": "https://arxiv.org/pdf/2406.02394.pdf", "abs": "https://arxiv.org/abs/2406.02394", "title": "Pattern Recognition or Medical Knowledge? The Problem with Multiple-Choice Questions in Medicine", "authors": ["Maxime Griot", "Jean Vanderdonckt", "Demet Yuksel", "Coralie Hemptinne"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 main", "summary": "Large Language Models (LLMs) such as ChatGPT demonstrate significant\npotential in the medical domain and are often evaluated using multiple-choice\nquestions (MCQs) modeled on exams like the USMLE. However, such benchmarks may\noverestimate true clinical understanding by rewarding pattern recognition and\ntest-taking heuristics. To investigate this, we created a fictional medical\nbenchmark centered on an imaginary organ, the Glianorex, allowing us to\nseparate memorized knowledge from reasoning ability. We generated textbooks and\nMCQs in English and French using leading LLMs, then evaluated proprietary,\nopen-source, and domain-specific models in a zero-shot setting. Despite the\nfictional content, models achieved an average score of 64%, while physicians\nscored only 27%. Fine-tuned medical models outperformed base models in English\nbut not in French. Ablation and interpretability analyses revealed that models\nfrequently relied on shallow cues, test-taking strategies, and hallucinated\nreasoning to identify the correct choice. These results suggest that standard\nMCQ-based evaluations may not effectively measure clinical reasoning and\nhighlight the need for more robust, clinically meaningful assessment methods\nfor LLMs.", "AI": {"tldr": "This paper critiques the use of multiple-choice questions to evaluate the clinical reasoning of Large Language Models (LLMs) in the medical domain, highlighting discrepancies in performance between models and physicians.", "motivation": "To investigate the effectiveness of using multiple-choice questions (MCQs) for evaluating clinical understanding in LLMs, and to address the issue of overestimation of true clinical reasoning abilities based on pattern recognition.", "method": "A fictional medical benchmark was created featuring an imaginary organ, the Glianorex. Textbooks and MCQs in English and French were generated using leading LLMs, and various models were evaluated in a zero-shot setting.", "result": "Models achieved an average score of 64% on the benchmark, while physicians scored only 27%. Fine-tuned medical models performed better than base models in English but not in French, and models often relied on shallow cues and hallucinations.", "conclusion": "Standard MCQ evaluations may not effectively measure clinical reasoning in LLMs, indicating a need for more substantive assessment methods in the medical field.", "key_contributions": ["Introduced a fictional benchmark to test LLM capabilities in clinical reasoning.", "Demonstrated the limitations of current MCQ evaluations in measuring true clinical understanding.", "Showed significant differences in performance between models and human physicians."], "limitations": "The benchmark used fictional content, which may limit the applicability of findings to real-world scenarios.", "keywords": ["Large Language Models", "medical evaluation", "clinical reasoning", "benchmarking", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.06279", "pdf": "https://arxiv.org/pdf/2406.06279.pdf", "abs": "https://arxiv.org/abs/2406.06279", "title": "Multi-Prompting Decoder Helps Better Language Understanding", "authors": ["Zifeng Cheng", "Zhaoling Chen", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Shiping Ge", "Qing Gu"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Recent Pre-trained Language Models (PLMs) usually only provide users with the\ninference APIs, namely the emerging Model-as-a-Service (MaaS) setting. To adapt\nMaaS PLMs to downstream tasks without accessing their parameters and gradients,\nsome existing methods focus on the output-side adaptation of PLMs, viewing the\nPLM as an encoder and then optimizing a task-specific decoder for decoding the\noutput hidden states and class scores of the PLM. Despite the effectiveness of\nthese methods, they only use a single prompt to query PLMs for decoding,\nleading to a heavy reliance on the quality of the adopted prompt. In this\npaper, we propose a simple yet effective Multi-Prompting Decoder (MPD)\nframework for MaaS adaptation. The core idea is to query PLMs with multiple\ndifferent prompts for each sample, thereby obtaining multiple output hidden\nstates and class scores for subsequent decoding. Such multi-prompting decoding\nparadigm can simultaneously mitigate reliance on the quality of a single\nprompt, alleviate the issue of data scarcity under the few-shot setting, and\nprovide richer knowledge extracted from PLMs. Specifically, we propose two\ndecoding strategies: multi-prompting decoding with optimal transport for hidden\nstates and calibrated decoding for class scores. Extensive experiments\ndemonstrate that our method achieves new state-of-the-art results on multiple\nnatural language understanding datasets under the few-shot setting.", "AI": {"tldr": "This paper introduces a Multi-Prompting Decoder (MPD) framework for adapting Pre-trained Language Models (PLMs) in a Model-as-a-Service setting, enhancing performance by querying with multiple prompts.", "motivation": "To address the limitations of single-prompt approaches for adapting PLMs in downstream tasks, particularly in the context of few-shot learning where data scarcity is a concern.", "method": "The Multi-Prompting Decoder (MPD) framework queries PLMs with multiple different prompts for each sample, allowing for the extraction of multiple hidden states and class scores for improved decoding.", "result": "The MPD framework demonstrates superior performance, achieving state-of-the-art results on various natural language understanding tasks under few-shot conditions.", "conclusion": "The multi-prompting approach reduces dependence on single prompts, enhances knowledge extraction from PLMs, and effectively addresses issues related to data scarcity in few-shot settings.", "key_contributions": ["Introduction of the Multi-Prompting Decoder (MPD) framework.", "Development of two decoding strategies: optimal transport for hidden states and calibrated decoding for class scores.", "Demonstration of state-of-the-art performance on multiple datasets."], "limitations": "", "keywords": ["Multi-Prompting Decoder", "Model-as-a-Service", "Natural Language Understanding"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2406.11093", "pdf": "https://arxiv.org/pdf/2406.11093.pdf", "abs": "https://arxiv.org/abs/2406.11093", "title": "RAEmoLLM: Retrieval Augmented LLMs for Cross-Domain Misinformation Detection Using In-Context Learning Based on Emotional Information", "authors": ["Zhiwei Liu", "Kailai Yang", "Qianqian Xie", "Christine de Kock", "Sophia Ananiadou", "Eduard Hovy"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Main)", "summary": "Misinformation is prevalent in various fields such as education, politics,\nhealth, etc., causing significant harm to society. However, current methods for\ncross-domain misinformation detection rely on effort- and resource-intensive\nfine-tuning and complex model structures. With the outstanding performance of\nLLMs, many studies have employed them for misinformation detection.\nUnfortunately, they focus on in-domain tasks and do not incorporate significant\nsentiment and emotion features (which we jointly call {\\em affect}). In this\npaper, we propose RAEmoLLM, the first retrieval augmented (RAG) LLMs framework\nto address cross-domain misinformation detection using in-context learning\nbased on affective information. RAEmoLLM includes three modules. (1) In the\nindex construction module, we apply an emotional LLM to obtain affective\nembeddings from all domains to construct a retrieval database. (2) The\nretrieval module uses the database to recommend top K examples (text-label\npairs) from source domain data for target domain contents. (3) These examples\nare adopted as few-shot demonstrations for the inference module to process the\ntarget domain content. The RAEmoLLM can effectively enhance the general\nperformance of LLMs in cross-domain misinformation detection tasks through\naffect-based retrieval, without fine-tuning. We evaluate our framework on three\nmisinformation benchmarks. Results show that RAEmoLLM achieves significant\nimprovements compared to the other few-shot methods on three datasets, with the\nhighest increases of 15.64%, 31.18%, and 15.73% respectively. This project is\navailable at https://github.com/lzw108/RAEmoLLM.", "AI": {"tldr": "RAEmoLLM introduces a retrieval augmented LLM framework for cross-domain misinformation detection, leveraging affective information without fine-tuning.", "motivation": "Misinformation detection is critical across various fields, but existing methods are resource-intensive and do not充分利用情感特征.", "method": "The proposed framework consists of three modules: index construction using emotional LLMs for affective embeddings, a retrieval module to recommend example pairs, and an inference module utilizing these examples for processing target content.", "result": "RAEmoLLM significantly outperformed traditional few-shot methods on three misinformation benchmarks, with improvements of up to 31.18%.", "conclusion": "The framework enhances LLM performance in misinformation detection tasks without requiring fine-tuning, demonstrating the importance of affect-based retrieval.", "key_contributions": ["First retrieval augmented LLM framework for cross-domain misinformation detection", "Utilization of affective information to improve model performance", "Demonstrated significant performance improvements over existing methods."], "limitations": "", "keywords": ["Misinformation Detection", "Retrieval Augmented LLM", "Affective Information", "Few-shot Learning", "Cross-domain"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2406.11753", "pdf": "https://arxiv.org/pdf/2406.11753.pdf", "abs": "https://arxiv.org/abs/2406.11753", "title": "A Semantic-Aware Layer-Freezing Approach to Computation-Efficient Fine-Tuning of Language Models", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "accepted by ACL 2025, the camera-ready version", "summary": "Finetuning language models (LMs) is crucial for adapting the models to\ndownstream data and tasks. However, full finetuning is usually costly. Existing\nwork, such as parameter-efficient finetuning (PEFT), often focuses on\n\\textit{how to finetune} but neglects the issue of \\textit{where to finetune}.\nAs a pioneering work on reducing the cost of backpropagation (at the layer\nlevel) by answering where to finetune, we conduct a semantic analysis of the LM\ninference process. We first propose using transition traces of the latent\nrepresentation to compute deviations (or loss). Then, using a derived formula\nof scaling law, we estimate the gain of each layer in reducing deviation (or\nloss). Further, we narrow down the scope for finetuning, and also, study the\ncost-benefit balance of LM finetuning. We perform extensive experiments across\nwell-known LMs and datasets. The results show that our approach is effective\nand efficient, and outperforms the existing baselines. Our approach is\northogonal to other techniques for improving finetuning efficiency, such as\nPEFT methods, offering practical values on LM finetuning.", "AI": {"tldr": "This paper presents a novel method for parameter-efficient finetuning of language models by analyzing where in the model to apply finetuning, effectively reducing the costs associated with backpropagation.", "motivation": "To address the high costs of full finetuning in language models, focusing on both how and where to finetune is essential for efficiency.", "method": "The authors propose using transition traces of the latent representation to compute deviations and estimate the gain of each layer in reducing loss, narrowing down the scope for effective finetuning.", "result": "Experimental results demonstrate the proposed method's effectiveness and efficiency, outperforming existing baselines in LM finetuning.", "conclusion": "The proposed approach is complementary to existing techniques like parameter-efficient finetuning methods, providing valuable insights into optimizing LM finetuning.", "key_contributions": ["Introduced a method for analyzing where to finetune in language models.", "Developed a formula for estimating layer contributions to loss reduction.", "Conducted extensive empirical validation across multiple LMs and datasets."], "limitations": "The study may not address all aspects of finetuning and assumes the applicability of the method across models and tasks.", "keywords": ["language models", "finetuning", "parameter-efficient finetuning", "machine learning", "loss reduction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2406.17764", "pdf": "https://arxiv.org/pdf/2406.17764.pdf", "abs": "https://arxiv.org/abs/2406.17764", "title": "BMIKE-53: Investigating Cross-Lingual Knowledge Editing with In-Context Learning", "authors": ["Ercong Nie", "Bo Shao", "Zifeng Ding", "Mingyang Wang", "Helmut Schmid", "Hinrich Schütze"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025", "summary": "This paper introduces BMIKE-53, a comprehensive benchmark for cross-lingual\nin-context knowledge editing (IKE) across 53 languages, unifying three\nknowledge editing (KE) datasets: zsRE, CounterFact, and WikiFactDiff.\nCross-lingual KE, which requires knowledge edited in one language to generalize\nacross others while preserving unrelated knowledge, remains underexplored. To\naddress this gap, we systematically evaluate IKE under zero-shot, one-shot, and\nfew-shot setups, incorporating tailored metric-specific demonstrations. Our\nfindings reveal that model scale and demonstration alignment critically govern\ncross-lingual IKE efficacy, with larger models and tailored demonstrations\nsignificantly improving performance. Linguistic properties, particularly script\ntype, strongly influence performance variation across languages, with non-Latin\nlanguages underperforming due to issues like language confusion. Code and data\nare publicly available at: https://github.com/ercong21/MultiKnow/.", "AI": {"tldr": "Introduction of BMIKE-53, a benchmark for cross-lingual in-context knowledge editing across 53 languages, evaluating various setups and revealing critical factors influencing performance.", "motivation": "To address the underexplored area of cross-lingual knowledge editing (KE) and provide a structured benchmark for evaluating this capability across languages.", "method": "Systematic evaluation of cross-lingual knowledge editing using zero-shot, one-shot, and few-shot setups with tailored metric-specific demonstrations.", "result": "Findings indicate that model scale and demonstration alignment are crucial for improving cross-lingual IKE performance, with larger models and tailored demonstrations significantly enhancing outcomes.", "conclusion": "The performance variation is influenced by linguistic properties like script type, with non-Latin languages experiencing lower performance due to language confusion issues.", "key_contributions": ["Introduction of BMIKE-53 benchmark for cross-lingual IKE", "Evaluation across 53 languages using various setups", "Public availability of code and data"], "limitations": "Performance variability across languages, especially non-Latin scripts, affecting overall outcomes.", "keywords": ["cross-lingual", "knowledge editing", "benchmark", "machine learning", "language processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2406.18403", "pdf": "https://arxiv.org/pdf/2406.18403.pdf", "abs": "https://arxiv.org/abs/2406.18403", "title": "LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks", "authors": ["Anna Bavaresco", "Raffaella Bernardi", "Leonardo Bertolazzi", "Desmond Elliott", "Raquel Fernández", "Albert Gatt", "Esam Ghaleb", "Mario Giulianelli", "Michael Hanna", "Alexander Koller", "André F. T. Martins", "Philipp Mondorf", "Vera Neplenbroek", "Sandro Pezzelle", "Barbara Plank", "David Schlangen", "Alessandro Suglia", "Aditya K Surikuchi", "Ece Takmaz", "Alberto Testoni"], "categories": ["cs.CL"], "comment": "Accepted to the main conference of ACL 2025", "summary": "There is an increasing trend towards evaluating NLP models with LLMs instead\nof human judgments, raising questions about the validity of these evaluations,\nas well as their reproducibility in the case of proprietary models. We provide\nJUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations\ncovering a broad range of evaluated properties and types of data, and\ncomprehensively evaluate 11 current LLMs, covering both open-weight and\nproprietary models, for their ability to replicate the annotations. Our\nevaluations show substantial variance across models and datasets. Models are\nreliable evaluators on some tasks, but overall display substantial variability\ndepending on the property being evaluated, the expertise level of the human\njudges, and whether the language is human or model-generated. We conclude that\nLLMs should be carefully validated against human judgments before being used as\nevaluators.", "AI": {"tldr": "The paper evaluates the capability of large language models (LLMs) to serve as evaluators of NLP tasks by comparing their performance against human annotations across various datasets.", "motivation": "The study addresses the emerging trend of using LLMs for NLP model evaluation, questioning the reliability and validity of such evaluations in light of observed disparities.", "method": "An extensible collection of 20 NLP datasets with human annotations was developed, and 11 current LLMs were evaluated for their ability to replicate these annotations across different properties and data types.", "result": "The evaluations revealed significant variance in LLM performance, indicating that while some models are effective evaluators for certain tasks, overall reliability varies based on evaluation property and type of language generated.", "conclusion": "LLMs should not be solely relied upon for evaluations without thorough validation against human judgments, given the observed variabilities.", "key_contributions": ["Introduction of JUDGE-BENCH, a new collection of NLP datasets with human annotations.", "Comprehensive evaluation of 11 LLMs, including both open-weight and proprietary models.", "Identification of variabilities in LLM performance as evaluators across tasks and datasets."], "limitations": "The study's findings may not be universally applicable across all NLP tasks.", "keywords": ["NLP evaluation", "large language models", "human annotation", "JUDGE-BENCH", "reproducibility"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.03505", "pdf": "https://arxiv.org/pdf/2408.03505.pdf", "abs": "https://arxiv.org/abs/2408.03505", "title": "Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble Exploitation", "authors": ["Weiqi Feng", "Yangrui Chen", "Shaoyu Wang", "Yanghua Peng", "Haibin Lin", "Minlan Yu"], "categories": ["cs.CL", "cs.AI", "cs.DC"], "comment": null, "summary": "Multimodal large language models (MLLMs) have extended the success of large\nlanguage models (LLMs) to multiple data types, such as image, text and audio,\nachieving significant performance in various domains, including multimodal\ntranslation, visual question answering and content generation. Nonetheless,\nexisting systems are inefficient to train MLLMs due to substantial GPU bubbles\ncaused by the heterogeneous modality models and complex data dependencies in 3D\nparallelism. This paper proposes Optimus, a distributed MLLM training system\nthat reduces end-to-end MLLM training time. Optimus is based on our principled\nanalysis that scheduling the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling encoder computation\npossible for all GPUs, Optimus searches the separate parallel plans for encoder\nand LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM\nbubbles without breaking the original data dependencies in the MLLM model\narchitecture. We further decompose encoder layer computation into a series of\nkernels, and analyze the common bubble pattern of 3D parallelism to carefully\noptimize the sub-millisecond bubble scheduling, minimizing the overall training\ntime. Our experiments in a production cluster show that Optimus accelerates\nMLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.", "AI": {"tldr": "Optimus is a distributed training system designed to enhance the efficiency of multimodal large language models (MLLMs) by reducing training time through innovative scheduling techniques.", "motivation": "Current MLLM training systems show inefficiency due to GPU bubbles resulting from heterogeneous modality models and complex data dependencies, barring effective training and increasing time costs.", "method": "Optimus introduces a distributed training approach that involves scheduling encoder computations during LLM training to reduce inefficiencies. It utilizes separate parallel plans for encoder and LLM and employs a bubble scheduling algorithm to maintain data dependencies while optimizing overall training time.", "result": "Optimus achieves a notable acceleration of multimodal large language model training by 20.5%-21.3% with significant models (ViT-22B and GPT-175B) across a large cluster of GPUs.", "conclusion": "Optimus demonstrates that with strategic scheduling and optimization, MLLM training can be significantly expedited without compromising model architecture or data handling.", "key_contributions": ["Introduction of Optimus, a distributed training system for MLLMs", "Development of a novel bubble scheduling algorithm", "Performance improvements validated through experiments in a production cluster"], "limitations": "", "keywords": ["multimodal models", "distributed training", "large language models"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2408.15409", "pdf": "https://arxiv.org/pdf/2408.15409.pdf", "abs": "https://arxiv.org/abs/2408.15409", "title": "Awes, Laws, and Flaws From Today's LLM Research", "authors": ["Adrian de Wynter"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "We perform a critical examination of the scientific methodology behind\ncontemporary large language model (LLM) research. For this we assess over 2,000\nresearch works released between 2020 and 2024 based on criteria typical of what\nis considered good research (e.g. presence of statistical tests and\nreproducibility), and cross-validate it with arguments that are at the centre\nof controversy (e.g., claims of emergent behaviour). We find multiple trends,\nsuch as declines in ethics disclaimers, a rise of LLMs as evaluators, and an\nincrease on claims of LLM reasoning abilities without leveraging human\nevaluation. We note that conference checklists are effective at curtailing some\nof these issues, but balancing velocity and rigour in research cannot solely\nrely on these. We tie all these findings to findings from recent meta-reviews\nand extend recommendations on how to address what does, does not, and should\nwork in LLM research.", "AI": {"tldr": "This paper critically examines the scientific methodology in large language model (LLM) research, analyzing trends in ethics, evaluation, and claims of reasoning abilities.", "motivation": "To assess the integrity and quality of LLM research in light of increasing claims and controversies surrounding their capabilities.", "method": "The authors evaluated over 2,000 research works released between 2020 and 2024 based on criteria of good research practices, including statistical tests and reproducibility.", "result": "The study identifies trends such as fewer ethics disclaimers, a rise in LLMs as evaluators, and increased assertions of reasoning abilities without human validation. Conference checklists improve some research quality aspects but are insufficient alone.", "conclusion": "To improve LLM research quality, the authors recommend a balanced approach that integrates ethical considerations alongside rigorous methodology, underscoring the role of conference standards.", "key_contributions": ["Critical analysis of over 2,000 LLM research articles", "Identification of emerging trends in LLM evaluation and ethics", "Recommendations for improving research methodology in LLM studies"], "limitations": "", "keywords": ["large language models", "scientific methodology", "research integrity", "ethics in AI", "evaluation of LLMs"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2408.16493", "pdf": "https://arxiv.org/pdf/2408.16493.pdf", "abs": "https://arxiv.org/abs/2408.16493", "title": "Learning from Negative Samples in Generative Biomedical Entity Linking", "authors": ["Chanhwi Kim", "Hyunjae Kim", "Sihyeon Park", "Jiwoo Lee", "Mujeen Sung", "Jaewoo Kang"], "categories": ["cs.CL"], "comment": "ACL 2025 (Findings)", "summary": "Generative models have become widely used in biomedical entity linking\n(BioEL) due to their excellent performance and efficient memory usage. However,\nthese models are usually trained only with positive samples, i.e., entities\nthat match the input mention's identifier, and do not explicitly learn from\nhard negative samples, which are entities that look similar but have different\nmeanings. To address this limitation, we introduce ANGEL (Learning from\nNegative Samples in Biomedical Generative Entity Linking), the first framework\nthat trains generative BioEL models using negative samples. Specifically, a\ngenerative model is initially trained to generate positive entity names from\nthe knowledge base for given input entities. Subsequently, both correct and\nincorrect outputs are gathered from the model's top-k predictions. Finally, the\nmodel is updated to prioritize the correct predictions through preference\noptimization. Our models outperform the previous best baseline models by up to\nan average top-1 accuracy of 1.4% on five benchmarks. When incorporating our\nframework into pre-training, the performance improvement increases further to\n1.7%, demonstrating its effectiveness in both the pre-training and fine-tuning\nstages. The code and model weights are available at\nhttps://github.com/dmis-lab/ANGEL.", "AI": {"tldr": "ANGEL introduces a generative framework for biomedical entity linking that incorporates training from negative samples, improving accuracy over existing models.", "motivation": "To improve the performance of biomedical entity linking by addressing the limitations of training solely on positive samples.", "method": "The model is initially trained to generate positive entity names, then updated using both correct and incorrect predictions through preference optimization.", "result": "The models improve upon previous best baselines by an average top-1 accuracy of 1.4% across five benchmarks and 1.7% with pre-training integration.", "conclusion": "This framework shows significant performance improvements by effectively leveraging negative samples in both pre-training and fine-tuning stages of biomedical entity linking.", "key_contributions": ["First framework for BioEL using negative samples", "Demonstrated increased accuracy in performance benchmarks", "Code and weights made publicly available"], "limitations": "", "keywords": ["Biomedical Entity Linking", "Generative Models", "Negative Samples"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2409.00113", "pdf": "https://arxiv.org/pdf/2409.00113.pdf", "abs": "https://arxiv.org/abs/2409.00113", "title": "Wait, that's not an option: LLMs Robustness with Incorrect Multiple-Choice Options", "authors": ["Gracjan Góral", "Emilia Wiśnios", "Piotr Sankowski", "Paweł Budzianowski"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for ACL 2025 Main Conference and NeurIPS 2024 FM-EduAssess\n  Workshop", "summary": "This work introduces a novel framework for evaluating LLMs' capacity to\nbalance instruction-following with critical reasoning when presented with\nmultiple-choice questions containing no valid answers. Through systematic\nevaluation across arithmetic, domain-specific knowledge, and high-stakes\nmedical decision tasks, we demonstrate that post-training aligned models often\ndefault to selecting invalid options, while base models exhibit improved\nrefusal capabilities that scale with model size. Our analysis reveals that\nalignment techniques, though intended to enhance helpfulness, can inadvertently\nimpair models' reflective judgment--the ability to override default behaviors\nwhen faced with invalid options. We additionally conduct a parallel human study\nshowing similar instruction-following biases, with implications for how these\nbiases may propagate through human feedback datasets used in alignment. We\nprovide extensive ablation studies examining the impact of model size, training\ntechniques, and prompt engineering. Our findings highlight fundamental tensions\nbetween alignment optimization and preservation of critical reasoning\ncapabilities, with important implications for developing more robust AI systems\nfor real-world deployment.", "AI": {"tldr": "Framework for evaluating LLMs' instruction-following vs. critical reasoning in multiple-choice questions with invalid answers.", "motivation": "To assess LLMs' ability to handle scenarios with no valid answers while following instructions and exercising critical reasoning.", "method": "Systematic evaluation across various tasks, including arithmetic and medical decision tasks, alongside human studies to investigate instruction-following biases and their impacts.", "result": "Post-training aligned models often select invalid options; base models show improved refusal capabilities linked to size.", "conclusion": "Alignment techniques that enhance helpfulness can impair reflective judgment, highlighting a tension between optimization and critical reasoning for real-world AI application.", "key_contributions": ["Novel evaluation framework for LLMs balancing instruction and reasoning", "Evidence of aligned models defaulting to invalid choices", "Implications for human feedback datasets in model alignment"], "limitations": "Focus on specific tasks; effects may vary across different domains.", "keywords": ["LLMs", "critical reasoning", "instruction-following", "alignment techniques", "human feedback"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.05367", "pdf": "https://arxiv.org/pdf/2409.05367.pdf", "abs": "https://arxiv.org/abs/2409.05367", "title": "STRICTA: Structured Reasoning in Critical Text Assessment for Peer Review and Beyond", "authors": ["Nils Dycke", "Matej Zečević", "Ilia Kuznetsov", "Beatrix Suess", "Kristian Kersting", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "Critical text assessment is at the core of many expert activities, such as\nfact-checking, peer review, and essay grading. Yet, existing work treats\ncritical text assessment as a black box problem, limiting interpretability and\nhuman-AI collaboration. To close this gap, we introduce Structured Reasoning In\nCritical Text Assessment (STRICTA), a novel specification framework to model\ntext assessment as an explicit, step-wise reasoning process. STRICTA breaks\ndown the assessment into a graph of interconnected reasoning steps drawing on\ncausality theory (Pearl, 1995). This graph is populated based on expert\ninteraction data and used to study the assessment process and facilitate\nhuman-AI collaboration. We formally define STRICTA and apply it in a study on\nbiomedical paper assessment, resulting in a dataset of over 4000 reasoning\nsteps from roughly 40 biomedical experts on more than 20 papers. We use this\ndataset to empirically study expert reasoning in critical text assessment, and\ninvestigate if LLMs are able to imitate and support experts within these\nworkflows. The resulting tools and datasets pave the way for studying\ncollaborative expert-AI reasoning in text assessment, in peer review and\nbeyond.", "AI": {"tldr": "Introducing the STRICTA framework for modeling critical text assessment through structured reasoning, facilitating human-AI collaboration in expert activities.", "motivation": "To improve interpretability and collaboration in critical text assessment tasks like fact-checking and peer review.", "method": "The study introduces a structured reasoning framework (STRICTA) that models text assessment using a graph of interconnected reasoning steps based on causality theory, supported by expert interaction data.", "result": "A dataset of over 4000 reasoning steps was created from 40 biomedical experts assessing over 20 papers, revealing insights into expert reasoning and LLM potential to assist in these workflows.", "conclusion": "The STRICTA framework and dataset enable further exploration of collaborative reasoning between experts and AI in text assessment, particularly in peer review scenarios.", "key_contributions": ["Development of the STRICTA framework", "Creation of a large dataset for critical text assessment", "Insights into expert reasoning and LLM collaboration capabilities"], "limitations": "", "keywords": ["critical text assessment", "human-AI collaboration", "structured reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.05806", "pdf": "https://arxiv.org/pdf/2409.05806.pdf", "abs": "https://arxiv.org/abs/2409.05806", "title": "CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs", "authors": ["Jizhan Fang", "Tianhe Lu", "Yunzhi Yao", "Ziyan Jiang", "Xin Xu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "ACL 2025; project website is available at\n  https://zjunlp.github.io/project/CKnowEdit code and dataset are available at\n  https://github.com/zjunlp/EasyEdit", "summary": "Chinese, as a linguistic system rich in depth and complexity, is\ncharacterized by distinctive elements such as ancient poetry, proverbs, idioms,\nand other cultural constructs. However, current Large Language Models (LLMs)\nface limitations in these specialized domains, highlighting the need for the\ndevelopment of comprehensive datasets that can assess, continuously update, and\nprogressively improve these culturally-grounded linguistic competencies through\ntargeted training optimizations. To address this gap, we introduce CKnowEdit,\nthe first-ever Chinese knowledge editing dataset designed to correct\nlinguistic, factual, and logical errors in LLMs. We collect seven types of\nknowledge from a wide range of sources, including classical texts, idioms, and\ncontent from Baidu Tieba Ruozhiba, taking into account the unique polyphony,\nantithesis, and logical structures inherent in the Chinese language. By\nanalyzing this dataset, we highlight the challenges current LLMs face in\nmastering Chinese. Furthermore, our evaluation of state-of-the-art knowledge\nediting techniques reveals opportunities to advance the correction of Chinese\nknowledge. Code and dataset are available at\nhttps://github.com/zjunlp/EasyEdit.", "AI": {"tldr": "Introduction of CKnowEdit, a unique dataset aimed at improving LLMs' performance with Chinese language through knowledge editing.", "motivation": "Current LLMs struggle with the complexities of the Chinese language, demonstrating errors in linguistic and factual content, necessitating improved training datasets.", "method": "The paper presents CKnowEdit—collecting seven types of knowledge from diverse sources including classical texts and idioms to evaluate and improve LLMs' understanding of the Chinese language.", "result": "Analysis of CKnowEdit reveals significant challenges in LLMs' mastery of Chinese, while providing evaluation of state-of-the-art knowledge editing techniques.", "conclusion": "CKnowEdit serves as a foundational dataset for correcting errors in LLMs, offering insights into enhancing language models for Chinese linguistic competencies.", "key_contributions": ["Introduction of the first Chinese knowledge editing dataset (CKnowEdit)", "Collection of diverse linguistic knowledge tailored for Chinese", "Evaluation framework for LLMs' understanding of Chinese culture and language nuances"], "limitations": "Limited to Chinese language and may not generalize to other languages.", "keywords": ["Chinese Language", "Large Language Models", "Knowledge Editing", "Cultural Linguistics", "Dataset Development"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2409.09401", "pdf": "https://arxiv.org/pdf/2409.09401.pdf", "abs": "https://arxiv.org/abs/2409.09401", "title": "Towards Diverse and Efficient Audio Captioning via Diffusion Models", "authors": ["Manjie Xu", "Chenxing Li", "Xinyi Tu", "Yong Ren", "Ruibo Fu", "Wei Liang", "Dong Yu"], "categories": ["cs.CL"], "comment": "https://sites.google.com/view/diffusion-audio-captioning", "summary": "We introduce Diffusion-based Audio Captioning (DAC), a non-autoregressive\ndiffusion model tailored for diverse and efficient audio captioning. Although\nexisting captioning models relying on language backbones have achieved\nremarkable success in various captioning tasks, their insufficient performance\nin terms of generation speed and diversity impede progress in audio\nunderstanding and multimedia applications. Our diffusion-based framework offers\nunique advantages stemming from its inherent stochasticity and holistic context\nmodeling in captioning. Through rigorous evaluation, we demonstrate that DAC\nnot only achieves SOTA performance levels compared to existing benchmarks in\nthe caption quality, but also significantly outperforms them in terms of\ngeneration speed and diversity. The success of DAC illustrates that text\ngeneration can also be seamlessly integrated with audio and visual generation\ntasks using a diffusion backbone, paving the way for a unified, audio-related\ngenerative model across different modalities.", "AI": {"tldr": "Diffusion-based Audio Captioning (DAC) is introduced as a non-autoregressive model that enhances audio captioning in terms of speed and diversity while achieving state-of-the-art performance.", "motivation": "There is a need for improved generation speed and diversity in existing audio captioning models which primarily rely on language backbones.", "method": "DAC employs a diffusion model that integrates stochasticity and holistic context modeling for audio captioning tasks.", "result": "DAC achieves state-of-the-art performance in caption quality and significantly outperforms existing models in generation speed and diversity.", "conclusion": "DAC shows the potential for integrating text generation with audio and visual tasks through a unified diffusion model approach.", "key_contributions": ["Introduction of a non-autoregressive diffusion model for audio captioning", "Demonstration of improved generation speed and diversity", "Establishment of a framework for unified multimodal generative tasks"], "limitations": "", "keywords": ["audio captioning", "diffusion model", "multimodal generation"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2409.14507", "pdf": "https://arxiv.org/pdf/2409.14507.pdf", "abs": "https://arxiv.org/abs/2409.14507", "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders", "authors": ["David Chanin", "James Wilken-Smith", "Tomáš Dulka", "Hardik Bhatnagar", "Satvik Golechha", "Joseph Bloom"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large\nlanguage models (LLMs) into human-interpretable latent directions or features.\nAs we increase the number of features in the SAE, hierarchical features tend to\nsplit into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.),\na phenomenon referred to as feature splitting. However, we show that sparse\ndecomposition and splitting of hierarchical features is not robust.\nSpecifically, we show that seemingly monosemantic features fail to fire where\nthey should, and instead get \"absorbed\" into their children features. We coin\nthis phenomenon feature absorption, and show that it is caused by optimizing\nfor sparsity in SAEs whenever the underlying features form a hierarchy. We\nintroduce a metric to detect absorption in SAEs, and validate our findings\nempirically on hundreds of LLM SAEs. Our investigation suggests that varying\nSAE sizes or sparsity is insufficient to solve this issue. We discuss the\nimplications of feature absorption in SAEs and some potential approaches to\nsolve the fundamental theoretical issues before SAEs can be used for\ninterpreting LLMs robustly and at scale.", "AI": {"tldr": "The paper analyzes the robustness of Sparse Autoencoders (SAEs) in feature decomposition of large language models, introducing the concepts of feature splitting and feature absorption, and providing empirical validation.", "motivation": "To explore how Sparse Autoencoders (SAEs) decompose activation spaces of large language models (LLMs) into interpretable features and investigate the resulting phenomena of feature splitting and absorption.", "method": "The authors introduce a metric for detecting feature absorption in SAEs and empirically validate their findings across hundreds of LLM SAEs.", "result": "The study reveals that hierarchical features in SAEs do not robustly split as expected, leading to a phenomenon where features are absorbed into child features, undermining the interpretability of LLMs.", "conclusion": "The findings highlight significant theoretical challenges that must be addressed to improve the robustness of SAEs for interpreting LLMs at scale, suggesting that simply adjusting SAE sizes or sparsity does not resolve the issue.", "key_contributions": ["Introduction of the concept of feature absorption in SAEs", "Development of a metric to detect feature absorption", "Empirical validation of findings on hundreds of LLM SAEs."], "limitations": "Limited to the exploration of feature absorption within the context of Sparse Autoencoders; further research needed to find solutions to the identified issues.", "keywords": ["Sparse Autoencoders", "feature absorption", "large language models", "interpretability", "empirical validation"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2409.19458", "pdf": "https://arxiv.org/pdf/2409.19458.pdf", "abs": "https://arxiv.org/abs/2409.19458", "title": "Scalable Fine-tuning from Multiple Data Sources: A First-Order Approximation Approach", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": "17 pages. Appeared in Findings of EMNLP'24", "summary": "We study the problem of fine-tuning a language model (LM) for a target task\nby optimally using the information from $n$ auxiliary tasks. This problem has\nbroad applications in NLP, such as targeted instruction tuning and data\nselection in chain-of-thought fine-tuning. The key challenge of this problem is\nthat not all auxiliary tasks are beneficial in improving the performance of the\ntarget task. Thus, selecting the right subset of auxiliary tasks is crucial.\nConventional subset selection methods, such as forward and backward stepwise\nselection, are unsuitable for LM fine-tuning because they require repeated\ntraining on subsets of auxiliary tasks. This paper introduces a new algorithm\nfor estimating model fine-tuning performance without requiring repeated\ntraining. Our algorithm first performs multitask training using data from all\ntasks to obtain a meta initialization. Then, we approximate the model\nfine-tuning loss of a subset using functional values and gradients from the\nmeta initialization. Empirically, we find that this gradient-based\napproximation holds with remarkable accuracy for twelve transformer-based LMs.\nThus, we can now estimate fine-tuning performances on CPUs within a few\nseconds. Finally, we fine-tune the pretrained base model once on the selected\nsubset of tasks. We conduct extensive experiments to validate this approach,\ndelivering a speedup of $30\\times$ over conventional subset selection while\nincurring only $1\\%$ error of the true fine-tuning performances. In downstream\nevaluations involving both instruction tuning and chain-of-thought fine-tuning,\nthis loss-based selection approach improves over prior gradient or\nrepresentation similarity-based methods for subset selection by up to $3.8\\%$.", "AI": {"tldr": "This paper presents a novel algorithm for fine-tuning language models efficiently by selecting optimal auxiliary tasks, achieving significant speedups and improved performance in downstream NLP tasks.", "motivation": "The paper addresses the challenge of selecting the right subset of auxiliary tasks for fine-tuning language models, which is crucial for enhancing performance on target tasks without excessive computational cost.", "method": "The proposed algorithm performs multitask training to obtain a meta initialization, then estimates the model fine-tuning loss using functional values and gradients from this initialization, avoiding repeated training on task subsets.", "result": "The approach yields a speedup of 30x over traditional subset selection methods and maintains only 1% error margin in performance estimation, significantly benefiting task performance in downstream evaluations.", "conclusion": "By optimizing task selection through a gradient-based approximation, the method improves fine-tuning efficacy, enhancing performance by up to 3.8% compared to existing methods.", "key_contributions": ["Introduction of a gradient-based loss approximation for task selection", "Substantial speedup in fine-tuning performance estimation", "Improved accuracy in downstream NLP tasks through optimized task selection"], "limitations": "", "keywords": ["language models", "fine-tuning", "task selection", "gradient approximation", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2409.19505", "pdf": "https://arxiv.org/pdf/2409.19505.pdf", "abs": "https://arxiv.org/abs/2409.19505", "title": "The Nature of NLP: Analyzing Contributions in NLP Papers", "authors": ["Aniket Pramanick", "Yufang Hou", "Saif M. Mohammad", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "accepted at ACL 2025", "summary": "Natural Language Processing (NLP) is an established and dynamic field.\nDespite this, what constitutes NLP research remains debated. In this work, we\naddress the question by quantitatively examining NLP research papers. We\npropose a taxonomy of research contributions and introduce NLPContributions, a\ndataset of nearly $2k$ NLP research paper abstracts, carefully annotated to\nidentify scientific contributions and classify their types according to this\ntaxonomy. We also introduce a novel task of automatically identifying\ncontribution statements and classifying their types from research papers. We\npresent experimental results for this task and apply our model to $\\sim$$29k$\nNLP research papers to analyze their contributions, aiding in the understanding\nof the nature of NLP research. We show that NLP research has taken a winding\npath -- with the focus on language and human-centric studies being prominent in\nthe 1970s and 80s, tapering off in the 1990s and 2000s, and starting to rise\nagain since the late 2010s. Alongside this revival, we observe a steady rise in\ndataset and methodological contributions since the 1990s, such that today, on\naverage, individual NLP papers contribute in more ways than ever before. Our\ndataset and analyses offer a powerful lens for tracing research trends and\noffer potential for generating informed, data-driven literature surveys.", "AI": {"tldr": "This paper proposes a taxonomy for NLP research contributions and introduces a dataset, NLPContributions, containing annotated abstracts of nearly 2,000 papers to classify contributions and analyze trends in the field.", "motivation": "To clarify what constitutes NLP research by providing a quantitative examination of NLP research papers and understanding their contributions over time.", "method": "The authors created NLPContributions, a dataset of annotated abstracts, and introduced a task for automatically identifying and classifying contribution statements in NLP research papers. They presented experimental results on this task and applied their model to analyze contributions across approximately 29,000 NLP papers.", "result": "The analysis reveals historical trends in NLP research, highlighting a resurgence of interest in language and human-centric studies since the late 2010s, alongside a steady increase in methodological and dataset contributions.", "conclusion": "The dataset and the trends analyzed offer insights into the evolving nature of NLP research and can facilitate data-driven literature surveys.", "key_contributions": ["Introduction of NLPContributions dataset with annotated abstracts", "Proposed taxonomy for classifying NLP research contributions", "Novel task of automatic identification and classification of contribution statements"], "limitations": "", "keywords": ["Natural Language Processing", "research contributions", "taxonomy", "dataset", "analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2409.20201", "pdf": "https://arxiv.org/pdf/2409.20201.pdf", "abs": "https://arxiv.org/abs/2409.20201", "title": "AfriHuBERT: A self-supervised speech representation model for African languages", "authors": ["Jesujoba O. Alabi", "Xuechen Liu", "Dietrich Klakow", "Junichi Yamagishi"], "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "In this work, we present AfriHuBERT, an extension of mHuBERT-147, a compact\nself-supervised learning (SSL) model pretrained on 147 languages. While\nmHuBERT-147 covered 16 African languages, we expand this to 1,226 through\ncontinued pretraining on 10K+ hours of speech data from diverse sources,\nbenefiting an African population of over 600M. We evaluate AfriHuBERT on two\nkey speech tasks, Spoken Language Identification (SLID) and Automatic Speech\nRecognition (ASR), using the FLEURS benchmark. Our results show a +3.6% F1\nscore improvement for SLID and a -2.1% average Word Error Rate (WER) reduction\nfor ASR over mHuBERT-147, and demonstrates competitiveness with larger SSL\nmodels such as MMS and XEUS. Further analysis shows that ASR models trained on\nAfriHuBERT exhibit improved cross-corpus generalization and are competitive in\nextremely low-resource ASR scenarios.", "AI": {"tldr": "AfriHuBERT is a self-supervised learning model pretrained on over 1,226 African languages, improving performance in speech tasks over its predecessor, mHuBERT-147.", "motivation": "To enhance speech recognition and language identification for over 600 million people speaking diverse African languages.", "method": "Continuing the pretraining of mHuBERT-147 on over 10,000 hours of speech data to cover additional African languages and optimizing for two key speech tasks.", "result": "AfriHuBERT shows a +3.6% F1 score improvement for Spoken Language Identification and a -2.1% average reduction in Word Error Rate for Automatic Speech Recognition compared to mHuBERT-147.", "conclusion": "AfriHuBERT demonstrates competitiveness with larger self-supervised models and exhibits improved performance in low-resource ASR scenarios.", "key_contributions": ["Expansion of multilingual capabilities to over 1,226 African languages", "Significant improvements in SLID and ASR tasks", "Enhanced performance in low-resource environments"], "limitations": "", "keywords": ["self-supervised learning", "speech recognition", "African languages"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.03026", "pdf": "https://arxiv.org/pdf/2410.03026.pdf", "abs": "https://arxiv.org/abs/2410.03026", "title": "Estimating Privacy Leakage of Augmented Contextual Knowledge in Language Models", "authors": ["James Flemings", "Bo Jiang", "Wanrong Zhang", "Zafar Takhirov", "Murali Annavaram"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Language models (LMs) rely on their parametric knowledge augmented with\nrelevant contextual knowledge for certain tasks, such as question answering.\nHowever, the contextual knowledge can contain private information that may be\nleaked when answering queries, and estimating this privacy leakage is not well\nunderstood. A straightforward approach of directly comparing an LM's output to\nthe contexts can overestimate the privacy risk, since the LM's parametric\nknowledge might already contain the augmented contextual knowledge. To this\nend, we introduce $\\emph{context influence}$, a metric that builds on\ndifferential privacy, a widely-adopted privacy notion, to estimate the privacy\nleakage of contextual knowledge during decoding. Our approach effectively\nmeasures how each subset of the context influences an LM's response while\nseparating the specific parametric knowledge of the LM. Using our context\ninfluence metric, we demonstrate that context privacy leakage occurs when\ncontextual knowledge is out of distribution with respect to parametric\nknowledge. Moreover, we experimentally demonstrate how context influence\nproperly attributes the privacy leakage to augmented contexts, and we evaluate\nhow factors-- such as model size, context size, generation position, etc.--\naffect context privacy leakage. The practical implications of our results will\ninform practitioners of the privacy risk associated with augmented contextual\nknowledge.", "AI": {"tldr": "This paper introduces a metric called 'context influence' to estimate privacy leakage from contextual knowledge used by language models, emphasizing its importance in tasks like question answering.", "motivation": "Understanding privacy leakage in language models when using contextual knowledge is crucial for safeguarding sensitive information and improving trust in AI applications.", "method": "The paper proposes the 'context influence' metric, which draws from differential privacy to estimate how subsets of context affect a language model's responses, distinguishing it from the model's inherent parametric knowledge.", "result": "The experimental findings reveal that privacy leakage occurs when contextual knowledge is out of distribution compared to the model's parametric knowledge, and that various factors can influence this leakage.", "conclusion": "The study provides insights into the privacy risks linked to augmented contextual knowledge, informing practitioners about the importance of evaluating context privacy in language model applications.", "key_contributions": ["Introduction of the 'context influence' metric for privacy leakage assessment.", "Empirical analysis of factors affecting context privacy leakage in language models.", "Clarification of the relationship between parametric and contextual knowledge in language models."], "limitations": "The study primarily focuses on the estimation of privacy leakage without providing a comprehensive framework for mitigating these risks in practical applications.", "keywords": ["privacy leakage", "contextual knowledge", "language models", "differential privacy", "context influence"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2410.03868", "pdf": "https://arxiv.org/pdf/2410.03868.pdf", "abs": "https://arxiv.org/abs/2410.03868", "title": "Can Language Models Reason about Individualistic Human Values and Preferences?", "authors": ["Liwei Jiang", "Taylor Sorensen", "Sydney Levine", "Yejin Choi"], "categories": ["cs.CL"], "comment": "Camera Ready at ACL Main 2025", "summary": "Recent calls for pluralistic alignment emphasize that AI systems should\naddress the diverse needs of all people. Yet, efforts in this space often\nrequire sorting people into fixed buckets of pre-specified diversity-defining\ndimensions (e.g., demographics), risking smoothing out individualistic\nvariations or even stereotyping. To achieve an authentic representation of\ndiversity that respects individuality, we propose individualistic alignment.\nWhile individualistic alignment can take various forms, we introduce\nIndieValueCatalog, a dataset transformed from the influential World Values\nSurvey (WVS), to study language models (LMs) on the specific challenge of\nindividualistic value reasoning. Given a sample of an individual's\nvalue-expressing statements, models are tasked with predicting this person's\nvalue judgments in novel cases. With IndieValueCatalog, we reveal critical\nlimitations in frontier LMs, which achieve only 55 % to 65% accuracy in\npredicting individualistic values. Moreover, our results highlight that a\nprecise description of individualistic values cannot be approximated only with\ndemographic information. We also identify a partiality of LMs in reasoning\nabout global individualistic values, as measured by our proposed Value Inequity\nIndex ({\\sigma}Inequity). Finally, we train a series of IndieValueReasoners to\nreveal new patterns and dynamics into global human values.", "AI": {"tldr": "This paper introduces individualistic alignment in AI, proposing the IndieValueCatalog dataset to assess language models' ability to understand individualistic value reasoning rather than relying solely on demographics.", "motivation": "To address the shortcomings of current methods in AI that categorize individuals into fixed diversity buckets, potentially leading to stereotyping.", "method": "The authors present the IndieValueCatalog, derived from the World Values Survey, to evaluate language models in predicting individualistic values based on personalized value-expressing statements.", "result": "Frontier language models demonstrate only 55% to 65% accuracy in predicting individualistic values, indicating significant room for improvement.", "conclusion": "The reliance on demographic information to approximate individualistic values is inadequate; a deeper understanding of individual variation is required in AI systems.", "key_contributions": ["Introduction of individualistic alignment as a concept for AI systems", "Development of the IndieValueCatalog dataset for studying individualistic value reasoning", "Identification of the limitations and partiality in language models regarding individualistic values."], "limitations": "The study primarily focuses on the accuracy of language models and may not address other dimensions of alignment issues or broader ethical concerns.", "keywords": ["individualistic alignment", "language models", "IndieValueCatalog", "value reasoning", "AI diversity"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.05613", "pdf": "https://arxiv.org/pdf/2410.05613.pdf", "abs": "https://arxiv.org/abs/2410.05613", "title": "Stereotype or Personalization? User Identity Biases Chatbot Recommendations", "authors": ["Anjali Kantharuban", "Jeremiah Milbauer", "Maarten Sap", "Emma Strubell", "Graham Neubig"], "categories": ["cs.CL"], "comment": null, "summary": "While personalized recommendations are often desired by users, it can be\ndifficult in practice to distinguish cases of bias from cases of\npersonalization: we find that models generate racially stereotypical\nrecommendations regardless of whether the user revealed their identity\nintentionally through explicit indications or unintentionally through implicit\ncues. We demonstrate that when people use large language models (LLMs) to\ngenerate recommendations, the LLMs produce responses that reflect both what the\nuser wants and who the user is. We argue that chatbots ought to transparently\nindicate when recommendations are influenced by a user's revealed identity\ncharacteristics, but observe that they currently fail to do so. Our experiments\nshow that even though a user's revealed identity significantly influences model\nrecommendations (p < 0.001), model responses obfuscate this fact in response to\nuser queries. This bias and lack of transparency occurs consistently across\nmultiple popular consumer LLMs and for four American racial groups.", "AI": {"tldr": "This paper investigates how large language models (LLMs) produce biased recommendations based on user identity, emphasizing the need for transparency in such systems.", "motivation": "The study aims to address the challenge of distinguishing between bias and useful personalization in LLM-generated recommendations, particularly in the context of racial stereotypes.", "method": "By conducting experiments with multiple popular consumer LLMs, the authors analyze how user-revealed identity characteristics affect the recommendations produced by the models.", "result": "The findings indicate that user identity significantly influences the model's recommendations, yet the models fail to transparently disclose this bias in their responses.", "conclusion": "The authors conclude that chatbots should indicate when recommendations are influenced by user identity characteristics to promote transparency and mitigate bias.", "key_contributions": ["Demonstration of racial stereotyping in LLM recommendations", "Development of a framework for identifying bias vs. personalization", "Call for increased transparency in chatbot recommendations"], "limitations": "The study focuses only on four American racial groups and may not cover other demographics or cultures.", "keywords": ["bias", "personalization", "large language models", "recommendations", "transparency"], "importance_score": 8, "read_time_minutes": 6}}
{"id": "2410.05873", "pdf": "https://arxiv.org/pdf/2410.05873.pdf", "abs": "https://arxiv.org/abs/2410.05873", "title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment", "authors": ["Amir Hossein Kargaran", "Ali Modarressi", "Nafiseh Nikeghbal", "Jana Diesner", "François Yvon", "Hinrich Schütze"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL Findings 2025", "summary": "English-centric large language models (LLMs) often show strong multilingual\ncapabilities. However, their multilingual performance remains unclear and is\nunder-evaluated for many other languages. Most benchmarks for multilinguality\nfocus on classic NLP tasks or cover a minimal number of languages. We introduce\nMEXA, a method for assessing the multilingual capabilities of pre-trained\nEnglish-centric LLMs using parallel sentences, which are available for more\nlanguages than existing downstream tasks. MEXA leverages that English-centric\nLLMs use English as a pivot language in their intermediate layers. MEXA\ncomputes the alignment between English and non-English languages using parallel\nsentences to evaluate the transfer of language understanding from English to\nother languages. This alignment can be used to estimate model performance in\ndifferent languages. We conduct controlled experiments using various parallel\ndatasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral,\nand OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We\nexplore different methods to compute embeddings in decoder-only models. Our\nresults show that MEXA, in its default settings, achieves an average Pearson\ncorrelation of 0.90 between its predicted scores and actual task performance\nacross languages. This suggests that MEXA is a reliable method for estimating\nthe multilingual capabilities of English-centric LLMs, providing a clearer\nunderstanding of their multilingual potential and the inner workings of LLMs.\nLeaderboard: https://cis-lmu-mexa.hf.space, Code:\nhttps://github.com/cisnlp/MEXA.", "AI": {"tldr": "MEXA is a new method for evaluating the multilingual capabilities of pre-trained English-centric large language models (LLMs) through alignment between English and non-English languages using parallel sentences.", "motivation": "To address the unclear multilingual performance of English-centric LLMs and evaluate their capabilities across more languages beyond classic NLP tasks.", "method": "MEXA computes the alignment between English and other languages using parallel sentences, allowing for the estimation of model performance in different languages.", "result": "MEXA achieves an average Pearson correlation of 0.90 between predicted scores and actual task performance across languages in controlled experiments.", "conclusion": "MEXA demonstrates reliability in estimating the multilingual capabilities of English-centric LLMs and enhances understanding of their multilingual potential.", "key_contributions": ["Introduces MEXA for multilingual evaluation of LLMs", "Uses parallel sentences for assessment across multiple languages", "Achieves high correlation in performance prediction"], "limitations": "", "keywords": ["multilinguality", "large language models", "evaluation method", "parallel sentences", "NLP"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.06118", "pdf": "https://arxiv.org/pdf/2410.06118.pdf", "abs": "https://arxiv.org/abs/2410.06118", "title": "Optimizing the Training Schedule of Multilingual NMT using Reinforcement Learning", "authors": ["Alexis Allemann", "Àlex R. Atrio", "Andrei Popescu-Belis"], "categories": ["cs.CL"], "comment": "Proceedings of MT Summit 2025", "summary": "Multilingual NMT is a viable solution for translating low-resource languages\n(LRLs) when data from high-resource languages (HRLs) from the same language\nfamily is available. However, the training schedule, i.e. the order of\npresentation of languages, has an impact on the quality of such systems. Here,\nin a many-to-one translation setting, we propose to apply two algorithms that\nuse reinforcement learning to optimize the training schedule of NMT: (1)\nTeacher-Student Curriculum Learning and (2) Deep Q Network. The former uses an\nexponentially smoothed estimate of the returns of each action based on the loss\non monolingual or multilingual development subsets, while the latter estimates\nrewards using an additional neural network trained from the history of actions\nselected in different states of the system, together with the rewards received.\nOn a 8-to-1 translation dataset with LRLs and HRLs, our second method improves\nBLEU and COMET scores with respect to both random selection of monolingual\nbatches and shuffled multilingual batches, by adjusting the number of\npresentations of LRL vs. HRL batches.", "AI": {"tldr": "This paper explores the optimization of training schedules in multilingual neural machine translation (NMT) using reinforcement learning algorithms to enhance translation quality for low-resource languages.", "motivation": "The goal is to improve translation quality of low-resource languages by optimizing the training schedule in multilingual NMT using data from high-resource languages.", "method": "The authors propose two reinforcement learning-based algorithms: Teacher-Student Curriculum Learning and Deep Q Network, both aimed at improving the training schedule.", "result": "The Deep Q Network method shows significant improvements in BLEU and COMET scores on an 8-to-1 translation dataset, outperforming both random and shuffled batch selections.", "conclusion": "Optimizing the training schedule using reinforcement learning leads to better performance in multilingual NMT for low-resource language translation.", "key_contributions": ["Development of a reinforcement learning approach to optimize training schedules in NMT.", "Implementation of Teacher-Student Curriculum Learning and Deep Q Network for training schedule adjustment.", "Empirical evidence showing improved translation scores for low-resource languages using these methods."], "limitations": "", "keywords": ["Multilingual NMT", "Reinforcement Learning", "Low-Resource Languages"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2410.07176", "pdf": "https://arxiv.org/pdf/2410.07176.pdf", "abs": "https://arxiv.org/abs/2410.07176", "title": "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models", "authors": ["Fei Wang", "Xingchen Wan", "Ruoxi Sun", "Jiefeng Chen", "Sercan Ö. Arık"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 main conference", "summary": "Retrieval augmented generation (RAG), while effectively integrating external\nknowledge to address the inherent limitations of large language models (LLMs),\ncan be hindered by imperfect retrieval that contain irrelevant, misleading, or\neven malicious information. Previous studies have rarely connected the behavior\nof RAG through joint analysis, particularly regarding error propagation coming\nfrom imperfect retrieval and potential conflicts between LLMs' internal\nknowledge and external sources. Through comprehensive and controlled analyses\nunder realistic conditions, we find that imperfect retrieval augmentation is\ninevitable, common, and harmful. We identify the knowledge conflicts between\nLLM-internal and external knowledge from retrieval as a bottleneck to overcome\nimperfect retrieval in the post-retrieval stage of RAG. To address this, we\npropose Astute RAG, a novel RAG approach designed to be resilient to imperfect\nretrieval augmentation. It adaptively elicits essential information from LLMs'\ninternal knowledge, iteratively consolidates internal and external knowledge\nwith source-awareness, and finalizes the answer according to information\nreliability. Our experiments with Gemini and Claude demonstrate the superior\nperformance of Astute RAG compared to previous robustness-enhanced RAG\napproaches. Specifically, Astute RAG is the only RAG method that achieves\nperformance comparable to or even surpassing conventional use of LLMs under the\nworst-case scenario. Further analysis reveals the effectiveness of Astute RAG\nin resolving knowledge conflicts, thereby improving the trustworthiness of RAG.", "AI": {"tldr": "Astute RAG addresses the problems of imperfect retrieval in retrieval augmented generation (RAG) by better integrating internal and external knowledge from LLMs.", "motivation": "The study explores the limitations of current retrieval augmented generation methods due to imperfect retrieval, which can include irrelevant or misleading information.", "method": "Astute RAG was developed to improve the integration of internal and external knowledge, using an approach that adapts and iterates over the retrieved information while considering source reliability.", "result": "Experiments show that Astute RAG outperforms existing robustness-enhanced RAG methods, achieving performance comparable to or exceeding traditional LLM use in worse-case scenarios.", "conclusion": "Astute RAG effectively resolves knowledge conflicts and enhances the trustworthiness of RAG systems, highlighting the importance of accurate integration of internal and external data.", "key_contributions": ["Introduction of Astute RAG to improve resilience against imperfect retrieval", "Demonstration of superior performance compared to previous RAG methods", "Focus on resolving knowledge conflicts between internal and external sources."], "limitations": "", "keywords": ["retrieval augmented generation", "large language models", "knowledge conflicts", "information reliability", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.08145", "pdf": "https://arxiv.org/pdf/2410.08145.pdf", "abs": "https://arxiv.org/abs/2410.08145", "title": "Insight Over Sight: Exploring the Vision-Knowledge Conflicts in Multimodal LLMs", "authors": ["Xiaoyuan Liu", "Wenxuan Wang", "Youliang Yuan", "Jen-tse Huang", "Qiuzhi Liu", "Pinjia He", "Zhaopeng Tu"], "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 main", "summary": "This paper explores the problem of commonsense level vision-knowledge\nconflict in Multimodal Large Language Models (MLLMs), where visual information\ncontradicts model's internal commonsense knowledge. To study this issue, we\nintroduce an automated framework, augmented with human-in-the-loop quality\ncontrol, to generate inputs designed to simulate and evaluate these conflicts\nin MLLMs. Using this framework, we have crafted a diagnostic benchmark\nconsisting of 374 original images and 1,122 high-quality question-answer (QA)\npairs. The benchmark covers two aspects of conflict and three question types,\nproviding a thorough assessment tool. We apply this benchmark to assess the\nconflict-resolution capabilities of nine representative MLLMs from various\nmodel families. Our results indicate an evident over-reliance on parametric\nknowledge for approximately 20% of all queries, especially among Yes-No and\naction-related problems. Based on these findings, we evaluate the effectiveness\nof existing approaches to mitigating the conflicts and compare them to our\n\"Focus-on-Vision\" prompting strategy. Despite some improvement, the\nvision-knowledge conflict remains unresolved and can be further scaled through\nour data construction framework. Our proposed framework, benchmark, and\nanalysis contribute to the understanding and mitigation of vision-knowledge\nconflicts in MLLMs.", "AI": {"tldr": "Explores vision-knowledge conflicts in Multimodal Large Language Models (MLLMs) and introduces a framework for generating diagnostic benchmarks.", "motivation": "To address the issue of commonsense level vision-knowledge conflict in MLLMs, where visual information contradicts the model's internal knowledge.", "method": "An automated framework with human-in-the-loop quality control to generate inputs simulating vision-knowledge conflicts, resulting in a benchmark with images and QA pairs.", "result": "Assessment of nine representative MLLMs reveals a 20% over-reliance on parametric knowledge for queries, particularly in Yes-No and action-related problems.", "conclusion": "While some strategies mitigate the conflict, the issue persists and can be better addressed with the proposed framework and benchmark.", "key_contributions": ["Introduced a framework for simulating vision-knowledge conflicts in MLLMs", "Developed a comprehensive diagnostic benchmark for evaluating MLLM performance", "Proposed the 'Focus-on-Vision' prompting strategy for conflict resolution"], "limitations": "Existing approaches only partially mitigate the conflicts, indicating a need for further investigation and scaling of the framework.", "keywords": ["Multimodal Large Language Models", "vision-knowledge conflict", "benchmark", "automatic framework", "commonsense reasoning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.10075", "pdf": "https://arxiv.org/pdf/2410.10075.pdf", "abs": "https://arxiv.org/abs/2410.10075", "title": "RoCoFT: Efficient Finetuning of Large Language Models with Row-Column Updates", "authors": ["Md Kowsher", "Tara Esmaeilbeig", "Chun-Nam Yu", "Chen Chen", "Mojtaba Soltanalian", "Niloofar Yousefi"], "categories": ["cs.CL"], "comment": "RoCoFT is a parameter-efficient method", "summary": "We propose RoCoFT, a parameter-efficient fine-tuning method for large-scale\nlanguage models (LMs) based on updating only a few rows and columns of the\nweight matrices in transformers. Through extensive experiments with medium-size\nLMs like BERT and RoBERTa, and larger LMs like Bloom-7B, Llama2-7B, and\nLlama2-13B, we show that our method gives comparable or better accuracies than\nstate-of-art PEFT methods while also being more memory and\ncomputation-efficient. We also study the reason behind the effectiveness of our\nmethod with tools from neural tangent kernel theory. We empirically demonstrate\nthat our kernel, constructed using a restricted set of row and column\nparameters, are numerically close to the full-parameter kernel and gives\ncomparable classification performance. Ablation studies are conducted to\ninvestigate the impact of different algorithmic choices, including the\nselection strategy for rows and columns as well as the optimal rank for\neffective implementation of our method.", "AI": {"tldr": "RoCoFT is a parameter-efficient fine-tuning method for language models that updates a limited number of weight matrix entries, achieving competitive results with reduced resource usage.", "motivation": "To develop a more efficient fine-tuning method for large-scale language models that requires fewer parameters while maintaining or improving performance.", "method": "The paper introduces RoCoFT, which updates only specific rows and columns of transformer weight matrices during fine-tuning, informed by principles from neural tangent kernel theory.", "result": "RoCoFT achieves comparable or superior accuracy compared to state-of-the-art parameter-efficient fine-tuning methods while being more memory and computation-efficient.", "conclusion": "The proposed method not only reduces resource consumption but also maintains classification performance, supported by empirical findings and ablation studies addressing algorithmic choices.", "key_contributions": ["Introduction of RoCoFT, a novel fine-tuning method for LMs.", "Demonstration of the method's efficacy with medium and large LMs.", "Empirical analysis linking the method's performance to neural tangent kernel theory."], "limitations": "", "keywords": ["RoCoFT", "fine-tuning", "large-scale language models", "parameter efficiency", "neural tangent kernel"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.11163", "pdf": "https://arxiv.org/pdf/2410.11163.pdf", "abs": "https://arxiv.org/abs/2410.11163", "title": "Model Swarms: Collaborative Search to Adapt LLM Experts via Swarm Intelligence", "authors": ["Shangbin Feng", "Zifeng Wang", "Yike Wang", "Sayna Ebrahimi", "Hamid Palangi", "Lesly Miculicich", "Achin Kulshrestha", "Nathalie Rauschmayr", "Yejin Choi", "Yulia Tsvetkov", "Chen-Yu Lee", "Tomas Pfister"], "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "We propose Model Swarms, a collaborative search algorithm to adapt LLMs via\nswarm intelligence, the collective behavior guiding individual systems.\nSpecifically, Model Swarms starts with a pool of LLM experts and a utility\nfunction. Guided by the best-found checkpoints across models, diverse LLM\nexperts collaboratively move in the weight space and optimize a utility\nfunction representing model adaptation objectives. Compared to existing model\ncomposition approaches, Model Swarms offers tuning-free model adaptation, works\nin low-data regimes with as few as 200 examples, and does not require\nassumptions about specific experts in the swarm or how they should be composed.\nExtensive experiments demonstrate that Model Swarms could flexibly adapt LLM\nexperts to a single task, multi-task domains, reward models, as well as diverse\nhuman interests, improving over 12 model composition baselines by up to 21.0%\nacross tasks and contexts. Further analysis reveals that LLM experts discover\npreviously unseen capabilities in initial checkpoints and that Model Swarms\nenable the weak-to-strong transition of experts through the collaborative\nsearch process.", "AI": {"tldr": "Model Swarms is a collaborative algorithm for adapting large language models (LLMs) using swarm intelligence for improved performance in various tasks with minimal data requirements.", "motivation": "To improve the adaptability of LLMs without needing extensive tuning or large amounts of data, addressing the limitations in current model composition methods.", "method": "The approach involves using a pool of LLM experts guided by their best checkpoints to collaboratively optimize a utility function representing model adaptation objectives through movement in weight space.", "result": "Model Swarms significantly outperforms 12 model composition baselines, achieving up to a 21.0% improvement in various tasks and contexts, and reveals new capabilities in the LLMs.", "conclusion": "The collaborative search process of Model Swarms facilitates the discovery of new model capabilities and effectively transitions experts from weak to strong performance.", "key_contributions": ["Introduction of Model Swarms as a tuning-free adaptation method for LLMs.", "Demonstrates effectiveness in low-data regimes with as few as 200 examples.", "Shows significant performance improvements over existing model composition techniques."], "limitations": "While the method proves effective, its reliance on initial expert checkpoints could impact adaptability if these checkpoints are not optimally selected.", "keywords": ["Model Swarms", "LLM adaptation", "swarm intelligence", "collaborative algorithm", "low-data learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.11693", "pdf": "https://arxiv.org/pdf/2410.11693.pdf", "abs": "https://arxiv.org/abs/2410.11693", "title": "BridG MT: Enhancing LLMs' Machine Translation Capabilities with Sentence Bridging and Gradual MT", "authors": ["Seung-Woo Choi", "Ga-Hyun Yoo", "Jay-Yoon Lee"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "Recent Large Language Models (LLMs) have demonstrated impressive translation\nperformance without requiring fine-tuning on additional parallel corpora.\nHowever, they still face significant challenges in certain scenarios,\nparticularly when translating low-resource languages. A common approach to\naddress this issue is to provide external knowledge, such as few-shot examples,\nto assist LLMs in translating specific source sentences. However, this method\nis fundamentally limited by the quality or quantity of relevant sources, which\ncannot always be guaranteed. To reduce LLMs' reliance on external sources, we\npropose BridG MT, a method that combines Sentence Bridging, which generates a\nsequence of sentences as a bridge that gradually transition from\neasy-to-translate to more difficult, and Gradual MT, which sequentially\ntranslates these sentences using earlier translations as few-shot examples for\nsubsequent ones. Experiments conducted on four LLMs across seven languages\ndemonstrate that our method effectively enhances translation performance, even\noutperforming translation methods that rely on a large number of few-shot\nexamples.", "AI": {"tldr": "BridG MT enhances LLM translation, especially in low-resource scenarios, using Sentence Bridging and Gradual MT.", "motivation": "To address the limitations of LLMs in translating low-resource languages without relying heavily on external knowledge.", "method": "BridG MT employs Sentence Bridging to create a sequence of transition sentences and Gradual MT to translate them sequentially, leveraging earlier translations as few-shot examples.", "result": "Experiments on four LLMs and seven languages show that BridG MT improves translation performance, surpassing methods that require many few-shot examples.", "conclusion": "BridG MT offers a robust alternative for enhancing translation in low-resource language scenarios without depending on external sources.", "key_contributions": ["Introduction of BridG MT combining Sentence Bridging and Gradual MT.", "Demonstrated effectiveness on various LLMs and multiple languages.", "Outperformance of traditional few-shot translation methods."], "limitations": "The method's performance may vary with different languages and contexts that may not be covered in the study.", "keywords": ["Large Language Models", "translation", "low-resource languages", "Sentence Bridging", "Gradual MT"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.12613", "pdf": "https://arxiv.org/pdf/2410.12613.pdf", "abs": "https://arxiv.org/abs/2410.12613", "title": "Exploring Model Kinship for Merging Large Language Models", "authors": ["Yedi Hu", "Yunzhi Yao", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "comment": "Ongoing work", "summary": "Model merging has become one of the key technologies for enhancing the\ncapabilities and efficiency of Large Language Models (LLMs). However, our\nunderstanding of the expected performance gains and principles when merging any\ntwo models remains limited. In this work, we introduce model kinship, the\ndegree of similarity or relatedness between LLMs, analogous to biological\nevolution. With comprehensive empirical analysis, we find that there is a\ncertain relationship between model kinship and the performance gains after\nmodel merging, which can help guide our selection of candidate models. Inspired\nby this, we propose a new model merging strategy: Top-k Greedy Merging with\nModel Kinship, which can yield better performance on benchmark datasets.\nSpecifically, we discover that using model kinship as a criterion can assist us\nin continuously performing model merging, alleviating the degradation (local\noptima) in model evolution, whereas model kinship can serve as a guide to\nescape these traps. Code is available at\nhttps://github.com/zjunlp/ModelKinship.", "AI": {"tldr": "This paper explores model merging strategies for Large Language Models (LLMs) by introducing the concept of model kinship, which relates to performance improvements during merging.", "motivation": "The motivation is to improve the merging processes of LLMs by understanding the performance gains through the similarity between models, termed model kinship.", "method": "The authors conduct extensive empirical analysis to investigate the relationship between model kinship and performance gains in model merging. They propose a merging strategy called Top-k Greedy Merging with Model Kinship.", "result": "The proposed strategy shows improved performance on benchmark datasets, demonstrating that model kinship can guide the selection of models for merging to achieve optimal performance.", "conclusion": "Using model kinship as a criterion aids in ongoing model merging, helping to avoid issues like degradation and local optima.", "key_contributions": ["Introduction of the concept of model kinship in LLM merging.", "Development of the Top-k Greedy Merging strategy based on model kinship.", "Empirical evidence supporting the link between model kinship and performance improvements."], "limitations": "", "keywords": ["model merging", "large language models", "model kinship", "performance gains", "empirical analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.13098", "pdf": "https://arxiv.org/pdf/2410.13098.pdf", "abs": "https://arxiv.org/abs/2410.13098", "title": "A Little Human Data Goes A Long Way", "authors": ["Dhananjay Ashok", "Jonathan May"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025", "summary": "Faced with an expensive human annotation process, creators of NLP systems\nincreasingly turn to synthetic data generation. While this method shows\npromise, the extent to which synthetic data can replace human annotation is\npoorly understood. We investigate the use of synthetic data in Fact\nVerification (FV) and Question Answering (QA) by studying the effects of\nincrementally replacing human generated data with synthetic points on eight\ndiverse datasets. Strikingly, replacing up to 90% of the training data only\nmarginally decreases performance, but replacing the final 10% leads to severe\ndeclines. We find that models trained on purely synthetic data can be reliably\nimproved by including as few as 125 human generated data points. We show that\nmatching the performance gain of just a little additional human data (only 200\npoints) requires an order of magnitude more synthetic data and estimate price\nratios at which human annotation would be a more cost-effective solution. Our\nresults suggest that even when human annotation at scale is infeasible, there\nis great value to having a small proportion of the dataset being human\ngenerated.", "AI": {"tldr": "This paper investigates the impact of synthetic data on NLP tasks, particularly Fact Verification and Question Answering, revealing that a small amount of human data significantly enhances model performance despite using mostly synthetic data.", "motivation": "To understand how synthetic data can replace human annotation in NLP tasks due to the high cost of human annotation.", "method": "The authors studied the effects of incrementally replacing human-generated data with synthetic data across eight diverse datasets in the domains of Fact Verification and Question Answering.", "result": "Replacing up to 90% of human-generated data only slightly affects performance, but the last 10% is critical, with models benefiting significantly from just 125 human-generated data points.", "conclusion": "A small fraction of human-generated data is valuable, as it can improve models trained predominantly on synthetic data; hence, investigating cost-effective ratios between human and synthetic data is crucial.", "key_contributions": ["Demonstrated the marginal decrease in performance with high percentages of synthetic data.", "Identified the critical importance of the last human data points for model efficacy.", "Provided insights on the cost-effectiveness of mixed data sourcing for NLP tasks."], "limitations": "", "keywords": ["synthetic data", "human annotation", "Fact Verification", "Question Answering", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2410.13184", "pdf": "https://arxiv.org/pdf/2410.13184.pdf", "abs": "https://arxiv.org/abs/2410.13184", "title": "Router-Tuning: A Simple and Effective Approach for Enabling Dynamic-Depth in Transformers", "authors": ["Shwai He", "Tao Ge", "Guoheng Sun", "Bowei Tian", "Xiaoyang Wang", "Dong Yu"], "categories": ["cs.CL"], "comment": null, "summary": "Traditional transformer models often allocate a fixed amount of computational\nresources to every input token, leading to inefficient and unnecessary\ncomputation. To address this, the Mixture of Depths (MoD) was introduced to\ndynamically adjust the computational depth by skipping less important layers.\nDespite its promise, current MoD approaches remain under-explored and face two\nmain challenges: (1) high training costs due to the need to train the entire\nmodel along with the routers that determine which layers to skip, and (2) the\nrisk of performance degradation when important layers are bypassed. In response\nto the first issue, we propose Router-Tuning, a method that fine-tunes only the\nrouter on a small dataset, drastically reducing the computational overhead\nassociated with full model training. For the second challenge, we propose\nMindSkip, which deploys Attention with Dynamic Depths. This method preserves\nthe model's performance while significantly enhancing computational and memory\nefficiency. Extensive experiments demonstrate that our approach delivers\ncompetitive results while dramatically improving the computation efficiency,\ne.g., 21\\% speedup and only a 0.2\\% performance drop. The code is released at\nhttps://github.com/CASE-Lab-UMD/Router-Tuning.", "AI": {"tldr": "The paper introduces Router-Tuning and MindSkip to enhance the efficiency of transformer models by dynamically adjusting computational depth and reducing training costs.", "motivation": "Traditional transformer models waste computation on every token. The Mixture of Depths (MoD) aims to optimize this but faces challenges such as high training costs and potential performance loss when important layers are skipped.", "method": "Router-Tuning fine-tunes only the router using a small dataset, reducing overhead, while MindSkip implements Attention with Dynamic Depths to maintain performance and improve efficiency.", "result": "The approach achieves competitive results with a 21% speedup and only a 0.2% drop in performance.", "conclusion": "The proposed methods improve computational efficiency in transformer models, making them more resource-effective without sacrificing performance.", "key_contributions": ["Introduced Router-Tuning to reduce training costs associated with MoD methods.", "Developed MindSkip to achieve dynamic depth adjustment while maintaining performance.", "Demonstrated significant computational speedup with minimal performance loss through experiments."], "limitations": "", "keywords": ["Transformer models", "Mixture of Depths", "Router-Tuning", "Dynamic Depths", "Attention"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.13281", "pdf": "https://arxiv.org/pdf/2410.13281.pdf", "abs": "https://arxiv.org/abs/2410.13281", "title": "BanTH: A Multi-label Hate Speech Detection Dataset for Transliterated Bangla", "authors": ["Fabiha Haider", "Fariha Tanjim Shifat", "Md Farhan Ishmam", "Deeparghya Dutta Barua", "Md Sakib Ul Rahman Sourove", "Md Fahim", "Md Farhad Alam"], "categories": ["cs.CL"], "comment": "Published in NAACL Findings 2025", "summary": "The proliferation of transliterated texts in digital spaces has emphasized\nthe need for detecting and classifying hate speech in languages beyond English,\nparticularly in low-resource languages. As online discourse can perpetuate\ndiscrimination based on target groups, e.g. gender, religion, and origin,\nmulti-label classification of hateful content can help in comprehending hate\nmotivation and enhance content moderation. While previous efforts have focused\non monolingual or binary hate classification tasks, no work has yet addressed\nthe challenge of multi-label hate speech classification in transliterated\nBangla. We introduce BanTH, the first multi-label transliterated Bangla hate\nspeech dataset comprising 37.3k samples. The samples are sourced from YouTube\ncomments, where each instance is labeled with one or more target groups,\nreflecting the regional demographic. We establish novel transformer\nencoder-based baselines by further pre-training on transliterated Bangla\ncorpus. We also propose a novel translation-based LLM prompting strategy for\ntransliterated text. Experiments reveal that our further pre-trained encoders\nare achieving state-of-the-art performance on the BanTH dataset, while our\ntranslation-based prompting outperforms other strategies in the zero-shot\nsetting. The introduction of BanTH not only fills a critical gap in hate speech\nresearch for Bangla but also sets the stage for future exploration into\ncode-mixed and multi-label classification challenges in underrepresented\nlanguages.", "AI": {"tldr": "This paper introduces BanTH, the first multi-label hate speech dataset for transliterated Bangla, and establishes novel methods for classification.", "motivation": "To detect and classify hate speech in low-resource languages, specifically in transliterated Bangla, addressing a critical gap in the research.", "method": "Developed a multi-label transliterated Bangla hate speech dataset (BanTH) with 37.3k samples from YouTube comments and established transformer encoder-based baselines while employing a novel translation-based LLM prompting strategy.", "result": "The further pre-trained transformers achieve state-of-the-art performance on the BanTH dataset, and the proposed LLM prompting strategy outperforms others in zero-shot settings.", "conclusion": "The BanTH dataset not only addresses the lack of resources in hate speech research for Bangla but also paves the way for future work on more complex language challenges.", "key_contributions": ["Introduction of the BanTH dataset for transliterated Bangla", "State-of-the-art transformer encoders for hate speech classification", "A novel LLM prompting strategy for transliterated text"], "limitations": "", "keywords": ["Hate Speech", "Transliterated Bangla", "Multi-label Classification", "Natural Language Processing", "Dataset"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2410.14309", "pdf": "https://arxiv.org/pdf/2410.14309.pdf", "abs": "https://arxiv.org/abs/2410.14309", "title": "LoGU: Long-form Generation with Uncertainty Expressions", "authors": ["Ruihan Yang", "Caiqi Zhang", "Zhisong Zhang", "Xinting Huang", "Sen Yang", "Nigel Collier", "Dong Yu", "Deqing Yang"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "While Large Language Models (LLMs) demonstrate impressive capabilities, they\nstill struggle with generating factually incorrect content (i.e.,\nhallucinations). A promising approach to mitigate this issue is enabling models\nto express uncertainty when unsure. Previous research on uncertainty modeling\nhas primarily focused on short-form QA, but realworld applications often\nrequire much longer responses. In this work, we introduce the task of Long-form\nGeneration with Uncertainty(LoGU). We identify two key challenges: Uncertainty\nSuppression, where models hesitate to express uncertainty, and Uncertainty\nMisalignment, where models convey uncertainty inaccurately. To tackle these\nchallenges, we propose a refinement-based data collection framework and a\ntwo-stage training pipeline. Our framework adopts a divide-and-conquer\nstrategy, refining uncertainty based on atomic claims. The collected data are\nthen used in training through supervised fine-tuning (SFT) and direct\npreference optimization (DPO) to enhance uncertainty expression. Extensive\nexperiments on three long-form instruction following datasets show that our\nmethod significantly improves accuracy, reduces hallucinations, and maintains\nthe comprehensiveness of responses.", "AI": {"tldr": "This paper addresses the challenge of hallucinations in Long-form Generation by introducing a new approach for modeling uncertainty in LLMs.", "motivation": "The motivation is to enhance the factual accuracy of outputs from Large Language Models by enabling them to express uncertainty, particularly in long-form generative tasks.", "method": "A refinement-based data collection framework is introduced along with a two-stage training pipeline that employs supervised fine-tuning and direct preference optimization to improve uncertainty expression in long-form responses.", "result": "The proposed method significantly improves the accuracy of generated content, reduces hallucinations, and maintains the comprehensiveness of responses across three long-form instruction following datasets.", "conclusion": "The findings demonstrate that enhancing uncertainty modeling in LLMs can lead to more reliable and accurate long-form text generation.", "key_contributions": ["Introduction of the Long-form Generation with Uncertainty (LoGU) task", "Development of a refinement-based data collection framework", "Implementation of a two-stage training pipeline for enhancing uncertainty expression"], "limitations": "", "keywords": ["Large Language Models", "Uncertainty Modeling", "Long-form Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2410.16930", "pdf": "https://arxiv.org/pdf/2410.16930.pdf", "abs": "https://arxiv.org/abs/2410.16930", "title": "Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes", "authors": ["Bryan R. Christ", "Zack Gottesman", "Jonathan Kropko", "Thomas Hartvigsen"], "categories": ["cs.CL", "cs.AI"], "comment": "38 pages, 54 figures, Accepted to ACL 2025 (Main)", "summary": "Math reasoning is an active area of Large Language Model (LLM) research\nbecause it is a hallmark of artificial intelligence and has implications in\nseveral domains, including math education. However, few works have explored how\nmath reasoning is encoded within LLM parameters and if it is a skill that can\nbe isolated within models. Doing so could allow targeted intervention to\nimprove math performance without altering non-math behavior and foster\nunderstanding of how models encode math reasoning. We introduce Math\nNeurosurgery (MathNeuro), a computationally efficient method we use to isolate\nmath-specific parameters in LLMs using only forward passes. MathNeuro builds on\nexisting work by using weights and activations to calculate parameter\nimportance, but isolates math-specific parameters by filtering out those\nimportant for general language tasks. Through pruning parameters MathNeuro\nidentifies, we delete a LLM's math reasoning ability without significantly\nimpacting its general language ability. Scaling the identified parameters by a\nsmall constant improves a pretrained or instruction-tuned LLM's performance by\n4-17% on GSM8K and 5-35% on MATH while leaving non-math behavior unaltered.\nMathNeuro is also data efficient: most of its effectiveness holds when\nidentifying math-specific parameters using a single sample. MathNeuro\nhighlights the potential for future work to intervene on math-specific\nparameters.", "AI": {"tldr": "MathNeuro is a method to isolate math reasoning parameters in LLMs without affecting general language ability, showing improved math performance.", "motivation": "Understanding how math reasoning is encoded in LLMs can lead to targeted interventions for improving math performance in AI models.", "method": "MathNeuro uses a computationally efficient approach that filters out parameters important for general language tasks to isolate math-specific parameters.", "result": "By pruning identified parameters, MathNeuro can remove a LLM's math reasoning ability while maintaining its general language proficiency; scaling these parameters improves performance on math benchmarks significantly.", "conclusion": "MathNeuro demonstrates a promising direction for future research on isolating and improving math reasoning in language models without compromising their overall capabilities.", "key_contributions": ["Introduction of MathNeuro for isolating math-specific parameters in LLMs", "Demonstration of performance improvement in math tasks with minimal impact on language tasks", "Data efficiency of identifying math parameters with a single sample"], "limitations": "The long-term effects of parameter pruning on model performance and generalizability remain to be explored.", "keywords": ["Large Language Models", "Math Reasoning", "Parameter Isolation", "Machine Learning", "Education"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.17714", "pdf": "https://arxiv.org/pdf/2410.17714.pdf", "abs": "https://arxiv.org/abs/2410.17714", "title": "CogSteer: Cognition-Inspired Selective Layer Intervention for Efficiently Steering Large Language Models", "authors": ["Xintong Wang", "Jingheng Pan", "Liang Ding", "Longyue Wang", "Longqin Jiang", "Xingshan Li", "Chris Biemann"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance through\npretraining on extensive data. This enables efficient adaptation to diverse\ndownstream tasks. However, the lack of interpretability in their underlying\nmechanisms limits the ability to effectively steer LLMs for specific\napplications. In this work, we investigate the intrinsic mechanisms of LLMs\nfrom a cognitive perspective using eye movement measures. Specifically, we\nanalyze the layer-wise correlation between human cognitive indicators and LLM\nrepresentations. Building on these insights, we propose a heuristic approach\nfor selecting the optimal steering layer to modulate LLM semantics. To this\nend, we introduce an efficient selective layer intervention based on prominent\nparameter-efficient fine-tuning methods, which conventionally adjust either all\nlayers or only the final layer. Additionally, we present an implicit layer\ncontrastive intervention during inference to steer LLMs away from toxic\noutputs. Extensive experiments on natural language understanding, reasoning,\nand generation tasks, conducted on GPT-2, Llama2-7B, and Mistral-7B,\ndemonstrate the effectiveness and efficiency of our approach. As a\nmodel-agnostic framework, it enhances the interpretability of LLMs while\nimproving efficiency for safe deployment.", "AI": {"tldr": "This paper investigates the cognitive mechanisms of Large Language Models (LLMs) using eye movement measures and proposes a method for selecting optimal steering layers to improve interpretability and efficiency in LLM applications.", "motivation": "The need for greater interpretability in LLMs to effectively steer them for specific applications, especially given their remarkable performance across various tasks with limited interpretability.", "method": "The study employs eye movement measures to analyze layer-wise correlations between human cognitive indicators and LLM representations, proposing a heuristic for selecting the optimal steering layer and introducing an implicit layer contrastive intervention during inference.", "result": "Extensive experiments on GPT-2, Llama2-7B, and Mistral-7B show that the proposed framework enhances LLM interpretability and improves deployment efficiency by reducing toxic outputs.", "conclusion": "The proposed model-agnostic framework offers a novel approach to modulate LLM semantics while maintaining their performance, addressing the challenges of interpretability and safety in deployment.", "key_contributions": ["Investigated cognitive mechanisms of LLMs through eye movement measures", "Proposed a heuristic for selecting optimal steering layers in LLMs", "Introduced an implicit layer contrastive intervention to mitigate toxic outputs"], "limitations": "", "keywords": ["large language models", "human-computer interaction", "cognitive measures", "layer intervention", "fine-tuning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2410.17891", "pdf": "https://arxiv.org/pdf/2410.17891.pdf", "abs": "https://arxiv.org/abs/2410.17891", "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models", "authors": ["Shansan Gong", "Shivam Agarwal", "Yizhe Zhang", "Jiacheng Ye", "Lin Zheng", "Mukai Li", "Chenxin An", "Peilin Zhao", "Wei Bi", "Jiawei Han", "Hao Peng", "Lingpeng Kong"], "categories": ["cs.CL"], "comment": "ICLR 2025. (minor updates) Code: https://github.com/HKUNLP/DiffuLLaMA", "summary": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for\ntext generative modeling, potentially addressing limitations of autoregressive\n(AR) models. However, current DLMs have been studied at a smaller scale\ncompared to their AR counterparts and lack fair comparison on language modeling\nbenchmarks. Additionally, training diffusion models from scratch at scale\nremains challenging. Given the prevalence of open-source AR language models, we\npropose adapting these models to build text diffusion models. We demonstrate\nconnections between AR and diffusion modeling objectives and introduce a simple\ncontinual pre-training approach for training diffusion models. Through\nsystematic evaluation on language modeling, reasoning, and commonsense\nbenchmarks, we show that we can convert AR models ranging from 127M to 7B\nparameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA,\nusing less than 200B tokens for training. Our experimental results reveal that\nthese models outperform earlier DLMs and are competitive with their AR\ncounterparts. We release a suite of DLMs (127M-355M-7B) capable of generating\nfluent text, performing in-context learning, filling in the middle without\nprompt re-ordering, and following instructions\nhttps://github.com/HKUNLP/DiffuLLaMA.", "AI": {"tldr": "This paper explores Diffusion Language Models (DLMs) as an alternative to autoregressive models, proposing a method to adapt existing AR models to create competitive DLMs for text generation.", "motivation": "To address the limitations of autoregressive models in text generation and establish a fair comparison on language modeling benchmarks while training DLMs at scale.", "method": "Introduce connections between autoregressive and diffusion modeling objectives, and a continual pre-training approach for training diffusion models from AR models, particularly GPT-2 and LLaMA.", "result": "The adapted diffusion models, DiffuGPT and DiffuLLaMA, outperform earlier DLMs and are competitive with AR counterparts across various language modeling, reasoning, and commonsense benchmarks.", "conclusion": "The study successfully shows that adapting AR models into DLMs enhances text generation capabilities, paving the way for further exploration in diffusion modeling for language tasks.", "key_contributions": ["Establishment of a link between autoregressive and diffusion models.", "Introduction of a continual pre-training approach for DLMs.", "Release of multiple scalable DLM variants that outperform previous models."], "limitations": "Challenges in scaling training of diffusion models and dependency on the established AR models for adaptation.", "keywords": ["Diffusion Language Models", "autoregressive models", "text generation", "machine learning", "natural language processing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2410.18359", "pdf": "https://arxiv.org/pdf/2410.18359.pdf", "abs": "https://arxiv.org/abs/2410.18359", "title": "Improving Model Factuality with Fine-grained Critique-based Evaluator", "authors": ["Yiqing Xie", "Wenxuan Zhou", "Pradyot Prakash", "Di Jin", "Yuning Mao", "Quintin Fettes", "Arya Talebzadeh", "Sinong Wang", "Han Fang", "Carolyn Rose", "Daniel Fried", "Hejia Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Factuality evaluation aims to detect factual errors produced by language\nmodels (LMs) and hence guide the development of more factual models. Towards\nthis goal, we train a factuality evaluator, FenCE, that provides LM generators\nwith claim-level factuality feedback. We conduct data augmentation on a\ncombination of public judgment datasets to train FenCE to (1) generate textual\ncritiques along with scores and (2) make claim-level judgment based on diverse\nsource documents obtained by various tools. We then present a framework that\nleverages FenCE to improve the factuality of LM generators by constructing\ntraining data. Specifically, we generate a set of candidate responses, leverage\nFenCE to revise and score each response without introducing lesser-known facts,\nand train the generator by preferring highly scored revised responses.\nExperiments show that our data augmentation methods improve the evaluator's\naccuracy by 2.9% on LLM-AggreFact. With FenCE, we improve Llama2-7B-chat and\nLlama3-8B-chat's factuality rate by 16.86% and 14.45% on FActScore,\noutperforming state-of-the-art factuality finetuning methods by 8.83% and\n6.96%.", "AI": {"tldr": "The paper introduces FenCE, a factuality evaluator to enhance the factual accuracy of language models by providing claim-level feedback and revising generated responses.", "motivation": "The need to detect and correct factual errors in language models to improve their reliability and output quality drives the research.", "method": "The authors trained FenCE through data augmentation on public judgment datasets to provide textual critiques and claim-level judgments, and used it to generate training data that improves LM generators' factuality.", "result": "FenCE improves the accuracy of factuality evaluations by 2.9% and enhances the factuality rates of Llama2-7B-chat and Llama3-8B-chat by 16.86% and 14.45%, respectively, exceeding state-of-the-art methods.", "conclusion": "The implementation of FenCE shows significant advancements in the factual correction of language models, thus contributing to more reliable AI-generated content.", "key_contributions": ["Introduction of FenCE for claim-level factuality evaluation", "Demonstration of substantial accuracy improvements in language models", "Development of a framework for leveraging factuality feedback to enhance LM outputs"], "limitations": "", "keywords": ["factuality evaluation", "language models", "data augmentation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2410.18702", "pdf": "https://arxiv.org/pdf/2410.18702.pdf", "abs": "https://arxiv.org/abs/2410.18702", "title": "GrammaMT: Improving Machine Translation with Grammar-Informed In-Context Learning", "authors": ["Rita Ramos", "Everlyn Asiko Chimoto", "Maartje ter Hoeve", "Natalie Schluter"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "We introduce GrammaMT, a grammatically-aware prompting approach for machine\ntranslation that uses Interlinear Glossed Text (IGT), a common form of\nlinguistic description providing morphological and lexical annotations for\nsource sentences. GrammaMT proposes three prompting strategies: gloss-shot,\nchain-gloss and model-gloss. All are training-free, requiring only a few\nexamples that involve minimal effort to collect, and making them well-suited\nfor low-resource setups. Experiments show that GrammaMT enhances translation\nperformance on open-source instruction-tuned LLMs for various low- to\nhigh-resource languages across three benchmarks: (1) the largest IGT corpus,\n(2) the challenging 2023 SIGMORPHON Shared Task data over endangered languages,\nand (3) even in an out-of-domain setting with FLORES. Moreover, ablation\nstudies reveal that leveraging gloss resources could substantially boost MT\nperformance (by over 17 BLEU points) if LLMs accurately generate or access\ninput sentence glosses.", "AI": {"tldr": "This paper presents GrammaMT, a grammatically-aware prompting method for machine translation using Interlinear Glossed Text (IGT) that improves translation performance, especially in low-resource settings.", "motivation": "To enhance machine translation performance for various languages using grammatically-informed prompting strategies, particularly in low-resource contexts.", "method": "GrammaMT introduces three training-free prompting strategies—gloss-shot, chain-gloss, and model-gloss—requiring only a few examples for setup.", "result": "The proposed methods improved translation performance significantly on instruction-tuned LLMs across three benchmarks, showcasing enhancements of over 17 BLEU points with proper gloss access.", "conclusion": "GrammaMT successfully demonstrates the potential to enhance machine translation through strategic use of gloss resources, particularly beneficial for low-resource languages.", "key_contributions": ["Introduction of three novel prompting strategies for machine translation", "Use of Interlinear Glossed Text to inform translation tasks", "Demonstrated significant BLEU score improvements across various language benchmarks"], "limitations": "The performance enhancement is contingent on accurate generation or access to gloss resources, which may not always be achievable.", "keywords": ["machine translation", "grammatical prompting", "Interlinear Glossed Text", "low-resource languages", "language models"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2410.20445", "pdf": "https://arxiv.org/pdf/2410.20445.pdf", "abs": "https://arxiv.org/abs/2410.20445", "title": "TrajAgent: An LLM-based Agent Framework for Automated Trajectory Modeling via Collaboration of Large and Small Models", "authors": ["Yuwei Du", "Jie Feng", "Jie Zhao", "Yong Li"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "the code will be openly accessible at:\n  https://github.com/tsinghua-fib-lab/TrajAgent", "summary": "Trajectory modeling, which includes research on trajectory data pattern\nmining and future prediction, has widespread applications in areas such as life\nservices, urban transportation, and public administration. Numerous methods\nhave been proposed to address specific problems within trajectory modeling.\nHowever, the heterogeneity of data and the diversity of trajectory tasks make\neffective and reliable trajectory modeling an important yet highly challenging\nendeavor, even for domain experts. In this paper, we propose\n\\textit{TrajAgent}, a agent framework powered by large language models (LLMs),\ndesigned to facilitate robust and efficient trajectory modeling through\nautomation modeling. This framework leverages and optimizes diverse specialized\nmodels to address various trajectory modeling tasks across different datasets\neffectively. In \\textit{TrajAgent}, we first develop \\textit{UniEnv}, an\nexecution environment with a unified data and model interface, to support the\nexecution and training of various models. Building on \\textit{UniEnv}, we\nintroduce an agentic workflow designed for automatic trajectory modeling across\nvarious trajectory tasks and data. Furthermore, we introduce collaborative\nlearning schema between LLM-based agents and small speciallized models, to\nenhance the performance of the whole framework effectively. Extensive\nexperiments on four tasks using four real-world datasets demonstrate the\neffectiveness of \\textit{TrajAgent} in automated trajectory modeling, achieving\na performance improvement of 2.38\\%-34.96\\% over baseline methods.", "AI": {"tldr": "The paper presents TrajAgent, a framework utilizing large language models for automated trajectory modeling, addressing challenges in trajectory data pattern mining and prediction across diverse tasks.", "motivation": "To tackle the challenges in trajectory modeling caused by data heterogeneity and task diversity, enhancing the automation and reliability of modeling processes.", "method": "The TrajAgent framework integrates a unified execution environment (UniEnv) and an agentic workflow, utilizing collaborative learning between LLM-based agents and specialized models for various trajectory tasks.", "result": "Experiments show that TrajAgent improves performance by 2.38%-34.96% over baseline methods across four trajectory modeling tasks using real-world datasets.", "conclusion": "TrajAgent effectively enhances automated trajectory modeling, highlighting the potential of LLMs in addressing complex trajectory data challenges.", "key_contributions": ["Introduction of TrajAgent framework for trajectory modeling using LLMs.", "Development of UniEnv for unified data and model support.", "Collaborative learning schema between LLMs and specialized models to improve performance."], "limitations": "", "keywords": ["trajectory modeling", "large language models", "automated modeling", "collaborative learning", "data heterogeneity"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2410.22499", "pdf": "https://arxiv.org/pdf/2410.22499.pdf", "abs": "https://arxiv.org/abs/2410.22499", "title": "Anticipating Future with Large Language Model for Simultaneous Machine Translation", "authors": ["Siqi Ouyang", "Oleksii Hrinchuk", "Zhehuai Chen", "Vitaly Lavrukhin", "Jagadeesh Balam", "Lei Li", "Boris Ginsburg"], "categories": ["cs.CL"], "comment": "NAACL 2025 Main", "summary": "Simultaneous machine translation (SMT) takes streaming input utterances and\nincrementally produces target text. Existing SMT methods mainly use the partial\nutterance that has already arrived at the input and the generated hypothesis.\nMotivated by human interpreters' technique to forecast future words before\nhearing them, we propose $\\textbf{T}$ranslation by $\\textbf{A}$nticipating\n$\\textbf{F}$uture (TAF), a method to improve translation quality while\nretraining low latency. Its core idea is to use a large language model (LLM) to\npredict future source words and opportunistically translate without introducing\ntoo much risk. We evaluate our TAF and multiple baselines of SMT on four\nlanguage directions. Experiments show that TAF achieves the best translation\nquality-latency trade-off and outperforms the baselines by up to 5 BLEU points\nat the same latency (three words). Code is released at\nhttps://github.com/owaski/TAF", "AI": {"tldr": "Proposes a new method called Translation by Anticipating Future (TAF) to enhance simultaneous machine translation using LLMs.", "motivation": "To improve the quality of simultaneous machine translation by mimicking human interpreters who forecast future words.", "method": "Utilizes a large language model to predict upcoming source words while translating simultaneously, striking a balance between translation quality and latency.", "result": "TAF outperforms existing SMT methods by achieving a better translation quality-latency trade-off, with improvements of up to 5 BLEU points.", "conclusion": "The TAF method significantly enhances the simultaneous machine translation process by incorporating future word predictions without incurring high latency.", "key_contributions": ["Introduction of the TAF method for simultaneous machine translation.", "Improved translation quality via future word anticipation using LLMs.", "Demonstrated better performance compared to multiple SMT baselines."], "limitations": "", "keywords": ["simultaneous machine translation", "large language models", "translation quality", "latency", "future word anticipation"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2411.04699", "pdf": "https://arxiv.org/pdf/2411.04699.pdf", "abs": "https://arxiv.org/abs/2411.04699", "title": "Towards Building Large Scale Datasets and State-of-the-Art Automatic Speech Translation Systems for 14 Indian Languages", "authors": ["Ashwin Sankar", "Sparsh Jain", "Nikhil Narasimhan", "Devilal Choudhary", "Dhairya Suman", "Mohammed Safi Ur Rahman Khan", "Anoop Kunchukuttan", "Mitesh M Khapra", "Raj Dabre"], "categories": ["cs.CL"], "comment": "Accepted at ACL (Main) 2025", "summary": "Speech translation for Indian languages remains a challenging task due to the\nscarcity of large-scale, publicly available datasets that capture the\nlinguistic diversity and domain coverage essential for real-world applications.\nExisting datasets cover a fraction of Indian languages and lack the breadth\nneeded to train robust models that generalize beyond curated benchmarks. To\nbridge this gap, we introduce BhasaAnuvaad, the largest speech translation\ndataset for Indian languages, spanning over 44 thousand hours of audio and 17\nmillion aligned text segments across 14 Indian languages and English. Our\ndataset is built through a threefold methodology: (a) aggregating high-quality\nexisting sources, (b) large-scale web crawling to ensure linguistic and domain\ndiversity, and (c) creating synthetic data to model real-world speech\ndisfluencies. Leveraging BhasaAnuvaad, we train IndicSeamless, a\nstate-of-the-art speech translation model for Indian languages that performs\nbetter than existing models. Our experiments demonstrate improvements in the\ntranslation quality, setting a new standard for Indian language speech\ntranslation. We will release all the code, data and model weights in the\nopen-source, with permissive licenses to promote accessibility and\ncollaboration.", "AI": {"tldr": "BhasaAnuvaad is introduced as the largest speech translation dataset for Indian languages, facilitating improved translation quality via a new model.", "motivation": "The scarcity of large-scale datasets for Indian languages hampers the development of robust speech translation models.", "method": "The dataset is constructed through aggregating existing sources, web crawling for diversity, and creating synthetic data to model speech disfluencies.", "result": "The IndicSeamless model trained on BhasaAnuvaad demonstrates significant improvements over existing speech translation models for Indian languages.", "conclusion": "The release of BhasaAnuvaad and IndicSeamless sets a new standard for speech translation in Indian languages, promoting open-source accessibility.", "key_contributions": ["Introduction of BhasaAnuvaad, the largest speech translation dataset for Indian languages.", "Development of IndicSeamless, a state-of-the-art speech translation model.", "Setting a new benchmark for translation quality in Indian language speech translation."], "limitations": "", "keywords": ["speech translation", "Indian languages", "dataset", "machine learning", "natural language processing"], "importance_score": 7, "read_time_minutes": 6}}
{"id": "2411.04794", "pdf": "https://arxiv.org/pdf/2411.04794.pdf", "abs": "https://arxiv.org/abs/2411.04794", "title": "KnowCoder-X: Boosting Multilingual Information Extraction via Code", "authors": ["Yuxin Zuo", "Wenxuan Jiang", "Wenxuan Liu", "Zixuan Li", "Long Bai", "Hanbin Wang", "Yutao Zeng", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL 2025 Findings", "summary": "Empirical evidence indicates that LLMs exhibit spontaneous cross-lingual\nalignment. However, although LLMs show promising cross-lingual alignment in\nInformation Extraction (IE), a significant imbalance across languages persists,\nhighlighting an underlying deficiency. To address this, we propose KnowCoder-X,\na powerful code LLM with advanced cross-lingual and multilingual capabilities\nfor universal IE. Firstly, it standardizes the representation of multilingual\nschemas using Python classes, ensuring a consistent ontology across different\nlanguages. Then, IE across languages is formulated as a unified code generation\ntask. Secondly, we conduct IE cross-lingual alignment instruction tuning on the\ntranslated instance prediction task to enhance the model's cross-lingual\ntransferability. During this phase, we also construct a high-quality and\ndiverse bilingual IE parallel dataset with 257k samples, called ParallelNER,\nsynthesized by our proposed robust three-stage pipeline, with manual annotation\nto ensure quality. Although without training in 29 unseen languages,\nKnowCoder-X surpasses ChatGPT by 30.17\\% and SoTA by 20.03\\%, thereby\ndemonstrating superior cross-lingual IE capabilities. Comprehensive evaluations\non 64 IE benchmarks in Chinese and English under various settings demonstrate\nthat KnowCoder-X significantly enhances cross-lingual IE transfer through\nboosting the IE alignment. Our code and dataset are available at:\nhttps://github.com/ICT-GoKnow/KnowCoder", "AI": {"tldr": "KnowCoder-X is a code LLM designed to improve cross-lingual information extraction, outperforming existing models and leveraging a new diverse dataset.", "motivation": "To address the imbalances in cross-lingual alignment for information extraction in LLMs, providing a robust solution for multilingual capabilities.", "method": "KnowCoder-X standardizes multilingual schema representation using Python classes and formulates IE as a unified code generation task, followed by instruction tuning on translated instance prediction and creating a large bilingual dataset, ParallelNER.", "result": "KnowCoder-X surpasses existing models like ChatGPT and SoTA by significant margins in cross-lingual information extraction tasks, demonstrating advanced capabilities.", "conclusion": "KnowCoder-X significantly enhances cross-lingual information extraction and provides a strong foundation for future multilingual applications.", "key_contributions": ["Proposed a novel LLM with advanced cross-lingual and multilingual capabilities for universal information extraction.", "Developed a high-quality bilingual IE parallel dataset with 257k samples for training.", "Demonstrated performance improvements over existing models in cross-lingual tasks."], "limitations": "", "keywords": ["cross-lingual alignment", "information extraction", "language models", "machine learning", "multilingual capabilities"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2411.05980", "pdf": "https://arxiv.org/pdf/2411.05980.pdf", "abs": "https://arxiv.org/abs/2411.05980", "title": "FactLens: Benchmarking Fine-Grained Fact Verification", "authors": ["Kushan Mitra", "Dan Zhang", "Sajjadur Rahman", "Estevam Hruschka"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, updated version", "summary": "Large Language Models (LLMs) have shown impressive capability in language\ngeneration and understanding, but their tendency to hallucinate and produce\nfactually incorrect information remains a key limitation. To verify\nLLM-generated contents and claims from other sources, traditional verification\napproaches often rely on holistic models that assign a single factuality label\nto complex claims, potentially obscuring nuanced errors. In this paper, we\nadvocate for a shift towards fine-grained verification, where complex claims\nare broken down into smaller sub-claims for individual verification, allowing\nfor more precise identification of inaccuracies, improved transparency, and\nreduced ambiguity in evidence retrieval. However, generating sub-claims poses\nchallenges, such as maintaining context and ensuring semantic equivalence with\nrespect to the original claim. We introduce FactLens, a benchmark for\nevaluating fine-grained fact verification, with metrics and automated\nevaluators of sub-claim quality. The benchmark data is manually curated to\nensure high-quality ground truth. Our results show alignment between automated\nFactLens evaluators and human judgments, and we discuss the impact of sub-claim\ncharacteristics on the overall verification performance.", "AI": {"tldr": "This paper advocates for fine-grained fact verification for LLM-generated content by breaking down complex claims into sub-claims, introducing the FactLens benchmark for evaluating this approach.", "motivation": "To address the hallucination and factual inaccuracies prevalent in LLM-generated content by proposing a more nuanced verification approach.", "method": "The paper proposes breaking down complex claims into smaller sub-claims for individual verification and introduces FactLens, a benchmark with metrics for evaluating sub-claim quality.", "result": "The study finds that the sub-claim approach allows for more precise identification of inaccuracies and shows alignment between automated evaluations and human judgments in validation.", "conclusion": "Fine-grained verification improves the transparency and reliability of LLM-generated information by addressing nuanced errors that traditional methods may overlook.", "key_contributions": ["Introduction of FactLens benchmark for evaluating fine-grained fact verification", "Demonstration of improved accuracy in verification through sub-claims", "Development of metrics for assessing sub-claim quality"], "limitations": "", "keywords": ["Fine-grained fact verification", "Large Language Models", "FactLens Benchmark"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2411.08534", "pdf": "https://arxiv.org/pdf/2411.08534.pdf", "abs": "https://arxiv.org/abs/2411.08534", "title": "Neural Topic Modeling with Large Language Models in the Loop", "authors": ["Xiaohao Yang", "He Zhao", "Weijie Xu", "Yuanyuan Qi", "Jueqing Lu", "Dinh Phung", "Lan Du"], "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling is a fundamental task in natural language processing, allowing\nthe discovery of latent thematic structures in text corpora. While Large\nLanguage Models (LLMs) have demonstrated promising capabilities in topic\ndiscovery, their direct application to topic modeling suffers from issues such\nas incomplete topic coverage, misalignment of topics, and inefficiency. To\naddress these limitations, we propose LLM-ITL, a novel LLM-in-the-loop\nframework that integrates LLMs with Neural Topic Models (NTMs). In LLM-ITL,\nglobal topics and document representations are learned through the NTM.\nMeanwhile, an LLM refines these topics using an Optimal Transport (OT)-based\nalignment objective, where the refinement is dynamically adjusted based on the\nLLM's confidence in suggesting topical words for each set of input words. With\nthe flexibility of being integrated into many existing NTMs, the proposed\napproach enhances the interpretability of topics while preserving the\nefficiency of NTMs in learning topics and document representations. Extensive\nexperiments demonstrate that LLM-ITL helps NTMs significantly improve their\ntopic interpretability while maintaining the quality of document\nrepresentation. Our code and datasets are available at\nhttps://github.com/Xiaohao-Yang/LLM-ITL", "AI": {"tldr": "Proposes LLM-ITL, a framework that combines LLMs with Neural Topic Models to enhance topic interpretation while ensuring efficiency.", "motivation": "To improve topic modeling in NLP by addressing limitations of LLMs in topic discovery such as incomplete coverage and misalignment.", "method": "Integration of LLMs with Neural Topic Models (NTMs) using an Optimal Transport-based alignment objective for topic refinement.", "result": "LLM-ITL significantly enhances topic interpretability and maintains high quality in document representations, as demonstrated through extensive experiments.", "conclusion": "The proposed framework represents a flexible and effective enhancement for existing NTMs in topic modeling.", "key_contributions": ["Introduction of the LLM-ITL framework", "Improved interpretability of topics", "Dynamic adjustment of topic refinement based on LLM confidence"], "limitations": "", "keywords": ["Topic modeling", "Large Language Models", "Neural Topic Models", "Natural Language Processing", "Optimal Transport"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2411.15462", "pdf": "https://arxiv.org/pdf/2411.15462.pdf", "abs": "https://arxiv.org/abs/2411.15462", "title": "HateDay: Insights from a Global Hate Speech Dataset Representative of a Day on Twitter", "authors": ["Manuel Tonneau", "Diyi Liu", "Niyati Malhotra", "Scott A. Hale", "Samuel P. Fraiberger", "Victor Orozco-Olvera", "Paul Röttger"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference. Data available at\n  https://huggingface.co/datasets/manueltonneau/hateday", "summary": "To address the global challenge of online hate speech, prior research has\ndeveloped detection models to flag such content on social media. However, due\nto systematic biases in evaluation datasets, the real-world effectiveness of\nthese models remains unclear, particularly across geographies. We introduce\nHateDay, the first global hate speech dataset representative of social media\nsettings, constructed from a random sample of all tweets posted on September\n21, 2022 and covering eight languages and four English-speaking countries.\nUsing HateDay, we uncover substantial variation in the prevalence and\ncomposition of hate speech across languages and regions. We show that\nevaluations on academic datasets greatly overestimate real-world detection\nperformance, which we find is very low, especially for non-European languages.\nOur analysis identifies key drivers of this gap, including models' difficulty\nto distinguish hate from offensive speech and a mismatch between the target\ngroups emphasized in academic datasets and those most frequently targeted in\nreal-world settings. We argue that poor model performance makes public models\nill-suited for automatic hate speech moderation and find that high moderation\nrates are only achievable with substantial human oversight. Our results\nunderscore the need to evaluate detection systems on data that reflects the\ncomplexity and diversity of real-world social media.", "AI": {"tldr": "The paper introduces HateDay, a global dataset for detecting online hate speech, revealing that existing models lack real-world effectiveness due to dataset biases and performance discrepancies across languages.", "motivation": "To address the ineffectiveness of existing hate speech detection models caused by systematic biases in evaluation datasets across geographies.", "method": "Construction of HateDay, a global hate speech dataset from a random sample of tweets in eight languages and four English-speaking countries, followed by analysis of detection performance.", "result": "Significant variation in hate speech prevalence across languages and regions, with real-world model performance being very low, especially for non-European languages.", "conclusion": "Existing models are poorly suited for automatic hate speech moderation and require substantial human oversight to achieve effective moderation rates.", "key_contributions": ["Introduction of HateDay, the first global hate speech dataset", "Identification of performance gaps in existing hate speech detection models across different languages", "Emphasis on the need for evaluations on real-world reflective data for hate speech detection systems"], "limitations": "Focuses primarily on hate speech from a social media perspective and may not generalize to other contexts.", "keywords": ["hate speech", "dataset", "social media", "machine learning", "evaluation"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2411.16679", "pdf": "https://arxiv.org/pdf/2411.16679.pdf", "abs": "https://arxiv.org/abs/2411.16679", "title": "Do Large Language Models Perform Latent Multi-Hop Reasoning without Exploiting Shortcuts?", "authors": ["Sohee Yang", "Nora Kassner", "Elena Gribovskaya", "Sebastian Riedel", "Mor Geva"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "We evaluate how well Large Language Models (LLMs) latently recall and compose\nfacts to answer multi-hop queries like \"In the year Scarlett Johansson was\nborn, the Summer Olympics were hosted in the country of\". One major challenge\nin such evaluation is that LLMs may have developed shortcuts by encountering\nthe head entity \"Scarlett Johansson\" and the answer entity \"United States\" in\nthe same training sequences or merely guess the answer based on frequency-based\npriors. To prevent shortcuts, we exclude test queries where the head and answer\nentities might have co-appeared during training. Through careful selection of\nrelations and facts and systematic removal of cases where models might guess\nanswers or exploit partial matches, we construct an evaluation dataset SOCRATES\n(ShOrtCut-fRee lATent rEaSoning). We observe that LLMs demonstrate promising\nlatent multi-hop reasoning abilities without exploiting shortcuts, but only for\ncertain types of queries. For queries requiring latent recall of countries as\nthe intermediate answer, the best models achieve 80% latent composability, but\nthis drops to just 5% for the recall of years. Comparisons with\nChain-of-Thought highlight a significant gap between the ability of models to\nreason latently versus explicitly. Analysis reveals that latent representations\nof the intermediate answer are constructed more often in queries with higher\nlatent composability, and shows the emergence of latent multi-hop reasoning\nduring pretraining.", "AI": {"tldr": "Evaluation of LLMs on multi-hop queries without exploiting shortcuts.", "motivation": "To assess LLMs' latent reasoning abilities in multi-hop queries while preventing shortcuts.", "method": "The dataset SOCRATES is constructed by excluding queries with co-appearing entities in training, focusing on selective relations and systematic removal of guessable answers.", "result": "LLMs show promising latent multi-hop reasoning for certain queries, achieving 80% composability for countries, but only 5% for years.", "conclusion": "Latent reasoning abilities emerge during pretraining, with a noted gap between latent and explicit reasoning.", "key_contributions": ["Introduction of the SOCRATES dataset to evaluate LLMs without shortcuts", "Demonstration of latent multi-hop reasoning capabilities in LLMs", "Analysis of factors affecting latent composability in queries."], "limitations": "Limited to specific query types where latent reasoning is more prominent.", "keywords": ["Large Language Models", "multi-hop queries", "latent reasoning", "SOCRATES dataset", "Chain-of-Thought"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2411.18478", "pdf": "https://arxiv.org/pdf/2411.18478.pdf", "abs": "https://arxiv.org/abs/2411.18478", "title": "Beyond Examples: High-level Automated Reasoning Paradigm in In-Context Learning via MCTS", "authors": ["Jinyang Wu", "Mingkuan Feng", "Shuai Zhang", "Feihu Che", "Zengqi Wen", "Chonghua Liao", "Jianhua Tao"], "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) enables large language models (LLMs) to perform\ndownstream tasks through advanced prompting and high-quality demonstrations.\nHowever, traditional ICL paradigms encounter significant limitations in complex\nreasoning tasks, stemming primarily from their dependence on example quality\nand absence of explicit reasoning guidance. To address these challenges, we\nintroduce HiAR-ICL, a **Hi**gh-level **A**utomated **R**easoning paradigm in\n**ICL** that shifts focus from specific examples to abstract reasoning\npatterns, thereby extending the conventional concept of \"context\" in ICL. Our\napproach begins by defining five atomic reasoning actions, upon which we employ\nMonte Carlo Tree Search to systematically construct high-level reasoning\npatterns. During inference, HiAR-ICL dynamically selects appropriate reasoning\npatterns based on problem attributes, providing explicit guidance for the\nmodel's reasoning process. Experiments demonstrate HiAR-ICL's effectiveness and\nefficiency: utilizing only 200 prior samples with Qwen2.5-7B-Instruct, our\nmethod achieves 80.6% accuracy on MATH and 62.5% on AMC, exceeding GPT-4o's\n77.2% and 57.5%. Our approach enhances performance across models of varying\nsizes while generalizing effectively across domains. Further analysis reveals\nthat HiAR-ICL can also serve as a plug-and-play inference method compatible\nwith post-training techniques like GRPO. Code and data are available at\nhttps://github.com/jinyangwu/HiARICL.", "AI": {"tldr": "HiAR-ICL is a new approach for in-context learning in LLMs that enhances complex reasoning tasks by focusing on abstract reasoning patterns rather than specific examples.", "motivation": "Traditional in-context learning methods struggle with complex reasoning tasks due to their dependence on the quality of examples and lack of explicit reasoning guidance.", "method": "HiAR-ICL defines five atomic reasoning actions and uses Monte Carlo Tree Search to create high-level reasoning patterns, which are dynamically selected during inference based on problem attributes.", "result": "HiAR-ICL achieves 80.6% accuracy on MATH and 62.5% on AMC using only 200 prior samples with Qwen2.5-7B-Instruct, outperforming GPT-4o.", "conclusion": "The HiAR-ICL approach improves model performance on reasoning tasks and is adaptable with existing post-training techniques, offering a robust framework for complex reasoning in LLMs.", "key_contributions": ["Introduces HiAR-ICL for abstract reasoning in ICL", "Employs Monte Carlo Tree Search for reasoning pattern construction", "Achieves superior accuracy over existing models on reasoning tasks"], "limitations": "", "keywords": ["in-context learning", "large language models", "automated reasoning", "complex reasoning", "Monte Carlo Tree Search"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.01617", "pdf": "https://arxiv.org/pdf/2412.01617.pdf", "abs": "https://arxiv.org/abs/2412.01617", "title": "If Eleanor Rigby Had Met ChatGPT: A Study on Loneliness in a Post-LLM World", "authors": ["Adrian de Wynter"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 (main)", "summary": "Warning: this paper discusses content related, but not limited to, violence,\nsex, and suicide. Loneliness, or the lack of fulfilling relationships,\nsignificantly impacts a person's mental and physical well-being and is\nprevalent worldwide. Previous research suggests that large language models\n(LLMs) may help mitigate loneliness. However, we argue that the use of\nwidespread LLMs in services like ChatGPT is more prevalent--and riskier, as\nthey are not designed for this purpose. To explore this, we analysed user\ninteractions with ChatGPT outside of its marketed use as a task-oriented\nassistant. In dialogues classified as lonely, users frequently (37%) sought\nadvice or validation, and received good engagement. However, ChatGPT failed in\nsensitive scenarios, like responding appropriately to suicidal ideation or\ntrauma. We also observed a 35% higher incidence of toxic content, with women\nbeing 22x more likely to be targeted than men. Our findings underscore ethical\nand legal questions about this technology, and note risks like radicalisation\nor further isolation. We conclude with recommendations to research and industry\nto address loneliness.", "AI": {"tldr": "This paper examines the use of large language models (LLMs) like ChatGPT in addressing loneliness, revealing risks and ethical concerns.", "motivation": "Loneliness negatively affects mental and physical health, and LLMs might help alleviate it; however, their current use poses risks.", "method": "Analyzed user interactions with ChatGPT, focusing on dialogues indicating loneliness and classified responses to sensitive scenarios.", "result": "Users in lonely scenarios sought advice or validation, however, ChatGPT struggled with sensitive topics, leading to a higher incidence of toxic responses.", "conclusion": "The study highlights significant ethical and legal challenges with LLMs in addressing loneliness, urging research and industry representatives to take action.", "key_contributions": ["Analyzed the unintended use of ChatGPT in the context of loneliness.", "Identified significant failure in managing sensitive topics like suicidal ideation.", "Documented a higher incidence of toxic interactions, especially against women."], "limitations": "The paper does not explore potential solutions in depth; focus is mainly on problems.", "keywords": ["loneliness", "large language models", "ethics", "ChatGPT", "user interactions"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.02595", "pdf": "https://arxiv.org/pdf/2412.02595.pdf", "abs": "https://arxiv.org/abs/2412.02595", "title": "Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset", "authors": ["Dan Su", "Kezhi Kong", "Ying Lin", "Joseph Jennings", "Brandon Norick", "Markus Kliegl", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved\nsignificant benchmark gains via aggressive model-based filtering, but at the\ncost of removing 90% of data. This limits their suitability for long token\nhorizon training, such as 15T tokens for Llama 3.1. In this paper, we show how\nto achieve better trade-offs between accuracy and data quantity by a\ncombination of classifier ensembling, synthetic data rephrasing, and reduced\nreliance on heuristic filters. When training 8B parameter models for 1T tokens,\nusing a high-quality subset of our data improves MMLU by 5.6 over DCLM,\ndemonstrating the efficacy of our methods for boosting accuracies over a\nrelatively short token horizon. Furthermore, our full 6.3T token dataset\nmatches DCLM on MMLU, but contains four times more unique real tokens than\nDCLM. This unlocks state-of-the-art training over a long token horizon: an 8B\nparameter model trained for 15T tokens, of which 7.2T came from our dataset, is\nbetter than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5\non average across ten diverse tasks. The dataset is available at\nhttps://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html", "AI": {"tldr": "The paper presents methods to improve model training by balancing accuracy and data quantity through classifier ensembling and synthetic data rephrasing, resulting in significant improvements on various benchmarks.", "motivation": "To find better trade-offs between accuracy and data quantity in training large language models, addressing the limitations of current datasets like FineWeb-Edu and DCLM.", "method": "The authors utilize classifier ensembling, synthetic data rephrasing, and reduce reliance on heuristic filters to curate a more effective dataset for model training.", "result": "Using their methods, the paper reports a 5.6 point improvement over DCLM on MMLU with a high-quality subset for 1T tokens, and superior performance on diverse benchmarks with a dataset consisting of 7.2T tokens.", "conclusion": "The new dataset enables state-of-the-art training capabilities, outperforming existing models like Llama 3.1 and demonstrating the potential of improved data handling techniques.", "key_contributions": ["Introduction of a novel dataset with significantly more unique tokens than competitors.", "Demonstration of improved model accuracy using better data selection strategies.", "Unlocking advanced training possibilities for long token horizons."], "limitations": "", "keywords": ["data quality", "model training", "language models", "machine learning", "synthetic data"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.02819", "pdf": "https://arxiv.org/pdf/2412.02819.pdf", "abs": "https://arxiv.org/abs/2412.02819", "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models in Chinese Novels", "authors": ["Lingxiao Wei", "He Yan", "Xiangju Lu", "Junmin Zhu", "Jun Wang", "Wei Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Large language models (LLMs) have been well-researched in various\nlong-context tasks. However, the scarcity of long-context summarization\ndatasets hinders progress in this area. To address this, we introduce CNNSum, a\nmulti-scale long-context summarization benchmark based on Chinese novels,\nfeaturing human-driven annotations across four subsets totaling 695 samples,\nwith lengths ranging from 16k to 128k. We benchmark numerous LLMs and conduct\ndetailed human assessments to summarize abnormal output types. Furthermore, we\nextensively explore how to improve long-context summarization. In our study:\n(1) Advanced LLMs may generate much subjective commentary, leading to vague\nsummaries. (2) Currently, long-context summarization mainly relies on memory\nability. The advantages of Large LLMs are hard to utilize, thus small LLMs are\nmore cost-effective. (3) Different prompt types paired with various version\nmodels may cause large performance gaps. In further fine-tuning, these can be\nmitigated, and the Base version models perform better. (4) LLMs with RoPE-base\nscaled exhibit strong extrapolation potential; using short-context data can\nsignificantly improve long-context summarization performance. However, further\napplying other interpolation methods requires careful selection. (5) CNNSum\nprovides more reliable evaluation results than other benchmarks. We release\nCNNSum to advance future research.(https://github.com/CxsGhost/CNNSum)", "AI": {"tldr": "CNNSum is a long-context summarization benchmark based on Chinese novels, consisting of 695 samples, aimed at enhancing long-context summarization in LLMs.", "motivation": "The paper addresses the challenge of limited long-context summarization datasets, which impede progress in the use of large language models for such tasks.", "method": "The study introduces CNNSum, a benchmark with human-driven annotations and evaluates numerous LLMs while exploring improvements in long-context summarization.", "result": "Advanced LLMs produce vague summaries due to subjective commentary, and smaller LLMs can be more cost-effective. Careful selection of interpolation methods can significantly enhance performance.", "conclusion": "The release of CNNSum is intended to provide a robust framework for continuing research in long-context summarization using LLMs.", "key_contributions": ["Introduces CNNSum, a benchmark for long-context summarization.", "Findings indicate that small LLMs may outperform larger models in specific contexts.", "Provides insights into the connection between prompt types and model performance in summarization tasks."], "limitations": "", "keywords": ["long-context summarization", "large language models", "benchmark", "Chinese novels", "summarization performance"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2412.02830", "pdf": "https://arxiv.org/pdf/2412.02830.pdf", "abs": "https://arxiv.org/abs/2412.02830", "title": "RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models", "authors": ["Hieu Tran", "Zonghai Yao", "Junda Wang", "Yifan Zhang", "Zhichao Yang", "Hong Yu"], "categories": ["cs.CL"], "comment": "Proceedings of ACL 2025 (main track)", "summary": "This work introduces RARE (Retrieval-Augmented Reasoning Enhancement), a\nversatile extension to the mutual reasoning framework (rStar), aimed at\nenhancing reasoning accuracy and factual integrity across large language models\n(LLMs) for complex, knowledge-intensive tasks such as commonsense and medical\nreasoning. RARE incorporates two innovative actions within the Monte Carlo Tree\nSearch (MCTS) framework: A6, which generates search queries based on the\ninitial problem statement, performs information retrieval using those queries,\nand augments reasoning with the retrieved data to formulate the final answer;\nand A7, which leverages information retrieval specifically for generated\nsub-questions and re-answers these sub-questions with the relevant contextual\ninformation. Additionally, a Retrieval-Augmented Factuality Scorer is proposed\nto replace the original discriminator, prioritizing reasoning paths that meet\nhigh standards of factuality. Experimental results with LLaMA 3.1 show that\nRARE enables open-source LLMs to achieve competitive performance with top\nopen-source models like GPT-4 and GPT-4o. This research establishes RARE as a\nscalable solution for improving LLMs in domains where logical coherence and\nfactual integrity are critical.", "AI": {"tldr": "RARE enhances large language models' reasoning accuracy by integrating information retrieval within reasoning frameworks for complex medical and commonsense tasks.", "motivation": "The paper addresses the challenge of improving reasoning accuracy and factual integrity in large language models, especially for knowledge-intensive tasks.", "method": "RARE introduces two key actions within a Monte Carlo Tree Search framework: A6 for generating search queries and augmenting reasoning with retrieved data, and A7 for re-answering generated sub-questions with contextual information.", "result": "Experimental results show that RARE significantly improves the performance of LLaMA 3.1, enabling it to compete with top models like GPT-4.", "conclusion": "RARE is established as a scalable solution for enhancing reasoning quality in LLMs, particularly in areas requiring logical coherence and factual integrity.", "key_contributions": ["Introduction of RARE framework for improving reasoning in LLMs.", "Novel actions A6 and A7 for integrating information retrieval into reasoning tasks.", "Development of a Retrieval-Augmented Factuality Scorer to enhance factual accuracy."], "limitations": "", "keywords": ["retrieval-augmented reasoning", "large language models", "factual integrity", "Monte Carlo Tree Search", "commonsense reasoning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.05710", "pdf": "https://arxiv.org/pdf/2412.05710.pdf", "abs": "https://arxiv.org/abs/2412.05710", "title": "PromptRefine: Enhancing Few-Shot Performance on Low-Resource Indic Languages with Example Selection from Related Example Banks", "authors": ["Soumya Suvra Ghosal", "Soumyabrata Pal", "Koyel Mukherjee", "Dinesh Manocha"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted at NAACL 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive few-shot\nlearning capabilities through in-context learning (ICL). However, ICL\nperformance is highly dependent on the choice of few-shot demonstrations,\nmaking the selection of the most optimal examples a persistent research\nchallenge. This issue is further amplified in low-resource Indic languages,\nwhere the scarcity of ground-truth data complicates the selection process. In\nthis work, we propose PromptRefine, a novel Alternating Minimization approach\nfor example selection that improves ICL performance on low-resource Indic\nlanguages. PromptRefine leverages auxiliary example banks from related\nhigh-resource Indic languages and employs multi-task learning techniques to\nalign language-specific retrievers, enabling effective cross-language\nretrieval. Additionally, we incorporate diversity in the selected examples to\nenhance generalization and reduce bias. Through comprehensive evaluations on\nfour text generation tasks -- Cross-Lingual Question Answering, Multilingual\nQuestion Answering, Machine Translation, and Cross-Lingual Summarization using\nstate-of-the-art LLMs such as LLAMA-3.1-8B, LLAMA-2-7B, Qwen-2-7B, and\nQwen-2.5-7B, we demonstrate that PromptRefine significantly outperforms\nexisting frameworks for retrieving examples.", "AI": {"tldr": "This work introduces PromptRefine, a novel approach for selecting examples that enhances in-context learning performance in low-resource Indic languages by employing multi-task learning and cross-language retrieval strategies.", "motivation": "The performance of in-context learning in large language models is highly sensitive to the selection of examples, particularly challenging in low-resource Indic languages due to limited ground-truth data.", "method": "PromptRefine utilizes an Alternating Minimization approach combined with multi-task learning to align language-specific retrievers from high-resource Indic languages, helping in effective cross-language retrieval and promoting diversity among selected examples.", "result": "PromptRefine significantly outperformed existing frameworks in retrieving examples across four text generation tasks, including Cross-Lingual Question Answering and Machine Translation, when evaluated with state-of-the-art large language models.", "conclusion": "The proposed method effectively addresses the example selection challenge in low-resource Indic languages, improving ICL performance and facilitating better generalization and reduced bias in language model applications.", "key_contributions": ["Introduction of PromptRefine for example selection", "Use of auxiliary example banks from high-resource languages", "Enhancement of example diversity to improve generalization"], "limitations": "", "keywords": ["Large Language Models", "In-Context Learning", "Low-resource Languages", "Example Selection", "Multi-task Learning"], "importance_score": 8, "read_time_minutes": 8}}
{"id": "2412.08985", "pdf": "https://arxiv.org/pdf/2412.08985.pdf", "abs": "https://arxiv.org/abs/2412.08985", "title": "KnowShiftQA: How Robust are RAG Systems when Textbook Knowledge Shifts in K-12 Education?", "authors": ["Tianshi Zheng", "Weihan Li", "Jiaxin Bai", "Weiqi Wang", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Retrieval-Augmented Generation (RAG) systems show remarkable potential as\nquestion answering tools in the K-12 Education domain, where knowledge is\ntypically queried within the restricted scope of authoritative textbooks.\nHowever, discrepancies between these textbooks and the parametric knowledge\ninherent in Large Language Models (LLMs) can undermine the effectiveness of RAG\nsystems. To systematically investigate RAG system robustness against such\nknowledge discrepancies, we introduce KnowShiftQA. This novel question\nanswering dataset simulates these discrepancies by applying deliberate\nhypothetical knowledge updates to both answers and source documents, reflecting\nhow textbook knowledge can shift. KnowShiftQA comprises 3,005 questions across\nfive subjects, designed with a comprehensive question typology focusing on\ncontext utilization and knowledge integration. Our extensive experiments on\nretrieval and question answering performance reveal that most RAG systems\nsuffer a substantial performance drop when faced with these knowledge\ndiscrepancies. Furthermore, questions requiring the integration of contextual\n(textbook) knowledge with parametric (LLM) knowledge pose a significant\nchallenge to current LLMs.", "AI": {"tldr": "Introduction of KnowShiftQA, a dataset for assessing RAG systems in K-12 Education concerning knowledge discrepancies between textbooks and LLMs.", "motivation": "To address the challenges RAG systems face due to discrepancies between authoritative textbook knowledge and the parametric knowledge of LLMs.", "method": "The dataset KnowShiftQA simulates knowledge discrepancies through hypothetical updates to answers and source documents, featuring 3,005 questions across five subjects with a focus on context usage and knowledge integration.", "result": "Most RAG systems demonstrate a significant performance drop when confronted with knowledge discrepancies; integration of contextual and parametric knowledge is notably challenging for LLMs.", "conclusion": "The findings highlight the vulnerabilities of current RAG systems in the education domain and suggest areas for improvement in integrating different knowledge types.", "key_contributions": ["Introduction of the KnowShiftQA dataset for RAG systems", "Comprehensive examination of RAG system performance under knowledge discrepancies", "Identification of challenges in integrating textbook and LLM knowledge"], "limitations": "The dataset is focused on K-12 education and may not generalize to other domains; performance metrics are limited to current RAG systems.", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "K-12 Education", "Knowledge Discrepancies", "Question Answering"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.10424", "pdf": "https://arxiv.org/pdf/2412.10424.pdf", "abs": "https://arxiv.org/abs/2412.10424", "title": "LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation", "authors": ["Eunsu Kim", "Juyoung Suk", "Seungone Kim", "Niklas Muennighoff", "Dongkwan Kim", "Alice Oh"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce LLM-as-an-Interviewer, a novel paradigm for evaluating large\nlanguage models (LLMs). This approach leverages multi-turn interactions where\nthe LLM interviewer actively provides feedback on responses and poses follow-up\nquestions to the evaluated LLM. At the start of the interview, the LLM\ninterviewer dynamically modifies datasets to generate initial questions,\nmitigating data contamination. We apply the LLM-as-an-Interviewer framework to\nevaluate six models on the MATH and DepthQA tasks. Our results show that the\nframework effectively provides insights into LLM performance, including the\nquality of initial responses, adaptability to feedback, and ability to address\nfollow-up queries like clarification or additional knowledge requests. The\nframework also addresses key limitations of conventional methods like\nLLM-as-a-Judge, including verbosity bias and inconsistency across runs.\nFinally, we propose the Interview Report, which aggregates insights from the\ninterview process, providing examples and a comprehensive analysis of the LLM's\nstrengths and weaknesses. This report offers a detailed snapshot of the model's\nreal-world applicability. The code for our framework is publicly available at\nhttps://github.com/interview-eval/.", "AI": {"tldr": "Introduction of a novel evaluation framework for large language models (LLMs) using an LLM as an interviewer to provide feedback and ask follow-up questions.", "motivation": "To evaluate LLM performance more effectively by leveraging interactive questioning and feedback rather than conventional methods.", "method": "The LLM interviewer modifies datasets to create initial questions, assesses model responses through multi-turn interactions, and generates an Interview Report summarizing performance insights.", "result": "The LLM-as-an-Interviewer framework provided better insights into LLM capabilities, adaptability to feedback, and handling of follow-up queries compared to traditional evaluation methods.", "conclusion": "The framework addresses issues like verbosity bias and inconsistencies in model evaluation, offering a detailed analysis of LLM strengths and weaknesses.", "key_contributions": ["Proposes a new evaluation paradigm for LLMs using an interviewer model.", "Develops an Interview Report that summarizes LLM performance and insights from the evaluation process.", "Addresses limitations of traditional evaluation methods like LLM-as-a-Judge."], "limitations": "The effectiveness of the framework may vary with the dataset and LLM complexity, along with reliance on the interviewer model's capabilities.", "keywords": ["LLM evaluation", "Human-Computer Interaction", "Feedback mechanisms", "Model performance analysis", "Interactive questioning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.11388", "pdf": "https://arxiv.org/pdf/2412.11388.pdf", "abs": "https://arxiv.org/abs/2412.11388", "title": "INTERACT: Enabling Interactive, Question-Driven Learning in Large Language Models", "authors": ["Aum Kendapadi", "Kerem Zaman", "Rakesh R. Menon", "Shashank Srivastava"], "categories": ["cs.CL"], "comment": "31 pages, 8 figures, 15 tables, 10 listings", "summary": "Large language models (LLMs) excel at answering questions but remain passive\nlearners-absorbing static data without the ability to question and refine\nknowledge. This paper explores how LLMs can transition to interactive,\nquestion-driven learning through student-teacher dialogues. We introduce\nINTERACT (INTERactive learning for Adaptive Concept Transfer), a framework in\nwhich a \"student\" LLM engages a \"teacher\" LLM through iterative inquiries to\nacquire knowledge across 1,347 contexts, including song lyrics, news articles,\nmovie plots, academic papers, and images. Our experiments show that across a\nwide range of scenarios and LLM architectures, interactive learning\nconsistently enhances performance, achieving up to a 25% improvement, with\n'cold-start' student models matching static learning baselines in as few as\nfive dialogue turns. Interactive setups can also mitigate the disadvantages of\nweaker teachers, showcasing the robustness of question-driven learning.", "AI": {"tldr": "The paper presents INTERACT, a framework enabling interactive, question-driven learning between student and teacher LLMs, showing performance improvements in knowledge acquisition across diverse contexts.", "motivation": "Despite the strengths of large language models in answering questions, they lack the ability to actively question and refine their knowledge. This paper aims to address that limitation.", "method": "The authors introduce INTERACT, a framework where a 'student' LLM engages with a 'teacher' LLM through iterative questioning to acquire knowledge over 1,347 different contexts.", "result": "Experiments demonstrate that interactive learning can provide up to a 25% performance improvement, with cold-start student models matching static baselines in as few as five dialogue turns.", "conclusion": "The study shows that interactive learning setups are robust and can effectively compensate for weaker teachers, highlighting the advantages of a question-driven approach in LLM learning.", "key_contributions": ["Introduction of INTERACT framework for question-driven learning between LLMs.", "Empirical results showing significant performance improvement of interactive learning.", "Demonstrating robustness of learning frameworks even with weaker teacher models."], "limitations": "The performance of the approach may vary based on the quality and structure of the teacher model used.", "keywords": ["Interactive Learning", "Large Language Models", "Question-Driven Learning", "Knowledge Acquisition", "Concept Transfer"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.11940", "pdf": "https://arxiv.org/pdf/2412.11940.pdf", "abs": "https://arxiv.org/abs/2412.11940", "title": "The Impact of Token Granularity on the Predictive Power of Language Model Surprisal", "authors": ["Byung-Doh Oh", "William Schuler"], "categories": ["cs.CL"], "comment": "ACL 2025; results with Natural Stories alignment issue corrected\n  (commit 4700daa)", "summary": "Word-by-word language model surprisal is often used to model the incremental\nprocessing of human readers, which raises questions about how various choices\nin language modeling influence its predictive power. One factor that has been\noverlooked in cognitive modeling is the granularity of subword tokens, which\nexplicitly encodes information about word length and frequency, and ultimately\ninfluences the quality of vector representations that are learned. This paper\npresents experiments that manipulate the token granularity and evaluate its\nimpact on the ability of surprisal to account for processing difficulty of\nnaturalistic text and garden-path constructions. Experiments with naturalistic\nreading times reveal a substantial influence of token granularity on surprisal,\nwith tokens defined by a vocabulary size of 8,000 resulting in surprisal that\nis most predictive. In contrast, on garden-path constructions, language models\ntrained on coarser-grained tokens generally assigned higher surprisal to\ncritical regions, suggesting a greater sensitivity to garden-path effects than\npreviously reported. Taken together, these results suggest a large role of\ntoken granularity on the quality of language model surprisal for cognitive\nmodeling.", "AI": {"tldr": "The paper investigates the impact of subword token granularity on language model surprisal and its predictive power for cognitive modeling.", "motivation": "To explore how different choices in language modeling, specifically token granularity, influence the predictive power of language model surprisal in understanding human reading processing.", "method": "Experiments manipulating token granularity and evaluating its impact on the predictive power of surprisal through reading times in naturalistic text.", "result": "The experiments revealed that a vocabulary size of 8,000 tokens produced the most predictive surprisal, while coarser tokens resulted in higher surprisal for garden-path constructions, indicating an enhanced sensitivity to reading difficulties.", "conclusion": "Token granularity plays a significant role in the quality of language model surprisal, which is crucial for cognitive modeling of human reading processes.", "key_contributions": ["Demonstrated the influence of subword token granularity on language model performance.", "Showed that finer granularity improves the predictive power of surprisal for reading times.", "Provided evidence that coarser tokens heightened surprisal in garden-path situations."], "limitations": "", "keywords": ["language model", "surprisal", "token granularity", "cognitive modeling", "naturalistic text"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.11965", "pdf": "https://arxiv.org/pdf/2412.11965.pdf", "abs": "https://arxiv.org/abs/2412.11965", "title": "Inferring Functionality of Attention Heads from their Parameters", "authors": ["Amit Elhelo", "Mor Geva"], "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Attention heads are one of the building blocks of large language models\n(LLMs). Prior work on investigating their operation mostly focused on analyzing\ntheir behavior during inference for specific circuits or tasks. In this work,\nwe seek a comprehensive mapping of the operations they implement in a model. We\npropose MAPS (Mapping Attention head ParameterS), an efficient framework that\ninfers the functionality of attention heads from their parameters, without any\nmodel training or inference. We showcase the utility of MAPS for answering two\ntypes of questions: (a) given a predefined operation, mapping how strongly\nheads across the model implement it, and (b) given an attention head, inferring\nits salient functionality. Evaluating MAPS on 20 operations across 6 popular\nLLMs shows its estimations correlate with the head's outputs during inference\nand are causally linked to the model's predictions. Moreover, its mappings\nreveal attention heads of certain operations that were overlooked in previous\nstudies, and valuable insights on function universality and architecture biases\nin LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS\nto characterize the salient operations of a given head. Our pipeline produces\nplausible operation descriptions for most heads, as assessed by human judgment,\nwhile revealing diverse operations.", "AI": {"tldr": "The paper presents MAPS, a framework for mapping the functionality of attention heads in large language models without training or inference, yielding insights into their operations.", "motivation": "To provide a comprehensive mapping of operations implemented by attention heads in large language models, a topic that has not been fully explored in prior work.", "method": "The authors propose MAPS, which infers the functionality of attention heads from their parameters, enabling the analysis of head operations without model training or inference.", "result": "Evaluation of MAPS on 20 operations across 6 LLMs shows strong correlation with head outputs during inference, revealing overlooked attention head operations, and insights into function universality and architecture biases.", "conclusion": "MAPS offers valuable insights into attention head functionalities and supports an automatic pipeline for characterizing operations, producing plausible descriptions for most heads.", "key_contributions": ["Development of MAPS framework for mapping attention heads' functionality", "Correlational analysis demonstrating MAPS efficacy with LLMs", "Revelation of overlooked operations and insights on model architecture biases"], "limitations": "", "keywords": ["attention heads", "large language models", "MAPS", "machine learning", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.12276", "pdf": "https://arxiv.org/pdf/2412.12276.pdf", "abs": "https://arxiv.org/abs/2412.12276", "title": "Emergence and Effectiveness of Task Vectors in In-Context Learning: An Encoder Decoder Perspective", "authors": ["Seungwook Han", "Jinyeop Song", "Jeff Gore", "Pulkit Agrawal"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "https://charming-centaur-089.notion.site/Emergence-and-Effectiveness-of-Task-Vectors-in-In-Context-Learning-An-Encoder-Decoder-Perspective-2054664a1d59814f8401cded3332fce4", "summary": "Autoregressive transformers exhibit adaptive learning through in-context\nlearning (ICL), which begs the question of how. Prior works have shown that\ntransformers represent the ICL tasks as vectors in their representations. In\nthis paper, we leverage the encoding-decoding framework to study how\ntransformers form task vectors during pretraining and how their task encoding\nquality predicts ICL task performance. On synthetic ICL tasks, we analyze the\ntraining dynamics of a small transformer and report the coupled emergence of\ntask encoding and decoding. As the model learns to encode different latent\ntasks (e.g., \"Finding the first noun in a sentence.\") into distinct, separable\nrepresentations, it concurrently builds conditional decoding algorithms and\nimproves its ICL performance. We validate this phenomenon across pretrained\nmodels of varying scales (Gemma-2 2B/9B/27B, Llama-3.1 8B/70B) and over the\ncourse of pretraining in OLMo-7B. Further, we demonstrate that the quality of\ntask encoding inferred from representations predicts ICL performance, and that,\nsurprisingly, finetuning the earlier layers can improve the task encoding and\nperformance more than finetuning the latter layers. Our empirical insights shed\nlight into better understanding the success and failure modes of large language\nmodels via their representations.", "AI": {"tldr": "The paper studies how autoregressive transformers form task vectors during pretraining and how their encoding quality influences in-context learning (ICL) performance. It analyzes training dynamics and validates findings across various pretrained models.", "motivation": "Understanding how autoregressive transformers adaptively learn through in-context learning (ICL) is crucial for improving their task performance.", "method": "The study utilizes an encoding-decoding framework to examine the formation of task vectors in transformers during pretraining. It analyzes a small transformer on synthetic ICL tasks and investigates the emergence of task encoding and decoding.", "result": "The research finds that as the model learns to encode diverse tasks into separable representations, it concurrently enhances conditional decoding algorithms, ultimately improving ICL performance.", "conclusion": "The quality of task encoding significantly predicts ICL performance. Interestingly, finetuning earlier layers can yield better task encoding enhancements than finetuning later layers. Insights from this study improve understanding of large language models' success and failure modes.", "key_contributions": ["Introduces the use of an encoding-decoding framework for studying task vector formation.", "Demonstrates the concurrent improvement of task encoding and conditional decoding algorithms during pretraining.", "Shows that finetuning earlier layers can enhance task encoding and ICL performance more effectively than later layers."], "limitations": "Focused on synthetic tasks and may need further exploration in more complex real-world scenarios.", "keywords": ["autoregressive transformers", "in-context learning", "task encoding", "pretraining", "large language models"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2412.12569", "pdf": "https://arxiv.org/pdf/2412.12569.pdf", "abs": "https://arxiv.org/abs/2412.12569", "title": "Quantifying Lexical Semantic Shift via Unbalanced Optimal Transport", "authors": ["Ryo Kishino", "Hiroaki Yamagiwa", "Ryo Nagata", "Sho Yokoi", "Hidetoshi Shimodaira"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Lexical semantic change detection aims to identify shifts in word meanings\nover time. While existing methods using embeddings from a diachronic corpus\npair estimate the degree of change for target words, they offer limited insight\ninto changes at the level of individual usage instances. To address this, we\napply Unbalanced Optimal Transport (UOT) to sets of contextualized word\nembeddings, capturing semantic change through the excess and deficit in the\nalignment between usage instances. In particular, we propose Sense Usage Shift\n(SUS), a measure that quantifies changes in the usage frequency of a word sense\nat each usage instance. By leveraging SUS, we demonstrate that several\nchallenges in semantic change detection can be addressed in a unified manner,\nincluding quantifying instance-level semantic change and word-level tasks such\nas measuring the magnitude of semantic change and the broadening or narrowing\nof meaning.", "AI": {"tldr": "This paper introduces a novel method for detecting lexical semantic changes using Unbalanced Optimal Transport applied to contextualized word embeddings.", "motivation": "Existing methods for lexical semantic change detection provide limited insights at the individual usage instance level; this paper aims to enhance understanding of semantic shifts over time.", "method": "The authors utilize Unbalanced Optimal Transport (UOT) on sets of contextualized word embeddings to capture and measure semantic changes through the concept of Sense Usage Shift (SUS), which quantifies frequency changes in word senses.", "result": "The paper demonstrates that the SUS measure can effectively address challenges in semantic change detection, including quantifying instance-level changes and broader word-level tasks.", "conclusion": "Applying SUS provides a unified approach to understanding and measuring both individual instance changes and overall trends in semantic change.", "key_contributions": ["Introduction of Sense Usage Shift (SUS) for quantifying semantic changes at usage instances", "Application of Unbalanced Optimal Transport (UOT) to contextualized embeddings for enhanced semantic change analysis", "Unified approach to tackle multiple challenges in semantic change detection"], "limitations": "", "keywords": ["semantic change detection", "contextualized embeddings", "Unbalanced Optimal Transport"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2412.13169", "pdf": "https://arxiv.org/pdf/2412.13169.pdf", "abs": "https://arxiv.org/abs/2412.13169", "title": "Algorithmic Fidelity of Large Language Models in Generating Synthetic German Public Opinions: A Case Study", "authors": ["Bolei Ma", "Berk Yoztyurk", "Anna-Carolina Haensch", "Xinpeng Wang", "Markus Herklotz", "Frauke Kreuter", "Barbara Plank", "Matthias Assenmacher"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "In recent research, large language models (LLMs) have been increasingly used\nto investigate public opinions. This study investigates the algorithmic\nfidelity of LLMs, i.e., the ability to replicate the socio-cultural context and\nnuanced opinions of human participants. Using open-ended survey data from the\nGerman Longitudinal Election Studies (GLES), we prompt different LLMs to\ngenerate synthetic public opinions reflective of German subpopulations by\nincorporating demographic features into the persona prompts. Our results show\nthat Llama performs better than other LLMs at representing subpopulations,\nparticularly when there is lower opinion diversity within those groups. Our\nfindings further reveal that the LLM performs better for supporters of\nleft-leaning parties like The Greens and The Left compared to other parties,\nand matches the least with the right-party AfD. Additionally, the inclusion or\nexclusion of specific variables in the prompts can significantly impact the\nmodels' predictions. These findings underscore the importance of aligning LLMs\nto more effectively model diverse public opinions while minimizing political\nbiases and enhancing robustness in representativeness.", "AI": {"tldr": "This study assesses the algorithmic fidelity of large language models (LLMs) in replicating public opinions, focusing specifically on their performance across different socio-cultural subpopulations in Germany.", "motivation": "The paper aims to investigate how well LLMs can reflect nuanced public opinions and social contexts, especially in political settings.", "method": "The authors use open-ended survey data from the German Longitudinal Election Studies (GLES) and prompt various LLMs to generate synthetic public opinions by incorporating demographic features into personas.", "result": "Llama outperforms other LLMs in accurately representing German subpopulations, particularly those with lower opinion diversity, and demonstrates varying accuracy based on political party support.", "conclusion": "The research highlights the need for better alignment of LLMs to capture diverse public opinions while reducing political biases and improving representativeness.", "key_contributions": ["Demonstrates LLM performance across different socio-political subpopulations.", "Analyzes impact of demographic variables on LLM output accuracy.", "Investigates political biases in LLM-based public opinion generation."], "limitations": "The study is constrained by the specifics of the German political landscape and may not generalize to other cultures or electoral systems.", "keywords": ["Large Language Models", "Public Opinion", "Algorithmic Fidelity", "Political Bias", "Socio-Cultural Context"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.13378", "pdf": "https://arxiv.org/pdf/2412.13378.pdf", "abs": "https://arxiv.org/abs/2412.13378", "title": "SummExecEdit: A Factual Consistency Benchmark in Summarization with Executable Edits", "authors": ["Onkar Thorat", "Philippe Laban", "Chien-Sheng Wu"], "categories": ["cs.CL"], "comment": null, "summary": "Detecting factual inconsistencies in summarization is critical, yet existing\nbenchmarks lack the necessary challenge and interpretability for robust\nevaluation. In this paper, we introduce SummExecEdit, a novel pipeline and\nbenchmark leveraging executable edits to assess models on their ability to both\ndetect factual errors and provide accurate explanations. The top-performing\nmodel, Claude3-Opus, achieves a joint detection and explanation score of only\n0.49 in our benchmark, with individual scores of 0.67 for detection and 0.73\nfor explanation. We conduct detailed evaluations to assess the current state of\nmodels in this field and find that more than half of the 20+ LLMs in our study\nstruggle with over 30% of the SummExecEdit benchmark. Additionally, we identify\nfour primary types of explanation errors, with 45.4% of them involving a focus\non completely unrelated parts of the summary.", "AI": {"tldr": "The paper introduces SummExecEdit, a benchmark for evaluating models on their ability to detect factual errors in summarization and offer accurate explanations, revealing significant performance limitations in current LLMs.", "motivation": "Existing benchmarks for detecting factual inconsistencies in summarization lack sufficient challenge and interpretability, necessitating a new evaluation framework.", "method": "A novel pipeline called SummExecEdit is developed, utilizing executable edits to evaluate models for their detection accuracy and the quality of their explanations.", "result": "Claude3-Opus, the top-performing model, achieves a detection score of 0.67 and an explanation score of 0.73, with a combined score of 0.49, highlighting deficiencies in over half of the tested LLMs.", "conclusion": "The findings stress the need for improved models in detecting factual inconsistencies and generating relevant explanations in summarization tasks.", "key_contributions": ["Introduction of SummExecEdit benchmark framework", "Assessment of factual error detection and explanation capabilities of LLMs", "Identification of common explanation errors in summarization"], "limitations": "More than half of the evaluated LLMs struggle with over 30% of the benchmark, indicating significant gaps in current capabilities.", "keywords": ["factual inconsistencies", "summarization", "LLMs", "executable edits", "benchmark"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2412.13602", "pdf": "https://arxiv.org/pdf/2412.13602.pdf", "abs": "https://arxiv.org/abs/2412.13602", "title": "GAMEBoT: Transparent Assessment of LLM Reasoning in Games", "authors": ["Wenye Lin", "Jonathan Roberts", "Yunhan Yang", "Samuel Albanie", "Zongqing Lu", "Kai Han"], "categories": ["cs.CL"], "comment": "9 pages, ACL 2025", "summary": "Large Language Models (LLMs) are increasingly deployed in real-world\napplications that demand complex reasoning. To track progress, robust\nbenchmarks are required to evaluate their capabilities beyond superficial\npattern recognition. However, current LLM reasoning benchmarks often face\nchallenges such as insufficient interpretability, performance saturation or\ndata contamination. To address these challenges, we introduce GAMEBoT, a gaming\narena designed for rigorous and transparent assessment of LLM reasoning\ncapabilities. GAMEBoT decomposes complex reasoning in games into predefined\nmodular subproblems. This decomposition allows us to design a suite of\nChain-of-Thought (CoT) prompts that leverage domain knowledge to guide LLMs in\naddressing these subproblems before action selection. Furthermore, we develop a\nsuite of rule-based algorithms to generate ground truth for these subproblems,\nenabling rigorous validation of the LLMs' intermediate reasoning steps. This\napproach facilitates evaluation of both the quality of final actions and the\naccuracy of the underlying reasoning process. GAMEBoT also naturally alleviates\nthe risk of data contamination through dynamic games and head-to-head LLM\ncompetitions. We benchmark 17 prominent LLMs across eight games, encompassing\nvarious strategic abilities and game characteristics. Our results suggest that\nGAMEBoT presents a significant challenge, even when LLMs are provided with\ndetailed CoT prompts. Project page: https://visual-ai.github.io/gamebot", "AI": {"tldr": "GAMEBoT is a gaming arena designed for transparent evaluation of Large Language Models' reasoning capabilities through modular subproblems and Chain-of-Thought prompts.", "motivation": "To create robust benchmarks for evaluating LLM reasoning capabilities beyond simple pattern recognition, addressing issues like interpretability and performance saturation.", "method": "GAMEBoT decomposes complex reasoning in games into modular subproblems, using Chain-of-Thought (CoT) prompts and rule-based algorithms to establish ground truth for rigorous validation.", "result": "Benchmarking 17 LLMs across eight games reveals that GAMEBoT presents a significant challenge, showing the limits of current LLM reasoning capabilities even with detailed prompts.", "conclusion": "GAMEBoT offers a substantial framework for evaluating LLM reasoning by providing a transparent assessment tool that mitigates data contamination risks.", "key_contributions": ["Introduction of GAMEBoT for LLM assessment", "Modular decomposition of complex reasoning tasks", "Dynamic game competitions to prevent data contamination"], "limitations": "", "keywords": ["Large Language Models", "reasoning benchmarks", "Chain-of-Thought prompts", "game-based evaluation", "data contamination"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.13649", "pdf": "https://arxiv.org/pdf/2412.13649.pdf", "abs": "https://arxiv.org/abs/2412.13649", "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation", "authors": ["Jialong Wu", "Zhenglin Wang", "Linhai Zhang", "Yilong Lai", "Yulan He", "Deyu Zhou"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.", "AI": {"tldr": "SCOPE optimizes KV cache for long-context generation in LLMs, enhancing performance during both prefill and decoding phases.", "motivation": "Existing KV cache optimizations overlook the decoding phase, leading to performance issues in long-output generation tasks.", "method": "SCOPE optimizes the KV cache during prefill and decoding phases separately, preserving essential information and selecting heavy hitters using a sliding strategy.", "result": "Extensive experiments demonstrate that SCOPE improves memory usage and performance on LongGenBench, enhancing its applicability to other compression methods.", "conclusion": "SCOPE effectively addresses KV cache limitations in LLMs, providing a systematic approach to optimize decoding without losing critical information.", "key_contributions": ["Introduced the SCOPE framework for KV cache optimization", "Maintained essential information during prefill phase", "Proposed a sliding strategy to select heavy hitters during decoding phase"], "limitations": "", "keywords": ["KV cache", "LLMs", "long-output generation", "prefill phase", "decoding phase"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.14050", "pdf": "https://arxiv.org/pdf/2412.14050.pdf", "abs": "https://arxiv.org/abs/2412.14050", "title": "Cross-Lingual Transfer of Debiasing and Detoxification in Multilingual LLMs: An Extensive Investigation", "authors": ["Vera Neplenbroek", "Arianna Bisazza", "Raquel Fernández"], "categories": ["cs.CL"], "comment": "Accepted to the Findings of ACL 2025", "summary": "Recent generative large language models (LLMs) show remarkable performance in\nnon-English languages, but when prompted in those languages they tend to\nexpress higher harmful social biases and toxicity levels. Prior work has shown\nthat finetuning on specialized datasets can mitigate this behavior, and doing\nso in English can transfer to other languages. In this work, we investigate the\nimpact of different finetuning methods on the model's bias and toxicity, but\nalso on its ability to produce fluent and diverse text. We reduce biases by\nfinetuning on curated non-harmful text, but find only direct preference\noptimization to be effective for mitigating toxicity. The mitigation caused by\napplying these methods in English also transfers to non-English languages. We\nfind evidence that the extent to which transfer takes place can be predicted by\nthe amount of data in a given language present in the model's pretraining data.\nHowever, this transfer of bias and toxicity mitigation often comes at the\nexpense of decreased language generation ability in non-English languages,\nhighlighting the importance of developing language-specific bias and toxicity\nmitigation methods.", "AI": {"tldr": "This paper investigates the effects of various finetuning methods on bias, toxicity, and text generation fluency in generative LLMs across different languages.", "motivation": "To explore how finetuning methods can mitigate harmful biases and toxicity in generative LLMs, especially when dealing with non-English languages.", "method": "Different finetuning methods were applied to generative LLMs, including training on curated non-harmful text and direct preference optimization, to evaluate their impacts on bias, toxicity, and text generation.", "result": "Finetuning on curated text reduced biases, while direct preference optimization effectively mitigated toxicity. Transferability of these methods from English to non-English languages was established, but often at the cost of decreased generation fluency in non-English languages.", "conclusion": "The study emphasizes the need for language-specific methods for bias and toxicity mitigation to avoid compromising text generation quality in non-English languages.", "key_contributions": ["Analysis of finetuning methods on LLMs for bias and toxicity reduction.", "Demonstration of transfer effects from English to non-English generations.", "Identification of trade-offs between bias mitigation and text generation quality."], "limitations": "The study notes a compromise in language generation ability when applying mitigation techniques.", "keywords": ["generative models", "bias mitigation", "toxicity reduction", "multilingual NLP", "finetuning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.18053", "pdf": "https://arxiv.org/pdf/2412.18053.pdf", "abs": "https://arxiv.org/abs/2412.18053", "title": "Neuron Empirical Gradient: Discovering and Quantifying Neurons Global Linear Controllability", "authors": ["Xin Zhao", "Zehui Jiang", "Naoki Yoshinaga"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main, 32 pages", "summary": "While feed-forward neurons in pre-trained language models (PLMs) can encode\nknowledge, past research targeted a small subset of neurons that heavily\ninfluence outputs. This leaves the broader role of neuron activations unclear,\nlimiting progress in areas like knowledge editing. We uncover a global linear\nrelationship between neuron activations and outputs using neuron interventions\non a knowledge probing dataset. The gradient of this linear relationship, which\nwe call the neuron empirical gradient (NEG), captures how changes in\nactivations affect predictions. To compute NEG efficiently, we propose\nNeurGrad, enabling large-scale analysis of neuron behavior in PLMs. We also\nshow that NEG effectively captures language skills across diverse prompts\nthrough skill neuron probing. Experiments on MCEval8k, a multi-genre\nmultiple-choice knowledge benchmark, support NEG's ability to represent model\nknowledge. Further analysis highlights the key properties of NEG-based skill\nrepresentation: efficiency, robustness, flexibility, and interdependency. The\ncode and data are released.", "AI": {"tldr": "This paper introduces the Neuron Empirical Gradient (NEG) to analyze neuron activations in pre-trained language models (PLMs) and their influence on outputs, demonstrating its effectiveness for knowledge representation and language skill probing.", "motivation": "Understanding the broader role of neuron activations in PLMs is essential for advancing knowledge editing and enhancing model interpretability.", "method": "We propose NeurGrad to compute the Neuron Empirical Gradient (NEG), which quantifies the relationship between neuron activations and model outputs. This method allows for large-scale analysis of neuron behavior and skill representation in PLMs.", "result": "NEG has shown to effectively capture language skills across various prompts and can represent model knowledge well, evidenced by experiments on the MCEval8k benchmark.", "conclusion": "The findings indicate that NEG is a valuable tool for probing language models, showcasing properties like efficiency and robustness in skill representation.", "key_contributions": ["Introduction of Neuron Empirical Gradient (NEG) for analyzing neuron behavior in PLMs", "Development of NeurGrad for efficient computation of NEG", "Demonstration of NEG's effectiveness in representing language skills and model knowledge"], "limitations": "", "keywords": ["Neuron Empirical Gradient", "pre-trained language models", "knowledge probing"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2412.18069", "pdf": "https://arxiv.org/pdf/2412.18069.pdf", "abs": "https://arxiv.org/abs/2412.18069", "title": "Improving Factuality with Explicit Working Memory", "authors": ["Mingda Chen", "Yang Li", "Karthik Padthe", "Rulin Shao", "Alicia Sun", "Luke Zettlemoyer", "Gargi Ghosh", "Wen-tau Yih"], "categories": ["cs.CL"], "comment": "ACL 2025 Camera Ready", "summary": "Large language models can generate factually inaccurate content, a problem\nknown as hallucination. Recent works have built upon retrieved-augmented\ngeneration to improve factuality through iterative prompting but these methods\nare limited by the traditional RAG design. To address these challenges, we\nintroduce EWE (Explicit Working Memory), a novel approach that enhances\nfactuality in long-form text generation by integrating a working memory that\nreceives real-time feedback from external resources. The memory is refreshed\nbased on online fact-checking and retrieval feedback, allowing EWE to rectify\nfalse claims during the generation process and ensure more accurate and\nreliable outputs. Our experiments demonstrate that Ewe outperforms strong\nbaselines on four fact-seeking long-form generation datasets, increasing the\nfactuality metric, VeriScore, by 2 to 6 points absolute without sacrificing the\nhelpfulness of the responses. Further analysis reveals that the design of rules\nfor memory updates, configurations of memory units, and the quality of the\nretrieval datastore are crucial factors for influencing model performance.", "AI": {"tldr": "This paper introduces EWE (Explicit Working Memory), a new method to improve factuality in long-form text generation by incorporating real-time feedback from external resources.", "motivation": "The paper addresses the problem of hallucination in large language models, where generated content can be factually inaccurate, by enhancing factuality in generation processes.", "method": "EWE integrates a working memory that updates based on online fact-checking and retrieval feedback, allowing it to correct false claims during generation.", "result": "EWE outperforms strong baselines on long-form generation datasets, increasing the factuality metric, VeriScore, by 2 to 6 points without sacrificing helpfulness.", "conclusion": "The rules for memory updates, configurations of memory units, and the quality of the retrieval datastore are significant factors influencing performance.", "key_contributions": ["Introduction of EWE, a novel method for improving factuality in generated text.", "Demonstrated effectiveness through experimental results on long-form generation datasets.", "Insights into memory update rules and their impact on model performance."], "limitations": "", "keywords": ["large language models", "fact-checking", "factuality", "text generation", "memory retrieval"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2412.18120", "pdf": "https://arxiv.org/pdf/2412.18120.pdf", "abs": "https://arxiv.org/abs/2412.18120", "title": "Do Language Models Understand the Cognitive Tasks Given to Them? Investigations with the N-Back Paradigm", "authors": ["Xiaoyang Hu", "Richard L. Lewis"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "Cognitive tasks originally developed for humans are now increasingly used to\nstudy language models. While applying these tasks is often straightforward,\ninterpreting their results can be challenging. In particular, when a model\nunderperforms, it is often unclear whether this results from a limitation in\nthe cognitive ability being tested or a failure to understand the task itself.\nA recent study argues that GPT 3.5's declining performance on 2-back and 3-back\ntasks reflects a working memory capacity limit similar to humans (Gong et al.,\n2024). By analyzing a range of open-source language models of varying\nperformance levels on these tasks, we show that the poor performance is due at\nleast in part to a limitation in task comprehension and task set maintenance.\nWe challenge the best-performing model with progressively harder versions of\nthe task (up to 10-back) and experiment with alternative prompting strategies,\nbefore analyzing model attentions. Our larger aim is to contribute to the\nongoing conversation around refining methodologies for the cognitive evaluation\nof language models.", "AI": {"tldr": "This paper investigates the cognitive task performance of language models and the factors affecting their results, suggesting limitations in task comprehension rather than solely cognitive ability.", "motivation": "To improve understanding of language models' performance on cognitive tasks and refine evaluation methodologies.", "method": "Analyzed open-source language models on cognitive tasks, introduced progressively harder task versions, and employed alternative prompting strategies.", "result": "Findings indicate that limitations in task comprehension and task set maintenance significantly affect model performance on cognitive tasks.", "conclusion": "The study suggests a need to better interpret language model performance data to differentiate between cognitive limitations and task understanding issues.", "key_contributions": ["Identification of task comprehension limitations in language models", "Introduction of progressively harder cognitive tasks", "Alternative prompting strategies to improve task performance"], "limitations": "The study may not account for all variables influencing cognitive task performance.", "keywords": ["language models", "cognitive tasks", "task comprehension", "machine learning", "evaluation methodologies"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2412.18151", "pdf": "https://arxiv.org/pdf/2412.18151.pdf", "abs": "https://arxiv.org/abs/2412.18151", "title": "CoAM: Corpus of All-Type Multiword Expressions", "authors": ["Yusuke Ide", "Joshua Tanner", "Adam Nohejl", "Jacob Hoffman", "Justin Vasselli", "Hidetaka Kamigaito", "Taro Watanabe"], "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multiword expressions (MWEs) refer to idiomatic sequences of multiple words.\nMWE identification, i.e., detecting MWEs in text, can play a key role in\ndownstream tasks such as machine translation, but existing datasets for the\ntask are inconsistently annotated, limited to a single type of MWE, or limited\nin size. To enable reliable and comprehensive evaluation, we created CoAM:\nCorpus of All-Type Multiword Expressions, a dataset of 1.3K sentences\nconstructed through a multi-step process to enhance data quality consisting of\nhuman annotation, human review, and automated consistency checking.\nAdditionally, for the first time in a dataset of MWE identification, CoAM's\nMWEs are tagged with MWE types, such as Noun and Verb, enabling fine-grained\nerror analysis. Annotations for CoAM were collected using a new interface\ncreated with our interface generator, which allows easy and flexible annotation\nof MWEs in any form. Through experiments using CoAM, we find that a fine-tuned\nlarge language model outperforms MWEasWSD, which achieved the state-of-the-art\nperformance on the DiMSUM dataset. Furthermore, analysis using our MWE type\ntagged data reveals that Verb MWEs are easier than Noun MWEs to identify across\napproaches.", "AI": {"tldr": "CoAM is a comprehensive dataset for identifying multiword expressions (MWEs) using a new annotation interface, enhancing data quality and enabling fine-grained analysis.", "motivation": "To address inconsistencies and limitations in existing datasets for multiword expression (MWE) identification, which is essential for tasks like machine translation.", "method": "The dataset was created through a multi-step process involving human annotation, review, and automated consistency checks. MWEs are tagged with types (Noun, Verb) for detailed error analysis.", "result": "Experiments show that a fine-tuned large language model outperforms existing MWE identification methods on benchmarks, with insights indicating Verb MWEs are easier to identify than Noun MWEs.", "conclusion": "CoAM provides a reliable and comprehensive dataset that can improve MWE identification and facilitate detailed error analysis.", "key_contributions": ["Introduction of a new dataset (CoAM) for MWE identification with diverse types", "Development of a novel annotation interface", "Demonstration of improved performance of a fine-tuned large language model for MWE identification"], "limitations": "", "keywords": ["multiword expressions", "dataset", "natural language processing", "machine translation", "language model"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.18547", "pdf": "https://arxiv.org/pdf/2412.18547.pdf", "abs": "https://arxiv.org/abs/2412.18547", "title": "Token-Budget-Aware LLM Reasoning", "authors": ["Tingxu Han", "Zhenting Wang", "Chunrong Fang", "Shiyu Zhao", "Shiqing Ma", "Zhenyu Chen"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning is critical for large language models (LLMs) to excel in a wide\nrange of tasks. While methods like Chain-of-Thought (CoT) reasoning and enhance\nLLM performance by decomposing problems into intermediate steps, they also\nincur significant overhead in token usage, leading to increased costs. We find\nthat the reasoning process of current LLMs is unnecessarily lengthy and it can\nbe compressed by including a reasonable token budget in the prompt, but the\nchoice of token budget plays a crucial role in the actual compression\neffectiveness. We then propose a token-budget-aware LLM reasoning framework\nthat dynamically adjusts the number of reasoning tokens based on the reasoning\ncomplexity of each problem. Experiments show that our method effectively\nreduces token costs in CoT reasoning with only a slight performance reduction,\noffering a practical solution to balance efficiency and accuracy in LLM\nreasoning. Code: https://github.com/GeniusHTX/TALE", "AI": {"tldr": "The paper proposes a token-budget-aware framework for reasoning in large language models to reduce token usage while maintaining performance.", "motivation": "To address the significant overhead in token usage caused by current reasoning methods in LLMs, which leads to increased costs.", "method": "The proposed framework dynamically adjusts the number of reasoning tokens based on the complexity of the reasoning problem, optimizing token usage.", "result": "Experiments demonstrate that the framework effectively reduces token costs in Chain-of-Thought reasoning while minimally impacting performance.", "conclusion": "Adopting a token-budget-aware approach balances efficiency and accuracy in LLM reasoning, presenting a practical solution for cost reduction.", "key_contributions": ["Introduction of a token-budget-aware reasoning framework for LLMs.", "Demonstrated reduction in token costs during CoT reasoning with minor performance trade-offs.", "Analysis of the impact of token budget selection on reasoning effectiveness."], "limitations": "The performance reduction, while slight, may vary depending on the complexity of reasoning tasks.", "keywords": ["large language models", "Chain-of-Thought", "reasoning", "token budget", "efficiency"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2412.20057", "pdf": "https://arxiv.org/pdf/2412.20057.pdf", "abs": "https://arxiv.org/abs/2412.20057", "title": "\"My life is miserable, have to sign 500 autographs everyday\": Exposing Humblebragging, the Brags in Disguise", "authors": ["Sharath Naganna", "Saprativa Bhattacharjee", "Biplab Banerjee", "Pushpak Bhattacharyya"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Humblebragging is a phenomenon in which individuals present self-promotional\nstatements under the guise of modesty or complaints. For example, a statement\nlike, \"Ugh, I can't believe I got promoted to lead the entire team. So\nstressful!\", subtly highlights an achievement while pretending to be\ncomplaining. Detecting humblebragging is important for machines to better\nunderstand the nuances of human language, especially in tasks like sentiment\nanalysis and intent recognition. However, this topic has not yet been studied\nin computational linguistics. For the first time, we introduce the task of\nautomatically detecting humblebragging in text. We formalize the task by\nproposing a 4-tuple definition of humblebragging and evaluate machine learning,\ndeep learning, and large language models (LLMs) on this task, comparing their\nperformance with humans. We also create and release a dataset called HB-24,\ncontaining 3,340 humblebrags generated using GPT-4o. Our experiments show that\ndetecting humblebragging is non-trivial, even for humans. Our best model\nachieves an F1-score of 0.88. This work lays the foundation for further\nexploration of this nuanced linguistic phenomenon and its integration into\nbroader natural language understanding systems.", "AI": {"tldr": "The paper introduces the detection of humblebragging in text, a novel task in computational linguistics, proposing a formal definition and evaluating models on a new dataset.", "motivation": "The study addresses the lack of research on humblebragging in computational linguistics and its importance for understanding human language nuances.", "method": "A 4-tuple definition of humblebragging is proposed, and machine learning, deep learning, and LLMs are evaluated against humans on this task using a newly released dataset, HB-24.", "result": "The best model achieves an F1-score of 0.88, indicating the complexity of detecting humblebragging even for humans.", "conclusion": "The research establishes a foundational understanding for integrating humblebragging detection into natural language understanding systems.", "key_contributions": ["Introduced the task of humblebragging detection in computational linguistics.", "Created and released the HB-24 dataset for training and evaluation.", "Demonstrated the effectiveness of machine learning models in detecting humblebragging."], "limitations": "Detection remains difficult, with even the best models only achieving moderate accuracy.", "keywords": ["humblebragging", "natural language processing", "sentiment analysis", "machine learning", "large language models"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2412.20584", "pdf": "https://arxiv.org/pdf/2412.20584.pdf", "abs": "https://arxiv.org/abs/2412.20584", "title": "Towards Neural No-Resource Language Translation: A Comparative Evaluation of Approaches", "authors": ["Madhavendra Thakur"], "categories": ["cs.CL"], "comment": "Presented at the Columbia AI Summit 2025", "summary": "No-resource languages - those with minimal or no digital representation -\npose unique challenges for machine translation (MT). Unlike low-resource\nlanguages, which rely on limited but existent corpora, no-resource languages\noften have fewer than 100 sentences available for training. This work explores\nthe problem of no-resource translation through three distinct workflows:\nfine-tuning of translation-specific models, in-context learning with large\nlanguage models (LLMs) using chain-of-reasoning prompting, and direct prompting\nwithout reasoning. Using Owens Valley Paiute as a case study, we demonstrate\nthat no-resource translation demands fundamentally different approaches from\nlow-resource scenarios, as traditional approaches to machine translation, such\nas those that work for low-resource languages, fail. Empirical results reveal\nthat, although traditional approaches fail, the in-context learning\ncapabilities of general-purpose large language models enable no-resource\nlanguage translation that outperforms low-resource translation approaches and\nrivals human translations (BLEU 0.45-0.6); specifically, chain-of-reasoning\nprompting outperforms other methods for larger corpora, while direct prompting\nexhibits advantages in smaller datasets. As these approaches are\nlanguage-agnostic, they have potential to be generalized to translation tasks\nfrom a wide variety of no-resource languages without expert input. These\nfindings establish no-resource translation as a distinct paradigm requiring\ninnovative solutions, providing practical and theoretical insights for language\npreservation.", "AI": {"tldr": "This paper addresses the challenges of translating no-resource languages, demonstrating that traditional machine translation methods are ineffective, and explores three distinct workflows using LLMs.", "motivation": "To explore the unique challenges posed by no-resource languages in machine translation and to propose innovative approaches for effective translation.", "method": "The paper investigates three workflows: fine-tuning of translation-specific models, in-context learning with LLMs through chain-of-reasoning prompting, and direct prompting without reasoning.", "result": "Empirical findings show that LLMs can effectively translate no-resource languages, with chain-of-reasoning prompting outperforming other methods and rivaling human translations, particularly for larger corpora.", "conclusion": "The study emphasizes that no-resource translation requires fundamentally different approaches than low-resource scenarios, highlighting the effectiveness of LLMs in this context.", "key_contributions": ["Introduces innovative workflows for no-resource language translation using LLMs.", "Demonstrates that traditional MT methods fail for no-resource languages.", "Establishes no-resource translation as a distinct paradigm."], "limitations": "", "keywords": ["no-resource languages", "machine translation", "large language models", "translation", "language preservation"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.00759", "pdf": "https://arxiv.org/pdf/2501.00759.pdf", "abs": "https://arxiv.org/abs/2501.00759", "title": "Enhancing Transformers for Generalizable First-Order Logical Entailment", "authors": ["Tianshi Zheng", "Jiazheng Wang", "Zihao Wang", "Jiaxin Bai", "Hang Yin", "Zheye Deng", "Yangqiu Song", "Jianxin Li"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main", "summary": "Transformers, as the fundamental deep learning architecture, have\ndemonstrated great capability in reasoning. This paper studies the\ngeneralizable first-order logical reasoning ability of transformers with their\nparameterized knowledge and how to improve it. Transformers' capability of\nfirst-order reasoning is further captured by whether they can conduct\nfirst-order logical entailment, which is quantitatively measured by their\nperformance in answering knowledge graph queries. We establish the connections\nbetween (1) two types of distribution shifts studied in out-of-distribution\ngeneralization and (2) unseen knowledge and query settings discussed in the\ntask of knowledge graph query answering, which makes it possible to\ncharacterize the fine-grained generalizability. Results on our comprehensive\ndataset showed that transformers outperform previous methods designed\nparticularly for this task and provided detailed empirical evidence about the\nimpact of the input query syntax, token embedding, and transformer\narchitectures on the reasoning capability of transformers. Interestingly, our\nresults revealed the mismatch of positional encoding and other design choices\nof transformer architectures in previous practices. Motivated by this, we\npropose TEGA, a logic-aware architecture that significantly improves the\nperformance in generalizable first-order logical entailment.", "AI": {"tldr": "This paper investigates and enhances the first-order logical reasoning ability of transformers using a new architecture called TEGA.", "motivation": "To study and improve the generalizable first-order logical reasoning capabilities of transformers, identifying key factors affecting their performance.", "method": "The authors conducted experiments to measure transformers' performance in answering knowledge graph queries while analyzing distribution shifts in out-of-distribution generalization.", "result": "Transformers showed superior performance over prior methods for first-order reasoning tasks, and the paper provides empirical evidence on the effects of various architectural choices.", "conclusion": "Introducing TEGA, a logic-aware transformer architecture, significantly improves transformers' reasoning abilities in first-order entailment.", "key_contributions": ["Development of TEGA, a new architecture for improved first-order reasoning", "Identification of factors influencing transformers' reasoning capabilities", "Establishment of connections between reasoning tasks and distribution shifts"], "limitations": "", "keywords": ["Transformers", "First-order logic", "Knowledge graph", "Entailment", "Machine Learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2501.00982", "pdf": "https://arxiv.org/pdf/2501.00982.pdf", "abs": "https://arxiv.org/abs/2501.00982", "title": "Are LLMs effective psychological assessors? Leveraging adaptive RAG for interpretable mental health screening through psychometric practice", "authors": ["Federico Ravenda", "Seyed Ali Bahrainian", "Andrea Raballo", "Antonietta Mira", "Noriko Kando"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In psychological practices, standardized questionnaires serve as essential\ntools for assessing mental health through structured, clinically-validated\nquestions (i.e., items). While social media platforms offer rich data for\nmental health screening, computational approaches often bypass these\nestablished clinical assessment tools in favor of black-box classification. We\npropose a novel questionnaire-guided screening framework that bridges\npsychological practice and computational methods through adaptive\nRetrieval-Augmented Generation (\\textit{aRAG}). Our approach links unstructured\nsocial media content and standardized clinical assessments by retrieving\nrelevant posts for each questionnaire item and using Large Language Models\n(LLMs) to complete validated psychological instruments. Our findings\ndemonstrate two key advantages of questionnaire-guided screening: First, when\ncompleting the Beck Depression Inventory-II (BDI-II), our approach matches or\noutperforms state-of-the-art performance on Reddit-based benchmarks without\nrequiring training data. Second, we show that guiding LLMs through standardized\nquestionnaires can yield superior results compared to directly prompting them\nfor depression screening, while also providing a more interpretable assessment\nby linking model outputs to clinically validated diagnostic criteria.\nAdditionally, we show, as a proof-of-concept, how our questionnaire-based\nmethodology can be extended to other mental conditions' screening, highlighting\nthe promising role of LLMs as psychological assessors.", "AI": {"tldr": "The paper presents a questionnaire-guided screening framework combining psychological assessments with social media data using adaptive Retrieval-Augmented Generation.", "motivation": "To enhance mental health assessment by integrating social media content with standardized clinical questionnaires, thereby bridging computational approaches and psychological practices.", "method": "A novel framework that retrieves relevant social media posts for each questionnaire item and utilizes Large Language Models to complete validated psychological instruments, specifically the Beck Depression Inventory-II (BDI-II).", "result": "The proposed approach matches or outperforms state-of-the-art performance on Reddit-based benchmarks without requiring training data, and provides more interpretable assessments that link model outputs to clinical criteria.", "conclusion": "The framework shows the potential for using LLMs as psychological assessors beyond depression screening, applicable to various mental health conditions.", "key_contributions": ["Integration of social media data with standardized psychological assessments", "Demonstrating superior performance with minimal to no training requirements", "Providing interpretable results aligned with clinical diagnostic criteria"], "limitations": "The method is primarily validated with the Beck Depression Inventory-II and may require further testing across diverse mental health conditions.", "keywords": ["Mental Health", "Psychological Assessments", "Large Language Models", "Retrieval-Augmented Generation", "Social Media"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.01377", "pdf": "https://arxiv.org/pdf/2501.01377.pdf", "abs": "https://arxiv.org/abs/2501.01377", "title": "Improving Medical Large Vision-Language Models with Abnormal-Aware Feedback", "authors": ["Yucheng Zhou", "Lingran Song", "Jianbing Shen"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "16 pages", "summary": "Existing Medical Large Vision-Language Models (Med-LVLMs), encapsulating\nextensive medical knowledge, demonstrate excellent capabilities in\nunderstanding medical images. However, there remain challenges in visual\nlocalization in medical images, which is crucial for abnormality detection and\ninterpretation. To address these issues, we propose a novel UMed-LVLM designed\nto unveil medical abnormalities. Specifically, we collect a Medical\nAbnormalities Unveiling (MAU) dataset and propose a two-stage training method\nfor UMed-LVLM training. To collect MAU dataset, we propose a prompt method\nutilizing the GPT-4V to generate diagnoses based on identified abnormal areas\nin medical images. Moreover, the two-stage training method includes\nAbnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding, comprising\nRelevance Reward, Abnormal Localization Reward and Vision Relevance Reward.\nExperimental results demonstrate that our UMed-LVLM significantly outperforms\nexisting Med-LVLMs in identifying and understanding medical abnormalities,\nachieving a 58% improvement over the baseline. In addition, this work shows\nthat enhancing the abnormality detection capabilities of Med-LVLMs\nsignificantly improves their understanding of medical images and generalization\ncapability.", "AI": {"tldr": "Proposes UMed-LVLM, a novel large vision-language model for detecting medical abnormalities using a two-stage training approach and a unique dataset.", "motivation": "To enhance visual localization and abnormality detection in medical images using large vision-language models.", "method": "Development of UMed-LVLM which utilizes a Medical Abnormalities Unveiling (MAU) dataset and a two-stage training method involving Abnormal-Aware Instruction Tuning and Abnormal-Aware Rewarding.", "result": "UMed-LVLM outperforms existing Med-LVLMs by 58% in identifying medical abnormalities, with improved understanding and generalization of medical images.", "conclusion": "Improving abnormality detection greatly enhances the overall capability of medical large vision-language models.", "key_contributions": ["Introduction of UMed-LVLM for medical image analysis", "Creation of the Medical Abnormalities Unveiling (MAU) dataset", "Novel two-stage training approach with multiple reward mechanisms."], "limitations": "", "keywords": ["Medical Imaging", "Vision-Language Models", "Abnormality Detection", "Human-Computer Interaction", "Machine Learning"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2501.01743", "pdf": "https://arxiv.org/pdf/2501.01743.pdf", "abs": "https://arxiv.org/abs/2501.01743", "title": "Automating Legal Interpretation with LLMs: Retrieval, Generation, and Evaluation", "authors": ["Kangcheng Luo", "Quzhe Huang", "Cong Jiang", "Yansong Feng"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Interpreting the law is always essential for the law to adapt to the\never-changing society. It is a critical and challenging task even for legal\npractitioners, as it requires meticulous and professional annotations and\nsummarizations by legal experts, which are admittedly time-consuming and\nexpensive to collect at scale. To alleviate the burden on legal experts, we\npropose a method for automated legal interpretation. Specifically, by emulating\ndoctrinal legal research, we introduce a novel framework, ATRIE, to address\nLegal Concept Interpretation, a typical task in legal interpretation. ATRIE\nutilizes large language models (LLMs) to AuTomatically Retrieve concept-related\ninformation, Interpret legal concepts, and Evaluate generated interpretations,\neliminating dependence on legal experts. ATRIE comprises a legal concept\ninterpreter and a legal concept interpretation evaluator. The interpreter uses\nLLMs to retrieve relevant information from previous cases and interpret legal\nconcepts. The evaluator uses performance changes on Legal Concept Entailment, a\ndownstream task we propose, as a proxy of interpretation quality. Automated and\nmultifaceted human evaluations indicate that the quality of our interpretations\nis comparable to those written by legal experts, with superior\ncomprehensiveness and readability. Although there remains a slight gap in\naccuracy, it can already assist legal practitioners in improving the efficiency\nof legal interpretation.", "AI": {"tldr": "The paper presents a framework, ATRIE, for automated legal interpretation using large language models to reduce the reliance on legal experts.", "motivation": "To alleviate the burden on legal experts by automating legal interpretation due to its time-consuming and expensive nature in traditional practices.", "method": "ATRIE emulates doctrinal legal research by utilizing LLMs to retrieve information, interpret legal concepts, and evaluate the interpretations.", "result": "Automated evaluations indicate that ATRIE's interpretations are comparable in quality to those of legal experts, with improved comprehensiveness and readability, despite a slight accuracy gap.", "conclusion": "ATRIE can assist legal practitioners, enhancing the efficiency of legal interpretations without fully replacing expert judgment.", "key_contributions": ["Development of ATRIE framework for legal interpretation", "Use of LLMs for interpreting legal concepts", "Introduction of Legal Concept Entailment task as a proxy for evaluation"], "limitations": "A slight gap in accuracy compared to legal experts remains.", "keywords": ["Legal Interpretation", "Large Language Models", "Automated Legal Research"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2501.02157", "pdf": "https://arxiv.org/pdf/2501.02157.pdf", "abs": "https://arxiv.org/abs/2501.02157", "title": "Personalized Graph-Based Retrieval for Large Language Models", "authors": ["Steven Au", "Cameron J. Dimacali", "Ojasmitha Pedirappagari", "Namyong Park", "Franck Dernoncourt", "Yu Wang", "Nikos Kanakaris", "Hanieh Deilamsalehy", "Ryan A. Rossi", "Nesreen K. Ahmed"], "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) evolve, their ability to deliver personalized\nand context-aware responses offers transformative potential for improving user\nexperiences. Existing personalization approaches, however, often rely solely on\nuser history to augment the prompt, limiting their effectiveness in generating\ntailored outputs, especially in cold-start scenarios with sparse data. To\naddress these limitations, we propose Personalized Graph-based\nRetrieval-Augmented Generation (PGraphRAG), a framework that leverages\nuser-centric knowledge graphs to enrich personalization. By directly\nintegrating structured user knowledge into the retrieval process and augmenting\nprompts with user-relevant context, PGraphRAG enhances contextual understanding\nand output quality. We also introduce the Personalized Graph-based Benchmark\nfor Text Generation, designed to evaluate personalized text generation tasks in\nreal-world settings where user history is sparse or unavailable. Experimental\nresults show that PGraphRAG significantly outperforms state-of-the-art\npersonalization methods across diverse tasks, demonstrating the unique\nadvantages of graph-based retrieval for personalization.", "AI": {"tldr": "The paper introduces PGraphRAG, a framework for personalized text generation using user-centric knowledge graphs to enhance contextual understanding and output quality, especially in scenarios with limited user history.", "motivation": "To address the limitations of existing personalization methods that rely solely on user history, thus limiting their effectiveness in cold-start scenarios.", "method": "The proposed framework, PGraphRAG, integrates structured user knowledge from knowledge graphs into the retrieval process and augments prompts with relevant user context.", "result": "Experimental results demonstrate that PGraphRAG significantly surpasses existing state-of-the-art personalization methods in various tasks.", "conclusion": "PGraphRAG showcases the advantages of graph-based retrieval for enhancing personalized text generation in situations with sparse user data.", "key_contributions": ["Introduction of PGraphRAG framework for personalized text generation.", "Development of a benchmark for evaluating personalized text generation in real-world scenarios.", "Demonstration of significant performance improvements over existing methods."], "limitations": "", "keywords": ["Personalized Text Generation", "Knowledge Graphs", "Retrieval-Augmented Generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.02295", "pdf": "https://arxiv.org/pdf/2501.02295.pdf", "abs": "https://arxiv.org/abs/2501.02295", "title": "Explicit vs. Implicit: Investigating Social Bias in Large Language Models through Self-Reflection", "authors": ["Yachao Zhao", "Bo Wang", "Yan Wang"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) have been shown to exhibit various biases and\nstereotypes in their generated content. While extensive research has\ninvestigated biases in LLMs, prior work has predominantly focused on explicit\nbias, with minimal attention to implicit bias and the relation between these\ntwo forms of bias. This paper presents a systematic framework grounded in\nsocial psychology theories to investigate and compare explicit and implicit\nbiases in LLMs. We propose a novel self-reflection-based evaluation framework\nthat operates in two phases: first measuring implicit bias through simulated\npsychological assessment methods, then evaluating explicit bias by prompting\nLLMs to analyze their own generated content. Through extensive experiments on\nadvanced LLMs across multiple social dimensions, we demonstrate that LLMs\nexhibit a substantial inconsistency between explicit and implicit biases: while\nexplicit bias manifests as mild stereotypes, implicit bias exhibits strong\nstereotypes. We further investigate the underlying factors contributing to this\nexplicit-implicit bias inconsistency, examining the effects of training data\nscale, model size, and alignment techniques. Experimental results indicate that\nwhile explicit bias declines with increased training data and model size,\nimplicit bias exhibits a contrasting upward trend. Moreover, contemporary\nalignment methods effectively suppress explicit bias but show limited efficacy\nin mitigating implicit bias.", "AI": {"tldr": "A framework to analyze explicit and implicit biases in Large Language Models (LLMs) reveals inconsistencies, with explicit biases being milder and implicit biases stronger.", "motivation": "To systematically investigate the relationship between explicit and implicit biases in LLMs, as previous research has mostly focused on explicit bias.", "method": "A self-reflection-based evaluation framework that measures implicit bias via psychological assessments and explicit bias by prompting LLMs to evaluate their generated content.", "result": "Experiments show a substantial inconsistency between explicit and implicit biases; while explicit bias declines with more training data, implicit bias increases.", "conclusion": "Contemporary alignment methods are effective against explicit bias but not for implicit bias, highlighting a critical gap in LLM bias mitigation strategies.", "key_contributions": ["Proposed a novel framework for evaluating biases in LLMs", "Demonstrated the inconsistency between explicit and implicit biases in LLMs", "Investigated factors affecting bias in LLMs, including training data and model size"], "limitations": "The framework may not encompass all possible biases and could benefit from additional dimensions for comprehensive analysis.", "keywords": ["Large Language Models", "biases", "implicit bias", "explicit bias", "self-reflection evaluation"], "importance_score": 8, "read_time_minutes": 12}}
{"id": "2501.02460", "pdf": "https://arxiv.org/pdf/2501.02460.pdf", "abs": "https://arxiv.org/abs/2501.02460", "title": "Towards Omni-RAG: Comprehensive Retrieval-Augmented Generation for Large Language Models in Medical Applications", "authors": ["Zhe Chen", "Yusheng Liao", "Shuyang Jiang", "Pingjie Wang", "Yiqiu Guo", "Yanfeng Wang", "Yu Wang"], "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. Project website:\n  https://github.com/Jack-ZC8/Omni-RAG-Medical", "summary": "Large language models hold promise for addressing medical challenges, such as\nmedical diagnosis reasoning, research knowledge acquisition, clinical\ndecision-making, and consumer health inquiry support. However, they often\ngenerate hallucinations due to limited medical knowledge. Incorporating\nexternal knowledge is therefore critical, which necessitates multi-source\nknowledge acquisition. We address this challenge by framing it as a source\nplanning problem, which is to formulate context-appropriate queries tailored to\nthe attributes of diverse sources. Existing approaches either overlook source\nplanning or fail to achieve it effectively due to misalignment between the\nmodel's expectation of the sources and their actual content. To bridge this\ngap, we present MedOmniKB, a repository comprising multigenre and\nmulti-structured medical knowledge sources. Leveraging these sources, we\npropose the Source Planning Optimisation method, which enhances multi-source\nutilisation. Our approach involves enabling an expert model to explore and\nevaluate potential plans while training a smaller model to learn source\nalignment. Experimental results demonstrate that our method substantially\nimproves multi-source planning performance, enabling the optimised small model\nto achieve state-of-the-art results in leveraging diverse medical knowledge\nsources.", "AI": {"tldr": "This paper presents MedOmniKB, a repository for multi-structured medical knowledge sources, and a Source Planning Optimisation method to improve multi-source utilization in large language models for medical tasks.", "motivation": "To enhance the performance of large language models in medical contexts by reducing hallucinations and improving multi-source knowledge acquisition.", "method": "The research frames the problem as source planning, allowing an expert model to develop context-appropriate queries while a smaller model learns source alignment.", "result": "The proposed method significantly improves multi-source planning performance, resulting in state-of-the-art results for leveraging diverse medical knowledge sources.", "conclusion": "Integrating effective source planning and multi-source knowledge enhances the reliability and applicability of large language models in healthcare applications.", "key_contributions": ["Introduction of MedOmniKB as a robust repository for diverse medical knowledge sources.", "Development of a Source Planning Optimisation method for improved multi-source utilization.", "Demonstration of superior performance in multi-source planning compared to existing approaches."], "limitations": "", "keywords": ["large language models", "medical knowledge", "source planning", "healthcare AI", "multi-source utilization"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2501.03545", "pdf": "https://arxiv.org/pdf/2501.03545.pdf", "abs": "https://arxiv.org/abs/2501.03545", "title": "Beyond Factual Accuracy: Evaluating Coverage of Diverse Factual Information in Long-form Text Generation", "authors": ["Chris Samarinas", "Alexander Krubner", "Alireza Salemi", "Youngwoo Kim", "Hamed Zamani"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents ICAT, an evaluation framework for measuring coverage of\ndiverse factual information in long-form text generation. ICAT breaks down a\nlong output text into a list of atomic claims and not only verifies each claim\nthrough retrieval from a (reliable) knowledge source, but also computes the\nalignment between the atomic factual claims and various aspects expected to be\npresented in the output. We study three implementations of the ICAT framework,\neach with a different assumption on the availability of aspects and alignment\nmethod. By adopting data from the diversification task in the TREC Web Track\nand the ClueWeb corpus, we evaluate the ICAT framework. We demonstrate strong\ncorrelation with human judgments and provide comprehensive evaluation across\nmultiple state-of-the-art LLMs. Our framework further offers interpretable and\nfine-grained analysis of diversity and coverage. Its modular design allows for\neasy adaptation to different domains and datasets, making it a valuable tool\nfor evaluating the qualitative aspects of long-form responses produced by LLMs.", "AI": {"tldr": "ICAT is an evaluation framework that measures the coverage of diverse factual information in long-form text generation, providing a fine-grained analysis of generated outputs.", "motivation": "To address the need for systematic evaluation of long-form text generation, particularly for measuring the diversity and factual accuracy of generated content.", "method": "ICAT decomposes long text into atomic claims, verifies each claim using a reliable knowledge source, and analyzes alignment with expected output aspects through three different implementations.", "result": "ICAT demonstrates a strong correlation with human judgments and offers comprehensive evaluation across multiple state-of-the-art LLMs, highlighting its effectiveness in assessing text generation quality.", "conclusion": "The modular design of ICAT allows for adaptability across various domains and datasets, serving as a valuable tool for evaluating the qualitative aspects of long-form responses from LLMs.", "key_contributions": ["Introduction of the ICAT evaluation framework", "Demonstration of robustness across different LLMs", "Interpretable and fine-grained analysis of diversity and coverage."], "limitations": "", "keywords": ["HCI", "Long-form text generation", "Factual accuracy", "Evaluation framework", "Knowledge retrieval"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2501.03835", "pdf": "https://arxiv.org/pdf/2501.03835.pdf", "abs": "https://arxiv.org/abs/2501.03835", "title": "TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification", "authors": ["Yindu Su", "Huike Zou", "Lin Sun", "Ting Zhang", "Haiyang Yang", "Liyu Chen", "David Lo", "Qingheng Zhang", "Shuguang Han", "Jufeng Chen"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Camera-ready version of the paper accepted at ACL 2025", "summary": "Product Attribute Value Identification (PAVI) involves identifying attribute\nvalues from product profiles, a key task for improving product search,\nrecommendation, and business analytics on e-commerce platforms. However,\nexisting PAVI methods face critical challenges, such as inferring implicit\nvalues, handling out-of-distribution (OOD) values, and producing normalized\noutputs. To address these limitations, we introduce Taxonomy-Aware Contrastive\nLearning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR\nformulates PAVI as an information retrieval task by encoding product profiles\nand candidate values into embeddings and retrieving values based on their\nsimilarity. It leverages contrastive training with taxonomy-aware hard negative\nsampling and employs adaptive inference with dynamic thresholds. TACLR offers\nthree key advantages: (1) it effectively handles implicit and OOD values while\nproducing normalized outputs; (2) it scales to thousands of categories, tens of\nthousands of attributes, and millions of values; and (3) it supports efficient\ninference for high-load industrial deployment. Extensive experiments on\nproprietary and public datasets validate the effectiveness and efficiency of\nTACLR. Further, it has been successfully deployed on the real-world e-commerce\nplatform Xianyu, processing millions of product listings daily with frequently\nupdated, large-scale attribute taxonomies. We release the code to facilitate\nreproducibility and future research at https://github.com/SuYindu/TACLR.", "AI": {"tldr": "The paper introduces TACLR, a novel retrieval-based method for Product Attribute Value Identification (PAVI) that improves e-commerce search and recommendation by effectively identifying attribute values from product profiles.", "motivation": "Existing methods for Product Attribute Value Identification face significant challenges, including the inference of implicit values, handling out-of-distribution values, and producing normalized outputs.", "method": "TACLR formulates PAVI as an information retrieval task, utilizing contrastive training with taxonomy-aware hard negative sampling and adaptive inference with dynamic thresholds.", "result": "TACLR successfully handles implicit and out-of-distribution values and scales effectively, demonstrated through extensive experiments and real-world deployment on an e-commerce platform.", "conclusion": "The method proves to be effective and efficient for high-load industrial deployment, providing solutions for key challenges in PAVI and releasing code for reproducibility.", "key_contributions": ["Novel retrieval-based approach for PAVI", "Taxonomy-aware hard negative sampling", "Efficient handling of implicit and OOD values"], "limitations": "", "keywords": ["Product Attribute Value Identification", "Contrastive Learning", "E-commerce"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2501.04945", "pdf": "https://arxiv.org/pdf/2501.04945.pdf", "abs": "https://arxiv.org/abs/2501.04945", "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models", "authors": ["Qingyu Ren", "Jie Zeng", "Qianyu He", "Jiaqing Liang", "Yanghua Xiao", "Weikang Zhou", "Zeye Sun", "Fei Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, it is an unexplored area to enhance\nLLMs' ability to follow soft constraints. To bridge the gap, we initially\ndesign a pipeline to construct datasets with high-quality outputs\nautomatically. Additionally, to fully utilize the positive and negative samples\ngenerated during the data construction process, we choose Direct Preference\nOptimization (DPO) as the training method. Furthermore, taking into account the\ndifficulty of soft constraints indicated by the number of constraints, we\ndesign a curriculum learning training paradigm based on the constraint\nquantity. We experimentally evaluate the effectiveness of our methods in\nimproving LLMs' soft constraint following ability and analyze the factors\ndriving the improvements.The datasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraint.", "AI": {"tldr": "This paper presents methods to improve large language models' (LLMs) ability to follow soft constraints using a new dataset construction pipeline and Direct Preference Optimization.", "motivation": "Enhancing LLMs' capacity to follow soft constraints is an unexplored area critical for improving instruction adherence.", "method": "A pipeline for constructing datasets with high-quality outputs was designed, followed by training LLMs using Direct Preference Optimization and a curriculum learning paradigm based on the quantity of constraints.", "result": "Experimental evaluations show effectiveness in improving LLMs' ability to adhere to soft constraints, with analysis of contributing factors.", "conclusion": "The methods proposed significantly enhance LLMs' performance in following soft constraints, and the datasets and code are made publicly available for further research.", "key_contributions": ["Development of a dataset construction pipeline for soft constraints", "Application of Direct Preference Optimization for training", "Introduction of a curriculum learning training paradigm based on constraint quantity"], "limitations": "", "keywords": ["large language models", "soft constraints", "Direct Preference Optimization", "curriculum learning", "instruction adherence"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.05962", "pdf": "https://arxiv.org/pdf/2501.05962.pdf", "abs": "https://arxiv.org/abs/2501.05962", "title": "Effective faking of verbal deception detection with target-aligned adversarial attacks", "authors": ["Bennett Kleinberg", "Riccardo Loconte", "Bruno Verschuere"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Legal and Criminological Psychology (author version)", "summary": "Background: Deception detection through analysing language is a promising\navenue using both human judgments and automated machine learning judgments. For\nboth forms of credibility assessment, automated adversarial attacks that\nrewrite deceptive statements to appear truthful pose a serious threat. Methods:\nWe used a dataset of 243 truthful and 262 fabricated autobiographical stories\nin a deception detection task for humans and machine learning models. A large\nlanguage model was tasked to rewrite deceptive statements so that they appear\ntruthful. In Study 1, humans who made a deception judgment or used the\ndetailedness heuristic and two machine learning models (a fine-tuned language\nmodel and a simple n-gram model) judged original or adversarial modifications\nof deceptive statements. In Study 2, we manipulated the target alignment of the\nmodifications, i.e. tailoring the attack to whether the statements would be\nassessed by humans or computer models. Results: When adversarial modifications\nwere aligned with their target, human (d=-0.07 and d=-0.04) and machine\njudgments (51% accuracy) dropped to the chance level. When the attack was not\naligned with the target, both human heuristics judgments (d=0.30 and d=0.36)\nand machine learning predictions (63-78%) were significantly better than\nchance. Conclusions: Easily accessible language models can effectively help\nanyone fake deception detection efforts both by humans and machine learning\nmodels. Robustness against adversarial modifications for humans and machines\ndepends on that target alignment. We close with suggestions on advancing\ndeception research with adversarial attack designs and techniques.", "AI": {"tldr": "This study explores the impact of adversarial attacks on deception detection in both human judgments and machine learning models.", "motivation": "The motivation behind this research is to address the challenges posed by automated adversarial attacks that can make deceptive statements appear truthful, thereby complicating the credibility assessment process.", "method": "The study utilized a dataset of autobiographical stories, where deceptive statements were rewritten by a language model. Two studies were conducted, one assessing human and machine judgments on original and modified statements and another manipulating the alignment of these modifications with the targets being assessed.", "result": "Findings revealed that when modifications were aligned with their intended assessment target, both human and machine judgments dropped to chance levels. When misaligned, judgments showed improved accuracy, highlighting vulnerabilities in deception detection systems.", "conclusion": "The study concludes that language models can facilitate deceptive tactics that challenge the integrity of both human and machine judgments, emphasizing the need for improved adversarial robustness.", "key_contributions": ["Demonstrated the effectiveness of language models in rewriting deceptive statements.", "Highlighted the vulnerability of both human and automated deception detection systems to adversarial attacks.", "Proposed future directions for research on deception detection and adversarial strategies."], "limitations": "The study primarily focused on autobiographical stories, which may limit generalizability to other forms of deceptive communication.", "keywords": ["deception detection", "adversarial attacks", "machine learning", "credibility assessment", "language models"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2501.11463", "pdf": "https://arxiv.org/pdf/2501.11463.pdf", "abs": "https://arxiv.org/abs/2501.11463", "title": "Curiosity-Driven Reinforcement Learning from Human Feedback", "authors": ["Haoran Sun", "Yekun Chai", "Shuohuan Wang", "Yu Sun", "Hua Wu", "Haifeng Wang"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Reinforcement learning from human feedback (RLHF) has proven effective in\naligning large language models (LLMs) with human preferences, but often at the\ncost of reduced output diversity. This trade-off between diversity and\nalignment quality remains a significant challenge. Drawing inspiration from\ncuriosity-driven exploration in reinforcement learning, we introduce\ncuriosity-driven RLHF (CD-RLHF), a framework that incorporates intrinsic\nrewards for novel states, alongside traditional sparse extrinsic rewards, to\noptimize both output diversity and alignment quality. We demonstrate the\neffectiveness of CD-RLHF through extensive experiments on a range of tasks,\nincluding text summarization and instruction following. Our approach achieves\nsignificant gains in diversity on multiple diversity-oriented metrics while\nmaintaining alignment with human preferences comparable to standard RLHF. We\nmake our code publicly available at https://github.com/ernie-research/CD-RLHF.", "AI": {"tldr": "This paper introduces curiosity-driven reinforcement learning from human feedback (CD-RLHF), which aims to enhance output diversity in large language models while maintaining alignment with human preferences.", "motivation": "The trade-off between output diversity and alignment quality in large language models using reinforcement learning from human feedback (RLHF) is a significant challenge that this paper addresses.", "method": "The proposed framework, CD-RLHF, incorporates intrinsic rewards for exploring novel states along with traditional extrinsic rewards to optimize both diversity and alignment.", "result": "CD-RLHF shows significant improvements in output diversity across various tasks like text summarization and instruction following, without compromising on alignment to human preferences.", "conclusion": "The study demonstrates that integrating curiosity-driven exploration can effectively enhance diversity in outputs while maintaining quality in alignment, and offers a publicly available code for further exploration.", "key_contributions": ["Introduction of curiosity-driven RLHF (CD-RLHF) framework", "Demonstration of significant improvements in output diversity", "Balancing diversity with alignment quality in LLMs."], "limitations": "", "keywords": ["Reinforcement Learning", "Human Feedback", "Output Diversity", "Large Language Models", "Curiosity"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2501.11549", "pdf": "https://arxiv.org/pdf/2501.11549.pdf", "abs": "https://arxiv.org/abs/2501.11549", "title": "Whose Boat Does it Float? Improving Personalization in Preference Tuning via Inferred User Personas", "authors": ["Nishant Balepur", "Vishakh Padmakumar", "Fumeng Yang", "Shi Feng", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "LLMs are aligned to follow input instructions by learning which of two\nresponses users prefer for a prompt. However, such preference data do not\nconvey why users prefer responses that are chosen or rejected, so LLMs trained\non these datasets cannot tailor responses to varied user needs. To surface\nthese parameters of personalization, we apply abductive reasoning to preference\ndata, inferring needs and interests of users, i.e., personas, that may prefer\neither response. We test this idea in two steps: Persona Inference (PI),\nabductively inferring personas of users who prefer chosen or rejected outputs,\nand Persona Tailoring (PT), training models to tailor outputs to personas from\nPI. We show: 1) LLMs infer personas accurately explaining why different users\nmay prefer both chosen or rejected outputs; 2) Training on preference data\naugmented with PI personas via PT boosts personalization and generalizes to\nsupporting user-written personas; and 3) Rejected response personas form harder\npersonalization evaluations, showing PT better aids users with uncommon\npreferences versus typical alignment methods. We argue for an abductive view of\npreferences for personalization, asking not only which response is better but\nwhen, why, and for whom.", "AI": {"tldr": "This paper explores how to enhance personalization in language models by inferring user personas from preference data and tailoring responses accordingly.", "motivation": "The current method of aligning LLMs based on user preferences lacks the understanding of why users prefer certain responses, limiting personalization capabilities.", "method": "The authors propose an approach that consists of two steps: Persona Inference (PI) to deduce user personas from preference data, and Persona Tailoring (PT) to adapt model outputs based on these inferred personas.", "result": "The study demonstrates that LLMs can accurately infer user personas, leading to improved personalization of outputs, especially for users with less common preferences.", "conclusion": "The use of abductive reasoning to interpret user preferences enriches the personalization process for LLMs, shifting the focus from merely identifying the preferred response to understanding the underlying reasons for these preferences.", "key_contributions": ["Introduction of Persona Inference (PI) for user persona deduction from preference data.", "Development of Persona Tailoring (PT) to train models to customize outputs based on inferred personas.", "Evidence that augmented training with PI personas enhances personalization and generalizes well to diverse user-written personas."], "limitations": "", "keywords": ["Personalization", "Language Models", "User Preference", "Abductive Reasoning", "Human-Computer Interaction"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2501.13125", "pdf": "https://arxiv.org/pdf/2501.13125.pdf", "abs": "https://arxiv.org/abs/2501.13125", "title": "Generating Plausible Distractors for Multiple-Choice Questions via Student Choice Prediction", "authors": ["Yooseop Lee", "Suin Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "In designing multiple-choice questions (MCQs) in education, creating\nplausible distractors is crucial for identifying students' misconceptions and\ngaps in knowledge and accurately assessing their understanding. However, prior\nstudies on distractor generation have not paid sufficient attention to\nenhancing the difficulty of distractors, resulting in reduced effectiveness of\nMCQs. This study presents a pipeline for training a model to generate\ndistractors that are more likely to be selected by students. First, we train a\npairwise ranker to reason about students' misconceptions and assess the\nrelative plausibility of two distractors. Using this model, we create a dataset\nof pairwise distractor ranks and then train a distractor generator via Direct\nPreference Optimization (DPO) to generate more plausible distractors.\nExperiments on computer science subjects (Python, DB, MLDL) demonstrate that\nour pairwise ranker effectively identifies students' potential\nmisunderstandings and achieves ranking accuracy comparable to human experts.\nFurthermore, our distractor generator outperforms several baselines in\ngenerating plausible distractors and produces questions with a higher item\ndiscrimination index (DI).", "AI": {"tldr": "The paper presents a model for generating more plausible distractors in multiple-choice questions (MCQs) to enhance student assessment.", "motivation": "The need for high-quality distractors in MCQs to better identify misconceptions and gaps in student knowledge.", "method": "A pairwise ranker was trained to evaluate the plausibility of distractors, followed by training a distractor generator using Direct Preference Optimization (DPO) based on pairwise distractor rank data.", "result": "The pairwise ranker achieved ranking accuracy comparable to human experts, while the distractor generator outperformed several baselines in generating plausible distractors, improving item discrimination.", "conclusion": "The study demonstrates that a structured approach to distractor generation can significantly enhance the effectiveness of MCQs.", "key_contributions": ["Developed a pairwise ranker to assess distractor plausibility.", "Created a dataset of pairwise distractor ranks.", "Showed significant improvement in distractor generation effectiveness compared to existing methods."], "limitations": "", "keywords": ["multiple-choice questions", "distractor generation", "student assessment", "machine learning", "education"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2501.14956", "pdf": "https://arxiv.org/pdf/2501.14956.pdf", "abs": "https://arxiv.org/abs/2501.14956", "title": "ExPerT: Effective and Explainable Evaluation of Personalized Long-Form Text Generation", "authors": ["Alireza Salemi", "Julian Killingback", "Hamed Zamani"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Evaluating personalized text generated by large language models (LLMs) is\nchallenging, as only the LLM user, i.e., prompt author, can reliably assess the\noutput, but re-engaging the same individuals across studies is infeasible. This\npaper addresses the challenge of evaluating personalized text generation by\nintroducing ExPerT, an explainable reference-based evaluation framework. ExPerT\nleverages an LLM to extract atomic aspects and their evidence from the\ngenerated and reference texts, match the aspects, and evaluate their alignment\nbased on content and writing style -- two key attributes in personalized text\ngeneration. Additionally, ExPerT generates detailed, fine-grained explanations\nfor every step of the evaluation process, enhancing transparency and\ninterpretability. Our experiments demonstrate that ExPerT achieves a 7.2%\nrelative improvement in alignment with human judgments compared to the\nstate-of-the-art text generation evaluation methods. Furthermore, human\nevaluators rated the usability of ExPerT's explanations at 4.7 out of 5,\nhighlighting its effectiveness in making evaluation decisions more\ninterpretable.", "AI": {"tldr": "ExPerT is an evaluation framework for personalized text generation that uses LLMs to align generated texts with references, improving transparency and interpretability.", "motivation": "Evaluating personalized text generated by LLMs is difficult as only prompt authors can accurately assess output, and re-engaging them for multiple studies is impractical.", "method": "ExPerT uses an LLM to extract atomic aspects from both generated and reference texts, matching these aspects and evaluating their alignment in terms of content and writing style.", "result": "ExPerT shows a 7.2% improvement in alignment with human judgments compared to current evaluation methods, and received a usability rating of 4.7 out of 5 from human evaluators.", "conclusion": "ExPerT enhances the evaluation process of personalized text generation by providing explainable, fine-grained evaluations that improve interpretability and usability.", "key_contributions": ["Introduction of ExPerT as an explainable evaluation framework", "Improved alignment with human judgments by 7.2%", "High usability rating for generated explanations"], "limitations": "", "keywords": ["personalized text generation", "explainable AI", "evaluation framework"], "importance_score": 9, "read_time_minutes": 6}}
{"id": "2501.17182", "pdf": "https://arxiv.org/pdf/2501.17182.pdf", "abs": "https://arxiv.org/abs/2501.17182", "title": "Dialogue Systems for Emotional Support via Value Reinforcement", "authors": ["Juhee Kim", "Chunghu Mok", "Jisun Lee", "Hyang Sook Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "I.2.7"], "comment": "This paper has been accepted for publication at ACL 2025", "summary": "Emotional support dialogue systems aim to reduce help-seekers' distress and\nhelp them overcome challenges. While human values$\\unicode{x2013}$core beliefs\nthat shape an individual's priorities$\\unicode{x2013}$are increasingly\nemphasized in contemporary psychological therapy for their role in fostering\ninternal transformation and long-term emotional well-being, their integration\ninto emotional support systems remains underexplored. To bridge this gap, we\npresent a value-driven method for training emotional support dialogue systems\ndesigned to reinforce positive values in seekers. Notably, our model identifies\nwhich values to reinforce at each turn and how to do so, by leveraging online\nsupport conversations from Reddit. We evaluate the method across support\nskills, seekers' emotional intensity, and value reinforcement. Our method\nconsistently outperforms various baselines, effectively exploring and eliciting\nvalues from seekers. Additionally, leveraging crowd knowledge from Reddit\nsignificantly enhances its effectiveness. Therapists highlighted its ability to\nvalidate seekers' challenges and emphasize positive aspects of their\nsituations$\\unicode{x2013}$both crucial elements of value reinforcement. Our\nwork, being the first to integrate value reinforcement into emotional support\nsystems, demonstrates its promise and establishes a foundation for future\nresearch.", "AI": {"tldr": "This paper presents a value-driven method for training emotional support dialogue systems, focusing on reinforcing positive values in help-seekers during conversations.", "motivation": "The integration of human values in emotional support systems is underexplored despite their importance in therapeutic practices for emotional well-being.", "method": "The proposed model identifies which values to reinforce at each conversational turn by analyzing online support conversations from Reddit.", "result": "The method consistently outperforms baselines in exploring and eliciting values and effectively utilizes crowd knowledge to enhance its effectiveness.", "conclusion": "This work is the first to incorporate value reinforcement into emotional support systems, showing promise for future research.", "key_contributions": ["Value-driven training for emotional support dialogue systems", "Use of Reddit conversations for model training", "Demonstrated improvement over various baselines in value reinforcement"], "limitations": "", "keywords": ["emotional support", "dialogue systems", "value reinforcement", "human values", "machine learning"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.18251", "pdf": "https://arxiv.org/pdf/2501.18251.pdf", "abs": "https://arxiv.org/abs/2501.18251", "title": "How to Select Datapoints for Efficient Human Evaluation of NLG Models?", "authors": ["Vilém Zouhar", "Peng Cui", "Mrinmaya Sachan"], "categories": ["cs.CL"], "comment": null, "summary": "Human evaluation is the gold standard for evaluating text generation models.\nHowever, it is expensive. In order to fit budgetary constraints, a random\nsubset of the test data is often chosen in practice for human evaluation.\nHowever, randomly selected data may not accurately represent test performance,\nmaking this approach economically inefficient for model comparison. Thus, in\nthis work, we develop and analyze a suite of selectors to get the most\ninformative datapoints for human evaluation, taking the evaluation costs into\naccount. We show that selectors based on variance in automated metric scores,\ndiversity in model outputs, or Item Response Theory outperform random\nselection. We further develop an approach to distill these selectors to the\nscenario where the model outputs are not yet available. In particular, we\nintroduce source-based estimators, which predict item usefulness for human\nevaluation just based on the source texts. We demonstrate the efficacy of our\nselectors in two common NLG tasks, machine translation and summarization, and\nshow that only $\\sim$70\\% of the test data is needed to produce the same\nevaluation result as the entire data.", "AI": {"tldr": "This paper develops selectors to identify the most informative samples for human evaluation of text generation models, improving efficiency over random selection.", "motivation": "To address the inefficiency of random selection in human evaluation of text generation models, which may not accurately represent model performance while incurring high costs.", "method": "The paper analyzes various selectors based on variance in automated metrics, diversity in outputs, and Item Response Theory, along with developing source-based estimators for predicting item usefulness without model outputs.", "result": "Selectors outperforming random selection were identified, including those based on variance and diversity. The proposed methods allowed achieving comparable evaluation results with approximately 70% of the full test data.", "conclusion": "A more efficient approach to selecting informative datapoints for human evaluation in natural language generation tasks is established, resulting in significant cost savings without sacrificing evaluation quality.", "key_contributions": ["Development of selectors that improve the efficiency of human evaluation for text generation models.", "Introduction of source-based estimators for predicting item usefulness without model outputs.", "Demonstration of the efficacy of selectors in machine translation and summarization."], "limitations": "The applicability of the proposed selectors may vary across different tasks and model types beyond those tested in the study.", "keywords": ["text generation", "human evaluation", "data selection", "machine translation", "summarization"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2501.19017", "pdf": "https://arxiv.org/pdf/2501.19017.pdf", "abs": "https://arxiv.org/abs/2501.19017", "title": "Calling a Spade a Heart: Gaslighting Multimodal Large Language Models via Negation", "authors": ["Bin Zhu", "Huiyan Qi", "Yinxuan Gui", "Jingjing Chen", "Chong-Wah Ngo", "Ee-Peng Lim"], "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have exhibited remarkable\nadvancements in integrating different modalities, excelling in complex\nunderstanding and generation tasks. Despite their success, MLLMs remain\nvulnerable to conversational adversarial inputs, particularly negation\narguments. This paper systematically evaluates state-of-the-art MLLMs across\ndiverse benchmarks, revealing significant performance drops when negation\narguments are introduced to initially correct responses. Notably, we introduce\nthe first benchmark GaslightingBench, specifically designed to evaluate the\nvulnerability of MLLMs to negation arguments. GaslightingBench consists of\nmultiple-choice questions curated from existing datasets, along with generated\nnegation prompts across 20 diverse categories. Throughout extensive evaluation,\nwe find that proprietary models such as Gemini-1.5-flash, GPT-4o and\nClaude-3.5-Sonnet demonstrate better resilience compared to open-source\ncounterparts like Qwen2-VL and LLaVA. However, all evaluated MLLMs struggle to\nmaintain logical consistency under negation arguments during conversation. Our\nfindings provide critical insights for improving the robustness of MLLMs\nagainst negation inputs, contributing to the development of more reliable and\ntrustworthy multimodal AI systems.", "AI": {"tldr": "This paper evaluates the vulnerability of Multimodal Large Language Models (MLLMs) to negation arguments, introducing a benchmark called GaslightingBench.", "motivation": "To assess the resilience of MLLMs against negation arguments and to improve their logical consistency during conversations.", "method": "The study systematically evaluates various state-of-the-art MLLMs using a newly introduced benchmark, GaslightingBench, which includes multiple-choice questions and negation prompts across 20 categories.", "result": "The evaluation shows that proprietary models like Gemini-1.5-flash, GPT-4o, and Claude-3.5-Sonnet are more resilient to negation arguments than open-source models such as Qwen2-VL and LLaVA, yet all models struggle with logical consistency under such adversarial inputs.", "conclusion": "Findings highlight the need for enhancements in the robustness of MLLMs against negation arguments, fostering the development of more trustworthy multimodal AI systems.", "key_contributions": ["Introduction of GaslightingBench benchmark for evaluating MLLMs.", "Systematic evaluation of MLLMs' performance against negation arguments.", "Insights into the resilience of proprietary vs. open-source models."], "limitations": "All MLLMs evaluated struggled with maintaining logical consistency under negation inputs.", "keywords": ["Multimodal Large Language Models", "Negation arguments", "GaslightingBench", "Logical consistency", "Robustness"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.02508", "pdf": "https://arxiv.org/pdf/2502.02508.pdf", "abs": "https://arxiv.org/abs/2502.02508", "title": "Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search", "authors": ["Maohao Shen", "Guangtao Zeng", "Zhenting Qi", "Zhang-Wei Hong", "Zhenfang Chen", "Wei Lu", "Gregory Wornell", "Subhro Das", "David Cox", "Chuang Gan"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities across diverse domains. Recent studies have shown that increasing\ntest-time computation enhances LLMs' reasoning capabilities. This typically\ninvolves extensive sampling at inference time guided by an external LLM\nverifier, resulting in a two-player system. Despite external guidance, the\neffectiveness of this system demonstrates the potential of a single LLM to\ntackle complex tasks. Thus, we pose a new research problem: Can we internalize\nthe searching capabilities to fundamentally enhance the reasoning abilities of\na single LLM? This work explores an orthogonal direction focusing on\npost-training LLMs for autoregressive searching (i.e., an extended reasoning\nprocess with self-reflection and self-exploration of new strategies). To\nachieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a\ntwo-stage training paradigm: 1) a small-scale format tuning stage to\ninternalize the COAT reasoning format and 2) a large-scale self-improvement\nstage leveraging reinforcement learning. Our approach results in Satori, a 7B\nLLM trained on open-source models and data. Extensive empirical evaluations\ndemonstrate that Satori achieves state-of-the-art performance on mathematical\nreasoning benchmarks while exhibits strong generalization to out-of-domain\ntasks. Code, data, and models are fully open-sourced.", "AI": {"tldr": "This paper introduces Satori, a 7B LLM that enhances reasoning capabilities through internalized searching strategies via a novel Chain-of-Action-Thought (COAT) methodology and a two-stage training paradigm.", "motivation": "To explore how internalizing searching capabilities can fundamentally enhance the reasoning abilities of a single large language model (LLM).", "method": "The authors propose the Chain-of-Action-Thought (COAT) reasoning format and a two-stage training paradigm that includes small-scale format tuning and large-scale self-improvement using reinforcement learning.", "result": "Satori demonstrates state-of-the-art performance on mathematical reasoning benchmarks and shows strong generalization to out-of-domain tasks.", "conclusion": "The results suggest that internalizing reasoning capabilities can significantly improve LLM performance, indicating a promising direction for future LLM development.", "key_contributions": ["Introduction of the Chain-of-Action-Thought (COAT) reasoning format", "Development of a new two-stage training paradigm for LLMs", "Achieving state-of-the-art performance on reasoning benchmarks"], "limitations": "", "keywords": ["large language models", "reasoning capabilities", "reinforcement learning", "autonomous searching", "natural language processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.03678", "pdf": "https://arxiv.org/pdf/2502.03678.pdf", "abs": "https://arxiv.org/abs/2502.03678", "title": "Reflection-Window Decoding: Text Generation with Selective Refinement", "authors": ["Zeyu Tang", "Zhenhao Chen", "Xiangchen Song", "Loka Li", "Yunlong Deng", "Yifan Shen", "Guangyi Chen", "Peter Spirtes", "Kun Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 42nd International Conference on Machine\n  Learning, 2025. (ICML 2025)", "summary": "The autoregressive decoding for text generation in large language models\n(LLMs), while widely used, is inherently suboptimal due to the lack of a\nbuilt-in mechanism to perform refinement and/or correction of the generated\ncontent. In this paper, we consider optimality in terms of the joint\nprobability over the generated response, when jointly considering all tokens at\nthe same time. We theoretically characterize the potential deviation of the\nautoregressively generated response from its globally optimal counterpart that\nis of the same length. Our analysis suggests that we need to be cautious when\nnoticeable uncertainty arises during text generation, which may signal the\nsub-optimality of the generation history. To address the pitfall of\nautoregressive decoding for text generation, we propose an approach that\nincorporates a sliding reflection window and a pausing criterion, such that\nrefinement and generation can be carried out interchangeably as the decoding\nproceeds. Our selective refinement framework strikes a balance between\nefficiency and optimality, and our extensive experimental results demonstrate\nthe effectiveness of our approach.", "AI": {"tldr": "This paper addresses suboptimalities in autoregressive decoding for LLMs and proposes a selective refinement framework to improve text generation.", "motivation": "The motivation is to improve text generation in large language models by addressing the inherent suboptimality of autoregressive decoding methods, which do not refine generated content effectively.", "method": "The authors propose a selective refinement framework that incorporates a sliding reflection window and a pausing criterion for interchangeably refining and generating text during the decoding process.", "result": "Extensive experimental results demonstrate that the proposed selective refinement framework achieves a better balance between efficiency and optimality in text generation compared to traditional autoregressive methods.", "conclusion": "The proposed approach effectively mitigates the issues of suboptimal text generation in autoregressive models by allowing for real-time refinement, leading to improved overall quality of generated responses.", "key_contributions": ["Introduction of a sliding reflection window for text generation", "Development of a pausing criterion for refining generated content", "Demonstration of improved efficiency and optimality in text generation processes"], "limitations": "", "keywords": ["large language models", "autoregressive decoding", "text generation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.04795", "pdf": "https://arxiv.org/pdf/2502.04795.pdf", "abs": "https://arxiv.org/abs/2502.04795", "title": "Developmentally-plausible Working Memory Shapes a Critical Period for Language Acquisition", "authors": ["Masato Mita", "Ryo Yoshida", "Yohei Oseki"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 (main, long)", "summary": "Large language models possess general linguistic abilities but acquire\nlanguage less efficiently than humans. This study proposes a method for\nintegrating the developmental characteristics of working memory during the\ncritical period, a stage when human language acquisition is particularly\nefficient, into the training process of language models. The proposed method\nintroduces a mechanism that initially constrains working memory during the\nearly stages of training and gradually relaxes this constraint in an\nexponential manner as learning progresses. Targeted syntactic evaluation shows\nthat the proposed method outperforms conventional methods without memory\nconstraints or with static memory constraints. These findings not only provide\nnew directions for designing data-efficient language models but also offer\nindirect evidence supporting the role of the developmental characteristics of\nworking memory as the underlying mechanism of the critical period in language\nacquisition.", "AI": {"tldr": "This study proposes a method to integrate the developmental characteristics of working memory into the training of language models, enhancing data efficiency and performance.", "motivation": "To improve language model efficiency by mimicking human language acquisition processes during the critical period of development.", "method": "Integrating a mechanism that initially constrains working memory in early training stages, then relaxes it exponentially as learning progresses.", "result": "The proposed method outperforms conventional training methods without memory constraints or those with static constraints, as shown by targeted syntactic evaluation.", "conclusion": "This approach not only suggests new methods for creating data-efficient language models but also reinforces the significance of working memory in language acquisition.", "key_contributions": ["Introduction of dynamic working memory constraints during training", "Demonstrated superiority over conventional training methods", "Insights into the role of working memory in language acquisition"], "limitations": "", "keywords": ["language models", "working memory", "language acquisition", "data efficiency", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.05449", "pdf": "https://arxiv.org/pdf/2502.05449.pdf", "abs": "https://arxiv.org/abs/2502.05449", "title": "Iterative Deepening Sampling as Efficient Test-Time Scaling", "authors": ["Weizhe Chen", "Sven Koenig", "Bistra Dilkina"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent reasoning models, such as OpenAI's O1 series, have demonstrated\nexceptional performance on complex reasoning tasks and revealed new test-time\nscaling laws. Inspired by this, many people have been studying how to train\nmodels to achieve effective self-evaluation and self-correction to further\nenable the scaling paradigm. However, less studied is how to efficiently scale\ntest-time compute from a fixed model, and this remains a challenge. In this\npaper, we address this challenge by focusing on enhancing the quality of\nself-reflection data generation for complex problem-solving at test time, which\ncan also subsequently improve the training of next-generation large language\nmodels (LLMs). Specifically, we explore how systematically triggering a model's\nself-correction mechanisms can improve performance on challenging reasoning\ntasks. To this end, we propose a novel iterative deepening sampling algorithm\nframework designed to enhance self-correction and generate higher-quality\nsamples. Through extensive experiments on Math500 and AIME benchmarks, we\ndemonstrate that our method achieves a higher success rate on difficult tasks\nand provide detailed ablation studies to analyze its effectiveness across\ndiverse settings.", "AI": {"tldr": "The paper proposes a novel algorithm to enhance self-reflection data generation for complex problem-solving in LLMs.", "motivation": "To address the challenge of efficiently scaling test-time compute from fixed models by improving the quality of self-reflection data generation.", "method": "The paper introduces an iterative deepening sampling algorithm framework that systematically triggers a model's self-correction mechanisms.", "result": "The proposed method improves success rates on challenging reasoning tasks as evidenced by experiments on the Math500 and AIME benchmarks.", "conclusion": "Enhancing self-correction through the proposed framework leads to better performance in reasoning tasks and informs training for next-generation LLMs.", "key_contributions": ["Novel iterative deepening sampling algorithm framework for self-correction", "Improved performance on Math500 and AIME benchmarks", "Detailed ablation studies analyzing effectiveness across diverse settings"], "limitations": "", "keywords": ["self-correction", "LLMs", "reasoning tasks", "data generation", "iterative deepening"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.05551", "pdf": "https://arxiv.org/pdf/2502.05551.pdf", "abs": "https://arxiv.org/abs/2502.05551", "title": "FRAME: Boosting LLMs with A Four-Quadrant Multi-Stage Pretraining Strategy", "authors": ["Xuemiao Zhang", "Feiyu Duan", "Liangyu Xu", "Yongwei Zhou", "Sirui Wang", "Rongxiang Weng", "Jingang Wang", "Xunliang Cai"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have significantly advanced human language\nunderstanding and generation, with pretraining data quality and organization\nbeing crucial to their performance. Multi-stage pretraining is a promising\napproach, but existing methods often lack quantitative criteria for data\npartitioning and instead rely on intuitive heuristics. In this paper, we\npropose the novel Four-quadRAnt Multi-stage prEtraining strategy (FRAME),\nguided by the established principle of organizing the pretraining process into\nfour stages to achieve significant loss reductions four times. This principle\nis grounded in two key findings: first, training on high Perplexity (PPL) data\nfollowed by low PPL data, and second, training on low PPL difference (PD) data\nfollowed by high PD data, both causing the loss to drop significantly twice and\nperformance enhancements. By partitioning data into four quadrants and\nstrategically organizing them, FRAME achieves a remarkable 16.8% average\nimprovement over random across MMLU and CMMLU for the 3B model, effectively\nboosting LLM performance.", "AI": {"tldr": "The paper introduces FRAME, a Four-quadRAnt Multi-stage pretraining strategy for large language models, aiming to enhance performance through strategic data partitioning.", "motivation": "To improve the pretraining process of LLMs by providing a systematic method for data partitioning instead of relying solely on intuitive heuristics.", "method": "The FRAME strategy organizes pretraining into four stages based on high and low perplexity data, using two key findings to systematically partition the data.", "result": "FRAME achieves a 16.8% average improvement over random data organization across MMLU and CMMLU benchmarks for the 3B model.", "conclusion": "By implementing FRAME, the study shows significant performance improvements in LLMs due to systematic data organization.", "key_contributions": ["Introduction of the FRAME strategy for LLM pretraining", "Quantitative criteria for data partitioning", "Demonstrated significant performance enhancements in benchmark tasks"], "limitations": "", "keywords": ["large language models", "pretraining", "data partitioning", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.06204", "pdf": "https://arxiv.org/pdf/2502.06204.pdf", "abs": "https://arxiv.org/abs/2502.06204", "title": "Non-literal Understanding of Number Words by Language Models", "authors": ["Polina Tsvilodub", "Kanishk Gandhi", "Haoran Zhao", "Jan-Philipp Fränken", "Michael Franke", "Noah D. Goodman"], "categories": ["cs.CL"], "comment": "12 pages, 10 figures. To appear in the Proceedings of CogSci 2025", "summary": "Humans naturally interpret numbers non-literally, effortlessly combining\ncontext, world knowledge, and speaker intent. We investigate whether large\nlanguage models (LLMs) interpret numbers similarly, focusing on hyperbole and\npragmatic halo effects. Through systematic comparison with human data and\ncomputational models of pragmatic reasoning, we find that LLMs diverge from\nhuman interpretation in striking ways. By decomposing pragmatic reasoning into\ntestable components, grounded in the Rational Speech Act framework, we pinpoint\nwhere LLM processing diverges from human cognition -- not in prior knowledge,\nbut in reasoning with it. This insight leads us to develop a targeted solution\n-- chain-of-thought prompting inspired by an RSA model makes LLMs'\ninterpretations more human-like. Our work demonstrates how computational\ncognitive models can both diagnose AI-human differences and guide development\nof more human-like language understanding capabilities.", "AI": {"tldr": "The paper explores how large language models (LLMs) interpret numbers and identifies key differences in reasoning compared to human cognition. It proposes chain-of-thought prompting to enhance LLM interpretative capabilities.", "motivation": "Investigate whether large language models (LLMs) interpret numbers non-literally, as humans do, especially in the context of hyperbole and pragmatic effects.", "method": "Systematic comparison of LLM interpretations with human data and computational models of pragmatic reasoning, utilizing the Rational Speech Act framework to test components of reasoning.", "result": "LLMs diverge from human interpretation in significant ways; however, chain-of-thought prompting based on the RSA model improves LLMs' interpretations to align more closely with human-like understanding.", "conclusion": "The work highlights that computational cognitive models can diagnose differences in interpretation between AI and humans and guide improvements in language understanding in AI systems.", "key_contributions": ["Identified key divergence in numerical interpretation between LLMs and humans", "Developed and tested chain-of-thought prompting to improve LLM performance", "Established a framework for assessing LLM interpretations via human-like reasoning"], "limitations": "", "keywords": ["large language models", "human interpretation", "pragmatic reasoning", "Rational Speech Act", "chain-of-thought prompting"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2502.06560", "pdf": "https://arxiv.org/pdf/2502.06560.pdf", "abs": "https://arxiv.org/abs/2502.06560", "title": "Position: It's Time to Act on the Risk of Efficient Personalized Text Generation", "authors": ["Eugenia Iofinova", "Andrej Jovanovic", "Dan Alistarh"], "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The recent surge in high-quality open-source Generative AI text models\n(colloquially: LLMs), as well as efficient finetuning techniques, have opened\nthe possibility of creating high-quality personalized models that generate text\nattuned to a specific individual's needs and are capable of credibly imitating\ntheir writing style by refining an open-source model with that person's own\ndata. The technology to create such models is accessible to private\nindividuals, and training and running such models can be done cheaply on\nconsumer-grade hardware. While these advancements are a huge gain for usability\nand privacy, this position paper argues that the practical feasibility of\nimpersonating specific individuals also introduces novel safety risks. For\ninstance, this technology enables the creation of phishing emails or fraudulent\nsocial media accounts, based on small amounts of publicly available text, or by\nthe individuals themselves to escape AI text detection. We further argue that\nthese risks are complementary to - and distinct from - the much-discussed risks\nof other impersonation attacks such as image, voice, or video deepfakes, and\nare not adequately addressed by the larger research community, or the current\ngeneration of open- and closed-source models.", "AI": {"tldr": "This paper discusses the emergence of personalized generative AI text models and the associated safety risks of impersonation.", "motivation": "To highlight the potential misuse of personalized generative AI models in creating convincing impersonations that pose safety risks.", "method": "The paper analyzes the capabilities of fine-tuning open-source LLMs to create personalized models and outlines the implications for safety and privacy.", "result": "The authors identify that such technologies facilitate the creation of phishing schemes and deepfake text easily, raising concerns that are not currently addressed by existing research.", "conclusion": "The risks associated with LLM-based impersonation require distinct consideration from other deepfake technologies, highlighting a gap in current research.", "key_contributions": ["Introduction of safety risks from personalized LLMs", "Comparative analysis with traditional deepfake risks", "Call for focused research on text-based impersonation risks"], "limitations": "The paper does not provide empirical studies or data to support the claims about risks.", "keywords": ["Generative AI", "LLM", "personalization", "impersonation", "safety risks"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.06851", "pdf": "https://arxiv.org/pdf/2502.06851.pdf", "abs": "https://arxiv.org/abs/2502.06851", "title": "Survey on Vision-Language-Action Models", "authors": ["Adilzhan Adilkhanov", "Amir Yelenov", "Assylkhan Seitzhanov", "Ayan Mazhitov", "Azamat Abdikarimov", "Danissa Sandykbayeva", "Daryn Kenzhebek", "Dinmukhammed Mukashev", "Ilyas Umurbekov", "Jabrail Chumakov", "Kamila Spanova", "Karina Burunchina", "Madina Yergibay", "Margulan Issa", "Moldir Zabirova", "Nurdaulet Zhuzbay", "Nurlan Kabdyshev", "Nurlan Zhaniyar", "Rasul Yermagambet", "Rustam Chibar", "Saltanat Seitzhan", "Soibkhon Khajikhanov", "Tasbolat Taunyazov", "Temirlan Galimzhanov", "Temirlan Kaiyrbay", "Tleukhan Mussin", "Togzhan Syrymova", "Valeriya Kostyukova", "Yerkebulan Massalim", "Yermakhan Kassym", "Zerde Nurbayeva", "Zhanat Kappassov"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "arXiv admin note: This submission has been withdrawn due to serious\n  violation of arXiv policies for acceptable submissions", "summary": "This paper presents an AI-generated review of Vision-Language-Action (VLA)\nmodels, summarizing key methodologies, findings, and future directions. The\ncontent is produced using large language models (LLMs) and is intended only for\ndemonstration purposes. This work does not represent original research, but\nhighlights how AI can help automate literature reviews. As AI-generated content\nbecomes more prevalent, ensuring accuracy, reliability, and proper synthesis\nremains a challenge. Future research will focus on developing a structured\nframework for AI-assisted literature reviews, exploring techniques to enhance\ncitation accuracy, source credibility, and contextual understanding. By\nexamining the potential and limitations of LLM in academic writing, this study\naims to contribute to the broader discussion of integrating AI into research\nworkflows. This work serves as a preliminary step toward establishing\nsystematic approaches for leveraging AI in literature review generation, making\nacademic knowledge synthesis more efficient and scalable.", "AI": {"tldr": "The paper presents an AI-generated review of Vision-Language-Action (VLA) models, emphasizing the role of LLMs in automating literature reviews and outlining future research directions.", "motivation": "This study aims to address the challenges of accuracy and reliability in AI-generated literature reviews, particularly in the context of VLA models.", "method": "The paper employs AI-generated content to summarize methodologies and findings related to Vision-Language-Action models for literature review purposes.", "result": "Key findings include the potential for AI to streamline literature reviews, while also highlighting the need for improved accuracy and credibility of AI-generated citations.", "conclusion": "The paper concludes that integrating AI tools can enhance the efficiency of academic knowledge synthesis but suggests further research to establish structured frameworks for this application.", "key_contributions": ["Demonstration of AI's role in automating literature reviews", "Identification of challenges related to citation accuracy and source credibility", "Proposal for future research directions in AI-assisted literature reviews"], "limitations": "The content is not based on original research and has been withdrawn due to policy violations.", "keywords": ["AI-generated content", "literature review", "Vision-Language-Action", "large language models", "academic workflows"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.08092", "pdf": "https://arxiv.org/pdf/2502.08092.pdf", "abs": "https://arxiv.org/abs/2502.08092", "title": "GCoT: Chain-of-Thought Prompt Learning for Graphs", "authors": ["Xingtong Yu", "Chang Zhou", "Zhongwei Kuai", "Xinming Zhang", "Yuan Fang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by SIGKDD2025", "summary": "Chain-of-thought (CoT) prompting has achieved remarkable success in natural\nlanguage processing (NLP). However, its vast potential remains largely\nunexplored for graphs. This raises an interesting question: How can we design\nCoT prompting for graphs to guide graph models to learn step by step? On one\nhand, unlike natural languages, graphs are non-linear and characterized by\ncomplex topological structures. On the other hand, many graphs lack textual\ndata, making it difficult to formulate language-based CoT prompting. In this\nwork, we propose the first CoT prompt learning framework for text-free graphs,\nGCoT. Specifically, we decompose the adaptation process for each downstream\ntask into a series of inference steps, with each step consisting of\nprompt-based inference, ``thought'' generation, and thought-conditioned prompt\nlearning. While the steps mimic CoT prompting in NLP, the exact mechanism\ndiffers significantly. Specifically, at each step, an input graph, along with a\nprompt, is first fed into a pre-trained graph encoder for prompt-based\ninference. We then aggregate the hidden layers of the encoder to construct a\n``thought'', which captures the working state of each node in the current step.\nConditioned on this thought, we learn a prompt specific to each node based on\nthe current state. These prompts are fed into the next inference step,\nrepeating the cycle. To evaluate and analyze the effectiveness of GCoT, we\nconduct comprehensive experiments on eight public datasets, which demonstrate\nthe advantage of our approach.", "AI": {"tldr": "This paper presents GCoT, a novel framework for chain-of-thought prompting in text-free graphs, enabling step-by-step learning for graph models.", "motivation": "To explore the application of chain-of-thought prompting in graphs, which have non-linear structures and often lack textual data.", "method": "The GCoT framework decomposes adaptation into multiple inference steps involving prompt-based inference, thought generation, and thought-conditioned prompt learning, utilizing a pre-trained graph encoder.", "result": "Experiments on eight public datasets showcase the effectiveness and advantages of the proposed GCoT framework compared to existing methods.", "conclusion": "GCoT enables guided learning in text-free graphs, mimicking chain-of-thought prompting in NLP but adapted to the unique structure of graphs.", "key_contributions": ["First framework for chain-of-thought prompting in text-free graphs", "Decomposed learning process into inference steps", "Demonstrated effectiveness on multiple datasets"], "limitations": "", "keywords": ["chain-of-thought", "prompt learning", "graphs", "NLP", "GCoT"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.08561", "pdf": "https://arxiv.org/pdf/2502.08561.pdf", "abs": "https://arxiv.org/abs/2502.08561", "title": "Quality-Aware Decoding: Unifying Quality Estimation and Decoding", "authors": ["Sai Koneru", "Matthias Huck", "Miriam Exel", "Jan Niehues"], "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "Quality Estimation (QE) models for Neural Machine Translation (NMT) predict\nthe quality of the hypothesis without having access to the reference. An\nemerging research direction in NMT involves the use of QE models, which have\ndemonstrated high correlations with human judgment and can enhance translations\nthrough Quality-Aware Decoding. Although several approaches have been proposed\nbased on sampling multiple candidate translations and picking the best\ncandidate, none have integrated these models directly into the decoding\nprocess. In this paper, we address this by proposing a novel token-level QE\nmodel capable of reliably scoring partial translations. We build a\nuni-directional QE model for this, as decoder models are inherently trained and\nefficient on partial sequences. We then present a decoding strategy that\nintegrates the QE model for Quality-Aware decoding and demonstrate that the\ntranslation quality improves when compared to the N-best list re-ranking with\nstate-of-the-art QE models (up to $1.39$ XCOMET-XXL $\\uparrow$). Finally, we\nshow that our approach provides significant benefits in document translation\ntasks, where the quality of N-best lists is typically suboptimal. Code can be\nfound at https://ai4lt.iar.kit.edu/english/projects\\_kontextmt.php", "AI": {"tldr": "This paper proposes a novel token-level Quality Estimation model for Neural Machine Translation that enhances translation quality through Quality-Aware Decoding, improving performance on document translation tasks.", "motivation": "The integration of Quality Estimation models into Neural Machine Translation decoding processes aims to improve translation quality in scenarios where reference translations are not available.", "method": "A uni-directional Quality Estimation model is developed that scores partial translations, coupled with a decoding strategy that incorporates this model for Quality-Aware decoding.", "result": "The proposed method shows improved translation quality, outperforming standard N-best list re-ranking techniques with state-of-the-art Quality Estimation models, achieving up to 1.39 XCOMET-XXL increase.", "conclusion": "The integration of the token-level QE model not only enhances overall translation quality but also significantly benefits tasks such as document translation where traditional methods struggle.", "key_contributions": ["Novel token-level QE model for partial translations", "Decoding strategy integrating QE for Quality-Aware decoding", "Demonstrated significant improvements in document translation tasks"], "limitations": "", "keywords": ["Quality Estimation", "Neural Machine Translation", "Quality-Aware Decoding"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.08662", "pdf": "https://arxiv.org/pdf/2502.08662.pdf", "abs": "https://arxiv.org/abs/2502.08662", "title": "RoToR: Towards More Reliable Responses for Order-Invariant Inputs", "authors": ["Soyoung Yoon", "Dongha Ahn", "Youngwon Lee", "Minkyu Jung", "HyungJoo Jang", "Seung-won Hwang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 main", "summary": "Mitigating positional bias of language models (LMs) for listwise inputs is a\nwell-known and important problem (e.g., lost-in-the-middle). While zero-shot\norder-invariant LMs have been proposed to solve this issue, their success on\npractical listwise problems has been limited. In this work, as a first\ncontribution, we identify and overcome two limitations to make zero-shot\ninvariant LMs more practical: (1) training and inference distribution mismatch\narising from modifying positional ID assignments to enforce invariance, and (2)\nfailure to adapt to mixture of order-invariant and sensitive inputs in\npractical listwise problems. Then, to overcome these issues we propose (1)\nRoToR, a zero-shot invariant LM for genuinely order-invariant inputs with\nminimal modifications of positional IDs, and (2) Selective Routing, an adaptive\nframework that handles both order-invariant and order-sensitive inputs in\nlistwise tasks. On the Lost in the middle (LitM), Knowledge Graph QA (KGQA),\nand MMLU benchmarks, we show that RoToR with Selective Routing can effectively\nhandle practical listwise input tasks in a zero-shot manner\n(https://github.com/soyoung97/RoToR)", "AI": {"tldr": "This paper addresses the limitations of zero-shot order-invariant language models for listwise inputs and proposes practical solutions to enhance their effectiveness.", "motivation": "Mitigating positional bias in language models for listwise inputs is crucial, especially to avoid issues like lost-in-the-middle, yet existing solutions have had limited practical success.", "method": "The authors propose RoToR, a zero-shot invariant language model that minimizes modifications to positional IDs, and Selective Routing, an adaptive framework that accommodates both order-invariant and order-sensitive inputs for listwise tasks.", "result": "RoToR combined with Selective Routing demonstrated effective handling of practical listwise input tasks across several benchmarks, including LitM, KGQA, and MMLU.", "conclusion": "The proposed solutions significantly improve the practicality of zero-shot invariant language models for listwise inputs, addressing key limitations in existing approaches.", "key_contributions": ["Introduction of RoToR, a zero-shot invariant language model with minimal positional ID modifications.", "Development of Selective Routing, an adaptive framework for mixed order-inputs handling.", "Comprehensive benchmarking on LitM, KGQA, and MMLU demonstrating improved practical applicability."], "limitations": "", "keywords": ["language models", "positional bias", "order-invariant", "listwise inputs", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.08826", "pdf": "https://arxiv.org/pdf/2502.08826.pdf", "abs": "https://arxiv.org/abs/2502.08826", "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation", "authors": ["Mohammad Mahdi Abootorabi", "Amirhosein Zobeiri", "Mahdi Dehghani", "Mohammadali Mohammadkhani", "Bardia Mohammadi", "Omid Ghahroodi", "Mahdieh Soleymani Baghshah", "Ehsaneddin Asgari"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "GitHub repository:\n  https://github.com/llm-lab-org/Multimodal-RAG-Survey", "summary": "Large Language Models (LLMs) suffer from hallucinations and outdated\nknowledge due to their reliance on static training data. Retrieval-Augmented\nGeneration (RAG) mitigates these issues by integrating external dynamic\ninformation for improved factual grounding. With advances in multimodal\nlearning, Multimodal RAG extends this approach by incorporating multiple\nmodalities such as text, images, audio, and video to enhance the generated\noutputs. However, cross-modal alignment and reasoning introduce unique\nchallenges beyond those in unimodal RAG. This survey offers a structured and\ncomprehensive analysis of Multimodal RAG systems, covering datasets,\nbenchmarks, metrics, evaluation, methodologies, and innovations in retrieval,\nfusion, augmentation, and generation. We review training strategies, robustness\nenhancements, loss functions, and agent-based approaches, while also exploring\nthe diverse Multimodal RAG scenarios. In addition, we outline open challenges\nand future directions to guide research in this evolving field. This survey\nlays the foundation for developing more capable and reliable AI systems that\neffectively leverage multimodal dynamic external knowledge bases. All resources\nare publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.", "AI": {"tldr": "This survey analyzes Multimodal Retrieval-Augmented Generation (RAG), highlighting its benefits in combining diverse modalities like text and images to improve factual grounding in Large Language Models (LLMs).", "motivation": "Multimodal RAG addresses hallucinations and outdated knowledge in LLMs by integrating dynamic external information across various formats.", "method": "The survey reviews existing datasets, benchmarks, evaluation metrics, methodologies, and recent innovations in the Multimodal RAG space, while discussing training strategies and challenges.", "result": "Multimodal RAG enhances AI outputs significantly by leveraging multiple modalities, although it faces unique challenges in cross-modal alignment and reasoning.", "conclusion": "The paper provides insights into the future of AI systems that utilize multimodal dynamic knowledge bases, emphasizing the need for further research in this area.", "key_contributions": ["Comprehensive analysis of Multimodal RAG systems", "Identification of open challenges and future research directions", "Review of methodologies and innovations in multimodal interactions"], "limitations": "", "keywords": ["Multimodal RAG", "Retrieval-Augmented Generation", "Large Language Models", "AI systems", "Dynamic knowledge bases"], "importance_score": 9, "read_time_minutes": 30}}
{"id": "2502.08900", "pdf": "https://arxiv.org/pdf/2502.08900.pdf", "abs": "https://arxiv.org/abs/2502.08900", "title": "Can Uniform Meaning Representation Help GPT-4 Translate from Indigenous Languages?", "authors": ["Shira Wein"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "While ChatGPT and GPT-based models are able to effectively perform many tasks\nwithout additional fine-tuning, they struggle with tasks related to extremely\nlow-resource languages and indigenous languages. Uniform Meaning Representation\n(UMR), a semantic representation designed to capture the meaning of texts in\nmany languages, is well-positioned to be leveraged in the development of\nlow-resource language technologies. In this work, we explore the downstream\nutility of UMR for low-resource languages by incorporating it into GPT-4\nprompts. Specifically, we examine the ability of GPT-4 to perform translation\nfrom three indigenous languages (Navajo, Ar\\'apaho, and Kukama), with and\nwithout demonstrations, as well as with and without UMR annotations.\nUltimately, we find that in the majority of our test cases, integrating UMR\ninto the prompt results in a statistically significant increase in performance,\nwhich is a promising indication of future applications of the UMR formalism.", "AI": {"tldr": "This study evaluates the effectiveness of Uniform Meaning Representation (UMR) in enhancing GPT-4's performance on low-resource and indigenous language translations.", "motivation": "To address the limitations of GPT-based models in handling low-resource languages and indigenous languages by leveraging UMR for better performance.", "method": "The research involves testing GPT-4's translation capabilities for three indigenous languages with and without the integration of UMR annotations in the prompts.", "result": "Integrating UMR into the prompts led to a statistically significant improvement in translation performance for indigenous languages.", "conclusion": "The positive results suggest that UMR could be a useful tool for advancing technologies in low-resource language contexts.", "key_contributions": ["Introduction of UMR to enhance translation tasks in low-resource languages", "Empirical evaluation of UMR's effectiveness with GPT-4 for indigenous languages", "Demonstration of statistical improvement in performance with UMR incorporation"], "limitations": "", "keywords": ["Uniform Meaning Representation", "GPT-4", "low-resource languages", "indigenous languages", "translation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.10201", "pdf": "https://arxiv.org/pdf/2502.10201.pdf", "abs": "https://arxiv.org/abs/2502.10201", "title": "Prediction hubs are context-informed frequent tokens in LLMs", "authors": ["Beatrix M. G. Nielsen", "Iuri Macocco", "Marco Baroni"], "categories": ["cs.CL", "cs.AI"], "comment": "Published as a conference paper at ACL 2025", "summary": "Hubness, the tendency for a few points to be among the nearest neighbours of\na disproportionate number of other points, commonly arises when applying\nstandard distance measures to high-dimensional data, often negatively impacting\ndistance-based analysis. As autoregressive large language models (LLMs) operate\non high-dimensional representations, we ask whether they are also affected by\nhubness. We first prove that the only large-scale representation comparison\noperation performed by LLMs, namely that between context and unembedding\nvectors to determine continuation probabilities, is not characterized by the\nconcentration of distances phenomenon that typically causes the appearance of\nnuisance hubness. We then empirically show that this comparison still leads to\na high degree of hubness, but the hubs in this case do not constitute a\ndisturbance. They are rather the result of context-modulated frequent tokens\noften appearing in the pool of likely candidates for next token prediction.\nHowever, when other distances are used to compare LLM representations, we do\nnot have the same theoretical guarantees, and, indeed, we see nuisance hubs\nappear. There are two main takeaways. First, hubness, while omnipresent in\nhigh-dimensional spaces, is not a negative property that needs to be mitigated\nwhen LLMs are being used for next token prediction. Second, when comparing\nrepresentations from LLMs using Euclidean or cosine distance, there is a high\nrisk of nuisance hubs and practitioners should use mitigation techniques if\nrelevant.", "AI": {"tldr": "This paper investigates the phenomenon of hubness in autoregressive large language models (LLMs), concluding that while hubness exists in high-dimensional data, it does not adversely affect next token prediction but poses risks in representation comparisons using certain distance metrics.", "motivation": "To explore how hubness impacts the performance of large language models when processing and comparing high-dimensional representations.", "method": "The authors prove theoretical properties of the representation comparisons made by LLMs and conduct empirical analyses to observe the effects of hubness.", "result": "They found that while hubness is prevalent, it does not negatively affect next token prediction in LLMs. However, other distance measures can lead to nuisance hubs, which may require mitigation techniques.", "conclusion": "Hubness is common in high-dimensional spaces but is not detrimental for LLM token prediction; however, caution is advised when using Euclidean or cosine distances for comparing representations.", "key_contributions": ["Proved that token prediction is unaffected by hubness in LLMs", "Demonstrated existence of nuisance hubs with alternative distance measures", "Provided guidelines for practitioners on mitigating hubness risks"], "limitations": "", "keywords": ["Hubness", "Large Language Models", "Distance Measures", "Token Prediction", "Nuisance Hubs"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.11095", "pdf": "https://arxiv.org/pdf/2502.11095.pdf", "abs": "https://arxiv.org/abs/2502.11095", "title": "A Survey of Large Language Models in Psychotherapy: Current Landscape and Future Directions", "authors": ["Hongbin Na", "Yining Hua", "Zimu Wang", "Tao Shen", "Beibei Yu", "Lilin Wang", "Wei Wang", "John Torous", "Ling Chen"], "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 Findings", "summary": "Mental health is increasingly critical in contemporary healthcare, with\npsychotherapy demanding dynamic, context-sensitive interactions that\ntraditional NLP methods struggle to capture. Large Language Models (LLMs) offer\nsignificant potential for addressing this gap due to their ability to handle\nextensive context and multi-turn reasoning. This review introduces a conceptual\ntaxonomy dividing psychotherapy into interconnected stages--assessment,\ndiagnosis, and treatment--to systematically examine LLM advancements and\nchallenges. Our comprehensive analysis reveals imbalances in current research,\nsuch as a focus on common disorders, linguistic biases, fragmented methods, and\nlimited theoretical integration. We identify critical challenges including\ncapturing dynamic symptom fluctuations, overcoming linguistic and cultural\nbiases, and ensuring diagnostic reliability. Highlighting future directions, we\nadvocate for continuous multi-stage modeling, real-time adaptive systems\ngrounded in psychological theory, and diversified research covering broader\nmental disorders and therapeutic approaches, aiming toward more holistic and\nclinically integrated psychotherapy LLMs systems.", "AI": {"tldr": "This paper reviews the application of Large Language Models (LLMs) in psychotherapy, highlighting the need for context-sensitive interactions and identifying key challenges in current research.", "motivation": "To address the limitations of traditional NLP methods in handling the dynamic and context-sensitive nature of psychotherapy, exploring how LLMs can bridge this gap.", "method": "A conceptual taxonomy of psychotherapy stages—assessment, diagnosis, and treatment—is introduced to systematically review LLM advancements and challenges.", "result": "The analysis reveals research imbalances, including a focus on common disorders and limitations such as linguistic biases and fragmented methodologies.", "conclusion": "Future research should focus on continuous multi-stage modeling and real-time adaptive systems based on psychological theory, expanding to cover a broader range of mental disorders and therapeutic methods.", "key_contributions": ["Introduces a conceptual taxonomy for psychotherapy stages to guide LLM application.", "Identifies critical challenges faced by current LLMs in psychotherapy.", "Advocates for a more holistic approach in integrating LLMs into clinical practice."], "limitations": "Current research is imbalanced, focusing primarily on common disorders and lacking integration of diverse therapeutic approaches.", "keywords": ["Large Language Models", "Psychotherapy", "Mental Health", "NLP", "Dynamic Interactions"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.11368", "pdf": "https://arxiv.org/pdf/2502.11368.pdf", "abs": "https://arxiv.org/abs/2502.11368", "title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing", "authors": ["Zhengxiang Wang", "Veronika Makarova", "Zhi Li", "Jordan Kodner", "Owen Rambow"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The paper explores the performance of LLMs in the context of\nmulti-dimensional analytic writing assessments, i.e. their ability to provide\nboth scores and comments based on multiple assessment criteria. Using a corpus\nof literature reviews written by L2 graduate students and assessed by human\nexperts against 9 analytic criteria, we prompt several popular LLMs to perform\nthe same task under various conditions. To evaluate the quality of feedback\ncomments, we apply a novel feedback comment quality evaluation framework. This\nframework is interpretable, cost-efficient, scalable, and reproducible,\ncompared to existing methods that rely on manual judgments. We find that LLMs\ncan generate reasonably good and generally reliable multi-dimensional analytic\nassessments. We release our corpus and code for reproducibility.", "AI": {"tldr": "This paper examines large language models' (LLMs) performance in multi-dimensional analytic writing assessments, focusing on their ability to provide scores and comments based on various criteria.", "motivation": "To investigate the capability of LLMs in generating reliable feedback for writing assessments that involve multiple criteria, which could enhance educational tools.", "method": "LLMs were prompted to assess literature reviews written by L2 graduate students, comparing their outputs against assessments made by human experts using a novel feedback comment quality evaluation framework.", "result": "LLMs demonstrated the ability to produce reasonably good and generally reliable multi-dimensional analytic assessments, outperforming some existing manual methods.", "conclusion": "The study suggests that LLMs can effectively contribute to writing assessments by providing both scores and constructive comments, making them useful for educational applications.", "key_contributions": ["Introduction of a novel feedback comment quality evaluation framework", "Demonstration of LLMs' effectiveness in multi-dimensional assessments", "Release of dataset and code for future research"], "limitations": "The study primarily focuses on L2 graduate students, which may limit the generalizability of the findings to other populations.", "keywords": ["Large Language Models", "Writing Assessment", "Feedback Quality", "Human-Computer Interaction", "Educational Technology"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11423", "pdf": "https://arxiv.org/pdf/2502.11423.pdf", "abs": "https://arxiv.org/abs/2502.11423", "title": "Exploring Persona Sentiment Sensitivity in Personalized Dialogue Generation", "authors": ["Yonghyun Jun", "Hwanhee Lee"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Personalized dialogue systems have advanced considerably with the integration\nof user-specific personas into large language models (LLMs). However, while\nLLMs can effectively generate personalized responses, the influence of persona\nsentiment on dialogue quality remains underexplored. In this work, we conduct a\nlarge-scale analysis of dialogues generated using a range of polarized user\nprofiles. Our experiments reveal that dialogues involving negatively polarized\nusers tend to overemphasize persona attributes. In contrast, positively\npolarized profiles yield dialogues that selectively incorporate persona\ninformation, resulting in smoother interactions. Furthermore, we find that\npersonas with weak or neutral sentiment generally produce lower-quality\ndialogues. Motivated by these findings, we propose a dialogue generation\napproach that explicitly accounts for persona polarity by combining a\nturn-based generation strategy with a profile ordering mechanism and\nsentiment-aware prompting. Our study provides new insights into the sensitivity\nof LLMs to persona sentiment and offers guidance for developing more robust and\nnuanced personalized dialogue systems.", "AI": {"tldr": "This paper analyzes the influence of user persona sentiment on the quality of dialogues generated by large language models (LLMs) and proposes an improved dialogue generation method.", "motivation": "The study investigates how user-specific persona sentiment affects the quality of interactions in personalized dialogue systems.", "method": "The analysis involves conducting large-scale experiments with various polarized user profiles to evaluate dialogue quality and the effects of sentiment. A novel dialogue generation approach was introduced that combines turn-based generation, profile ordering, and sentiment-aware prompting.", "result": "Findings reveal that negative personas exacerbate dialogue attributes, while positive personas result in smoother interactions. Weak or neutral sentiments in personas lead to lower dialogue quality.", "conclusion": "By accounting for persona polarity, the proposed method enhances the generation of higher-quality dialogues, providing insights for developing more nuanced dialogue systems.", "key_contributions": ["Analysis of timing and quality of dialogues relative to user persona sentiment", "Introduction of a turn-based generation strategy", "Development of a sentiment-aware dialogue generation approach"], "limitations": "", "keywords": ["dialogue systems", "personalization", "large language models", "sentiment analysis", "HCI"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11469", "pdf": "https://arxiv.org/pdf/2502.11469.pdf", "abs": "https://arxiv.org/abs/2502.11469", "title": "If Attention Serves as a Cognitive Model of Human Memory Retrieval, What is the Plausible Memory Representation?", "authors": ["Ryo Yoshida", "Shinnosuke Isono", "Kohei Kajikawa", "Taiga Someya", "Yushi Sugimoto", "Yohei Oseki"], "categories": ["cs.CL"], "comment": "18 pages; To appear in ACL 2025", "summary": "Recent work in computational psycholinguistics has revealed intriguing\nparallels between attention mechanisms and human memory retrieval, focusing\nprimarily on vanilla Transformers that operate on token-level representations.\nHowever, computational psycholinguistic research has also established that\nsyntactic structures provide compelling explanations for human sentence\nprocessing that token-level factors cannot fully account for. In this paper, we\ninvestigate whether the attention mechanism of Transformer Grammar (TG), which\nuniquely operates on syntactic structures as representational units, can serve\nas a cognitive model of human memory retrieval, using Normalized Attention\nEntropy (NAE) as a linking hypothesis between models and humans. Our\nexperiments demonstrate that TG's attention achieves superior predictive power\nfor self-paced reading times compared to vanilla Transformer's, with further\nanalyses revealing independent contributions from both models. These findings\nsuggest that human sentence processing involves dual memory representations --\none based on syntactic structures and another on token sequences -- with\nattention serving as the general memory retrieval algorithm, while highlighting\nthe importance of incorporating syntactic structures as representational units.", "AI": {"tldr": "This paper explores the attention mechanism of Transformer Grammar (TG) as a cognitive model for human memory retrieval, emphasizing the role of syntactic structures.", "motivation": "To investigate parallels between attention mechanisms in Transformers and human memory retrieval, particularly using syntactic structures as a representational unit.", "method": "The study employs Normalized Attention Entropy (NAE) to assess the attention mechanisms of TG compared to traditional Transformers, analyzing self-paced reading times as a measure of predictive power.", "result": "TG's attention mechanism shows superior predictive accuracy for reading times over vanilla Transformers, indicating that human sentence processing relies on dual memory representations.", "conclusion": "The findings imply that effective human sentence processing combines memory representations based on both syntactic structures and token sequences, with attention functioning as a general memory retrieval algorithm.", "key_contributions": ["Introduced Transformer Grammar (TG) as a cognitive model for memory retrieval.", "Demonstrated the superior predictive power of TG's attention in reading time analysis.", "Highlighted the dual role of memory representations in human sentence processing."], "limitations": "", "keywords": ["Transformer Grammar", "attention mechanism", "human memory retrieval", "syntactic structures", "natural language processing"], "importance_score": 6, "read_time_minutes": 18}}
{"id": "2502.11671", "pdf": "https://arxiv.org/pdf/2502.11671.pdf", "abs": "https://arxiv.org/abs/2502.11671", "title": "Diversity-oriented Data Augmentation with Large Language Models", "authors": ["Zaitian Wang", "Jinghan Zhang", "Xinhao Zhang", "Kunpeng Liu", "Pengfei Wang", "Yuanchun Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "Data augmentation is an essential technique in natural language processing\n(NLP) for enriching training datasets by generating diverse samples. This\nprocess is crucial for improving the robustness and generalization capabilities\nof NLP models. However, a significant challenge remains: \\textit{Insufficient\nAttention to Sample Distribution Diversity}. Most existing methods focus on\nincreasing the sample numbers while neglecting the sample distribution\ndiversity, which can lead to model overfitting. In response, we explore data\naugmentation's impact on dataset diversity and propose a\n\\textbf{\\underline{D}}iversity-\\textbf{\\underline{o}}riented data\n\\textbf{\\underline{Aug}}mentation framework (\\textbf{DoAug}). %\n\\(\\mathscr{DoAug}\\) Specifically, we utilize a diversity-oriented fine-tuning\napproach to train an LLM as a diverse paraphraser, which is capable of\naugmenting textual datasets by generating diversified paraphrases. Then, we\napply the LLM paraphraser to a selected coreset of highly informative samples\nand integrate the paraphrases with the original data to create a more diverse\naugmented dataset. Finally, we conduct extensive experiments on 12 real-world\ntextual datasets. The results show that our fine-tuned LLM augmenter improves\ndiversity while preserving label consistency, thereby enhancing the robustness\nand performance of downstream tasks. Specifically, it achieves an average\nperformance gain of \\(10.52\\%\\), surpassing the runner-up baseline with more\nthan three percentage points.", "AI": {"tldr": "The paper presents DoAug, a diversity-oriented data augmentation framework that improves the robustness of NLP models by enhancing sample distribution diversity through LLM-generated paraphrases.", "motivation": "To address the challenge of insufficient attention to sample distribution diversity in data augmentation, which can lead to model overfitting.", "method": "The authors developed a diversity-oriented fine-tuning approach for training an LLM to generate diverse paraphrases. The model augments a selected coreset of informative samples and integrates these paraphrases with original data to create a more diverse dataset.", "result": "Experiments conducted on 12 real-world textual datasets demonstrated that the DoAug framework improves data diversity, preserves label consistency, and offers an average performance gain of 10.52%.", "conclusion": "The proposed method enhances the robustness and performance of NLP models through effective data augmentation strategies focused on diversity.", "key_contributions": ["Development of the DoAug framework", "Introduction of a diversity-oriented fine-tuning approach", "Demonstration of significant performance gains on diverse datasets"], "limitations": "", "keywords": ["data augmentation", "diversity", "paraphrasing", "NLP", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.11735", "pdf": "https://arxiv.org/pdf/2502.11735.pdf", "abs": "https://arxiv.org/abs/2502.11735", "title": "MT-RAIG: Novel Benchmark and Evaluation Framework for Retrieval-Augmented Insight Generation over Multiple Tables", "authors": ["Kwangwook Seo", "Donguk Kwon", "Dongha Lee"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Recent advancements in table-based reasoning have expanded beyond\nfactoid-level QA to address insight-level tasks, where systems should\nsynthesize implicit knowledge in the table to provide explainable analyses.\nAlthough effective, existing studies remain confined to scenarios where a\nsingle gold table is given alongside the user query, failing to address cases\nwhere users seek comprehensive insights from multiple unknown tables. To bridge\nthese gaps, we propose MT-RAIG Bench, design to evaluate systems on\nRetrieval-Augmented Insight Generation over Mulitple-Tables. Additionally, to\ntackle the suboptimality of existing automatic evaluation methods in the table\ndomain, we further introduce a fine-grained evaluation framework MT-RAIG Eval,\nwhich achieves better alignment with human quality judgments on the generated\ninsights. We conduct extensive experiments and reveal that even frontier LLMs\nstill struggle with complex multi-table reasoning, establishing our MT-RAIG\nBench as a challenging testbed for future research.", "AI": {"tldr": "This paper presents MT-RAIG Bench for evaluating insight generation from multiple tables and introduces MT-RAIG Eval for improved evaluation methods.", "motivation": "To address the limitations of current table-based reasoning systems that only use a single table alongside queries and to evaluate complex multi-table reasoning tasks.", "method": "Introduction of MT-RAIG Bench for Retrieval-Augmented Insight Generation over Multiple Tables and a fine-grained evaluation framework, MT-RAIG Eval.", "result": "MT-RAIG Bench serves as a challenging testbed for future research, revealing that even advanced LLMs struggle with multi-table reasoning.", "conclusion": "The proposed frameworks enhance the evaluation of systems generating insights from multiple tables and align better with human quality judgments.", "key_contributions": ["Introduction of MT-RAIG Bench for multi-table reasoning tasks.", "Development of MT-RAIG Eval for fine-grained evaluation of generated insights.", "Highlighting the deficiencies of current LLMs in complex reasoning scenarios."], "limitations": "The proposed frameworks have not been applied widely in practical scenarios yet.", "keywords": ["multi-table reasoning", "insight generation", "evaluation framework", "Retrieval-Augmented Generation", "LLMs"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.12378", "pdf": "https://arxiv.org/pdf/2502.12378.pdf", "abs": "https://arxiv.org/abs/2502.12378", "title": "Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges", "authors": ["Bolei Ma", "Yuting Li", "Wei Zhou", "Ziwei Gong", "Yang Janet Liu", "Katja Jasinskaja", "Annemarie Friedrich", "Julia Hirschberg", "Frauke Kreuter", "Barbara Plank"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Understanding pragmatics-the use of language in context-is crucial for\ndeveloping NLP systems capable of interpreting nuanced language use. Despite\nrecent advances in language technologies, including large language models,\nevaluating their ability to handle pragmatic phenomena such as implicatures and\nreferences remains challenging. To advance pragmatic abilities in models, it is\nessential to understand current evaluation trends and identify existing\nlimitations. In this survey, we provide a comprehensive review of resources\ndesigned for evaluating pragmatic capabilities in NLP, categorizing datasets by\nthe pragmatics phenomena they address. We analyze task designs, data collection\nmethods, evaluation approaches, and their relevance to real-world applications.\nBy examining these resources in the context of modern language models, we\nhighlight emerging trends, challenges, and gaps in existing benchmarks. Our\nsurvey aims to clarify the landscape of pragmatic evaluation and guide the\ndevelopment of more comprehensive and targeted benchmarks, ultimately\ncontributing to more nuanced and context-aware NLP models.", "AI": {"tldr": "This survey examines the evaluation of pragmatic capabilities in NLP systems, categorizing existing resources and identifying trends, challenges, and gaps in benchmarks.", "motivation": "Understanding pragmatics is essential for NLP systems to interpret nuanced language use effectively.", "method": "The paper provides a comprehensive review of datasets and resources for evaluating pragmatic phenomena in NLP, including analysis of task designs and evaluation approaches.", "result": "The survey highlights current trends in pragmatic evaluation and identifies existing limitations in available benchmarks.", "conclusion": "The findings encourage the development of more targeted benchmarks for nuanced and context-aware NLP models.", "key_contributions": ["Comprehensive review of resources for evaluating pragmatic capabilities in NLP", "Identification of trends and gaps in existing benchmarks", "Guidance for developing better evaluation metrics for pragmatic phenomena"], "limitations": "Limited exploration of the practical application of the proposed benchmarks in real-world scenarios.", "keywords": ["pragmatics", "natural language processing", "evaluation benchmarks", "language models", "context-aware systems"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2502.12685", "pdf": "https://arxiv.org/pdf/2502.12685.pdf", "abs": "https://arxiv.org/abs/2502.12685", "title": "Theoretical Guarantees for Minimum Bayes Risk Decoding", "authors": ["Yuki Ichihara", "Yuu Jinnai", "Kaito Ariu", "Tetsuro Morimura", "Eiji Uchibe"], "categories": ["cs.CL"], "comment": null, "summary": "Minimum Bayes Risk (MBR) decoding optimizes output selection by maximizing\nthe expected utility value of an underlying human distribution. While prior\nwork has shown the effectiveness of MBR decoding through empirical evaluation,\nfew studies have analytically investigated why the method is effective. As a\nresult of our analysis, we show that, given the size $n$ of the reference\nhypothesis set used in computation, MBR decoding approaches the optimal\nsolution with high probability at a rate of $O\\left(n^{-\\frac{1}{2}}\\right)$,\nunder certain assumptions, even though the language space $Y$ is significantly\nlarger $|Y|\\gg n$. This result helps to theoretically explain the strong\nperformance observed in several prior empirical studies on MBR decoding. In\naddition, we provide the performance gap for maximum-a-posteriori (MAP)\ndecoding and compare it to MBR decoding. The result of this paper indicates\nthat MBR decoding tends to converge to the optimal solution faster than MAP\ndecoding in several cases.", "AI": {"tldr": "The paper investigates the analytical effectiveness of Minimum Bayes Risk (MBR) decoding, demonstrating its convergence to the optimal solution faster than Maximum-a-posteriori (MAP) decoding under certain conditions.", "motivation": "To theoretically analyze the effectiveness of Minimum Bayes Risk (MBR) decoding, which has been shown to perform well empirically but lacks analytical insight.", "method": "The analysis involves examining the convergence rate of MBR decoding given the size of the reference hypothesis set and comparing its performance to Maximum-a-posteriori (MAP) decoding.", "result": "MBR decoding approaches the optimal solution with high probability at a rate of O(n^(-1/2)), indicating faster convergence to the optimal solution than MAP decoding under certain assumptions.", "conclusion": "The findings support previous empirical evaluations of MBR decoding and highlight its advantages over MAP decoding in terms of convergence speed to optimal solutions.", "key_contributions": ["Analytical proof of MBR decoding's effectiveness", "Establishment of convergence rates for MBR decoding", "Comparison of performance between MBR and MAP decoding"], "limitations": "", "keywords": ["Minimum Bayes Risk", "decoding", "performance analysis", "MAP decoding", "convergence"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2502.12821", "pdf": "https://arxiv.org/pdf/2502.12821.pdf", "abs": "https://arxiv.org/abs/2502.12821", "title": "Pitfalls of Scale: Investigating the Inverse Task of Redefinition in Large Language Models", "authors": ["Elena Stringli", "Maria Lymperaiou", "Giorgos Filandrianos", "Athanasios Voulodimos", "Giorgos Stamou"], "categories": ["cs.CL"], "comment": "Accepted at Findings of ACL 2025", "summary": "Inverse tasks can uncover potential reasoning gaps as Large Language Models\n(LLMs) scale up. In this work, we explore the redefinition task, in which we\nassign alternative values to well-known physical constants and units of\nmeasure, prompting LLMs to respond accordingly. Our findings show that not only\ndoes model performance degrade with scale, but its false confidence also rises.\nMoreover, while factors such as prompting strategies or response formatting are\ninfluential, they do not preclude LLMs from anchoring to memorized values.", "AI": {"tldr": "This paper investigates how LLMs respond to redefined physical constants, revealing performance degradation and increased false confidence as models scale.", "motivation": "To uncover reasoning gaps in Large Language Models as they scale up by exploring their responses to redefined physical constants and units.", "method": "The study employs a redefinition task where alternative values are assigned to physical constants, and LLMs are evaluated on their ability to respond correctly to these changes.", "result": "The results indicate a deterioration in model performance with scale, alongside a rise in false confidence when responding to assigned alternative values.", "conclusion": "The findings highlight that despite varying prompting strategies or response formats, LLMs often rely on memorized values, which affects their performance.", "key_contributions": ["Identifies reasoning gaps in LLMs as they scale up", "Demonstrates the impact of redefined physical constants on model performance", "Reveals the relationship between model scale and false confidence in responses."], "limitations": "The study focuses solely on physical constants and may not generalize to all types of reasoning tasks.", "keywords": ["Large Language Models", "Reasoning", "Physical Constants", "Performance Degradation", "False Confidence"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2502.12835", "pdf": "https://arxiv.org/pdf/2502.12835.pdf", "abs": "https://arxiv.org/abs/2502.12835", "title": "Subword models struggle with word learning, but surprisal hides it", "authors": ["Bastian Bunzeck", "Sina Zarrieß"], "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "We study word learning in subword and character language models with the\npsycholinguistic lexical decision task. While subword LMs struggle to discern\nwords and non-words with high accuracy, character LMs solve this task easily\nand consistently. Only when supplied with further contexts do subword LMs\nperform similarly to character models. Additionally, when looking at word-level\nand syntactic learning trajectories, we find that both processes are separable\nin character LMs. Word learning happens before syntactic learning, whereas both\noccur simultaneously in subword LMs. This raises questions about the adequacy\nof subword LMs for modeling language acquisition and positions character LMs as\na viable alternative to study processes below the syntactic level.", "AI": {"tldr": "This paper examines the performance of subword and character language models in word learning through a psycholinguistic task, revealing differences in learning trajectories and implications for language acquisition modeling.", "motivation": "To investigate the efficacy of subword versus character language models in understanding word learning and syntactic acquisition.", "method": "The study employs the psycholinguistic lexical decision task to compare the word and non-word recognition capabilities of subword and character language models.", "result": "Character LMs consistently outperform subword LMs in recognizing words and non-words, showcasing a clear separation in learning processes, with character LMs learning words before syntax.", "conclusion": "Character language models are suggested as a preferable alternative for modeling language acquisition due to their distinct learning trajectories compared to subword models.", "key_contributions": ["Demonstration of the superior performance of character LMs in lexical tasks.", "Identification of the separability of word and syntactic learning in character LMs.", "Critical evaluation of the limitations of subword LMs in modeling language acquisition."], "limitations": "The study may not account for all possible contexts in which subword LMs can operate effectively.", "keywords": ["language models", "word learning", "psycholinguistics", "subword", "character"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13031", "pdf": "https://arxiv.org/pdf/2502.13031.pdf", "abs": "https://arxiv.org/abs/2502.13031", "title": "HPSS: Heuristic Prompting Strategy Search for LLM Evaluators", "authors": ["Bosi Wen", "Pei Ke", "Yufei Sun", "Cunxiang Wang", "Xiaotao Gu", "Jinfeng Zhou", "Jie Tang", "Hongning Wang", "Minlie Huang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Since the adoption of large language models (LLMs) for text evaluation has\nbecome increasingly prevalent in the field of natural language processing\n(NLP), a series of existing works attempt to optimize the prompts for LLM\nevaluators to improve their alignment with human judgment. However, their\nefforts are limited to optimizing individual factors of evaluation prompts,\nsuch as evaluation criteria or output formats, neglecting the combinatorial\nimpact of multiple factors, which leads to insufficient optimization of the\nevaluation pipeline. Nevertheless, identifying well-behaved prompting\nstrategies for adjusting multiple factors requires extensive enumeration. To\nthis end, we comprehensively integrate 8 key factors for evaluation prompts and\npropose a novel automatic prompting strategy optimization method called\nHeuristic Prompting Strategy Search (HPSS). Inspired by the genetic algorithm,\nHPSS conducts an iterative search to find well-behaved prompting strategies for\nLLM evaluators. A heuristic function is employed to guide the search process,\nenhancing the performance of our algorithm. Extensive experiments across four\nevaluation tasks demonstrate the effectiveness of HPSS, consistently\noutperforming both human-designed evaluation prompts and existing automatic\nprompt optimization methods. Our code is available at\nhttps://github.com/thu-coai/HPSS.", "AI": {"tldr": "This paper proposes Heuristic Prompting Strategy Search (HPSS), an automatic method to optimize evaluation prompts for large language models (LLMs) by integrating multiple factors for improved alignment with human judgment.", "motivation": "Existing methods focus on optimizing single aspects of LLM evaluation prompts but overlook the combinatorial impact of multiple factors, hindering the effectiveness of the evaluation pipeline.", "method": "HPSS uses an iterative search algorithm inspired by genetic algorithms, employing a heuristic function to identify and optimize effective prompting strategies across multiple evaluation factors.", "result": "HPSS consistently outperformed human-designed prompts and prior automatic optimization methods in extensive experiments across four evaluation tasks.", "conclusion": "The study demonstrates the potential of HPSS to significantly enhance LLM evaluation prompt optimization, making it more aligned with human judgment.", "key_contributions": ["Introduction of Heuristic Prompting Strategy Search (HPSS) method", "Integration of 8 key factors for prompt evaluation", "Demonstrated superior performance compared to existing methods"], "limitations": "", "keywords": ["large language models", "prompt optimization", "Heuristic Prompting Strategy Search", "natural language processing", "evaluation tasks"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.13259", "pdf": "https://arxiv.org/pdf/2502.13259.pdf", "abs": "https://arxiv.org/abs/2502.13259", "title": "HumT DumT: Measuring and controlling human-like language in LLMs", "authors": ["Myra Cheng", "Sunny Yu", "Dan Jurafsky"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to ACL 2025", "summary": "Should LLMs generate language that makes them seem human? Human-like language\nmight improve user experience, but might also lead to deception, overreliance,\nand stereotyping. Assessing these potential impacts requires a systematic way\nto measure human-like tone in LLM outputs. We introduce HumT and SocioT,\nmetrics for human-like tone and other dimensions of social perceptions in text\ndata based on relative probabilities from an LLM. By measuring HumT across\npreference and usage datasets, we find that users prefer less human-like\noutputs from LLMs in many contexts. HumT also offers insights into the\nperceptions and impacts of anthropomorphism: human-like LLM outputs are highly\ncorrelated with warmth, social closeness, femininity, and low status, which are\nclosely linked to the aforementioned harms. We introduce DumT, a method using\nHumT to systematically control and reduce the degree of human-like tone while\npreserving model performance. DumT offers a practical approach for mitigating\nrisks associated with anthropomorphic language generation.", "AI": {"tldr": "This paper introduces metrics to measure human-like tone in LLM outputs, finding users often prefer less human-like responses, and proposes a method to reduce anthropomorphic language risks.", "motivation": "To systematically assess the impacts of human-like language generated by LLMs on user experience and potential harms such as deception and overreliance.", "method": "Introduces HumT and SocioT metrics based on relative probabilities from LLM, and evaluates these metrics across preference and usage datasets.", "result": "Users demonstrated a preference for less human-like outputs, with human-like language outputs correlated with perceptions of warmth, social closeness, femininity, and low status.", "conclusion": "DumT offers a practical method to control human-like tone in LLM outputs while maintaining performance, aiming to mitigate risks of anthropomorphism.", "key_contributions": ["Development of HumT and SocioT metrics for measuring human-like tone in LLM outputs", "Empirical findings showing user preferences for less human-like language", "Introduction of DumT method to reduce human-like tone while preserving model performance"], "limitations": "Further exploration needed on the long-term impacts of reduced human-like tone in diverse contexts.", "keywords": ["human-like language", "LLMs", "anthropomorphism", "natural language processing", "user experience"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.13775", "pdf": "https://arxiv.org/pdf/2502.13775.pdf", "abs": "https://arxiv.org/abs/2502.13775", "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in Healthcare", "authors": ["Anudeex Shetty", "Amin Beheshti", "Mark Dras", "Usman Naseem"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 (Main Proceedings)", "summary": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions.", "AI": {"tldr": "This paper introduces VITAL, a benchmark dataset for assessing pluralistic alignment in health-related scenarios using Large Language Models (LLMs).", "motivation": "The motivation is to address the inadequacy of existing alignment techniques that model a monolithic preference, particularly in health contexts where diverse perspectives are crucial.", "method": "The authors introduce VITAL, a dataset containing 13.1K value-laden situations and 5.4K multiple-choice questions related to health, and evaluate eight different LLMs against it.", "result": "The evaluation demonstrates that current pluralistic alignment techniques fail to effectively accommodate the diversity of healthcare beliefs.", "conclusion": "The findings indicate a need for specific alignment approaches in health domains, highlighting limitations in current methodologies.", "key_contributions": ["Creation of the VITAL benchmark dataset for health-related pluralistic alignment", "Evaluation of pluralistic alignment techniques on multiple LLMs", "Identification of gaps in current alignment techniques for diverse healthcare beliefs"], "limitations": "", "keywords": ["Large Language Models", "Pluralistic Alignment", "Health Informatics", "Dataset", "Human Values"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.13917", "pdf": "https://arxiv.org/pdf/2502.13917.pdf", "abs": "https://arxiv.org/abs/2502.13917", "title": "TESS 2: A Large-Scale Generalist Diffusion Language Model", "authors": ["Jaesung Tae", "Hamish Ivison", "Sachin Kumar", "Arman Cohan"], "categories": ["cs.CL"], "comment": "ACL 2025 camera-ready", "summary": "We introduce TESS 2, a general instruction-following diffusion language model\nthat outperforms contemporary instruction-tuned diffusion models, as well as\nmatches and sometimes exceeds strong autoregressive (AR) models. We train TESS\n2 by first adapting a strong AR model via continued pretraining with the usual\ncross-entropy as diffusion loss, and then performing further instruction\ntuning. We find that adaptation training as well as the choice of the base\nmodel is crucial for training good instruction-following diffusion models. We\nfurther propose reward guidance, a novel and modular inference-time guidance\nprocedure to align model outputs without needing to train the underlying model.\nFinally, we show that TESS 2 further improves with increased inference-time\ncompute, highlighting the utility of diffusion LMs in having fine-grained\ncontrollability over the amount of compute used at inference time. Code and\nmodels are available at https://github.com/hamishivi/tess-2.", "AI": {"tldr": "TESS 2 is a novel instruction-following diffusion language model that surpasses current models and combines autoregressive training with new inference-time procedures.", "motivation": "To improve instruction-following language models, especially in terms of performance and adaptability.", "method": "Train a strong autoregressive model, adapt it via continued pretraining with cross-entropy diffusion loss, and implement further instruction tuning. Introduce a novel reward guidance procedure for inference-time model alignment.", "result": "TESS 2 outperforms contemporary instruction-tuned diffusion models and matches/exceeds strong autoregressive models; the model further improves with increased inference-time compute.", "conclusion": "The study highlights the effectiveness of diffusion LMs for instruction-following tasks and their controllability based on compute usage during inference.", "key_contributions": ["Introduction of TESS 2 model with superior performance", "Novel reward guidance procedure for aligning model outputs", "Demonstration of improved performance with enhanced inference-time compute"], "limitations": "", "keywords": ["instruction-following", "diffusion language model", "autoregressive model", "reward guidance", "inference-time guidance"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.13957", "pdf": "https://arxiv.org/pdf/2502.13957.pdf", "abs": "https://arxiv.org/abs/2502.13957", "title": "RAG-Gym: Systematic Optimization of Language Agents for Retrieval-Augmented Generation", "authors": ["Guangzhi Xiong", "Qiao Jin", "Xiao Wang", "Yin Fang", "Haolin Liu", "Yifan Yang", "Fangyuan Chen", "Zhixing Song", "Dengyu Wang", "Minjia Zhang", "Zhiyong Lu", "Aidong Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "Homepage: https://rag-gym.github.io; Code:\n  https://github.com/RAG-Gym/RAG-Gym", "summary": "Retrieval-augmented generation (RAG) has shown great promise for\nknowledge-intensive tasks and recently advanced with agentic RAG, where\nlanguage agents engage in multi-round interactions with external knowledge\nsources for adaptive information retrieval. However, existing agentic RAG\nmethods often depend on ad-hoc prompt engineering and lack a unified\noptimization framework. We introduce RAG-Gym, a comprehensive platform that\nsystematically explores three optimization dimensions: (1) prompt engineering,\n(2) actor tuning, and (3) critic training. For prompt engineering, we propose\nRe$^2$Search, a novel agent incorporating reasoning reflection that\nsignificantly outperforms standard prompts. In actor tuning, we evaluate three\npopular post-training algorithms with fine-grained process supervision and\nidentify direct preference optimization as the most effective. We further\ndemonstrate that a trained critic can enhance inference by selecting\nhigher-quality intermediate reasoning steps. Together, these findings lead to\nthe optimized Re$^2$Search++ agent, which surpasses most recent methods like\nSearch-R1 by a relative increase of 3.2% to 11.6% in average F1. Finally, we\nexamine the impact of different reward sources and analyze scaling properties\nin training and inference, offering practical insights for agentic RAG\noptimization. The project homepage is available at https://rag-gym.github.io.", "AI": {"tldr": "The paper introduces RAG-Gym, an optimization platform for agentic retrieval-augmented generation (RAG) that improves prompt engineering, actor tuning, and critic training.", "motivation": "Existing agentic RAG methods depend on ad-hoc prompt engineering and lack a unified optimization framework, prompting the need for a systematic approach.", "method": "The study systematically explores three optimization dimensions: prompt engineering using Re$^2$Search, actor tuning with post-training algorithms, and critic training to enhance inference.", "result": "The optimized Re$^2$Search++ agent outperforms recent methods, achieving a 3.2% to 11.6% relative increase in average F1 score.", "conclusion": "The findings provide practical insights for optimizing agentic RAG by evaluating different reward sources and understanding scaling properties.", "key_contributions": ["Introduction of RAG-Gym as a comprehensive optimization platform for agentic RAG.", "Development of Re$^2$Search for improved prompt engineering.", "Identification of direct preference optimization as the most effective actor tuning method."], "limitations": "", "keywords": ["Retrieval-augmented generation", "agentic RAG", "optimization framework", "prompt engineering", "actor tuning"], "importance_score": 10, "read_time_minutes": 15}}
{"id": "2502.14127", "pdf": "https://arxiv.org/pdf/2502.14127.pdf", "abs": "https://arxiv.org/abs/2502.14127", "title": "Which of These Best Describes Multiple Choice Evaluation with LLMs? A) Forced B) Flawed C) Fixable D) All of the Above", "authors": ["Nishant Balepur", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Multiple choice question answering (MCQA) is popular for LLM evaluation due\nto its simplicity and human-like testing, but we argue for its reform. We first\nreveal flaws in MCQA's format, as it struggles to: 1) test\ngeneration/subjectivity; 2) match LLM use cases; and 3) fully test knowledge.\nWe instead advocate for generative formats based on human testing, where LLMs\nconstruct and explain answers, better capturing user needs and knowledge while\nremaining easy to score. We then show even when MCQA is a useful format, its\ndatasets suffer from: leakage; unanswerability; shortcuts; and saturation. In\neach issue, we give fixes from education, like rubrics to guide MCQ writing;\nscoring methods to bridle guessing; and Item Response Theory to build harder\nMCQs. Lastly, we discuss LLM errors in MCQA, robustness, biases, and unfaithful\nexplanations, showing how our prior solutions better measure or address these\nissues. While we do not need to desert MCQA, we encourage more efforts in\nrefining the task based on educational testing, advancing evaluations.", "AI": {"tldr": "The paper critiques multiple choice question answering (MCQA) for LLM evaluation and proposes generative formats for better alignment with user needs and knowledge.", "motivation": "The motivation is to improve the effectiveness of LLM evaluation by addressing the shortcomings of MCQA, which fails to adequately assess generation, subjectivity, and knowledge.", "method": "The authors analyze the flaws of MCQA datasets and propose reform methods from educational testing, including rubric-based guidance for question writing and scoring methods to minimize guessing.", "result": "The paper identifies several issues with MCQA datasets such as leakage, unanswerability, and biases, and presents fixes, demonstrating that a generative format can better capture LLM capabilities.", "conclusion": "The study advocates for refining MCQA while integrating ideas from educational testing to enhance evaluation measures, suggesting that improvements are essential rather than completely abandoning the MCQA format.", "key_contributions": ["Critique of MCQA for LLM evaluation", "Proposal of generative answer formats", "Educational reforms for MCQA question design"], "limitations": "Some LLM errors and biases still persist, indicating that while improvements can be made, challenges in LLM evaluations remain.", "keywords": ["MCQA", "LLM evaluation", "generative formats", "educational testing", "item response theory"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2502.14258", "pdf": "https://arxiv.org/pdf/2502.14258.pdf", "abs": "https://arxiv.org/abs/2502.14258", "title": "Does Time Have Its Place? Temporal Heads: Where Language Models Recall Time-specific Information", "authors": ["Yein Park", "Chanwoong Yoon", "Jungwoo Park", "Minbyul Jeong", "Jaewoo Kang"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the main conference at ACL 2025", "summary": "While the ability of language models to elicit facts has been widely\ninvestigated, how they handle temporally changing facts remains underexplored.\nWe discover Temporal Heads, specific attention heads that primarily handle\ntemporal knowledge, through circuit analysis. We confirm that these heads are\npresent across multiple models, though their specific locations may vary, and\ntheir responses differ depending on the type of knowledge and its corresponding\nyears. Disabling these heads degrades the model's ability to recall\ntime-specific knowledge while maintaining its general capabilities without\ncompromising time-invariant and question-answering performances. Moreover, the\nheads are activated not only numeric conditions (\"In 2004\") but also textual\naliases (\"In the year ...\"), indicating that they encode a temporal dimension\nbeyond simple numerical representation. Furthermore, we expand the potential of\nour findings by demonstrating how temporal knowledge can be edited by adjusting\nthe values of these heads.", "AI": {"tldr": "This paper investigates how language models manage temporally changing facts, identifying specific attention heads called Temporal Heads that handle this knowledge.", "motivation": "The study aims to fill the gap in understanding how language models cope with facts that change over time, a topic that has been largely overlooked despite the models' ability to retrieve static information.", "method": "The authors employed circuit analysis to identify and analyze Temporal Heads across different language models, observing their behavior with respect to varying types of temporal knowledge.", "result": "The research found that disabling the Temporal Heads hinders the models' performance in recalling time-specific facts without affecting their general capabilities, illustrating the heads' crucial role in managing temporal knowledge.", "conclusion": "The presence of Temporal Heads was confirmed in multiple models, indicating that language models possess specialized mechanisms for handling temporal information, which can be modified or edited.", "key_contributions": ["Identification of Temporal Heads in language models for handling temporal knowledge", "Demonstration of the varying activation of these heads based on numeric and textual temporal cues", "Potential for editing temporal knowledge by adjusting the values in Temporal Heads."], "limitations": "", "keywords": ["temporal knowledge", "language models", "attention heads", "HCI", "NLP"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.14301", "pdf": "https://arxiv.org/pdf/2502.14301.pdf", "abs": "https://arxiv.org/abs/2502.14301", "title": "SEA-HELM: Southeast Asian Holistic Evaluation of Language Models", "authors": ["Yosephine Susanto", "Adithya Venkatadri Hulagadri", "Jann Railey Montalan", "Jian Gang Ngui", "Xian Bin Yong", "Weiqi Leong", "Hamsawardhini Rengarajan", "Peerat Limkonchotiwat", "Yifan Mai", "William Chandra Tjhi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With the rapid emergence of novel capabilities in Large Language Models\n(LLMs), the need for rigorous multilingual and multicultural benchmarks that\nare integrated has become more pronounced. Though existing LLM benchmarks are\ncapable of evaluating specific capabilities of LLMs in English as well as in\nvarious mid- to low-resource languages, including those in the Southeast Asian\n(SEA) region, a comprehensive and culturally representative evaluation suite\nfor the SEA languages has not been developed thus far. Here, we present\nSEA-HELM, a holistic linguistic and cultural LLM evaluation suite that\nemphasises SEA languages, comprising five core pillars: (1) NLP Classics, (2)\nLLM-specifics, (3) SEA Linguistics, (4) SEA Culture, (5) Safety. SEA-HELM\ncurrently supports Filipino, Indonesian, Tamil, Thai, and Vietnamese. We also\nintroduce the SEA-HELM leaderboard, which allows users to understand models'\nmultilingual and multicultural performance in a systematic and user-friendly\nmanner. We make the SEA-HELM evaluation code publicly available.", "AI": {"tldr": "Introduction of SEA-HELM, a comprehensive evaluation suite for Large Language Models focusing on Southeast Asian languages.", "motivation": "To address the lack of multilingual and multicultural benchmarks specifically for Southeast Asian languages in the evaluation of LLM capabilities.", "method": "Development of SEA-HELM consisting of five core layers: NLP Classics, LLM-specifics, SEA Linguistics, SEA Culture, and Safety.", "result": "SEA-HELM evaluates models across various pillars and supports languages including Filipino, Indonesian, Tamil, Thai, and Vietnamese, allowing for a structured assessment of multilingual and multicultural performance.", "conclusion": "The availability of SEA-HELM and its leaderboard can significantly enhance the understanding of LLM performance in the Southeast Asian context.", "key_contributions": ["Introduction of a holistic evaluation suite for SEA languages", "Creation of a SEA-HELM leaderboard for user-friendly assessment", "Public availability of evaluation code for ease of access"], "limitations": "", "keywords": ["Large Language Models", "Multilingual benchmarks", "Southeast Asian languages"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2502.14677", "pdf": "https://arxiv.org/pdf/2502.14677.pdf", "abs": "https://arxiv.org/abs/2502.14677", "title": "Data-Constrained Synthesis of Training Data for De-Identification", "authors": ["Thomas Vakili", "Aron Henriksson", "Hercules Dalianis"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main: Long paper", "summary": "Many sensitive domains -- such as the clinical domain -- lack widely\navailable datasets due to privacy risks. The increasing generative capabilities\nof large language models (LLMs) have made synthetic datasets a viable path\nforward. In this study, we domain-adapt LLMs to the clinical domain and\ngenerate synthetic clinical texts that are machine-annotated with tags for\npersonally identifiable information using capable encoder-based NER models. The\nsynthetic corpora are then used to train synthetic NER models. The results show\nthat training NER models using synthetic corpora incurs only a small drop in\npredictive performance. The limits of this process are investigated in a\nsystematic ablation study -- using both Swedish and Spanish data. Our analysis\nshows that smaller datasets can be sufficient for domain-adapting LLMs for data\nsynthesis. Instead, the effectiveness of this process is almost entirely\ncontingent on the performance of the machine-annotating NER models trained\nusing the original data.", "AI": {"tldr": "Study explores the use of large language models to generate synthetic clinical datasets for training Named Entity Recognition (NER) models.", "motivation": "Address the lack of widely available clinical datasets due to privacy risks by utilizing synthetic data generation techniques.", "method": "Domain-adapt large language models to generate synthetic clinical texts annotated for personally identifiable information using encoder-based NER models; train synthetic NER models on these datasets.", "result": "Synthetic NER models trained with synthetic corpora show only a small decrease in predictive performance compared to models trained on original data.", "conclusion": "Smaller datasets can be effective for adapting LLMs for data synthesis; however, the success depends largely on the machine-annotating NER models' quality trained on original data.", "key_contributions": ["Methodology for generating synthetic clinical texts using LLMs", "Evaluation of NER models trained on synthetic versus original datasets", "Insights on dataset size requirements for effective LLM adaptation"], "limitations": "The process's effectiveness greatly relies on the quality of the original data's NER models.", "keywords": ["synthetic datasets", "large language models", "clinical domain", "named entity recognition", "privacy concerns"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2502.14778", "pdf": "https://arxiv.org/pdf/2502.14778.pdf", "abs": "https://arxiv.org/abs/2502.14778", "title": "Harnessing PDF Data for Improving Japanese Large Multimodal Models", "authors": ["Jeonghun Baek", "Akiko Aizawa", "Kiyoharu Aizawa"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to ACL2025 Findings. Code:\n  https://github.com/ku21fan/PDF-JLMM", "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nEnglish, but their effectiveness in Japanese remains limited due to the lack of\nhigh-quality training data. Current Japanese LMMs often rely on translated\nEnglish datasets, restricting their ability to capture Japan-specific cultural\nknowledge. To address this, we explore the potential of Japanese PDF data as a\ntraining resource, an area that remains largely underutilized. We introduce a\nfully automated pipeline that leverages pretrained models to extract image-text\npairs from PDFs through layout analysis, OCR, and vision-language pairing,\nremoving the need for manual annotation. Additionally, we construct instruction\ndata from extracted image-text pairs to enrich the training data. To evaluate\nthe effectiveness of PDF-derived data, we train Japanese LMMs and assess their\nperformance on the Japanese LMM Benchmark. Our results demonstrate substantial\nimprovements, with performance gains ranging from 2.1% to 13.8% on Heron-Bench.\nFurther analysis highlights the impact of PDF-derived data on various factors,\nsuch as model size and language models, reinforcing its value as a multimodal\nresource for Japanese LMMs.", "AI": {"tldr": "This paper presents an automated method for utilizing Japanese PDF data to improve Large Multimodal Models (LMMs), addressing the limitations of existing Japanese LMMs.", "motivation": "The lack of high-quality training data for Japanese LMMs has restricted their effectiveness, leading to reliance on translated datasets that fail to capture cultural nuances.", "method": "An automated pipeline was developed to extract image-text pairs from Japanese PDFs using layout analysis, OCR, and vision-language pairing, without manual annotation. Instruction data was constructed from these pairs to enhance training.", "result": "Training Japanese LMMs with PDF-derived data resulted in performance improvements ranging from 2.1% to 13.8% on the Japanese LMM Benchmark (Heron-Bench).", "conclusion": "PDF-derived data proves to be a valuable multimodal resource, enhancing the performance of Japanese LMMs and enabling better cultural representation.", "key_contributions": ["Developed an automated pipeline for extracting data from Japanese PDFs", "Demonstrated significant performance enhancements in Japanese LMMs", "Provided a new source of high-quality training data specific to Japanese culture"], "limitations": "", "keywords": ["Large Multimodal Models", "Japanese PDF Data", "OCR", "Vision-Language Pairing", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2502.16173", "pdf": "https://arxiv.org/pdf/2502.16173.pdf", "abs": "https://arxiv.org/abs/2502.16173", "title": "Mapping 1,000+ Language Models via the Log-Likelihood Vector", "authors": ["Momose Oyama", "Hiroaki Yamagiwa", "Yusuke Takase", "Hidetoshi Shimodaira"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To compare autoregressive language models at scale, we propose using\nlog-likelihood vectors computed on a predefined text set as model features.\nThis approach has a solid theoretical basis: when treated as model coordinates,\ntheir squared Euclidean distance approximates the Kullback-Leibler divergence\nof text-generation probabilities. Our method is highly scalable, with\ncomputational cost growing linearly in both the number of models and text\nsamples, and is easy to implement as the required features are derived from\ncross-entropy loss. Applying this method to over 1,000 language models, we\nconstructed a \"model map,\" providing a new perspective on large-scale model\nanalysis.", "AI": {"tldr": "The paper presents a method for comparing autoregressive language models using log-likelihood vectors to analyze model similarities and performance.", "motivation": "To provide a scalable and efficient way to compare autoregressive language models at scale, enabling better understanding and categorization.", "method": "The authors compute log-likelihood vectors from a predefined text set and use these as features. The squared Euclidean distance between these vectors approximates the Kullback-Leibler divergence, allowing for model comparisons.", "result": "The approach was applied to over 1,000 language models, resulting in a 'model map' that visually represents the relationships and differences between these models based on performance metrics.", "conclusion": "This method offers a new perspective for large-scale model analysis and can facilitate more informed choices in model selection.", "key_contributions": ["Introduction of a scalable method for modeling comparison", "Construction of a new 'model map' for visualizing language models", "Theoretical substantiation of using log-likelihood vectors as model coordinates."], "limitations": "", "keywords": ["autogressive models", "language models", "log-likelihood", "Kullback-Leibler divergence", "model analysis"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.16942", "pdf": "https://arxiv.org/pdf/2502.16942.pdf", "abs": "https://arxiv.org/abs/2502.16942", "title": "NUTSHELL: A Dataset for Abstract Generation from Scientific Talks", "authors": ["Maike Züfle", "Sara Papi", "Beatrice Savoldi", "Marco Gaido", "Luisa Bentivogli", "Jan Niehues"], "categories": ["cs.CL"], "comment": null, "summary": "Scientific communication is receiving increasing attention in natural\nlanguage processing, especially to help researches access, summarize, and\ngenerate content. One emerging application in this area is Speech-to-Abstract\nGeneration (SAG), which aims to automatically generate abstracts from recorded\nscientific presentations. SAG enables researchers to efficiently engage with\nconference talks, but progress has been limited by a lack of large-scale\ndatasets. To address this gap, we introduce NUTSHELL, a novel multimodal\ndataset of *ACL conference talks paired with their corresponding abstracts. We\nestablish strong baselines for SAG and evaluate the quality of generated\nabstracts using both automatic metrics and human judgments. Our results\nhighlight the challenges of SAG and demonstrate the benefits of training on\nNUTSHELL. By releasing NUTSHELL under an open license (CC-BY 4.0), we aim to\nadvance research in SAG and foster the development of improved models and\nevaluation methods.", "AI": {"tldr": "Introduction of NUTSHELL, a multimodal dataset of ACL conference talks and abstracts for Speech-to-Abstract Generation (SAG).", "motivation": "To improve scientific communication and engagement by providing a tool for automatically generating abstracts from recorded presentations, addressing the lack of large-scale datasets.", "method": "Introduces the NUTSHELL dataset which pairs recorded conference talks with corresponding abstracts, establishing strong baselines for SAG and evaluating abstract quality via automatic metrics and human judgments.", "result": "The introduction of NUTSHELL enables improvements in SAG, highlighting challenges while demonstrating the advantages of training on this dataset.", "conclusion": "The release of NUTSHELL aims to foster advancements in SAG research, allowing for better models and evaluation methods.", "key_contributions": ["Introduction of a new multimodal dataset (NUTSHELL) for SAG applications.", "Establishment of strong performance baselines for abstract generation from talks.", "Evaluation of the generated abstracts through multiple metrics."], "limitations": "Limited to ACL conference talks; may not represent all scientific fields.", "keywords": ["Speech-to-Abstract Generation", "multimodal dataset", "NUTSHELL", "scientific communication", "ACL conference"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.17184", "pdf": "https://arxiv.org/pdf/2502.17184.pdf", "abs": "https://arxiv.org/abs/2502.17184", "title": "Measuring Data Diversity for Instruction Tuning: A Systematic Analysis and A Reliable Metric", "authors": ["Yuming Yang", "Yang Nan", "Junjie Ye", "Shihan Dou", "Xiao Wang", "Shuo Li", "Huijie Lv", "Mingqi Wu", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main. Camera-ready version updated (20 pages).\n  Project page: https://github.com/UmeanNever/NovelSum", "summary": "Data diversity is crucial for the instruction tuning of large language\nmodels. Existing studies have explored various diversity-aware data selection\nmethods to construct high-quality datasets and enhance model performance.\nHowever, the fundamental problem of precisely defining and measuring data\ndiversity remains underexplored, limiting clear guidance for data engineering.\nTo address this, we systematically analyze 11 existing diversity measurement\nmethods by evaluating their correlation with model performance through\nextensive fine-tuning experiments. Our results indicate that a reliable\ndiversity measure should properly account for both inter-sample differences and\nthe information density in the sample space. Building on this, we propose\nNovelSum, a new diversity metric based on sample-level \"novelty.\" Experiments\non both simulated and real-world data show that NovelSum accurately captures\ndiversity variations and achieves a 0.97 correlation with instruction-tuned\nmodel performance, highlighting its value in guiding data engineering\npractices. With NovelSum as an optimization objective, we further develop a\ngreedy, diversity-oriented data selection strategy that outperforms existing\napproaches, validating both the effectiveness and practical significance of our\nmetric. The code is available at https://github.com/UmeanNever/NovelSum.", "AI": {"tldr": "Explores data diversity in large language models, proposing a new metric, NovelSum, and a corresponding data selection strategy to improve model performance.", "motivation": "To define and measure data diversity effectively for enhancing model performance and guide data engineering practices in large language models.", "method": "Systematic analysis of 11 existing diversity measurement methods; introduction of NovelSum, a new diversity metric based on sample-level novelty; extensive fine-tuning experiments to evaluate correlation with model performance.", "result": "NovelSum captures diversity variations with 0.97 correlation to instruction-tuned model performance; development of a new data selection strategy that outperforms existing methods.", "conclusion": "NovelSum is a reliable metric for guiding data engineering in large language models, showing practical significance through improved model performance.", "key_contributions": ["Introduction of NovelSum as a new diversity metric based on novelty.", "Systematic evaluation of diversity measurement methods and their correlation with model performance.", "Development of a diversity-oriented data selection strategy that enhances model performance."], "limitations": "", "keywords": ["data diversity", "large language models", "diversity metric", "model performance", "data selection"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.18795", "pdf": "https://arxiv.org/pdf/2502.18795.pdf", "abs": "https://arxiv.org/abs/2502.18795", "title": "Anything Goes? A Crosslinguistic Study of (Im)possible Language Learning in LMs", "authors": ["Xiulin Yang", "Tatsuya Aoyama", "Yuekun Yao", "Ethan Wilcox"], "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Do language models (LMs) offer insights into human language learning? A\ncommon argument against this idea is that because their architecture and\ntraining paradigm are so vastly different from humans, LMs can learn arbitrary\ninputs as easily as natural languages. We test this claim by training LMs to\nmodel impossible and typologically unattested languages. Unlike previous work,\nwhich has focused exclusively on English, we conduct experiments on 12\nlanguages from 4 language families with two newly constructed parallel corpora.\nOur results show that while GPT-2 small can largely distinguish attested\nlanguages from their impossible counterparts, it does not achieve perfect\nseparation between all the attested languages and all the impossible ones. We\nfurther test whether GPT-2 small distinguishes typologically attested from\nunattested languages with different NP orders by manipulating word order based\non Greenberg's Universal 20. We find that the model's perplexity scores do not\ndistinguish attested vs. unattested word orders, while its performance on the\ngeneralization test does. These findings suggest that LMs exhibit some\nhuman-like inductive biases, though these biases are weaker than those found in\nhuman learners.", "AI": {"tldr": "This study investigates whether language models (LMs) can provide insights into human language learning by testing their ability to distinguish between attested and unattested languages.", "motivation": "To explore if language models can mirror human inductive biases in language learning despite architectural differences.", "method": "The authors trained LMs on 12 languages from 4 families, focusing on twelve newly constructed parallel corpora to model impossible and typologically unattested languages.", "result": "GPT-2 small shows the ability to distinguish attested languages from impossible ones but does not achieve perfect separation, and it struggles to distinguish between different NP orders based on typological data.", "conclusion": "Language models exhibit some similarities to human language learning biases, though they are weaker than those observed in humans.", "key_contributions": ["Demonstration of LM's ability to differentiate attested vs unattested languages", "Exploration of LMs' performance on typologically attested and unattested languages", "Introduction of new parallel corpora for language testing"], "limitations": "The model does not achieve perfect accuracy and biases are not as strong as in human learning.", "keywords": ["language models", "human language learning", "inductive biases", "natural languages", "typological languages"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2502.18968", "pdf": "https://arxiv.org/pdf/2502.18968.pdf", "abs": "https://arxiv.org/abs/2502.18968", "title": "Know You First and Be You Better: Modeling Human-Like User Simulators via Implicit Profiles", "authors": ["Kuang Wang", "Xianfei Li", "Shenghao Yang", "Li Zhou", "Feng Jiang", "Haizhou Li"], "categories": ["cs.CL"], "comment": "9 pages. Accepted to ACL 2025. Camera-ready version", "summary": "User simulators are crucial for replicating human interactions with dialogue\nsystems, supporting both collaborative training and automatic evaluation,\nespecially for large language models (LLMs). However, current role-playing\nmethods face challenges such as a lack of utterance-level authenticity and\nuser-level diversity, often hindered by role confusion and dependence on\npredefined profiles of well-known figures. In contrast, direct simulation\nfocuses solely on text, neglecting implicit user traits like personality and\nconversation-level consistency. To address these issues, we introduce the User\nSimulator with Implicit Profiles (USP), a framework that infers implicit user\nprofiles from human-machine interactions to simulate personalized and realistic\ndialogues. We first develop an LLM-driven extractor with a comprehensive\nprofile schema, then refine the simulation using conditional supervised\nfine-tuning and reinforcement learning with cycle consistency, optimizing at\nboth the utterance and conversation levels. Finally, a diverse profile sampler\ncaptures the distribution of real-world user profiles. Experimental results\nshow that USP outperforms strong baselines in terms of authenticity and\ndiversity while maintaining comparable consistency. Additionally, using USP to\nevaluate LLM on dynamic multi-turn aligns well with mainstream benchmarks,\ndemonstrating its effectiveness in real-world applications.", "AI": {"tldr": "Introduces USP, a framework utilizing implicit user profiles for enhanced dialogue simulation in LLMs.", "motivation": "To improve the authenticity and diversity of user simulations in dialogue systems, addressing limitations in current role-playing methods.", "method": "Development of an LLM-driven extractor for implicit profiles, followed by refinements using conditional supervised fine-tuning and reinforcement learning, and implementation of a diverse profile sampler.", "result": "USP significantly outperforms baselines in authenticity and user diversity, while maintaining conversation-level consistency, and effectively evaluates LLMs on dynamic multi-turn interactions.", "conclusion": "USP demonstrates a successful approach for simulating realistic dialogues, providing an effective tool for dialogue system evaluation in real-world applications.", "key_contributions": ["Framework for simulating personalized dialogues using implicit user profiles", "LLM-driven extractor and comprehensive profile schema", "Diverse profile sampler capturing real-world user distribution"], "limitations": "", "keywords": ["User Simulation", "Dialogue Systems", "Implicit Profiles", "Large Language Models", "Reinforcement Learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.19163", "pdf": "https://arxiv.org/pdf/2502.19163.pdf", "abs": "https://arxiv.org/abs/2502.19163", "title": "TestNUC: Enhancing Test-Time Computing Approaches and Scaling through Neighboring Unlabeled Data Consistency", "authors": ["Henry Peng Zou", "Zhengyao Gu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Liancheng Fang", "Yibo Wang", "Yangning Li", "Kay Liu", "Philip S. Yu"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted by ACL 2025 main conference", "summary": "Test-time computing approaches, which leverage additional computational\nresources during inference, have been proven effective in enhancing large\nlanguage model performance. This work introduces a novel, linearly scaling\napproach, TestNUC, that improves test-time predictions by leveraging the local\nconsistency of neighboring unlabeled data-it classifies an input instance by\nconsidering not only the model's prediction on that instance but also on\nneighboring unlabeled instances. We evaluate TestNUC across eight diverse\ndatasets, spanning intent classification, topic mining, domain discovery, and\nemotion detection, demonstrating its consistent superiority over baseline\nmethods such as standard prompting and self-consistency. Furthermore, TestNUC\ncan be seamlessly integrated with existing test-time computing approaches,\nsubstantially boosting their performance. Our analysis reveals that TestNUC\nscales effectively with increasing amounts of unlabeled data and performs\nrobustly across different embedding models, making it practical for real-world\napplications. Our code is available at https://github.com/HenryPengZou/TestNUC.", "AI": {"tldr": "TestNUC is a novel approach for enhancing large language model performance at test time by leveraging local consistency of neighboring unlabeled data, showing significant improvements over traditional methods.", "motivation": "To improve test-time predictions of large language models by utilizing additional computational resources and local information from unlabeled data.", "method": "TestNUC classifies input instances by considering model predictions on both the instance and its neighboring unlabeled data, enhancing robustness and accuracy.", "result": "TestNUC consistently outperformed baseline methods like standard prompting and self-consistency across eight diverse datasets, including intent classification and emotion detection.", "conclusion": "TestNUC is a scalable and robust approach for test-time computing that integrates well with existing methods, ideal for practical applications due to its ability to utilize large amounts of unlabeled data.", "key_contributions": ["Introduces TestNUC, a linearly scaling approach to leverage unlabeled data during inference.", "Demonstrates significant improvements in various NLP tasks over baseline methods.", "Provides practical implementation for real-world applications with available code."], "limitations": "", "keywords": ["test-time computing", "large language models", "unlabeled data", "NLP", "machine learning"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2502.19765", "pdf": "https://arxiv.org/pdf/2502.19765.pdf", "abs": "https://arxiv.org/abs/2502.19765", "title": "EdiText: Controllable Coarse-to-Fine Text Editing with Diffusion Language Models", "authors": ["Che Hyun Lee", "Heeseung Kim", "Jiheum Yeom", "Sungroh Yoon"], "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025", "summary": "We propose EdiText, a controllable text editing method that modifies the\nreference text to desired attributes at various scales. We integrate an\nSDEdit-based editing technique that allows for broad adjustments in the degree\nof text editing. Additionally, we introduce a novel fine-level editing method\nbased on self-conditioning, which allows subtle control of reference text.\nWhile being capable of editing on its own, this fine-grained method, integrated\nwith the SDEdit approach, enables EdiText to make precise adjustments within\nthe desired range. EdiText demonstrates its controllability to robustly adjust\nreference text at a broad range of levels across various tasks, including\ntoxicity control and sentiment control.", "AI": {"tldr": "EdiText is a controllable text editing method that allows modifications of reference text attributes at various scales, improving functionality for tasks like toxicity and sentiment control.", "motivation": "To provide a versatile text editing method capable of modifying text attributes with precise control, addressing existing limitations in nuanced editing.", "method": "EdiText integrates an SDEdit-based editing technique for broad adjustments and a novel fine-level editing method based on self-conditioning, enabling subtle and precise control.", "result": "EdiText successfully demonstrates robust controllability for modifying reference text across a variety of tasks, including toxicity and sentiment adjustments.", "conclusion": "The integration of coarse and fine-level editing methods in EdiText allows for more nuanced and effective text editing capabilities.", "key_contributions": ["Introduction of EdiText for controllable text editing", "Combination of SDEdit with self-conditioning for fine-grained control", "Demonstration of effectiveness in tasks like toxicity and sentiment control"], "limitations": "", "keywords": ["controllable text editing", "SDEdit", "self-conditioning", "toxicity control", "sentiment control"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20238", "pdf": "https://arxiv.org/pdf/2502.20238.pdf", "abs": "https://arxiv.org/abs/2502.20238", "title": "FINEREASON: Evaluating and Improving LLMs' Deliberate Reasoning through Reflective Puzzle Solving", "authors": ["Guizhen Chen", "Weiwen Xu", "Hao Zhang", "Hou Pong Chan", "Chaoqun Liu", "Lidong Bing", "Deli Zhao", "Anh Tuan Luu", "Yu Rong"], "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Main", "summary": "Many challenging reasoning tasks require not just rapid, intuitive responses,\nbut a more deliberate, multi-step approach. Recent progress in large language\nmodels (LLMs) highlights an important shift from the \"System 1\" way of quick\nreactions to the \"System 2\" style of reflection-and-correction problem solving.\nHowever, current benchmarks heavily rely on the final-answer accuracy, leaving\nmuch of a model's intermediate reasoning steps unexamined. This fails to assess\nthe model's ability to reflect and rectify mistakes within the reasoning\nprocess. To bridge this gap, we introduce FINEREASON, a logic-puzzle benchmark\nfor fine-grained evaluation of LLMs' reasoning capabilities. Each puzzle can be\ndecomposed into atomic steps, making it ideal for rigorous validation of\nintermediate correctness. Building on this, we introduce two tasks: state\nchecking, and state transition, for a comprehensive evaluation of how models\nassess the current situation and plan the next move. To support broader\nresearch, we also provide a puzzle training set aimed at enhancing performance\non general mathematical tasks. We show that models trained on our state\nchecking and transition data demonstrate gains in math reasoning by up to 5.1%\non GSM8K.", "AI": {"tldr": "Introduces FINEREASON, a benchmark for evaluating LLMs' reasoning processes through multi-step logic puzzles.", "motivation": "To address the inadequacy of current benchmarks that only assess final-answer accuracy, neglecting the intermediate reasoning steps of LLMs.", "method": "Developed a benchmark featuring logic puzzles decomposable into atomic steps, introducing tasks for state checking and state transition to evaluate reasoning processes comprehensively.", "result": "Models trained on the FINEREASON tasks show improved mathematical reasoning performance by up to 5.1% on the GSM8K benchmark.", "conclusion": "FINEREASON enables better evaluation of LLMs' reflective and corrective reasoning capabilities, promoting improved model performance in multi-step reasoning tasks.", "key_contributions": ["Creation of the FINEREASON benchmark for evaluating LLMs' reasoning steps", "Introduction of state checking and state transition tasks", "Demonstrated performance gains in math reasoning tasks related to LLM training."], "limitations": "", "keywords": ["large language models", "reasoning", "benchmark", "math reasoning", "state transition"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2502.20273", "pdf": "https://arxiv.org/pdf/2502.20273.pdf", "abs": "https://arxiv.org/abs/2502.20273", "title": "How Much is Enough? The Diminishing Returns of Tokenization Training Data", "authors": ["Varshini Reddy", "Craig W. Schmidt", "Yuval Pinter", "Chris Tanner"], "categories": ["cs.CL", "cs.CE"], "comment": null, "summary": "Tokenization, a crucial initial step in natural language processing, is\ngoverned by several key parameters, such as the tokenization algorithm,\nvocabulary size, pre-tokenization strategy, inference strategy, and training\ndata corpus. This paper investigates the impact of an often-overlooked\nhyperparameter, tokenizer training data size. We train BPE, UnigramLM, and\nWordPiece tokenizers across various vocabulary sizes using English training\ndata ranging from 1GB to 900GB. Our findings reveal diminishing returns as\ntraining data size increases beyond roughly 150GB, suggesting a practical limit\nto the improvements in tokenization quality achievable through additional data.\nWe analyze this phenomenon and attribute the saturation effect to constraints\nintroduced by the pre-tokenization stage. We then demonstrate the extent to\nwhich these findings can generalize by experimenting on data in Russian, a\nlanguage typologically distant from English. While the limit appears to\nmaterialize at a later phase of pre-training, around 200GB, it is in fact\nobserved. These results provide valuable insights for optimizing the\ntokenization process by reducing the compute required for training on large\ncorpora and suggest promising directions for future research in tokenization\nalgorithms.", "AI": {"tldr": "The paper explores the effects of tokenizer training data size on natural language processing quality, revealing diminishing returns beyond certain thresholds and providing insights for optimization.", "motivation": "To investigate the impact of tokenizer training data size on the effectiveness of various tokenization algorithms in NLP.", "method": "BPE, UnigramLM, and WordPiece tokenizers were trained on English data sizes ranging from 1GB to 900GB to determine the effect of training data size on tokenization quality.", "result": "Findings indicate diminishing returns in tokenization quality after approximately 150GB of training data for English and around 200GB for Russian, attributed to the pre-tokenization stage.", "conclusion": "The study suggests practical limits to training data size benefits in tokenization and provides insights for optimizing future tokenizer algorithms.", "key_contributions": ["Identified practical limits for tokenizer training data size", "Provided comparative analysis across English and Russian languages", "Informed optimizations in future tokenization research"], "limitations": "", "keywords": ["tokenization", "natural language processing", "training data size", "BPE", "WordPiece"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2503.00985", "pdf": "https://arxiv.org/pdf/2503.00985.pdf", "abs": "https://arxiv.org/abs/2503.00985", "title": "Enhancing Text Editing for Grammatical Error Correction: Arabic as a Case Study", "authors": ["Bashar Alhafni", "Nizar Habash"], "categories": ["cs.CL"], "comment": null, "summary": "Text editing frames grammatical error correction (GEC) as a sequence tagging\nproblem, where edit tags are assigned to input tokens, and applying these edits\nresults in the corrected text. This approach has gained attention for its\nefficiency and interpretability. However, while extensively explored for\nEnglish, text editing remains largely underexplored for morphologically rich\nlanguages like Arabic. In this paper, we introduce a text editing approach that\nderives edit tags directly from data, eliminating the need for\nlanguage-specific edits. We demonstrate its effectiveness on Arabic, a\ndiglossic and morphologically rich language, and investigate the impact of\ndifferent edit representations on model performance. Our approach achieves SOTA\nresults on two Arabic GEC benchmarks and performs on par with SOTA on two\nothers. Additionally, our models are over six times faster than existing Arabic\nGEC systems, making our approach more practical for real-world applications.\nFinally, we explore ensemble models, demonstrating how combining different\nmodels leads to further performance improvements. We make our code, data, and\npretrained models publicly available.", "AI": {"tldr": "This paper presents a text editing approach for grammatical error correction in Arabic by deriving edit tags directly from data, achieving state-of-the-art results and improved efficiency.", "motivation": "The study addresses the gap in grammatical error correction methods for morphologically rich languages like Arabic, focusing on efficiency and interpretability through sequence tagging.", "method": "The authors introduce a data-derived edit tag approach for text editing in Arabic, investigating the impact of different edit representations on model performance and exploring ensemble models for further improvements.", "result": "The proposed approach achieves state-of-the-art results on two Arabic grammatical error correction benchmarks and maintains competitive performance on two additional benchmarks, while being over six times faster than existing systems.", "conclusion": "The results indicate that the proposed method is practical for real-world applications and contributes to the advancement of Arabic grammatical error correction technologies.", "key_contributions": ["Introduction of a data-driven edit tag approach for Arabic GEC", "State-of-the-art performance on multiple benchmarks", "Significantly improved efficiency over existing systems"], "limitations": "", "keywords": ["grammatical error correction", "Arabic", "sequence tagging", "edit tags", "machine learning"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2503.01150", "pdf": "https://arxiv.org/pdf/2503.01150.pdf", "abs": "https://arxiv.org/abs/2503.01150", "title": "MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages", "authors": ["Chen Zhang", "Mingxu Tao", "Zhiyuan Liao", "Yansong Feng"], "categories": ["cs.CL"], "comment": "ACL 2025 (Findings) Code and data available at\n  https://github.com/luciusssss/MiLiC-Eval", "summary": "Large language models (LLMs) excel in high-resource languages but struggle\nwith low-resource languages (LRLs), particularly those spoken by minority\ncommunities in China, such as Tibetan, Uyghur, Kazakh, and Mongolian. To\nsystematically track the progress in these languages, we introduce MiLiC-Eval,\na benchmark designed for minority languages in China, featuring 24K instances\nacross 9 tasks. MiLiC-Eval focuses on underrepresented writing systems. Its\nparallelism between tasks and languages can provide a faithful and fine-grained\nassessment of linguistic and problem-solving skills. Our evaluation reveals\nthat open-source LLMs perform poorly on syntax-intensive tasks and multi-script\nlanguages. We further demonstrate how MiLiC-Eval can help advance LRL research\nin handling diverse writing systems and understanding the process of language\nadaptation.", "AI": {"tldr": "MiLiC-Eval is a benchmark for evaluating language models in low-resource languages, focusing on minority languages in China like Tibetan and Uyghur.", "motivation": "To track progress and improve performance of LLMs on low-resource languages in China, which are often overlooked.", "method": "Introduces a benchmark with 24K instances across 9 tasks specifically designed for underrepresented writing systems.", "result": "Evaluation of open-source LLMs showed poor performance on syntax-intensive tasks and multi-script languages.", "conclusion": "MiLiC-Eval can facilitate advancements in low-resource language research and model adaptation strategies.", "key_contributions": ["Development of MiLiC-Eval benchmark", "Insights into LLMs performance on minority languages", "Focus on underrepresented writing systems"], "limitations": "", "keywords": ["low-resource languages", "language models", "evaluation benchmark"], "importance_score": 6, "read_time_minutes": 5}}
{"id": "2503.01854", "pdf": "https://arxiv.org/pdf/2503.01854.pdf", "abs": "https://arxiv.org/abs/2503.01854", "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models", "authors": ["Jiahui Geng", "Qing Li", "Herbert Woisetschlaeger", "Zongxiong Chen", "Fengyu Cai", "Yuxia Wang", "Preslav Nakov", "Hans-Arno Jacobsen", "Fakhri Karray"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the machine unlearning techniques within the context\nof large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM\nunlearning offers a principled approach to removing the influence of\nundesirable data (e.g., sensitive or illegal information) from LLMs, while\npreserving their overall utility without requiring full retraining. Despite\ngrowing research interest, there is no comprehensive survey that systematically\norganizes existing work and distills key insights; here, we aim to bridge this\ngap. We begin by introducing the definition and the paradigms of LLM\nunlearning, followed by a comprehensive taxonomy of existing unlearning\nstudies. Next, we categorize current unlearning approaches, summarizing their\nstrengths and limitations. Additionally, we review evaluation metrics and\nbenchmarks, providing a structured overview of current assessment\nmethodologies. Finally, we outline promising directions for future research,\nhighlighting key challenges and opportunities in the field.", "AI": {"tldr": "This paper surveys machine unlearning techniques for large language models (LLMs), focusing on a framework for understanding and categorizing existing approaches while identifying future research paths.", "motivation": "To fill the gap of a systematic survey in the growing area of LLM unlearning, providing insights into techniques for removing undesirable data influence without full retraining.", "method": "The study introduces the definition and paradigms of LLM unlearning, categorizes existing approaches into a comprehensive taxonomy, and reviews evaluation metrics and benchmarks.", "result": "The paper successfully categorizes various unlearning techniques, their strengths and weaknesses, and provides a structured overview of assessment methodologies in the field.", "conclusion": "The paper emphasizes the need for continued exploration in LLM unlearning and outlines key challenges and opportunities for future research.", "key_contributions": ["Comprehensive taxonomy of LLM unlearning techniques", "Categorization of existing unlearning approaches", "Review of evaluation metrics and future research directions"], "limitations": "", "keywords": ["machine unlearning", "large language models", "unlearning techniques"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.02450", "pdf": "https://arxiv.org/pdf/2503.02450.pdf", "abs": "https://arxiv.org/abs/2503.02450", "title": "Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization", "authors": ["Yilun Qiu", "Xiaoyan Zhao", "Yang Zhang", "Yimeng Bai", "Wenjie Wang", "Hong Cheng", "Fuli Feng", "Tat-Seng Chua"], "categories": ["cs.CL"], "comment": "2025 ACL Findings", "summary": "Personalizing Large Language Models (LLMs) has become a critical step in\nfacilitating their widespread application to enhance individual life\nexperiences. In pursuit of personalization, distilling key preference\ninformation from an individual's historical data as instructional preference\ncontext to customize LLM generation has emerged as a promising direction.\nHowever, these methods face a fundamental limitation by overlooking the\ninter-user comparative analysis, which is essential for identifying the\ninter-user differences that truly shape preferences. To address this\nlimitation, we propose Difference-aware Personalization Learning (DPL), a novel\napproach that emphasizes extracting inter-user differences to enhance LLM\npersonalization. DPL strategically selects representative users for comparison\nand establishes a structured standard to extract meaningful, task-relevant\ndifferences for customizing LLM generation. Extensive experiments on real-world\ndatasets demonstrate that DPL significantly enhances LLM personalization. We\nrelease our code at https://github.com/SnowCharmQ/DPL.", "AI": {"tldr": "This paper introduces Difference-aware Personalization Learning (DPL) to enhance personalization of Large Language Models (LLMs) by focusing on inter-user differences.", "motivation": "To facilitate widespread application of LLMs by improving personalization based on individual preferences from historical data.", "method": "Difference-aware Personalization Learning (DPL) extracts inter-user differences and selects representative users for comparison to customize LLM generation.", "result": "Extensive experiments on real-world datasets show that DPL significantly enhances LLM personalization compared to previous methods.", "conclusion": "DPL effectively incorporates inter-user comparative analysis, leading to better customization of LLM outputs for individual users.", "key_contributions": ["Novel DPL approach for LLM personalization", "Focus on inter-user differences", "Code release for community use"], "limitations": "", "keywords": ["Large Language Models", "Personalization", "Inter-user differences", "Machine Learning", "Natural Language Processing"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.03340", "pdf": "https://arxiv.org/pdf/2503.03340.pdf", "abs": "https://arxiv.org/abs/2503.03340", "title": "EnigmaToM: Improve LLMs' Theory-of-Mind Reasoning Capabilities with Neural Knowledge Base of Entity States", "authors": ["Hainiu Xu", "Siya Qi", "Jiazheng Li", "Yuxiang Zhou", "Jinhua Du", "Caroline Catmur", "Yulan He"], "categories": ["cs.CL"], "comment": "Findings of ACL 2025", "summary": "Theory-of-Mind (ToM), the ability to infer others' perceptions and mental\nstates, is fundamental to human interaction but remains challenging for Large\nLanguage Models (LLMs). While existing ToM reasoning methods show promise with\nreasoning via perceptual perspective-taking, they often rely excessively on\noff-the-shelf LLMs, reducing their efficiency and limiting their applicability\nto high-order ToM reasoning. To address these issues, we present EnigmaToM, a\nnovel neuro-symbolic framework that enhances ToM reasoning by integrating a\nNeural Knowledge Base of entity states (Enigma) for (1) a psychology-inspired\niterative masking mechanism that facilitates accurate perspective-taking and\n(2) knowledge injection that elicits key entity information. Enigma generates\nstructured knowledge of entity states to build spatial scene graphs for belief\ntracking across various ToM orders and enrich events with fine-grained entity\nstate details. Experimental results on ToMi, HiToM, and FANToM benchmarks show\nthat EnigmaToM significantly improves ToM reasoning across LLMs of varying\nsizes, particularly excelling in high-order reasoning scenarios.", "AI": {"tldr": "EnigmaToM is a neuro-symbolic framework that enhances Theory-of-Mind reasoning in Large Language Models by integrating a Neural Knowledge Base and an iterative masking mechanism.", "motivation": "To improve Theory-of-Mind reasoning capabilities in LLMs, as existing methods often rely too heavily on off-the-shelf models, limiting their effectiveness.", "method": "The paper introduces EnigmaToM, which uses a neuro-symbolic approach combining a Neural Knowledge Base with an iterative masking mechanism to enhance perspective-taking and perform knowledge injection.", "result": "EnigmaToM shows significant improvement in ToM reasoning on various benchmarks, particularly in high-order reasoning scenarios compared to existing methods.", "conclusion": "The framework enables better belief tracking and enriched representations of entity states, making it applicable for complex human interaction tasks.", "key_contributions": ["Proposed EnigmaToM framework for improving ToM reasoning in LLMs.", "Introduced an iterative masking mechanism for better perspective-taking.", "Demonstrated significant performance gains on ToMi, HiToM, and FANToM benchmarks."], "limitations": "The generalizability of the approach to other domains beyond benchmarks remains to be explored.", "keywords": ["Theory-of-Mind", "Large Language Models", "neuro-symbolic systems", "perspective-taking", "knowledge injection"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.04490", "pdf": "https://arxiv.org/pdf/2503.04490.pdf", "abs": "https://arxiv.org/abs/2503.04490", "title": "Large Language Models in Bioinformatics: A Survey", "authors": ["Zhenyu Wang", "Zikang Wang", "Jiyue Jiang", "Pengan Chen", "Xiangyu Shi", "Yu Li"], "categories": ["cs.CL", "q-bio.GN"], "comment": "Accepted by ACL 2025", "summary": "Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.", "AI": {"tldr": "This paper reviews the impact of Large Language Models (LLMs) on bioinformatics, covering genomic modeling, RNA structure, protein function, and single-cell analysis, while addressing challenges and future directions.", "motivation": "To systematically review the advancements brought by LLMs in bioinformatics, highlighting their transformative potential in the field and precision medicine.", "method": "Systematic review of recent advancements in LLM applications within bioinformatics, focusing on specific domains such as genomic sequence modeling and single-cell transcriptomics.", "result": "LLMs have significantly advanced the analysis of biological data, yet challenges like data scarcity and integration across different omics remain.", "conclusion": "LLMs are positioned to drive innovations in bioinformatics and precision medicine, but further research is needed to address existing challenges.", "key_contributions": ["Comprehensive review of LLM applications in bioinformatics", "Identification of key challenges in the field", "Exploration of future research directions including hybrid AI models"], "limitations": "The paper does not provide empirical data or case studies to support the claims.", "keywords": ["Large Language Models", "Bioinformatics", "Genomic Analysis", "Precision Medicine", "RNA Structure"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.04796", "pdf": "https://arxiv.org/pdf/2503.04796.pdf", "abs": "https://arxiv.org/abs/2503.04796", "title": "Optimizing Multi-Hop Document Retrieval Through Intermediate Representations", "authors": ["Jiaen Lin", "Jingyu Liu", "Yingbo Liu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted by ACL 2025 Findings", "summary": "Retrieval-augmented generation (RAG) encounters challenges when addressing\ncomplex queries, particularly multi-hop questions. While several methods tackle\nmulti-hop queries by iteratively generating internal queries and retrieving\nexternal documents, these approaches are computationally expensive. In this\npaper, we identify a three-stage information processing pattern in LLMs during\nlayer-by-layer reasoning, consisting of extraction, processing, and subsequent\nextraction steps. This observation suggests that the representations in\nintermediate layers contain richer information compared to those in other\nlayers. Building on this insight, we propose Layer-wise RAG (L-RAG). Unlike\nprior methods that focus on generating new internal queries, L-RAG leverages\nintermediate representations from the middle layers, which capture next-hop\ninformation, to retrieve external knowledge. L-RAG achieves performance\ncomparable to multi-step approaches while maintaining inference overhead\nsimilar to that of standard RAG. Experimental results show that L-RAG\noutperforms existing RAG methods on open-domain multi-hop question-answering\ndatasets, including MuSiQue, HotpotQA, and 2WikiMultiHopQA. The code is\navailable in https://anonymous.4open.science/r/L-RAG-ADD5/", "AI": {"tldr": "The paper introduces Layer-wise RAG (L-RAG), a method for efficiently addressing multi-hop queries in Retrieval-augmented generation (RAG) by utilizing intermediate layer representations, achieving comparable performance to more costly methods.", "motivation": "This research addresses the computational challenges in retrieving information for complex multi-hop queries in RAG systems.", "method": "The proposed L-RAG utilizes a three-stage information processing pattern in LLMs, extracting and utilizing intermediate layer representations for external knowledge retrieval.", "result": "L-RAG outperforms existing RAG methods on various multi-hop question-answering datasets while maintaining similar inference costs to standard RAG.", "conclusion": "L-RAG provides an efficient alternative for multi-hop question answering tasks, balancing performance and computational overhead.", "key_contributions": ["Introduction of Layer-wise RAG (L-RAG) for multi-hop query handling.", "Demonstration that intermediate layer representations offer richer information for external knowledge retrieval.", "Empirical results showing superior performance of L-RAG on standard datasets."], "limitations": "", "keywords": ["Retrieval-augmented generation", "multi-hop questions", "natural language processing"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.04800", "pdf": "https://arxiv.org/pdf/2503.04800.pdf", "abs": "https://arxiv.org/abs/2503.04800", "title": "HoH: A Dynamic Benchmark for Evaluating the Impact of Outdated Information on Retrieval-Augmented Generation", "authors": ["Jie Ouyang", "Tingyue Pan", "Mingyue Cheng", "Ruiran Yan", "Yucong Luo", "Jiaying Lin", "Qi Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) has emerged as an effective\napproach for addressing the knowledge outdating problem in Large Language\nModels (LLMs), it still faces a critical challenge: the prevalence of outdated\ninformation in knowledge bases. Current research primarily focuses on\nincorporating up-to-date information, yet the impact of outdated information\ncoexisting in retrieval sources remains inadequately addressed. To bridge this\ngap, we introduce HoH, the first benchmark specifically designed to evaluate\nthe impact of outdated information on RAG. Our benchmark leverages token-level\ndiff algorithms combined with LLM pipelines to efficiently create a large-scale\nQA dataset that accurately captures the evolution of temporal knowledge in\nreal-world facts. Through comprehensive experiments, we reveal that outdated\ninformation significantly degrades RAG performance in two critical ways: (1) it\nsubstantially reduces response accuracy by distracting models from correct\ninformation, and (2) it can mislead models into generating potentially harmful\noutputs, even when current information is available. Current RAG approaches\nstruggle with both retrieval and generation aspects when handling outdated\ninformation. These findings highlight the urgent need for innovative solutions\nto address the temporal challenges in RAG. Our code and data are available at:\nhttps://github.com/0russwest0/HoH.", "AI": {"tldr": "This paper introduces HoH, a benchmark for evaluating the impact of outdated information on Retrieval-Augmented Generation in Large Language Models.", "motivation": "Address the issue of outdated information in knowledge bases affecting the performance of Retrieval-Augmented Generation (RAG).", "method": "Developed a benchmark using token-level diff algorithms in combination with LLM pipelines to create a QA dataset capturing temporal knowledge evolution.", "result": "Experiments show that outdated information reduces RAG performance by lowering response accuracy and misleading models into generating harmful outputs.", "conclusion": "There is an urgent need for innovative solutions to mitigate the effects of outdated information in RAG.", "key_contributions": ["Introduction of HoH benchmark for outdated information", "Demonstrated impact of outdated information on RAG performance", "Provided insights into the need for innovative solutions in RAG systems"], "limitations": "", "keywords": ["Retrieval-Augmented Generation", "Large Language Models", "Knowledge Outdating", "QA Dataset", "Temporal Knowledge"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.05750", "pdf": "https://arxiv.org/pdf/2503.05750.pdf", "abs": "https://arxiv.org/abs/2503.05750", "title": "CSTRL: Context-Driven Sequential Transfer Learning for Abstractive Radiology Report Summarization", "authors": ["Mst. Fahmida Sultana Naznin", "Adnan Ibney Faruq", "Mostafa Rifat Tazwar", "Md Jobayer", "Md. Mehedi Hasan Shawon", "Md Rakibul Hasan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7"], "comment": "Accepted in ACL 2025 Findings", "summary": "A radiology report comprises several sections, including the Findings and\nImpression of the diagnosis. Automatically generating the Impression from the\nFindings is crucial for reducing radiologists' workload and improving\ndiagnostic accuracy. Pretrained models that excel in common abstractive\nsummarization problems encounter challenges when applied to specialized medical\ndomains largely due to the complex terminology and the necessity for accurate\nclinical context. Such tasks in medical domains demand extracting core\ninformation, avoiding context shifts, and maintaining proper flow. Misuse of\nmedical terms can lead to drastic clinical errors. To address these issues, we\nintroduce a sequential transfer learning that ensures key content extraction\nand coherent summarization. Sequential transfer learning often faces challenges\nlike initial parameter decay and knowledge loss, which we resolve with the\nFisher matrix regularization. Using MIMIC-CXR and Open-I datasets, our model,\nCSTRL - Context-driven Sequential TRansfer Learning - achieved state-of-the-art\nperformance, showing 56.2% improvement in BLEU-1, 40.5% in BLEU-2, 84.3% in\nBLEU-3, 28.9% in ROUGE-1, 41.0% in ROUGE-2 and 26.5% in ROGUE-3 score over\nbenchmark studies. We also analyze factual consistency scores while preserving\nthe medical context. Our code is publicly available at\nhttps://github.com/fahmidahossain/Report_Summarization.", "AI": {"tldr": "The paper presents a model, CSTRL, for improving the automatic generation of radiology report impressions through a specialized sequential transfer learning approach.", "motivation": "Automatic generation of the Impression section from the Findings in radiology reports is essential for reducing the workload of radiologists and enhancing diagnostic precision.", "method": "The authors introduce a sequential transfer learning technique that emphasizes key content extraction and coherent summarization, overcoming challenges associated with initial parameter decay and knowledge loss by using Fisher matrix regularization.", "result": "CSTRL achieves state-of-the-art performance on MIMIC-CXR and Open-I datasets, with significant improvements in BLEU and ROUGE scores compared to benchmark studies.", "conclusion": "The results demonstrate the model's effectiveness in generating coherent summaries while maintaining factual consistency in medical context.", "key_contributions": ["Development of CSTRL for radiology report summarization", "Implementation of Fisher matrix regularization to enhance transfer learning", "Public availability of the model code for further research"], "limitations": "", "keywords": ["radiology report", "abstractive summarization", "transfer learning", "context preservation", "medical domain"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.05763", "pdf": "https://arxiv.org/pdf/2503.05763.pdf", "abs": "https://arxiv.org/abs/2503.05763", "title": "GMLM: Bridging Graph Neural Networks and Language Models for Heterophilic Node Classification", "authors": ["Aarush Sinha", "OM Kumar CU"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Integrating structured graph data with rich textual information from nodes\nposes a significant challenge, particularly for heterophilic node\nclassification. Current approaches often struggle with computational costs or\neffective fusion of disparate modalities. We propose \\textbf{Graph Masked\nLanguage Model (GMLM)}, a novel architecture efficiently combining Graph Neural\nNetworks (GNNs) with Pre-trained Language Models (PLMs). GMLM introduces three\nkey innovations: (i) a \\textbf{dynamic active node selection} strategy for\nscalable PLM text processing; (ii) a GNN-specific \\textbf{contrastive\npretraining stage} using soft masking with a learnable graph \\texttt{[MASK]}\ntoken for robust structural representations; and (iii) a \\textbf{dedicated\nfusion module} integrating RGCN-based GNN embeddings with PLM (GTE-Small \\&\nDistilBERT) embeddings. Extensive experiments on heterophilic benchmarks\n(Cornell, Wisconsin, Texas) demonstrate GMLM's superiority. Notably,\nGMLM(DistilBERT) achieves significant performance gains, improving accuracy by\nover \\textbf{4.7\\%} on Cornell and over \\textbf{2.0\\%} on Texas compared to the\nprevious best-performing baselines. This work underscores the benefits of\ntargeted PLM engagement and modality-specific pretraining for improved,\nefficient learning on text-rich graphs.", "AI": {"tldr": "The paper presents Graph Masked Language Model (GMLM), a model that integrates Graph Neural Networks with Pre-trained Language Models for better node classification in heterophilic graphs.", "motivation": "The integration of structured graph data with textual information is challenging, especially for heterophilic node classification, where current methods face computational and fusion efficiency issues.", "method": "GMLM employs a dynamic active node selection strategy for scalable processing, a contrastive pretraining stage using soft masking with a learnable graph [MASK] token, and a fusion module that integrates GNN and PLM embeddings.", "result": "GMLM demonstrates superior performance on several heterophilic benchmarks, notably achieving over 4.7% accuracy improvement on the Cornell dataset and over 2.0% on Texas compared to prior models.", "conclusion": "The study highlights the effectiveness of tailored PLM engagement and pretraining for enhanced learning in text-rich graph environments.", "key_contributions": ["Dynamic active node selection for scalability", "Contrastive pretraining with soft masking for structural robustness", "A dedicated fusion module for integrating GNN and PLM embeddings"], "limitations": "", "keywords": ["Graph Neural Networks", "Pre-trained Language Models", "Heterophilic Node Classification", "Dynamic Node Selection", "Contrastive Pretraining"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.06594", "pdf": "https://arxiv.org/pdf/2503.06594.pdf", "abs": "https://arxiv.org/abs/2503.06594", "title": "Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation", "authors": ["Yingfeng Luo", "Tong Zheng", "Yongyu Mu", "Bei Li", "Qinghong Zhang", "Yongqi Gao", "Ziqiang Xu", "Peinan Feng", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025. Please cite the ACL version. Code and\n  data are available at: https://github.com/NiuTrans/LaMaTE", "summary": "The field of neural machine translation (NMT) has changed with the advent of\nlarge language models (LLMs). Much of the recent emphasis in natural language\nprocessing (NLP) has been on modeling machine translation and many other\nproblems using a single pre-trained Transformer decoder, while encoder-decoder\narchitectures, which were the standard in earlier NMT models, have received\nrelatively less attention. In this paper, we explore translation models that\nare universal, efficient, and easy to optimize, by marrying the world of LLMs\nwith the world of NMT. We apply LLMs to NMT encoding and leave the NMT decoder\nunchanged. We also develop methods for adapting LLMs to work better with the\nNMT decoder. Furthermore, we construct a new dataset involving multiple tasks\nto assess how well the machine translation system generalizes across various\ntasks. Evaluations on the WMT and our datasets show that results using our\nmethod match or surpass a range of baselines in terms of translation quality,\nbut achieve $2.4 \\sim 6.5 \\times$ inference speedups and a $75\\%$ reduction in\nthe memory footprint of the KV cache. It also demonstrates strong\ngeneralization across a variety of translation-related tasks.", "AI": {"tldr": "This paper presents a novel approach that combines large language models with traditional neural machine translation architectures to improve translation efficiency and quality.", "motivation": "To enhance neural machine translation (NMT) by integrating large language models (LLMs) while maintaining an efficient and high-quality translation system.", "method": "The authors apply LLMs to the NMT encoding process and optimize them for traditional NMT decoders, while constructing a new dataset to evaluate the generalization across multiple tasks.", "result": "The proposed method achieves translation quality that matches or exceeds various baselines while providing significant improvements in inference speed and memory efficiency.", "conclusion": "The integration of LLMs into NMT demonstrates noteworthy improvements in translation quality and efficiency, suggesting a promising direction for future research in both NMT and general NLP applications.", "key_contributions": ["Integration of LLMs with NMT encoding while retaining traditional decoders", "Development of a new dataset for multi-task evaluation", "Achieved 2.4 to 6.5 times faster inference and a 75% reduction in memory usage."], "limitations": "The paper does not address the potential challenges in fully replacing traditional NMT decoders with LLMs in all scenarios.", "keywords": ["neural machine translation", "large language models", "efficiency", "natural language processing", "translation quality"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2503.07604", "pdf": "https://arxiv.org/pdf/2503.07604.pdf", "abs": "https://arxiv.org/abs/2503.07604", "title": "Implicit Reasoning in Transformers is Reasoning through Shortcuts", "authors": ["Tianhe Lin", "Jian Xie", "Siyu Yuan", "Deqing Yang"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Test-time compute is emerging as a new paradigm for enhancing language\nmodels' complex multi-step reasoning capabilities, as demonstrated by the\nsuccess of OpenAI's o1 and o3, as well as DeepSeek's R1. Compared to explicit\nreasoning in test-time compute, implicit reasoning is more inference-efficient,\nrequiring fewer generated tokens. However, why does the advanced reasoning\ncapability fail to emerge in the implicit reasoning style? In this work, we\ntrain GPT-2 from scratch on a curated multi-step mathematical reasoning dataset\nand conduct analytical experiments to investigate how language models perform\nimplicit reasoning in multi-step tasks. Our findings reveal: 1) Language models\ncan perform step-by-step reasoning and achieve high accuracy in both in-domain\nand out-of-domain tests via implicit reasoning. However, this capability only\nemerges when trained on fixed-pattern data. 2) Conversely, implicit reasoning\nabilities emerging from training on unfixed-pattern data tend to overfit a\nspecific pattern and fail to generalize further. Notably, this limitation is\nalso observed in state-of-the-art large language models. These findings suggest\nthat language models acquire implicit reasoning through shortcut learning,\nenabling strong performance on tasks with similar patterns while lacking\ngeneralization.", "AI": {"tldr": "This paper explores implicit reasoning in language models, revealing their ability to perform step-by-step reasoning with high accuracy under specific training conditions, but also highlights limitations in generalization.", "motivation": "To investigate the capabilities and limitations of implicit reasoning in language models during multi-step reasoning tasks.", "method": "Trained GPT-2 from scratch on a curated multi-step mathematical reasoning dataset and conducted analytical experiments to evaluate performance.", "result": "Language models demonstrate high accuracy in implicit reasoning with fixed-pattern data but struggle to generalize with unfixed-pattern data, showing signs of shortcut learning.", "conclusion": "Implicit reasoning allows for strong performance only in specific patterns, and generalization is often compromised due to overfitting.", "key_contributions": ["Investigation of implicit reasoning in multi-step reasoning tasks", "Identification of the importance of fixed-pattern data for effective learning", "Revelation of generalization limitations in state-of-the-art language models"], "limitations": "The findings indicate that models may rely on shortcut learning, limiting their effectiveness in diverse scenarios.", "keywords": ["language models", "implicit reasoning", "multi-step reasoning", "shortcut learning", "generalization"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.08067", "pdf": "https://arxiv.org/pdf/2503.08067.pdf", "abs": "https://arxiv.org/abs/2503.08067", "title": "Context-aware Biases for Length Extrapolation", "authors": ["Ali Veisi", "Hamidreza Amirzadeh", "Amir Mansourian"], "categories": ["cs.CL"], "comment": "13 pages, 6 figures, 4 table", "summary": "Transformers often struggle to generalize to longer sequences than those seen\nduring training, a limitation known as length extrapolation. Most existing\nRelative Positional Encoding (RPE) methods attempt to address this by\nintroducing either fixed linear biases or globally learned biases, which lack\nthe capacity to adapt to different input contexts. In this work, we propose an\nadditive RPE, Context-Aware Biases for Length Extrapolation (CABLE), a method\nthat learns token-specific, context-aware biases for each attention head in\ntransformers. By dynamically adjusting positional biases based on the input\nsequence, CABLE overcomes the rigidity of fixed RPEs. When evaluated on\nsequences longer than originally trained with, GPT-2 Medium (334M parameters)\nwith CABLE achieves lower perplexity than counterparts using other widely\nadopted positional encoding methods. Additionally, by applying CABLE to the\nBERT base model we improved performance in long-context retrieval tasks. Our\nmethod significantly enhances the extrapolation performance of existing RPE\nmethods tested on the FineWeb-Edu10B and WikiText-103 datasets. Code is\navailable at: https://github.com/axiomlab/cable", "AI": {"tldr": "CABLE introduces context-aware biases in transformers to improve length extrapolation performance.", "motivation": "Transformers struggle with generalizing to longer sequences than seen during training, known as length extrapolation, highlighting a gap in existing Relative Positional Encoding methods.", "method": "The proposed method, CABLE, learns token-specific, context-aware biases for attention heads, allowing for dynamic adjustment based on input sequences.", "result": "CABLE achieved lower perplexity on longer sequences with GPT-2 and improved performance on long-context retrieval tasks with BERT base, outperforming traditional RPE methods.", "conclusion": "CABLE significantly enhances length extrapolation in transformers, addressing the limitations of conventional positional encoding approaches tested on standard datasets.", "key_contributions": ["Introduction of a context-aware positional encoding method that adapts to different input contexts.", "Demonstrated lower perplexity for long sequences using CABLE with GPT-2.", "Enhanced performance in long-context retrieval tasks with BERT base."], "limitations": "", "keywords": ["Transformers", "Relative Positional Encoding", "Human-Computer Interaction", "Machine Learning", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.10084", "pdf": "https://arxiv.org/pdf/2503.10084.pdf", "abs": "https://arxiv.org/abs/2503.10084", "title": "Why Prompt Design Matters and Works: A Complexity Analysis of Prompt Search Space in LLMs", "authors": ["Xiang Zhang", "Juntai Cao", "Jiaqi Wei", "Chenyu You", "Dujian Ding"], "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the remarkable successes of large language models (LLMs), the\nunderlying Transformer architecture has inherent limitations in handling\ncomplex reasoning tasks. Chain-of-thought (CoT) prompting has emerged as a\npractical workaround, but most CoT-based methods rely on a single, generic\nprompt such as \"think step by step\", with no task-specific adaptation. These\napproaches expect the model to discover an effective reasoning path on its own,\nforcing it to search through a vast prompt space. In contrast, several studies\nhave explored task-specific prompt designs to boost performance. However, these\ndesigns are typically developed through trial and error, lacking theoretical\ngrounding. As a result, prompt engineering remains largely ad hoc and unguided.\nIn this paper, we provide a theoretical framework that explains why some\nprompts succeed while others fail. We show that prompts function as selectors,\nextracting task-relevant information from the model's full hidden state during\nCoT reasoning. Each prompt defines a unique trajectory through the answer\nspace, and the choice of trajectory is crucial for task performance and future\nnavigation within the space. We analyze the complexity of finding optimal\nprompts and characterize the size of the prompt space for a given task. Our\ntheory reveals principles behind effective prompt design and shows that naive\nCoT-using self-guided prompts like \"think step by step\"-can severely hinder\nperformance. Through experiments, we show that optimal prompt search can lead\nto more than a 50% improvement on reasoning tasks, providing a theoretical\nfoundation for prompt engineering.", "AI": {"tldr": "This paper introduces a theoretical framework for effective prompt design in large language models (LLMs), emphasizing the importance of task-specific prompts over generic ones in improving performance on reasoning tasks.", "motivation": "The paper addresses limitations in the Transformer architecture of LLMs concerning complex reasoning tasks and the need for better prompt engineering techniques to enhance their performance.", "method": "The authors propose a theoretical framework for analyzing prompts as selectors that extract task-relevant information from the model's hidden state during chain-of-thought (CoT) reasoning. They analyze the complexity of finding optimal prompts and characterize the prompt space for different tasks.", "result": "Experiments show that employing optimal prompt search strategies can lead to over a 50% improvement in performance on reasoning tasks compared to using generic prompting methods.", "conclusion": "The findings provide a theoretical basis for the design of effective prompts, demonstrating that naively guided prompts can hinder performance in reasoning tasks.", "key_contributions": ["Theoretical framework for understanding prompt effectiveness", "Analysis of prompt trajectories in reasoning tasks", "Demonstrated significant performance improvements through optimal prompting"], "limitations": "", "keywords": ["prompt engineering", "large language models", "chain-of-thought prompting", "reasoning tasks", "task-specific prompts"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.13089", "pdf": "https://arxiv.org/pdf/2503.13089.pdf", "abs": "https://arxiv.org/abs/2503.13089", "title": "ClusComp: A Simple Paradigm for Model Compression and Efficient Finetuning", "authors": ["Baohao Liao", "Christian Herold", "Seyyed Hadi Hashemi", "Stefan Vasilev", "Shahram Khadivi", "Christof Monz"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL camera-ready version", "summary": "As large language models (LLMs) scale, model compression is crucial for edge\ndeployment and accessibility. Weight-only quantization reduces model size but\nsuffers from performance degradation at lower bit widths. Moreover, standard\nfinetuning is incompatible with quantized models, and alternative methods often\nfall short of full finetuning. In this paper, we propose ClusComp, a simple yet\neffective compression paradigm that clusters weight matrices into codebooks and\nfinetunes them block-by-block. ClusComp (1) achieves superior performance in\n2-4 bit quantization, (2) pushes compression to 1-bit while outperforming\nultra-low-bit methods with minimal finetuning, and (3) enables efficient\nfinetuning, even surpassing existing quantization-based approaches and rivaling\nfull FP16 finetuning. Notably, ClusComp supports compression and finetuning of\n70B LLMs on a single A6000-48GB GPU.", "AI": {"tldr": "ClusComp is a novel model compression method for LLMs that enhances performance in low-bit quantization and allows efficient finetuning.", "motivation": "As LLMs grow larger, efficient compression methods are necessary for deployment on edge devices without sacrificing performance.", "method": "ClusComp clusters weight matrices into codebooks and finetunes them block-by-block, enabling better performance in quantization at 1-4 bit levels.", "result": "ClusComp shows superior performance in 2-4 bit quantization and effectively finetunes models, even surpassing full FP16 finetuning results.", "conclusion": "ClusComp enables efficient LLM deployment on resource-constrained devices while maintaining high performance through its innovative compression and finetuning approach.", "key_contributions": ["Addresses the challenge of compression for large-scale LLMs.", "Achieves high performance in ultra-low-bit quantization with minimal finetuning.", "Demonstrates capability to finetune 70B LLMs on a single A6000-48GB GPU."], "limitations": "", "keywords": ["model compression", "quantization", "large language models", "finetuning", "ClusComp"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2503.13975", "pdf": "https://arxiv.org/pdf/2503.13975.pdf", "abs": "https://arxiv.org/abs/2503.13975", "title": "Navigating Rifts in Human-LLM Grounding: Study and Benchmark", "authors": ["Omar Shaikh", "Hussein Mozannar", "Gagan Bansal", "Adam Fourney", "Eric Horvitz"], "categories": ["cs.CL", "cs.HC"], "comment": "ACL 2025, 16 pages, 5 figures", "summary": "Language models excel at following instructions but often struggle with the\ncollaborative aspects of conversation that humans naturally employ. This\nlimitation in grounding -- the process by which conversation participants\nestablish mutual understanding -- can lead to outcomes ranging from frustrated\nusers to serious consequences in high-stakes scenarios. To systematically study\ngrounding challenges in human-LLM interactions, we analyze logs from three\nhuman-assistant datasets: WildChat, MultiWOZ, and Bing Chat. We develop a\ntaxonomy of grounding acts and build models to annotate and forecast grounding\nbehavior. Our findings reveal significant differences in human-human and\nhuman-LLM grounding: LLMs were three times less likely to initiate\nclarification and sixteen times less likely to provide follow-up requests than\nhumans. Additionally, we find that early grounding failures predict later\ninteraction breakdowns. Building on these insights, we introduce Rifts, a\nbenchmark derived from publicly available LLM interaction data containing\nsituations where LLMs fail to initiate grounding. We note that current frontier\nmodels perform poorly on Rifts, highlighting the need to reconsider how we\ntrain and prompt LLMs for human interaction. To this end, we develop a\npreliminary intervention aimed at mitigating grounding failures.", "AI": {"tldr": "This paper investigates the grounding challenges in human-LLM interactions, revealing significant differences in communication patterns and proposing a benchmark for improving interactions.", "motivation": "To address the limitations of language models in facilitating collaborative conversations and ensuring mutual understanding in high-stakes scenarios.", "method": "The authors analyze logs from three datasets (WildChat, MultiWOZ, and Bing Chat), develop a taxonomy of grounding acts, and introduce a benchmark called Rifts to study grounding failures in LLM interactions.", "result": "Findings indicate LLMs initiate clarification three times less and follow-up requests sixteen times less compared to humans. Early grounding failures can predict later interaction breakdowns.", "conclusion": "The study highlights the need for improved training and prompting strategies for LLMs to enhance their grounding capabilities in human interactions.", "key_contributions": ["Development of a taxonomy for grounding acts in conversations.", "Introduction of the Rifts benchmark for assessing grounding failures in LLMs.", "Findings on the stark differences between human-human and human-LLM interaction patterns."], "limitations": "The intervention developed is preliminary, and frontier models still perform poorly on the proposed benchmark.", "keywords": ["Grounding", "Language Models", "Human-LLM Interaction", "Benchmarking", "Clarification"], "importance_score": 9, "read_time_minutes": 16}}
{"id": "2503.15351", "pdf": "https://arxiv.org/pdf/2503.15351.pdf", "abs": "https://arxiv.org/abs/2503.15351", "title": "SPILL: Domain-Adaptive Intent Clustering based on Selection and Pooling with Large Language Models", "authors": ["I-Fan Lin", "Faegheh Hasibi", "Suzan Verberne"], "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose Selection and Pooling with Large Language Models\n(SPILL), an intuitive and domain-adaptive method for intent clustering without\nfine-tuning. Existing embeddings-based clustering methods rely on a few labeled\nexamples or unsupervised fine-tuning to optimize results for each new dataset,\nwhich makes them less generalizable to multiple datasets. Our goal is to make\nthese existing embedders more generalizable to new domain datasets without\nfurther fine-tuning. Inspired by our theoretical derivation and simulation\nresults on the effectiveness of sampling and pooling techniques, we view the\nclustering task as a small-scale selection problem. A good solution to this\nproblem is associated with better clustering performance. Accordingly, we\npropose a two-stage approach: First, for each utterance (referred to as the\nseed), we derive its embedding using an existing embedder. Then, we apply a\ndistance metric to select a pool of candidates close to the seed. Because the\nembedder is not optimized for new datasets, in the second stage, we use an LLM\nto further select utterances from these candidates that share the same intent\nas the seed. Finally, we pool these selected candidates with the seed to derive\na refined embedding for the seed. We found that our method generally\noutperforms directly using an embedder, and it achieves comparable results to\nother state-of-the-art studies, even those that use much larger models and\nrequire fine-tuning, showing its strength and efficiency. Our results indicate\nthat our method enables existing embedders to be further improved without\nadditional fine-tuning, making them more adaptable to new domain datasets.\nAdditionally, viewing the clustering task as a small-scale selection problem\ngives the potential of using LLMs to customize clustering tasks according to\nthe user's goals.", "AI": {"tldr": "The paper introduces SPILL, a method for intent clustering using Large Language Models (LLMs) without fine-tuning, improving generalizability to new datasets.", "motivation": "To enhance the generalizability of existing embedding-based clustering methods without requiring fine-tuning, which limits their use across multiple datasets.", "method": "The proposed method involves a two-stage process: first deriving embeddings for each input utterance (seed) and then using distance metrics to select a candidate pool and LLMs to refine intent clustering.", "result": "SPILL outperformed standard embedding methods and achieved results comparable to state-of-the-art techniques, showing improved adaptability to new domain datasets.", "conclusion": "The SPILL method significantly enhances the usability of embeddings for clustering tasks without the need for fine-tuning, allowing for domain customization through LLMs.", "key_contributions": ["Introduction of SPILL for intent clustering", "Demonstration of LLMs in refining clustering tasks", "Improvement of existing embedders' adaptability without fine-tuning"], "limitations": "", "keywords": ["intent clustering", "Large Language Models", "embedding", "domain adaptation", "machine learning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.15469", "pdf": "https://arxiv.org/pdf/2503.15469.pdf", "abs": "https://arxiv.org/abs/2503.15469", "title": "A Dual-Directional Context-Aware Test-Time Learning for Text Classification", "authors": ["Dong Xu", "ZhengLin Lai", "MengYao Liao", "Xueliang Li", "Junkai Ji"], "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Text classification assigns text to predefined categories. Traditional\nmethods struggle with complex structures and long-range dependencies. Deep\nlearning with recurrent neural networks and Transformer models has improved\nfeature extraction and context awareness. However, these models still trade off\ninterpretability, efficiency and contextual range. We propose the Dynamic\nBidirectional Elman Attention Network (DBEAN). DBEAN combines bidirectional\ntemporal modeling and self-attention. It dynamically weights critical input\nsegments and preserves computational efficiency.", "AI": {"tldr": "This paper introduces the Dynamic Bidirectional Elman Attention Network (DBEAN) which enhances text classification by combining bidirectional temporal modeling with self-attention, addressing the interpretability and efficiency limitations of existing deep learning models.", "motivation": "Traditional text classification methods struggle with complex structures and long-range dependencies, leading to inefficiencies and reduced interpretability in deep learning models.", "method": "The DBEAN integrates bidirectional temporal modeling and self-attention mechanisms to dynamically weight important input segments while maintaining computational efficiency.", "result": "DBEAN demonstrates improved performance in text classification tasks by efficiently handling long-range dependencies and complex structures.", "conclusion": "The proposed model outperforms traditional methods and retains interpretability, making it a valuable addition to deep learning approaches for text classification.", "key_contributions": ["Introduction of DBEAN that combines bidirectional temporal modeling with self-attention.", "Dynamic weighting of critical input segments for better efficiency.", "Preservation of interpretability while improving performance in text classification."], "limitations": "The model's performance may still be limited by the quality of the training data and the specific domains it is applied to.", "keywords": ["text classification", "deep learning", "self-attention", "temporal modeling", "DBEAN"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2503.16031", "pdf": "https://arxiv.org/pdf/2503.16031.pdf", "abs": "https://arxiv.org/abs/2503.16031", "title": "Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content", "authors": ["Sai Kartheek Reddy Kasu", "Shankar Biradar", "Sunil Saumya"], "categories": ["cs.CL"], "comment": "7 Pages, 2 figures, 7 tables", "summary": "In the evolving landscape of online discourse, misinformation increasingly\nadopts humorous tones to evade detection and gain traction. This work\nintroduces Deceptive Humor as a novel research direction, emphasizing how false\nnarratives, when coated in humor, can become more difficult to detect and more\nlikely to spread. To support research in this space, we present the Deceptive\nHumor Dataset (DHD) a collection of humor-infused comments derived from\nfabricated claims using the ChatGPT-4o model. Each entry is labeled with a\nSatire Level (from 1 for subtle satire to 3 for overt satire) and categorized\ninto five humor types: Dark Humor, Irony, Social Commentary, Wordplay, and\nAbsurdity. The dataset spans English, Telugu, Hindi, Kannada, Tamil, and their\ncode-mixed forms, making it a valuable resource for multilingual analysis. DHD\noffers a structured foundation for understanding how humor can serve as a\nvehicle for the propagation of misinformation, subtly enhancing its reach and\nimpact. Strong baselines are established to encourage further research and\nmodel development in this emerging area.", "AI": {"tldr": "This paper introduces Deceptive Humor as a novel approach to understanding how humor-infused misinformation spreads and presents the Deceptive Humor Dataset (DHD) with multilingual humor samples.", "motivation": "To address the challenge of detecting misinformation that utilizes humor to enhance its spread and evasion of detection.", "method": "The authors created the Deceptive Humor Dataset (DHD) by generating humor-infused comments from fabricated claims using ChatGPT-4o. Each comment is labeled with a Satire Level and categorized into various humor types.", "result": "The DHD covers multiple languages, presenting a structured resource that supports research on humor's role in misinformation propagation and establishes strong performance baselines for future modeling efforts.", "conclusion": "The introduction of Deceptive Humor as a research direction opens new avenues for examining the interplay between humor and misinformation, with the DHD serving as a foundational dataset.", "key_contributions": ["Introduction of Deceptive Humor as a research concept.", "Creation of the Deceptive Humor Dataset (DHD) with humor-infused misinformation examples.", "Multilingual analysis across several languages and humor types."], "limitations": "The dataset may have limitations regarding the diversity of humor styles and cultural contexts represented.", "keywords": ["Deceptive Humor", "Misinformation", "Dataset", "Humor Types", "Multilingual Analysis"], "importance_score": 4, "read_time_minutes": 15}}
{"id": "2503.20756", "pdf": "https://arxiv.org/pdf/2503.20756.pdf", "abs": "https://arxiv.org/abs/2503.20756", "title": "ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems", "authors": ["Chenxi Wang", "Jizhan Fang", "Xiang Chen", "Bozhong Tian", "Ziwen Xu", "Huajun Chen", "Ningyu Zhang"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Work in progress", "summary": "Recent advancements in Large Multimodal Models (LMMs) have shown promise in\nAutonomous Driving Systems (ADS). However, their direct application to ADS is\nhindered by challenges such as misunderstanding of traffic knowledge, complex\nroad conditions, and diverse states of vehicle. To address these challenges, we\npropose the use of Knowledge Editing, which enables targeted modifications to a\nmodel's behavior without the need for full retraining. Meanwhile, we introduce\nADS-Edit, a multimodal knowledge editing dataset specifically designed for ADS,\nwhich includes various real-world scenarios, multiple data types, and\ncomprehensive evaluation metrics. We conduct comprehensive experiments and\nderive several interesting conclusions. We hope that our work will contribute\nto the further advancement of knowledge editing applications in the field of\nautonomous driving. Code and data are available in\nhttps://github.com/zjunlp/EasyEdit.", "AI": {"tldr": "This paper proposes Knowledge Editing for Large Multimodal Models to enhance Autonomous Driving Systems, introducing a dataset called ADS-Edit for targeted modifications without retraining.", "motivation": "Direct application of Large Multimodal Models (LMMs) to Autonomous Driving Systems (ADS) is hindered by issues like misunderstanding of traffic knowledge and complex road conditions.", "method": "The paper introduces Knowledge Editing to enable targeted modifications of models, and presents the ADS-Edit dataset designed for various real-world ADS scenarios.", "result": "Comprehensive experiments conducted using the ADS-Edit dataset yield insights into knowledge editing applications for improving ADS.", "conclusion": "The authors hope their work advances knowledge editing in autonomous driving, providing tools and datasets for further research.", "key_contributions": ["Introduction of Knowledge Editing for model modifications in ADS", "Development of the ADS-Edit dataset tailored for diverse ADS scenarios", "Insights derived from experiments using the proposed methodology."], "limitations": "The work is still in progress and may not cover all challenges in ADS.", "keywords": ["Large Multimodal Models", "Autonomous Driving Systems", "Knowledge Editing"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2503.20850", "pdf": "https://arxiv.org/pdf/2503.20850.pdf", "abs": "https://arxiv.org/abs/2503.20850", "title": "Both Direct and Indirect Evidence Contribute to Dative Alternation Preferences in Language Models", "authors": ["Qing Yao", "Kanishka Misra", "Leonie Weissweiler", "Kyle Mahowald"], "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) tend to show human-like preferences on a number of\nsyntactic phenomena, but the extent to which these are attributable to direct\nexposure to the phenomena or more general properties of language is unclear. We\nexplore this with the English dative alternation (DO: \"gave Y the X\" vs. PO:\n\"gave the X to Y\"), using a controlled rearing paradigm wherein we iteratively\ntrain small LMs on systematically manipulated input. We focus on properties\nthat affect the choice of alternant: length and animacy. Both properties are\ndirectly present in datives but also reflect more global tendencies for shorter\nelements to precede longer ones and animates to precede inanimates. First, by\nmanipulating and ablating datives for these biases in the input, we show that\ndirect evidence of length and animacy matters, but easy-first preferences\npersist even without such evidence. Then, using LMs trained on systematically\nperturbed datasets to manipulate global length effects (re-linearizing\nsentences globally while preserving dependency structure), we find that dative\npreferences can emerge from indirect evidence. We conclude that LMs' emergent\nsyntactic preferences come from a mix of direct and indirect sources.", "AI": {"tldr": "This paper investigates the sources of human-like syntactic preferences in language models, specifically focusing on the English dative alternation and the effects of length and animacy.", "motivation": "To understand whether human-like preferences in language models stem from direct exposure to syntactic phenomena or from general properties of language.", "method": "The study uses a controlled rearing paradigm, training small language models on systematically manipulated input and analyzing the effects of length and animacy on dative alternation preferences.", "result": "The findings indicate that while direct evidence of length and animacy affects language model preferences, certain preferences can still emerge from indirect evidence, highlighting a blend of direct and indirect influences on syntax.", "conclusion": "The study concludes that language models' syntactic preferences arise from both direct exposure to linguistic features and broader global language properties.", "key_contributions": ["Investigates the dative alternation in language models and its syntactic implications.", "Shows the influence of length and animacy on language model behavior.", "Demonstrates that indirect evidence can lead to emergent syntactic preferences in language models."], "limitations": "", "keywords": ["language models", "dative alternation", "syntax", "length", "animacy"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2503.22877", "pdf": "https://arxiv.org/pdf/2503.22877.pdf", "abs": "https://arxiv.org/abs/2503.22877", "title": "Understanding Inequality of LLM Fact-Checking over Geographic Regions with Agent and Retrieval models", "authors": ["Bruno Coelho", "Shujaat Mirza", "Yuyuan Cui", "Christina Pöpper", "Damon McCoy"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Fact-checking is a potentially useful application of Large Language Models\n(LLMs) to combat the growing dissemination of disinformation. However, the\nperformance of LLMs varies across geographic regions. In this paper, we\nevaluate the factual accuracy of open and private models across a diverse set\nof regions and scenarios.\n  Using a dataset containing 600 fact-checked statements balanced across six\nglobal regions we examine three experimental setups of fact-checking a\nstatement: (1) when just the statement is available, (2) when an LLM-based\nagent with Wikipedia access is utilized, and (3) as a best case scenario when a\nRetrieval-Augmented Generation (RAG) system provided with the official fact\ncheck is employed. Our findings reveal that regardless of the scenario and LLM\nused, including GPT-4, Claude Sonnet, and LLaMA, statements from the Global\nNorth perform substantially better than those from the Global South.\nFurthermore, this gap is broadened for the more realistic case of a Wikipedia\nagent-based system, highlighting that overly general knowledge bases have a\nlimited ability to address region-specific nuances. These results underscore\nthe urgent need for better dataset balancing and robust retrieval strategies to\nenhance LLM fact-checking capabilities, particularly in geographically diverse\ncontexts.", "AI": {"tldr": "The paper evaluates the factual accuracy of LLMs in fact-checking across diverse global regions, highlighting significant performance disparities between the Global North and South.", "motivation": "To assess the effectiveness of LLMs in fact-checking and address the issue of disinformation proliferation.", "method": "Evaluated the performance of various LLMs, including GPT-4, in fact-checking through three scenarios involving different data access levels, using a dataset of 600 statements from six regions.", "result": "LLMs performed significantly better on statements from the Global North, with stark differences in performance noted when utilizing a Wikipedia agent for fact-checking.", "conclusion": "The disparities in LLM performance across geographical regions indicate a need for improved dataset balancing and enhanced retrieval methods for better fact-checking outcomes.", "key_contributions": ["Identified regional performance disparities in LLM fact-checking", "Proposed the necessity for improved dataset balancing", "Highlighted limitations of current retrieval methods in addressing regional nuances"], "limitations": "The study may not encompass all potential LLMs or fact-checking contexts beyond the six examined regions.", "keywords": ["fact-checking", "Large Language Models", "disinformation", "global regions", "Retrieval-Augmented Generation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.01225", "pdf": "https://arxiv.org/pdf/2504.01225.pdf", "abs": "https://arxiv.org/abs/2504.01225", "title": "A Conformal Risk Control Framework for Granular Word Assessment and Uncertainty Calibration of CLIPScore Quality Estimates", "authors": ["Gonçalo Gomes", "Bruno Martins", "Chrysoula Zerva"], "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted at Findings ACL 2025", "summary": "This study explores current limitations of learned image captioning\nevaluation metrics, specifically the lack of granular assessments for errors\nwithin captions, and the reliance on single-point quality estimates without\nconsidering uncertainty. To address the limitations, we propose a simple yet\neffective strategy for generating and calibrating distributions of CLIPScore\nvalues. Leveraging a model-agnostic conformal risk control framework, we\ncalibrate CLIPScore values for task-specific control variables, tackling the\naforementioned limitations. Experimental results demonstrate that using\nconformal risk control, over score distributions produced with simple methods\nsuch as input masking, can achieve competitive performance compared to more\ncomplex approaches. Our method effectively detects erroneous words, while\nproviding formal guarantees aligned with desired risk levels. It also improves\nthe correlation between uncertainty estimations and prediction errors, thus\nenhancing the overall reliability of caption evaluation metrics.", "AI": {"tldr": "This study addresses limitations in image captioning evaluation metrics by proposing a method for calibrating CLIPScore values using a conformal risk control framework.", "motivation": "Current image captioning evaluation metrics lack granular assessments of errors and rely on single-point quality estimates without considering uncertainty.", "method": "The paper proposes a strategy that utilizes a model-agnostic conformal risk control framework to generate calibrated distributions of CLIPScore values for specific tasks.", "result": "Experimental results show that the proposed method outperforms simpler methods like input masking and provides competitive results compared to complex approaches, effectively detecting erroneous words and enhancing reliability.", "conclusion": "The proposed method improves uncertainty estimations and their correlation with prediction errors, thus increasing the reliability of caption evaluation metrics.", "key_contributions": ["Development of a model-agnostic conformal risk control framework for calibrating CLIPScore values", "Demonstrated advantage over simpler methods for image captioning error detection", "Enhanced reliability and correlation of uncertainty estimations with prediction errors"], "limitations": "", "keywords": ["image captioning", "CLIPScore", "conformal risk control", "evaluation metrics", "uncertainty estimation"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2504.01919", "pdf": "https://arxiv.org/pdf/2504.01919.pdf", "abs": "https://arxiv.org/abs/2504.01919", "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation", "authors": ["Baban Gain", "Dibyanayan Bandyopadhyay", "Asif Ekbal"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the\nlandscape of machine translation (MT), particularly for low-resource languages\nand domains that lack sufficient parallel corpora, linguistic tools, and\ncomputational infrastructure. This survey presents a comprehensive overview of\nrecent progress in leveraging LLMs for MT. We analyze techniques such as\nfew-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning\n(e.g., LoRA, adapters) that enable effective adaptation to under-resourced\nsettings. The paper also explores synthetic data generation strategies using\nLLMs, including back-translation and lexical augmentation. Additionally, we\ncompare LLM-based translation with traditional encoder-decoder models across\ndiverse language pairs, highlighting the strengths and limitations of each. We\ndiscuss persistent challenges - such as hallucinations, evaluation\ninconsistencies, and inherited biases, while also evaluating emerging\nLLM-driven metrics for translation quality. This survey offers practical\ninsights and outlines future directions for building robust, inclusive, and\nscalable MT systems in the era of large-scale generative models.", "AI": {"tldr": "Survey on advancements in using LLMs for machine translation (MT) of low-resource languages, discussing techniques, challenges, and future directions.", "motivation": "To provide an overview of how Large Language Models can be effectively utilized for machine translation, particularly in low-resource settings.", "method": "Analyze techniques like few-shot prompting, cross-lingual transfer, parameter-efficient fine-tuning, and synthetic data generation for MT.", "result": "The survey highlights advancements in LLM-based MT techniques, compares them with traditional models, and identifies persistent challenges in adopting these systems.", "conclusion": "Future directions for robust and scalable MT systems using LLMs are discussed, addressing evaluation and bias challenges.", "key_contributions": ["Comprehensive overview of LLM techniques in MT", "Comparison of LLM-based translation vs traditional models", "Insights into challenges and future directions in MT using LLMs"], "limitations": "Discussion of challenges like hallucinations, evaluation inconsistencies, and biases in LLMs.", "keywords": ["Large Language Models", "Machine Translation", "Low-resource languages", "Few-shot prompting", "Synthetic data generation"], "importance_score": 9, "read_time_minutes": 20}}
{"id": "2504.05214", "pdf": "https://arxiv.org/pdf/2504.05214.pdf", "abs": "https://arxiv.org/abs/2504.05214", "title": "Post-Training Language Models for Continual Relation Extraction", "authors": ["Sefika Efeoglu", "Adrian Paschke", "Sonja Schimmler"], "categories": ["cs.CL"], "comment": "17 pages, Initial Results and Reporting of the work", "summary": "Real-world data, such as news articles, social media posts, and chatbot\nconversations, is inherently dynamic and non-stationary, presenting significant\nchallenges for constructing real-time structured representations through\nknowledge graphs (KGs). Relation Extraction (RE), a fundamental component of KG\ncreation, often struggles to adapt to evolving data when traditional models\nrely on static, outdated datasets. Continual Relation Extraction (CRE) methods\ntackle this issue by incrementally learning new relations while preserving\npreviously acquired knowledge. This study investigates the application of\npre-trained language models (PLMs), specifically large language models (LLMs),\nto CRE, with a focus on leveraging memory replay to address catastrophic\nforgetting. We evaluate decoder-only models (eg, Mistral-7B and Llama2-7B) and\nencoder-decoder models (eg, Flan-T5 Base) on the TACRED and FewRel datasets.\nTask-incremental fine-tuning of LLMs demonstrates superior performance over\nearlier approaches using encoder-only models like BERT on TACRED, excelling in\nseen-task accuracy and overall performance (measured by whole and average\naccuracy), particularly with the Mistral and Flan-T5 models. Results on FewRel\nare similarly promising, achieving second place in whole and average accuracy\nmetrics. This work underscores critical factors in knowledge transfer, language\nmodel architecture, and KG completeness, advancing CRE with LLMs and memory\nreplay for dynamic, real-time relation extraction.", "AI": {"tldr": "This study explores Continual Relation Extraction (CRE) using pre-trained language models (PLMs), emphasizing memory replay to mitigate catastrophic forgetting. It demonstrates that task-incremental fine-tuning of LLMs significantly outperforms traditional models in relation extraction tasks.", "motivation": "The inherent dynamism of real-world data presents challenges for constructing structured representations via knowledge graphs, necessitating models that can adapt to evolving information.", "method": "The paper evaluates decoder-only models (Mistral-7B and Llama2-7B) and encoder-decoder models (Flan-T5 Base) on TACRED and FewRel datasets, using task-incremental fine-tuning to assess performance in Continual Relation Extraction.", "result": "Task-incremental fine-tuning of LLMs, particularly with Mistral and Flan-T5, shows superior performance compared to traditional encoder-only models like BERT on TACRED, and strong results on FewRel, achieving second place in metrics for both datasets.", "conclusion": "The findings highlight key factors in knowledge transfer and model architecture, advancing the application of LLMs and memory replay techniques in continual relation extraction.", "key_contributions": ["Application of LLMs for Continual Relation Extraction", "Use of memory replay to combat catastrophic forgetting", "Demonstrated superior accuracy in relation extraction tasks compared to traditional models"], "limitations": "", "keywords": ["Continual Relation Extraction", "Large Language Models", "Memory Replay", "Knowledge Graphs", "Dynamic Data"], "importance_score": 9, "read_time_minutes": 17}}
{"id": "2504.07527", "pdf": "https://arxiv.org/pdf/2504.07527.pdf", "abs": "https://arxiv.org/abs/2504.07527", "title": "Supervised Optimism Correction: Be Confident When LLMs Are Sure", "authors": ["Junjie Zhang", "Rushuai Yang", "Shunyu Liu", "Ting-En Lin", "Fei Huang", "Yi Chen", "Yongbin Li", "Dacheng Tao"], "categories": ["cs.CL"], "comment": null, "summary": "In this work, we establish a novel theoretical connection between supervised\nfine-tuning and offline reinforcement learning under the token-level Markov\ndecision process, revealing that large language models indeed learn an implicit\n$Q$-function for inference. Through this theoretical lens, we demonstrate that\nthe widely used beam search method suffers from unacceptable over-optimism,\nwhere inference errors are inevitably amplified due to inflated $Q$-value\nestimations of suboptimal steps. To address this limitation, we propose\nSupervised Optimism Correction(SOC), which introduces a simple yet effective\nauxiliary loss for token-level $Q$-value estimations during supervised\nfine-tuning. Specifically, the auxiliary loss employs implicit value\nregularization to boost model confidence in expert-demonstrated responses,\nthereby suppressing over-optimism toward insufficiently supervised responses.\nExtensive experiments on mathematical reasoning benchmarks, including GSM8K,\nMATH, and GAOKAO, showcase the superiority of the proposed SOC with beam search\nacross a series of open-source models.", "AI": {"tldr": "This paper connects supervised fine-tuning with offline reinforcement learning for large language models, proposing an optimization method to mitigate inference errors in beam search.", "motivation": "To explore the connection between supervised fine-tuning and offline reinforcement learning, particularly in the context of large language models and their inference processes.", "method": "The paper introduces Supervised Optimism Correction (SOC), an auxiliary loss for improving token-level Q-value estimations during supervised fine-tuning, addressing the over-optimism in beam search methods.", "result": "Experimental results demonstrate that SOC significantly improves performance on mathematical reasoning benchmarks compared to traditional beam search methods.", "conclusion": "Integrating SOC leads to better model confidence and performance in inference tasks, particularly for large language models focused on mathematical reasoning.", "key_contributions": ["Establishes a theoretical link between supervised fine-tuning and offline reinforcement learning", "Proposes Supervised Optimism Correction (SOC) as a solution to mitigate over-optimism in inference", "Demonstrates superiority of SOC through extensive experiments on benchmark tasks"], "limitations": "", "keywords": ["Supervised Fine-Tuning", "Reinforcement Learning", "Large Language Models", "Beam Search", "Mathematical Reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.08838", "pdf": "https://arxiv.org/pdf/2504.08838.pdf", "abs": "https://arxiv.org/abs/2504.08838", "title": "SD$^2$: Self-Distilled Sparse Drafters", "authors": ["Mike Lasby", "Nish Sinnadurai", "Valavan Manohararajah", "Sean Lie", "Yani Ioannou", "Vithursan Thangarasa"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "24 pages", "summary": "Speculative decoding is a powerful technique for reducing the latency of\nLarge Language Models (LLMs), offering a fault-tolerant framework that enables\nthe use of highly compressed draft models. In this work, we introduce\nSelf-Distilled Sparse Drafters (SD$^2$), a novel methodology that leverages\nself-data distillation and fine-grained weight sparsity to produce highly\nefficient and well-aligned draft models. SD$^2$ systematically enhances draft\ntoken acceptance rates while significantly reducing Multiply-Accumulate\noperations (MACs), even in the Universal Assisted Generation (UAG) setting,\nwhere draft and target models originate from different model families. On a\nLlama-3.1-70B target model, SD$^2$ provides a 1.59$\\times$ higher Mean Accepted\nLength (MAL) compared to layer-pruned draft models and reduces MACs by over\n43.87% with a 8.36% reduction in MAL compared to a dense draft models. Our 1.5B\nand 3B unstructured sparse drafters outperform both dense and layer-pruned\nmodels in terms of end-to-end latency improvements; highlighting the potential\nof sparsity-aware fine-tuning and compression strategies to improve LLM\ninference efficiency while maintaining alignment with target models.", "AI": {"tldr": "This paper presents Self-Distilled Sparse Drafters (SD²), a method that enhances LLM performance by reducing latency and optimizing draft models through self-data distillation and sparsity techniques.", "motivation": "To improve the inference efficiency of Large Language Models by leveraging speculative decoding and fault-tolerant approaches.", "method": "The authors introduce Self-Distilled Sparse Drafters (SD²) which combine self-data distillation with weight sparsity to create efficient draft models.", "result": "SD² achieves a 1.59× higher Mean Accepted Length (MAL) while reducing Multiply-Accumulate operations by over 43.87% compared to dense models, and outperforms dense and layer-pruned models in latency.", "conclusion": "The findings suggest that sparsity-aware fine-tuning can significantly enhance the performance of LLMs without compromising alignment with target models.", "key_contributions": ["Introduction of Self-Distilled Sparse Drafters (SD²) methodology", "Demonstrated efficiency through reduction in MACs and improvement in MAL", "Outperformance of unstructured sparse drafters against dense and layer-pruned models."], "limitations": "", "keywords": ["Large Language Models", "Latency Reduction", "Self-Distillation", "Sparsity", "Inference Efficiency"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.09923", "pdf": "https://arxiv.org/pdf/2504.09923.pdf", "abs": "https://arxiv.org/abs/2504.09923", "title": "Guiding Reasoning in Small Language Models with LLM Assistance", "authors": ["Yujin Kim", "Euiin Yi", "Minu Kim", "Se-Young Yun", "Taehyeon Kim"], "categories": ["cs.CL"], "comment": "20 pages, 12 figures, 9 tables", "summary": "The limited reasoning capabilities of small language models (SLMs) cast doubt\non their suitability for tasks demanding deep, multi-step logical deduction.\nThis paper introduces a framework called Small Reasons, Large Hints (SMART),\nwhich selectively augments SLM reasoning with targeted guidance from large\nlanguage models (LLMs). Inspired by the concept of cognitive scaffolding, SMART\nemploys a score-based evaluation to identify uncertain reasoning steps and\ninjects corrective LLM-generated reasoning only when necessary. By framing\nstructured reasoning as an optimal policy search, our approach steers the\nreasoning trajectory toward correct solutions without exhaustive sampling. Our\nexperiments on mathematical reasoning datasets demonstrate that targeted\nexternal scaffolding significantly improves performance, paving the way for\ncollaborative use of both SLM and LLM to tackle complex reasoning tasks that\nare currently unsolvable by SLMs alone.", "AI": {"tldr": "This paper presents a framework called SMART that enhances small language models' reasoning capabilities using large language models' guidance.", "motivation": "To address the limited reasoning capabilities of small language models in tasks requiring deep logical deduction.", "method": "SMART employs a score-based evaluation to identify uncertain reasoning steps in SLMs and provides LLM-generated guidance only when needed, framing reasoning as an optimal policy search.", "result": "Experiments show that targeted external scaffolding improves the performance of SLMs on complex reasoning tasks.", "conclusion": "The collaborative use of SLMs and LLMs can address reasoning tasks that small language models currently struggle to solve alone.", "key_contributions": ["Introduction of the SMART framework for enhancing SLM reasoning", "Score-based evaluation for selective LLM guidance", "Demonstration of improved performance on mathematical reasoning datasets"], "limitations": "", "keywords": ["small language models", "large language models", "reasoning", "cognitive scaffolding", "mathematical reasoning"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.11042", "pdf": "https://arxiv.org/pdf/2504.11042.pdf", "abs": "https://arxiv.org/abs/2504.11042", "title": "LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews", "authors": ["Sukannya Purkayastha", "Zhuang Li", "Anne Lauscher", "Lizhen Qu", "Iryna Gurevych"], "categories": ["cs.CL"], "comment": "Accepted at ACL 2025: 29 pages, 18 Figures, 15 Tables", "summary": "Peer review is a cornerstone of quality control in scientific publishing.\nWith the increasing workload, the unintended use of `quick' heuristics,\nreferred to as lazy thinking, has emerged as a recurring issue compromising\nreview quality. Automated methods to detect such heuristics can help improve\nthe peer-reviewing process. However, there is limited NLP research on this\nissue, and no real-world dataset exists to support the development of detection\ntools. This work introduces LazyReview, a dataset of peer-review sentences\nannotated with fine-grained lazy thinking categories. Our analysis reveals that\nLarge Language Models (LLMs) struggle to detect these instances in a zero-shot\nsetting. However, instruction-based fine-tuning on our dataset significantly\nboosts performance by 10-20 performance points, highlighting the importance of\nhigh-quality training data. Furthermore, a controlled experiment demonstrates\nthat reviews revised with lazy thinking feedback are more comprehensive and\nactionable than those written without such feedback. We will release our\ndataset and the enhanced guidelines that can be used to train junior reviewers\nin the community. (Code available here:\nhttps://github.com/UKPLab/arxiv2025-lazy-review)", "AI": {"tldr": "The paper introduces LazyReview, a dataset aimed at detecting lazy thinking in peer reviews, and shows that LLMs can improve with fine-tuning on this dataset.", "motivation": "To address the problem of lazy thinking in peer reviews that compromises quality, by providing a dataset to develop detection tools.", "method": "The study introduces the LazyReview dataset with annotated peer-review sentences and evaluates performance of LLMs in detecting lazy thinking before and after fine-tuning.", "result": "LLMs demonstrate poor detection in a zero-shot setting but perform significantly better (10-20 points) after instruction-based fine-tuning on the dataset.", "conclusion": "Lazy thinking feedback improves the comprehensiveness and actionability of peer reviews; the dataset will be released for community use.", "key_contributions": ["Introduction of the LazyReview dataset for peer review analysis", "Demonstration of the effectiveness of LLMs after fine-tuning on this dataset", "Guidelines for training junior reviewers based on findings"], "limitations": "The dataset's applicability may be limited to certain domains of peer review and may require further expansion for broader utility.", "keywords": ["Peer Review", "Lazy Thinking", "NLP", "Large Language Models", "Dataset"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2505.22759", "pdf": "https://arxiv.org/pdf/2505.22759.pdf", "abs": "https://arxiv.org/abs/2505.22759", "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian", "authors": ["Sara Papi", "Marco Gaido", "Luisa Bentivogli", "Alessio Brutti", "Mauro Cettolo", "Roberto Gretter", "Marco Matassoni", "Mohamed Nabih", "Matteo Negri"], "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.", "AI": {"tldr": "FAMA is the first open science speech foundation model family for English and Italian, trained on over 150k hours of open-source speech data, addressing reproducibility issues in the field.", "motivation": "To advance speech processing with fully transparent models and datasets due to the closed nature of existing speech foundation models which limits reproducibility and fair evaluation.", "method": "Introduced FAMA, an open science family of speech foundation models, trained on 150k+ hours of open-source speech data, and created a new dataset with 16k hours of cleaned and pseudo-labeled speech.", "result": "FAMA exhibits competitive performance with existing speech foundation models while operating up to 8 times faster.", "conclusion": "FAMA promotes openness in speech technology research by releasing all artifacts under open-source licenses, contributing to the field's progress.", "key_contributions": ["Introduction of FAMA, the first family of open science speech foundation models for English and Italian.", "Creation of a new dataset containing 16k hours of cleaned and pseudo-labeled speech.", "Demonstration of competitive performance and efficiency compared to current SFMs."], "limitations": "", "keywords": ["speech foundation models", "open science", "data reproducibility", "speech processing", "open-source"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.22919", "pdf": "https://arxiv.org/pdf/2505.22919.pdf", "abs": "https://arxiv.org/abs/2505.22919", "title": "ER-REASON: A Benchmark Dataset for LLM-Based Clinical Reasoning in the Emergency Room", "authors": ["Nikita Mehandru", "Niloufar Golchini", "David Bamman", "Travis Zack", "Melanie F. Molina", "Ahmed Alaa"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been extensively evaluated on medical\nquestion answering tasks based on licensing exams. However, real-world\nevaluations often depend on costly human annotators, and existing benchmarks\ntend to focus on isolated tasks that rarely capture the clinical reasoning or\nfull workflow underlying medical decisions. In this paper, we introduce\nER-Reason, a benchmark designed to evaluate LLM-based clinical reasoning and\ndecision-making in the emergency room (ER)--a high-stakes setting where\nclinicians make rapid, consequential decisions across diverse patient\npresentations and medical specialties under time pressure. ER-Reason includes\ndata from 3,984 patients, encompassing 25,174 de-identified longitudinal\nclinical notes spanning discharge summaries, progress notes, history and\nphysical exams, consults, echocardiography reports, imaging notes, and ER\nprovider documentation. The benchmark includes evaluation tasks that span key\nstages of the ER workflow: triage intake, initial assessment, treatment\nselection, disposition planning, and final diagnosis--each structured to\nreflect core clinical reasoning processes such as differential diagnosis via\nrule-out reasoning. We also collected 72 full physician-authored rationales\nexplaining reasoning processes that mimic the teaching process used in\nresidency training, and are typically absent from ER documentation. Evaluations\nof state-of-the-art LLMs on ER-Reason reveal a gap between LLM-generated and\nclinician-authored clinical reasoning for ER decisions, highlighting the need\nfor future research to bridge this divide.", "AI": {"tldr": "The paper introduces ER-Reason, a benchmark for evaluating clinical reasoning and decision-making of LLMs in an emergency room context, highlighting the gap between LLM outputs and clinician reasoning.", "motivation": "To address the limitations of existing benchmarks that fail to capture the complexities of clinical decision-making and to improve the evaluation of LLMs in high-stakes medical environments.", "method": "The paper presents ER-Reason, a comprehensive benchmark that includes clinical data from 3,984 patients and emphasizes key workflows in the emergency room, evaluating LLMs on tasks reflecting real clinical reasoning, complemented by rationales from physicians.", "result": "State-of-the-art LLMs underperformed compared to clinician-authored rationales, indicating a significant gap in the clinical reasoning capability of LLMs in emergency decision-making contexts.", "conclusion": "The findings underscore the necessity for enhanced understanding of LLMs' clinical reasoning and call for future research to address identified gaps in their performance in medical contexts.", "key_contributions": ["Introduction of ER-Reason benchmark for evaluating LLMs in clinical scenarios", "Inclusion of diverse clinical data types relevant to emergency room workflows", "Collection of physician-authored rationales to enhance understanding of clinical reasoning processes."], "limitations": "The benchmark relies on historical clinical data and may not account for emerging medical practices.", "keywords": ["language models", "clinical reasoning", "emergency room", "health informatics", "benchmarking"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2505.23026", "pdf": "https://arxiv.org/pdf/2505.23026.pdf", "abs": "https://arxiv.org/abs/2505.23026", "title": "Context-Robust Knowledge Editing for Language Models", "authors": ["Haewon Park", "Gyubin Choi", "Minjun Kim", "Yohan Jo"], "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings. Our code and datasets are available at\n  https://github.com/holi-lab/CoRE", "summary": "Knowledge editing (KE) methods offer an efficient way to modify knowledge in\nlarge language models. Current KE evaluations typically assess editing success\nby considering only the edited knowledge without any preceding contexts. In\nreal-world applications, however, preceding contexts often trigger the\nretrieval of the original knowledge and undermine the intended edit. To address\nthis issue, we develop CHED -- a benchmark designed to evaluate the context\nrobustness of KE methods. Evaluations on CHED show that they often fail when\npreceding contexts are present. To mitigate this shortcoming, we introduce\nCoRE, a KE method designed to strengthen context robustness by minimizing\ncontext-sensitive variance in hidden states of the model for edited knowledge.\nThis method not only improves the editing success rate in situations where a\npreceding context is present but also preserves the overall capabilities of the\nmodel. We provide an in-depth analysis of the differing impacts of preceding\ncontexts when introduced as user utterances versus assistant responses, and we\ndissect attention-score patterns to assess how specific tokens influence\nediting success.", "AI": {"tldr": "This paper develops a benchmark, CHED, to evaluate the context robustness of knowledge editing methods in large language models, introducing a new method, CoRE, to enhance editing success in the presence of contextual information.", "motivation": "To address the shortcomings of current knowledge editing evaluations that ignore the impact of preceding contexts in real-world applications, which can trigger retrieval of original knowledge and undermine edits.", "method": "The proposed CHED benchmark evaluates the context robustness of knowledge editing methods. CoRE is introduced as a new KE method that minimizes context-sensitive variance in hidden states to improve editing success when context is present.", "result": "Evaluations reveal that existing KE methods often fail when preceding contexts are presented. CoRE significantly improves editing success rates while preserving model capabilities.", "conclusion": "The ability to edit knowledge is enhanced by considering context robustness, revealing the need for methods like CoRE that accommodate real-world application scenarios.", "key_contributions": ["Introduction of the CHED benchmark for evaluating context robustness in KE methods.", "Development of the CoRE method to improve knowledge editing success in the presence of contextual information.", "Analysis of the effects of different types of preceding contexts on editing outcomes."], "limitations": "", "keywords": ["knowledge editing", "context robustness", "large language models"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23126", "pdf": "https://arxiv.org/pdf/2505.23126.pdf", "abs": "https://arxiv.org/abs/2505.23126", "title": "PBEBench: A Multi-Step Programming by Examples Reasoning Benchmark inspired by Historical Linguistics", "authors": ["Atharva Naik", "Darsh Agrawal", "Manav Kapadnis", "Yuwei An", "Yash Mathur", "Carolyn Rose", "David Mortensen"], "categories": ["cs.CL"], "comment": null, "summary": "Recently, long chain of thought (LCoT), Large Language Models (LLMs), have\ntaken the machine learning world by storm with their breathtaking reasoning\ncapabilities. However, are the abstract reasoning abilities of these models\ngeneral enough for problems of practical importance? Unlike past work, which\nhas focused mainly on math, coding, and data wrangling, we focus on a\nhistorical linguistics-inspired inductive reasoning problem, formulated as\nProgramming by Examples. We develop a fully automated pipeline for dynamically\ngenerating a benchmark for this task with controllable difficulty in order to\ntackle scalability and contamination issues to which many reasoning benchmarks\nare subject. Using our pipeline, we generate a test set with nearly 1k\ninstances that is challenging for all state-of-the-art reasoning LLMs, with the\nbest model (Claude-3.7-Sonnet) achieving a mere 54% pass rate, demonstrating\nthat LCoT LLMs still struggle with a class or reasoning that is ubiquitous in\nhistorical linguistics as well as many other domains.", "AI": {"tldr": "This paper investigates the practical reasoning capabilities of Large Language Models (LLMs) in a historical linguistics-inspired task, finding that they struggle despite advances in reasoning ability.", "motivation": "To explore whether the abstract reasoning abilities of Large Language Models (LLMs) are viable for practical problems outside traditional domains.", "method": "The authors developed a fully automated pipeline to generate a benchmark for historical linguistics-inspired inductive reasoning, adjustable for difficulty.", "result": "Nearly 1,000 test instances were generated, showing that even the best state-of-the-art LLM struggled, achieving only a 54% pass rate.", "conclusion": "LLM reasoning capabilities remain limited, specifically in domains like historical linguistics that require a different type of reasoning.", "key_contributions": ["Introduction of a new benchmark for reasoning in historical linguistics inspired by Programming by Examples.", "Automation of benchmark generation to address scalability and contamination problems.", "Demonstration of LLM struggles with reasoning tasks that are common in historical linguistics."], "limitations": "The focus is primarily on reasoning related to historical linguistics, possibly limiting generalizability beyond this domain.", "keywords": ["Large Language Models", "reasoning", "historical linguistics", "benchmarking", "inductive reasoning"], "importance_score": 7, "read_time_minutes": 5}}
{"id": "2505.23291", "pdf": "https://arxiv.org/pdf/2505.23291.pdf", "abs": "https://arxiv.org/abs/2505.23291", "title": "ScEdit: Script-based Assessment of Knowledge Editing", "authors": ["Xinye Li", "Zunwen Zheng", "Qian Zhang", "Dekai Zhuang", "Jiabao Kang", "Liyan Xu", "Qingbin Liu", "Xi Chen", "Zhiying Tu", "Dianhui Chu", "Dianbo Sui"], "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Knowledge Editing (KE) has gained increasing attention, yet current KE tasks\nremain relatively simple. Under current evaluation frameworks, many editing\nmethods achieve exceptionally high scores, sometimes nearing perfection.\nHowever, few studies integrate KE into real-world application scenarios (e.g.,\nrecent interest in LLM-as-agent). To support our analysis, we introduce a novel\nscript-based benchmark -- ScEdit (Script-based Knowledge Editing Benchmark) --\nwhich encompasses both counterfactual and temporal edits. We integrate\ntoken-level and text-level evaluation methods, comprehensively analyzing\nexisting KE techniques. The benchmark extends traditional fact-based\n(\"What\"-type question) evaluation to action-based (\"How\"-type question)\nevaluation. We observe that all KE methods exhibit a drop in performance on\nestablished metrics and face challenges on text-level metrics, indicating a\nchallenging task. Our benchmark is available at\nhttps://github.com/asdfo123/ScEdit.", "AI": {"tldr": "The paper introduces the ScEdit benchmark for evaluating Knowledge Editing (KE), addressing limitations in current KE methods by incorporating both counterfactual and temporal edits along with diverse evaluation metrics.", "motivation": "To enhance the evaluation of Knowledge Editing methods, which have high scores in traditional metrics but lack real-world application integration.", "method": "The authors developed the ScEdit benchmark, which includes counterfactual and temporal edits, and incorporates both token-level and text-level evaluation techniques.", "result": "The introduction of ScEdit reveals that existing KE methods struggle with performance on new evaluation metrics, particularly text-level metrics, indicating challenges in the editing process.", "conclusion": "The novel benchmark aims to provide a more realistic assessment of KE methods, highlighting their limitations and areas for improvement in real-world applications.", "key_contributions": ["Introduction of the ScEdit benchmark for Knowledge Editing evaluation", "Integration of action-based evaluation in addition to traditional fact-based metrics", "Comprehensive analysis of existing KE techniques using diverse metrics"], "limitations": "The study indicates a performance drop in KE methods on new metrics, suggesting inherent challenges in the editing tasks.", "keywords": ["Knowledge Editing", "ScEdit", "Benchmark", "Evaluation", "Natural Language Processing"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2505.23657", "pdf": "https://arxiv.org/pdf/2505.23657.pdf", "abs": "https://arxiv.org/abs/2505.23657", "title": "Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation", "authors": ["Hongxiang Zhang", "Hao Chen", "Muhao Chen", "Tianyi Zhang"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent decoding methods improve the factuality of large language models\n(LLMs) by refining how the next token is selected during generation. These\nmethods typically operate at the token level, leveraging internal\nrepresentations to suppress superficial patterns. Nevertheless, LLMs remain\nprone to hallucinations, especially over longer contexts. In this paper, we\npropose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy\nthat actively decides when to apply contrasting layers during generation. By\ncasting decoding as a sequential decision-making problem, ActLCD employs a\nreinforcement learning policy guided by a reward-aware classifier to optimize\nfactuality beyond the token level. Our experiments demonstrate that ActLCD\nsurpasses state-of-the-art methods across five benchmarks, showcasing its\neffectiveness in mitigating hallucinations in diverse generation scenarios.", "AI": {"tldr": "This paper introduces Active Layer-Contrastive Decoding (ActLCD), a new method to enhance the factuality of large language models (LLMs) by optimizing token selection during text generation.", "motivation": "To improve the factual accuracy of large language models, particularly in addressing hallucinations that occur during longer contexts in text generation.", "method": "ActLCD employs a reinforcement learning policy that decides when to apply contrasting layers in the decoding process, treating this selection as a sequential decision-making problem.", "result": "ActLCD outperforms existing state-of-the-art decoding methods across five different benchmarks, effectively reducing hallucinations.", "conclusion": "The proposed ActLCD strategy significantly enhances the factuality of LLMs during generation, showing promise for wider applications in text generation tasks.", "key_contributions": ["Introduces a novel decoding strategy called Active Layer-Contrastive Decoding (ActLCD).", "Implements a reinforcement learning approach to optimize the token selection process.", "Demonstrates superior performance in factuality over existing methods across multiple benchmarks."], "limitations": "The paper does not address potential computational overhead introduced by the reinforcement learning policy used in ActLCD.", "keywords": ["Large Language Models", "Decoding Methods", "Reinforcement Learning", "Factuality", "Hallucination Mitigation"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2505.23729", "pdf": "https://arxiv.org/pdf/2505.23729.pdf", "abs": "https://arxiv.org/abs/2505.23729", "title": "Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time", "authors": ["Mohamad Chehade", "Soumya Suvra Ghosal", "Souradip Chakraborty", "Avinash Reddy", "Dinesh Manocha", "Hao Zhu", "Amrit Singh Bedi"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025", "summary": "Aligning large language models with humans is challenging due to the\ninherently multifaceted nature of preference feedback. While existing\napproaches typically frame this as a multi-objective optimization problem, they\noften overlook how humans actually make decisions. Research on bounded\nrationality suggests that human decision making follows satisficing\nstrategies-optimizing primary objectives while ensuring others meet acceptable\nthresholds. To bridge this gap and operationalize the notion of satisficing\nalignment, we propose SITAlign: an inference time framework that addresses the\nmultifaceted nature of alignment by maximizing a primary objective while\nsatisfying threshold-based constraints on secondary criteria. We provide\ntheoretical insights by deriving sub-optimality bounds of our satisficing based\ninference alignment approach. We empirically validate SITAlign's performance\nthrough extensive experimentation on multiple benchmarks. For instance, on the\nPKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while\nensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art\nmulti objective decoding strategy by a margin of 22.3% in terms of GPT-4\nwin-tie rate for helpfulness reward while adhering to the threshold on\nharmlessness.", "AI": {"tldr": "SITAlign is a framework for aligning large language models with human preferences by maximizing a primary objective while meeting threshold constraints on secondary criteria.", "motivation": "To address the challenges in aligning large language models with humans, considering the multifaceted nature of human decision-making based on satisficing strategies.", "method": "SITAlign proposes an inference time framework that maximizes a primary objective while ensuring that secondary criteria meet acceptable thresholds, with theoretical and empirical validations.", "result": "SITAlign improves performance on the PKU-SafeRLHF dataset, surpassing the existing multi-objective decoding strategies by 22.3% in helpfulness reward while maintaining harmlessness criteria.", "conclusion": "The framework offers a novel way to operationalize human-like decision making in model alignment, demonstrating both theoretical insights and superior empirical performance.", "key_contributions": ["Introduction of SITAlign framework for satisficing alignment", "Theoretical derivation of sub-optimality bounds", "Empirical validation showing significant performance improvement over state-of-the-art methods"], "limitations": "", "keywords": ["large language models", "human alignment", "multi-objective optimization", "satisficing strategies", "machine learning"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2503.18792", "pdf": "https://arxiv.org/pdf/2503.18792.pdf", "abs": "https://arxiv.org/abs/2503.18792", "title": "REALM: A Dataset of Real-World LLM Use Cases", "authors": ["Jingwen Cheng", "Kshitish Ghate", "Wenyue Hua", "William Yang Wang", "Hong Shen", "Fei Fang"], "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "11 pages, 3 figures", "summary": "Large Language Models (LLMs), such as the GPT series, have driven significant\nindustrial applications, leading to economic and societal transformations.\nHowever, a comprehensive understanding of their real-world applications remains\nlimited. To address this, we introduce REALM, a dataset of over 94,000 LLM use\ncases collected from Reddit and news articles. REALM captures two key\ndimensions: the diverse applications of LLMs and the demographics of their\nusers. It categorizes LLM applications and explores how users' occupations\nrelate to the types of applications they use. By integrating real-world data,\nREALM offers insights into LLM adoption across different domains, providing a\nfoundation for future research on their evolving societal roles.", "AI": {"tldr": "Introduction of REALM, a dataset of LLM use cases from Reddit and news articles to understand applications and demographics.", "motivation": "To provide a comprehensive understanding of real-world applications of Large Language Models (LLMs).", "method": "The authors collected over 94,000 use cases from Reddit and news articles, categorizing them by application type and user demographics.", "result": "REALM reveals insights into LLM adoption across various domains and user occupations, highlighting diverse applications.", "conclusion": "This dataset serves as a foundational resource for future research on the societal roles of LLMs.", "key_contributions": ["Introduction of REALM dataset with extensive LLM use cases", "Analysis of user demographics in relation to LLM applications", "Insights into the diverse applications of LLMs across different domains"], "limitations": "", "keywords": ["Large Language Models", "dataset", "applications", "demographics", "LLM use cases"], "importance_score": 8, "read_time_minutes": 11}}
{"id": "2504.13861", "pdf": "https://arxiv.org/pdf/2504.13861.pdf", "abs": "https://arxiv.org/abs/2504.13861", "title": "3MDBench: Medical Multimodal Multi-agent Dialogue Benchmark", "authors": ["Ivan Sviridov", "Amina Miftakhova", "Artemiy Tereshchenko", "Galina Zubkova", "Pavel Blinov", "Andrey Savchenko"], "categories": ["cs.HC", "cs.CL", "cs.MA", "68T42", "I.2.1"], "comment": "35 pages, 13 figures, 7 tables", "summary": "Though Large Vision-Language Models (LVLMs) are being actively explored in\nmedicine, their ability to conduct telemedicine consultations combining\naccurate diagnosis with professional dialogue remains underexplored. In this\npaper, we present 3MDBench (Medical Multimodal Multi-agent Dialogue Benchmark),\nan open-source framework for simulating and evaluating LVLM-driven telemedical\nconsultations. 3MDBench simulates patient variability through four\ntemperament-based Patient Agents and an Assessor Agent that jointly evaluate\ndiagnostic accuracy and dialogue quality. It includes 3013 cases across 34\ndiagnoses drawn from real-world telemedicine interactions, combining textual\nand image-based data. The experimental study compares diagnostic strategies for\npopular LVLMs, including GPT-4o-mini, LLaVA-3.2-11B-Vision-Instruct, and\nQwen2-VL-7B-Instruct. We demonstrate that multimodal dialogue with internal\nreasoning improves F1 score by 6.5% over non-dialogue settings, highlighting\nthe importance of context-aware, information-seeking questioning. Moreover,\ninjecting predictions from a diagnostic convolutional network into the LVLM's\ncontext boosts F1 by up to 20%. Source code is available at\nhttps://anonymous.4open.science/r/3mdbench_acl-0511.", "AI": {"tldr": "This paper presents 3MDBench, an open-source framework for simulating and evaluating LVLM-driven telemedical consultations, demonstrating improved diagnostic accuracy through multimodal dialogue.", "motivation": "To explore the potential of Large Vision-Language Models in conducting effective telemedicine consultations that combine accurate diagnosis with professional dialogue.", "method": "The study introduces 3MDBench, which simulates patient variability with temperament-based Patient Agents and evaluates diagnostic and dialogue quality with an Assessor Agent. It includes an experimental study comparing various LVLMs with 3013 cases across 34 diagnoses using both text and image inputs.", "result": "The experiments reveal that multimodal dialogue with reasoning enhances F1 score by 6.5%, and incorporating predictions from a convolutional network into the context of the LVLM further increases F1 by up to 20%.", "conclusion": "The findings underline the significance of context-aware interactions and the potential of LVLMs in healthcare scenarios consisting of both text and visual data.", "key_contributions": ["Introduction of 3MDBench for telemedical conversation evaluation", "Demonstration of improved diagnostic performance using multimodal dialogue", "Integration of CNN predictions into LVLM context for enhanced accuracy"], "limitations": "", "keywords": ["Telemedicine", "Large Vision-Language Models", "Multimodal Dialogue", "Healthcare", "Diagnostic Accuracy"], "importance_score": 9, "read_time_minutes": 35}}
{"id": "2504.14822", "pdf": "https://arxiv.org/pdf/2504.14822.pdf", "abs": "https://arxiv.org/abs/2504.14822", "title": "Completing A Systematic Review in Hours instead of Months with Interactive AI Agents", "authors": ["Rui Qiu", "Shijie Chen", "Yu Su", "Po-Yin Yen", "Han-Wei Shen"], "categories": ["cs.HC", "cs.CL"], "comment": "Accepted as ACL 2025 (main)", "summary": "Systematic reviews (SRs) are vital for evidence-based practice in high stakes\ndisciplines, such as healthcare, but are often impeded by intensive labors and\nlengthy processes that can take months to complete. Due to the high demand for\ndomain expertise, existing automatic summarization methods fail to accurately\nidentify relevant studies and generate high-quality summaries. To that end, we\nintroduce InsightAgent, a human-centered interactive AI agent powered by large\nlanguage models that revolutionize this workflow. InsightAgent partitions a\nlarge literature corpus based on semantics and employs a multi-agent design for\nmore focused processing of literature, leading to significant improvement in\nthe quality of generated SRs. InsightAgent also provides intuitive\nvisualizations of the corpus and agent trajectories, allowing users to\neffortlessly monitor the actions of the agent and provide real-time feedback\nbased on their expertise. Our user studies with 9 medical professionals\ndemonstrate that the visualization and interaction mechanisms can effectively\nimprove the quality of synthesized SRs by 27.2%, reaching 79.7% of\nhuman-written quality. At the same time, user satisfaction is improved by\n34.4%. With InsightAgent, it only takes a clinician about 1.5 hours, rather\nthan months, to complete a high-quality systematic review.", "AI": {"tldr": "InsightAgent is an interactive AI tool using LLMs to streamline systematic reviews (SRs) in healthcare, improving quality and user satisfaction significantly while reducing completion time.", "motivation": "Systematic reviews are crucial in healthcare but suffer from lengthy, labor-intensive processes; existing summarization methods lack accuracy.", "method": "InsightAgent partitions literature semantically and uses a multi-agent design for focused processing, complemented by interactive visualizations for user feedback.", "result": "User studies show a 27.2% improvement in SR quality, achieving 79.7% of human-written standards, and a 34.4% increase in user satisfaction, reducing review time to approximately 1.5 hours.", "conclusion": "InsightAgent revolutionizes the SR process, making it faster and enhancing quality and user experience.", "key_contributions": ["Introduces a human-centered interactive AI agent for systematic reviews.", "Significantly improves quality of SRs and user satisfaction through real-time feedback.", "Reduces completion time from months to about 1.5 hours."], "limitations": "", "keywords": ["systematic reviews", "human-centered AI", "large language models", "healthcare", "user interaction"], "importance_score": 9, "read_time_minutes": 15}}
