{"id": "2507.22134", "pdf": "https://arxiv.org/pdf/2507.22134.pdf", "abs": "https://arxiv.org/abs/2507.22134", "title": "IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks", "authors": ["Yoonsu Kim", "Brandon Chin", "Kihoon Son", "Seoyoung Kim", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "While large language models (LLMs) are widely used for writing, users often\nstruggle to express their nuanced and evolving intents through prompt-based\ninterfaces. Intents -- low-level strategies or preferences for achieving a\nwriting goal -- are often vague, fluid, or even subconscious, making it\ndifficult for users to articulate and adjust them. To address this, we present\nIntentFlow, which supports the communication of dynamically evolving intents\nthroughout LLM-assisted writing. IntentFlow extracts goals and intents from\nuser prompts and presents them as editable interface components, which users\ncan revise, remove, or refine via direct manipulation or follow-up prompts.\nVisual links connect each component to the output segments it influences,\nhelping users understand model behavior. In a within-subjects study (N=12),\nparticipants using IntentFlow, compared to a chat-based baseline, expressed\ntheir intents more easily and in detail, engaged in more meaningful actions to\ncommunicate intents, such as adjusting and deleting, and produced outputs that\nbetter aligned with their evolving intents. We found that editable intent\nrepresentations help users refine and consolidate a final set of intents, which\ncan be reused across similar tasks to support consistent and transferable\nLLM-assisted writing.", "AI": {"tldr": "IntentFlow enhances LLM-assisted writing by allowing users to communicate and refine their evolving intents through editable interface components.", "motivation": "Users struggle to express nuanced writing intents through prompt-based interfaces when using LLMs, leading to challenges in effectively articulating and adjusting their goals.", "method": "The paper introduces IntentFlow, a system that extracts user intents from prompts and displays them as editable components linked to output segments, allowing users to revise their intents directly.", "result": "In a study with 12 participants, those using IntentFlow found it easier to express and modify their intents, leading to more aligned outputs with their goals compared to a chat-based baseline.", "conclusion": "Editable intent representations significantly aid users in refining final intents for LLM-assisted writing, facilitating better consistency and transferability across tasks.", "key_contributions": ["Introduction of IntentFlow for editable intent management in LLM writing", "Demonstration of improved user engagement and output alignment in writing tasks", "Illustration of the potential for reusable intents across similar writing tasks"], "limitations": "", "keywords": ["large language models", "intent management", "HCI", "user interaction", "writing assistance"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22153", "pdf": "https://arxiv.org/pdf/2507.22153.pdf", "abs": "https://arxiv.org/abs/2507.22153", "title": "Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality", "authors": ["Ethan Wilson", "Vincent Bindschaedler", "Sophie JÃ¶rg", "Sean Sheikholeslam", "Kevin Butler", "Eakta Jain"], "categories": ["cs.HC", "cs.CR"], "comment": null, "summary": "Photorealistic 3D avatar generation has rapidly improved in recent years, and\nrealistic avatars that match a user's true appearance are more feasible in\nMixed Reality (MR) than ever before. Yet, there are known risks to sharing\none's likeness online, and photorealistic MR avatars could exacerbate these\nrisks. If user likenesses were to be shared broadly, there are risks for cyber\nabuse or targeted fraud based on user appearances. We propose an alternate\navatar rendering scheme for broader social MR -- synthesizing realistic avatars\nthat preserve a user's demographic identity while being distinct enough from\nthe individual user to protect facial biometric information. We introduce a\nmethodology for privatizing appearance by isolating identity within the feature\nspace of identity-encoding generative models. We develop two algorithms that\nthen obfuscate identity: \\epsmethod{} provides differential privacy guarantees\nand \\thetamethod{} provides fine-grained control for the level of identity\noffset. These methods are shown to successfully generate de-identified virtual\navatars across multiple generative architectures in 2D and 3D. With these\ntechniques, it is possible to protect user privacy while largely preserving\nattributes related to sense of self. Employing these techniques in public\nsettings could enable the use of photorealistic avatars broadly in MR,\nmaintaining high realism and immersion without privacy risk.", "AI": {"tldr": "This paper presents a novel methodology for generating photorealistic avatars in Mixed Reality that protects user identity while preserving their demographic characteristics.", "motivation": "To address the privacy risks associated with sharing photorealistic avatars that could lead to cyber abuse or targeted fraud, the authors propose a solution to synthesize avatars that obscure user identity.", "method": "The approach involves isolating identity in the feature space of identity-encoding generative models and developing two algorithms: one for differential privacy guarantees and another for fine-grained identity obfuscation.", "result": "The proposed algorithms successfully generate de-identified virtual avatars across various generative architectures in both 2D and 3D.", "conclusion": "By employing these techniques, privacy can be maintained while preserving key attributes related to user identity, enabling safer use of photorealistic avatars in public Mixed Reality settings.", "key_contributions": ["Introduction of a methodology to synthesize avatars that maintain demographic identity without revealing biometric data.", "Development of two algorithms that provide privacy guarantees and control over identity obfuscation.", "Demonstration of effective generation of de-identified avatars across different generative architectures."], "limitations": "", "keywords": ["photorealistic avatars", "Mixed Reality", "user privacy", "identity obfuscation", "generative models"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22163", "pdf": "https://arxiv.org/pdf/2507.22163.pdf", "abs": "https://arxiv.org/abs/2507.22163", "title": "IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI", "authors": ["DaEun Choi", "Kihoon Son", "Jaesang Yu", "Hyunjoon Jung", "Juho Kim"], "categories": ["cs.HC"], "comment": null, "summary": "Generative AI opens new possibilities for design exploration by rapidly\ngenerating images aligned with user goals. However, our formative study (N=7)\nrevealed three key limitations hindering designers' broad and efficient\nexploration when interacting with these models. These include difficulty\nexpressing open-ended exploratory intent, lack of continuity in exploration,\nand limited support for reusing or iterating on previous ideas. We propose\nIdeaBlocks, where users can express their exploratory intents to generative AI\nwith structured input and modularize them into Exploration Blocks. These blocks\ncan be chained for continuous, non-linear exploration and reused across\ncontexts, enabling broad exploration without losing creative momentum. Our user\nstudy with 12 designers showed that participants using IdeaBlocks explored\n112.8% more images with 12.5% greater visual diversity than the baseline. They\nalso developed ideas in more iterative and continuous patterns, such as\nbranching, chaining, and revisiting ideas. We discuss design implications for\nfuture tools to better balance divergent and convergent support during\ndifferent phases of exploration, and to capture and leverage exploratory\nintents more effectively.", "AI": {"tldr": "The paper presents IdeaBlocks, a tool facilitating structured input for generative AI to enhance design exploration by enabling continuous and iterative idea development.", "motivation": "Generative AI offers rapid image generation, but existing methods pose limitations for designers in exploration, including expression of intent, continuity, and idea reuse.", "method": "Formative study with 7 participants identified limitations; user study with 12 designers tested IdeaBlocks for structured input and modular exploration.", "result": "Participants using IdeaBlocks explored 112.8% more images with 12.5% greater visual diversity compared to baseline, while developing ideas in an iterative pattern.", "conclusion": "Future tools should better support divergent and convergent exploration phases and effectively capture exploratory intents.", "key_contributions": ["Introduction of IdeaBlocks for structured exploratory intents", "Demonstrated increased image exploration and diversity", "Insights into design implications for future tools"], "limitations": "Limited to a small sample size of designers in the study.", "keywords": ["Generative AI", "design exploration", "iterative idea development", "IdeaBlocks", "visual diversity"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22193", "pdf": "https://arxiv.org/pdf/2507.22193.pdf", "abs": "https://arxiv.org/abs/2507.22193", "title": "DissolvPCB: Fully Recyclable 3D-Printed Electronics with Liquid Metal Conductors and PVA Substrates", "authors": ["Zeyu Yan", "SuHwan Hong", "Josiah Hester", "Tingyu Cheng", "Huaishu Peng"], "categories": ["cs.HC"], "comment": null, "summary": "We introduce DissolvPCB, an electronic prototyping technique for fabricating\nfully recyclable printed circuit board assemblies (PCBAs) using affordable FDM\n3D printing, with polyvinyl alcohol (PVA) as a water-soluble substrate and\neutectic gallium-indium (EGaIn) as the conductive material. When obsolete, the\nPCBA can be easily recycled by immersing it in water: the PVA dissolves, the\nEGaIn re-forms into a liquid metal bead, and the electronic components are\nrecovered. These materials can then be reused to fabricate a new PCBA.\n  We present the DissolvPCB workflow, characterize its design parameters,\nevaluate the performance of circuits produced with it, and quantify its\nenvironmental impact through a lifecycle assessment (LCA) comparing it to\nconventional CNC-milled FR-4 boards. We further develop a software plugin that\nautomatically converts PCB design files into 3D-printable circuit substrate\nmodels. To demonstrate the capabilities of DissolvPCB, we fabricate and recycle\nthree functional prototypes: a Bluetooth speaker featuring a double-sided PCB,\na finger fidget toy with a 3D circuit topology, and a shape-changing gripper\nenabled by Joule-heat-driven 4D printing. The paper concludes with a discussion\nof current technical limitations and opportunities for future directions.", "AI": {"tldr": "DissolvPCB is a technique for making recyclable printed circuit board assemblies using 3D printing and water-soluble materials, allowing easy recycling and reuse of components.", "motivation": "The need for environmentally friendly electronics that can be easily recycled and reused.", "method": "Utilizing FDM 3D printing with polyvinyl alcohol as a substrate and eutectic gallium-indium for conductivity, the paper presents a workflow for creating PCBAs and evaluates their performance and environmental impact.", "result": "DissolvPCB circuits were demonstrated through functional prototypes like a Bluetooth speaker and a gripper, showing effective recycling after use.", "conclusion": "The technique has limitations but offers significant potential for sustainable electronics manufacturing and design.", "key_contributions": ["Introduced DissolvPCB as a recyclable PCBA fabrication method.", "Developed a software plugin for automatic conversion of PCB design files.", "Performed lifecycle assessment comparing DissolvPCB with traditional manufacturing methods."], "limitations": "Current technical limitations that need to be addressed.", "keywords": ["DissolvPCB", "recyclable electronics", "3D printing", "polymers", "environmental impact"], "importance_score": 4, "read_time_minutes": 12}}
{"id": "2507.22159", "pdf": "https://arxiv.org/pdf/2507.22159.pdf", "abs": "https://arxiv.org/abs/2507.22159", "title": "IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian", "authors": ["Vanessa Rebecca Wiyono", "David Anugraha", "Ayu Purwarianti", "Genta Indra Winata"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Over 200 million people speak Indonesian, yet the language remains\nsignificantly underrepresented in preference-based research for large language\nmodels (LLMs). Most existing multilingual datasets are derived from English\ntranslations, often resulting in content that lacks cultural and linguistic\nauthenticity. To address this gap, we introduce IndoPref, the first fully\nhuman-authored and multi-domain Indonesian preference dataset specifically\ndesigned to evaluate the naturalness and quality of LLM-generated text. All\nannotations are natively written in Indonesian and evaluated using\nKrippendorff's alpha, demonstrating strong inter-annotator agreement.\nAdditionally, we benchmark the dataset across multiple LLMs and assess the\noutput quality of each model.", "AI": {"tldr": "IndoPref is a new, human-authored dataset for evaluating Indonesian preferences in LLM-generated text, addressing the lack of linguistic authenticity in existing multilingual datasets.", "motivation": "To provide a culturally and linguistically authentic dataset for preference-based research in Indonesian, filling a significant gap in multilingual models.", "method": "The IndoPref dataset is fully human-authored, with annotations written in Indonesian. It uses Krippendorff's alpha for evaluation of inter-annotator agreement and benchmarks across multiple LLMs to assess output quality.", "result": "Strong inter-annotator agreement was achieved, indicating reliability, and the dataset serves as a benchmark for quality evaluation in LLM outputs.", "conclusion": "IndoPref offers a valuable resource for improving LLM performance in understanding and generating Indonesian text.", "key_contributions": ["Introduction of the first human-authored Indonesian preference dataset", "Demonstration of strong inter-annotator reliability", "Benchmarking across multiple LLMs for output quality assessment"], "limitations": "", "keywords": ["Indonesian", "large language models", "preference dataset", "natural language processing", "multilingual"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.22241", "pdf": "https://arxiv.org/pdf/2507.22241.pdf", "abs": "https://arxiv.org/abs/2507.22241", "title": "Verisimilitude as Boon and Bane: How People Initiate Opportunistic Interactions at Professional Events in Social VR", "authors": ["Victoria Chang", "Caro Williams-Pierce", "Huaishu Peng", "Ge Gao"], "categories": ["cs.HC"], "comment": null, "summary": "Opportunistic interactions-the unstructured exchanges that emerge as\nindividuals become aware of each other's presence-are essential for\nrelationship building and information sharing in everyday life. Yet, fostering\neffective opportunistic interactions has proven challenging, especially at\nprofessional events that have increasingly transitioned from in person to\nonline formats. In the current paper, we offer an in-depth qualitative account\nof how people initiate opportunistic interactions in social VR. Our\nparticipants consisted of 16 individuals with ongoing experience attending\nVR-mediated events in their professional communities. We conducted extensive\nobservations with each participant during one or more events they attended. We\nalso interviewed them after every observed event, obtaining self-reflections on\ntheir attempts to navigate opportunistic interactions with others. Our analysis\nrevealed that participants sought to understand the extent to which social VR\npreserved the real-world meanings of various nonverbal cues, which we refer to\nas verisimilitude. We detailed the unique connections between a person's\nperceived verisimilitude and their social behaviors at each of the three steps\ntoward initiating opportunistic interactions: availability recognition,\nattention capture, and ice-breaking. Across these steps, the VR platform\ntypically replaces complex social mechanisms with feasible technical ones in\norder to function, thereby altering the preconditions necessary for a nonverbal\ncue's social meanings to remain intact. We identified a rich set of strategies\nthat participants developed to assess verisimilitude and act upon it, while\nalso confirming a lack of systematic knowledge guiding their practices. Based\non these findings, we provide actionable insights for social VR platform design\nthat can best support the initiation of opportunistic interactions for\nprofessional purposes.", "AI": {"tldr": "This paper examines how individuals initiate opportunistic interactions in social virtual reality (VR) for professional events, highlighting the importance of nonverbal cues and proposing design insights for VR platforms.", "motivation": "Opportunistic interactions are key for relationship building and information sharing, especially as professional events move online. However, facilitating these interactions in social VR poses unique challenges.", "method": "The study involved qualitative observations and interviews with 16 participants who attended VR-mediated events, allowing for in-depth insights into their experiences with opportunistic interactions.", "result": "Participants navigated interactions by assessing the verisimilitude of nonverbal cues in VR, which shaped their social behaviors during the steps of interaction initiation: recognizing availability, capturing attention, and ice-breaking.", "conclusion": "The findings indicate a disconnect between participants' practices and systematic knowledge on navigating social cues in VR, leading to recommendations for enhancing VR platform design to support professional interactions.", "key_contributions": ["Identification of the role of verisimilitude in social interactions in VR", "Detailed analysis of interaction initiation steps in social VR", "Recommendations for VR platform design to enhance opportunistic interactions"], "limitations": "The study is based on a small sample size and specific to certain professional communities, which may limit generalizability.", "keywords": ["opportunistic interactions", "social VR", "verisimilitude", "professional events", "user experience"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.22168", "pdf": "https://arxiv.org/pdf/2507.22168.pdf", "abs": "https://arxiv.org/abs/2507.22168", "title": "Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles", "authors": ["Kimberly Le Truong", "Riccardo Fogliato", "Hoda Heidari", "Zhiwei Steven Wu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Current benchmarks for evaluating Large Language Models (LLMs) often do not\nexhibit enough writing style diversity, with many adhering primarily to\nstandardized conventions. Such benchmarks do not fully capture the rich variety\nof communication patterns exhibited by humans. Thus, it is possible that LLMs,\nwhich are optimized on these benchmarks, may demonstrate brittle performance\nwhen faced with \"non-standard\" input. In this work, we test this hypothesis by\nrewriting evaluation prompts using persona-based LLM prompting, a low-cost\nmethod to emulate diverse writing styles. Our results show that, even with\nidentical semantic content, variations in writing style and prompt formatting\nsignificantly impact the estimated performance of the LLM under evaluation.\nNotably, we identify distinct writing styles that consistently trigger either\nlow or high performance across a range of models and tasks, irrespective of\nmodel family, size, and recency. Our work offers a scalable approach to augment\nexisting benchmarks, improving the external validity of the assessments they\nprovide for measuring LLM performance across linguistic variations.", "AI": {"tldr": "The paper explores the limitations of current benchmarks for evaluating Large Language Models (LLMs) due to a lack of writing style diversity, proposing a method to enhance benchmark evaluations by incorporating persona-based LLM prompting to emulate diverse styles.", "motivation": "Current LLM benchmarks fail to capture the variety of human communication styles, potentially leading to fragile model performance under non-standard input.", "method": "The authors rewrote evaluation prompts using a persona-based LLM prompting method to evaluate the impact of diverse writing styles on LLM performance.", "result": "Variations in writing style and prompt formatting significantly influence LLM performance, with specific styles consistently leading to either low or high performance across different LLMs.", "conclusion": "Incorporating diverse writing styles in benchmarks can improve their validity for assessing LLM performance across linguistic variations.", "key_contributions": ["Proposed a low-cost method for emulating diverse writing styles in LLM evaluations.", "Demonstrated that writing style variations impact LLM performance even with identical semantic content.", "Identified distinct writing styles that consistently affect LLM performance across various models."], "limitations": "", "keywords": ["Large Language Models", "writing style diversity", "benchmark evaluation", "persona-based prompting", "LLM performance"], "importance_score": 9, "read_time_minutes": 12}}
{"id": "2507.22252", "pdf": "https://arxiv.org/pdf/2507.22252.pdf", "abs": "https://arxiv.org/abs/2507.22252", "title": "Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving", "authors": ["Kexin Liang", "Jan Luca KÃ¤stleb", "Bani Anvarib", "Simeon C. Calverta", "J. W. C. van Lint"], "categories": ["cs.HC"], "comment": null, "summary": "When automated driving systems encounter complex situations beyond their\noperational capabilities, they issue takeover requests, prompting drivers to\nresume vehicle control and return to the driving loop as a critical safety\nbackup. However, this control transition places significant demands on drivers,\nrequiring them to promptly respond to takeover requests while executing\nhigh-quality interventions. To ensure safe and comfortable control transitions,\nit is essential to develop a deep understanding of the key factors influencing\nvarious takeover performance aspects. This study evaluates drivers' takeover\nperformance across three dimensions: response efficiency, user experience, and\ndriving safety - using a driving simulator experiment. EXtreme Gradient\nBoosting (XGBoost) models are used to investigate the contributions of two\ncritical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in\npredicting various takeover performance metrics by comparing the predictive\nresults to the baseline models that rely solely on basic Driver Characteristics\n(DC). The results reveal that (i) higher SA enables drivers to respond to\ntakeover requests more quickly, particularly for reflexive responses; and (ii)\nSC shows a greater overall impact on takeover quality than SA, where higher SC\ngenerally leads to enhanced subjective rating scores and objective execution\ntrajectories. These findings highlight the distinct yet complementary roles of\nSA and SC in shaping performance components, offering valuable insights for\noptimizing human-vehicle interactions and enhancing automated driving system\ndesign.", "AI": {"tldr": "This study evaluates drivers' performance in automated driving takeover scenarios, focusing on factors like Situational Awareness (SA) and Spare Capacity (SC) using driving simulations and XGBoost models.", "motivation": "To understand the key factors influencing drivers' responses to takeover requests in automated driving systems, ensuring safe and comfortable control transitions.", "method": "Driving simulator experiment assessing takeover performance along dimensions of response efficiency, user experience, and driving safety, analyzed using EXtreme Gradient Boosting (XGBoost) models.", "result": "Higher Situational Awareness (SA) leads to quicker responses to takeover requests, while Spare Capacity (SC) has a greater overall impact on the quality of takeovers, improving both subjective ratings and execution trajectories.", "conclusion": "The study highlights the complementary roles of SA and SC in enhancing performance in automated driving interactions, providing insights for future design improvements in automated driving systems.", "key_contributions": ["Evaluated the impact of Situational Awareness (SA) and Spare Capacity (SC) on drivers' takeover performance.", "Used driving simulators and XGBoost models to assess performance dimensions.", "Identified that SC has a greater influence on takeover quality than SA."], "limitations": "", "keywords": ["Automated Driving", "Takeover Requests", "Situational Awareness", "Spare Capacity", "Driving Performance"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22187", "pdf": "https://arxiv.org/pdf/2507.22187.pdf", "abs": "https://arxiv.org/abs/2507.22187", "title": "A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models", "authors": ["Adam M. Morgan", "Adeen Flinker"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present an automated pipeline for estimating Verb Frame Frequencies\n(VFFs), the frequency with which a verb appears in particular syntactic frames.\nVFFs provide a powerful window into syntax in both human and machine language\nsystems, but existing tools for calculating them are limited in scale,\naccuracy, or accessibility. We use large language models (LLMs) to generate a\ncorpus of sentences containing 476 English verbs. Next, by instructing an LLM\nto behave like an expert linguist, we had it analyze the syntactic structure of\nthe sentences in this corpus. This pipeline outperforms two widely used\nsyntactic parsers across multiple evaluation datasets. Furthermore, it requires\nfar fewer resources than manual parsing (the gold-standard), thereby enabling\nrapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF\ndatabase with broader verb coverage, finer-grained syntactic distinctions, and\nexplicit estimates of the relative frequencies of structural alternates\ncommonly studied in psycholinguistics. The pipeline is easily customizable and\nextensible to new verbs, syntactic frames, and even other languages. We present\nthis work as a proof of concept for automated frame frequency estimation, and\nrelease all code and data to support future research.", "AI": {"tldr": "This paper presents an automated pipeline utilizing large language models to estimate Verb Frame Frequencies (VFFs), which capture the frequency of verbs in specific syntactic frames, outperforming existing syntactic parsers while being resource-efficient.", "motivation": "Existing tools for calculating Verb Frame Frequencies are limited in scale, accuracy, or accessibility, creating a need for a more efficient approach.", "method": "Utilizing large language models to generate sentences with 476 English verbs and instructing an LLM to analyze their syntactic structures, leading to the development of a pipeline for estimating VFFs.", "result": "The pipeline outperforms two widely used syntactic parsers and enables rapid estimation of VFFs with broader verb coverage and finer-grained syntactic distinctions.", "conclusion": "The automated pipeline effectively demonstrates the feasibility of VFF estimation and releases code and data to aid further research.", "key_contributions": ["Automation of Verb Frame Frequency estimation using LLMs", "Outperformance of traditional syntactic parsers", "Creation of a new VFF database with extensive verb coverage"], "limitations": "", "keywords": ["Verb Frame Frequencies", "syntactic parsing", "large language models"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.22262", "pdf": "https://arxiv.org/pdf/2507.22262.pdf", "abs": "https://arxiv.org/abs/2507.22262", "title": "Towards Safe and Comfortable Vehicle Control Transitions: A Systematic Review of Takeover Time, Time Budget, and Takeover Performance", "authors": ["Kexin Liang", "Simeon C. Calvert", "J. W. C. van Lint"], "categories": ["cs.HC"], "comment": null, "summary": "Conditionally automated driving systems require human drivers to disengage\nfrom non-driving-related activities and resume vehicle control within limited\ntime budgets when encountering scenarios beyond system capabilities. Ensuring\nsafe and comfortable transitions is critical for reducing driving risks and\nimproving user experience. However, takeovers involve complex human-vehicle\ninteractions, resulting in substantial variability in drivers' responses,\nespecially in takeover time, defined as the duration needed to regain control.\nThis variability presents challenges in setting sufficient time budgets that\nare neither too short (risking safety and comfort) nor too long (reducing\ndriver alertness and transition efficiency).\n  Although previous research has examined the role of time budgets in\ninfluencing takeover time and performance, few studies have systematically\naddressed how to determine sufficient time budgets that adapt to diverse\nscenarios and driver needs. This review supports such efforts by examining the\nentire takeover sequence, including takeover time, time budget, and takeover\nperformance. Specifically, we (i) synthesize causal factors influencing\ntakeover time and propose a taxonomy of its determinants using the\ntask-capability interface model; (ii) review existing work on fixed and\nadaptive time budgets, introducing the concept of the takeover buffer to\ndescribe the gap between takeover time and allocated time budget; (iii) present\na second taxonomy to support standardized and context-sensitive measurement of\ntakeover performance; (iv) propose a conceptual model describing the\nrelationships among takeover time, time budget, and performance; and (v)\noutline a research agenda with six directions.", "AI": {"tldr": "The paper reviews the challenges and variables associated with human driver takeovers in conditionally automated driving systems, focusing on defining sufficient time budgets for safe and comfortable transitions.", "motivation": "To improve safety and user experience in conditionally automated driving by addressing the variability in human driver responses during takeovers.", "method": "The paper synthesizes causal factors influencing takeover time, reviews existing studies on time budgets, introduces a concept of takeover buffer, and proposes taxonomies and a conceptual model for measuring takeover performance.", "result": "The review identifies key determinants influencing takeover time, provides a framework for fixed and adaptive time budgets, and outlines six strategic directions for future research.", "conclusion": "A standardized approach to measuring takeover performance and establishing adaptable time budgets is essential for enhancing safety and efficiency in human-vehicle interaction during takeovers.", "key_contributions": ["Synthesis of causal factors influencing takeover time and a proposed taxonomy of its determinants.", "Introduction of the takeover buffer concept for measuring the gap between actual takeover time and allocated time budget.", "Development of a comprehensive conceptual model relating takeover time, time budget, and performance."], "limitations": "", "keywords": ["automated driving", "takeover time", "time budget", "human-vehicle interaction", "performance measurement"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.22201", "pdf": "https://arxiv.org/pdf/2507.22201.pdf", "abs": "https://arxiv.org/abs/2507.22201", "title": "The role of media memorability in facilitating startups' access to venture capital funding", "authors": ["L. Toschi", "S. Torrisi", "A. Fronzetti Colladon"], "categories": ["cs.CL", "cs.SI", "physics.soc-ph", "I.2.7; J.4; H.4.0"], "comment": null, "summary": "Media reputation plays an important role in attracting venture capital\ninvestment. However, prior research has focused too narrowly on general media\nexposure, limiting our understanding of how media truly influences funding\ndecisions. As informed decision-makers, venture capitalists respond to more\nnuanced aspects of media content. We introduce the concept of media\nmemorability - the media's ability to imprint a startup's name in the memory of\nrelevant investors. Using data from 197 UK startups in the micro and\nnanotechnology sector (funded between 1995 and 2004), we show that media\nmemorability significantly influences investment outcomes. Our findings suggest\nthat venture capitalists rely on detailed cues such as a startup's\ndistinctiveness and connectivity within news semantic networks. This\ncontributes to research on entrepreneurial finance and media legitimation. In\npractice, startups should go beyond frequent media mentions to strengthen brand\nmemorability through more targeted, meaningful coverage highlighting their\nuniqueness and relevance within the broader industry conversation.", "AI": {"tldr": "The paper explores how media memorability influences venture capital investment decisions in startups, showing that detailed media content cues affect funding outcomes.", "motivation": "To advance understanding of media influence on venture capital funding beyond just general media exposure.", "method": "Analysis of data from 197 UK micro and nanotechnology startups funded between 1995 and 2004, focusing on the significance of media memorability.", "result": "Media memorability significantly influences investment outcomes, independent of general media exposure, highlighting cues like distinctiveness and connectivity in news.", "conclusion": "Startups should aim for meaningful media coverage that enhances brand memorability over mere frequency of mentions.", "key_contributions": ["Introduction of the concept of media memorability", "Demonstration of the impact of media content nuances on venture funding", "Providing strategic recommendations for startups on media engagement"], "limitations": "", "keywords": ["media reputation", "venture capital", "investment outcomes", "entrepreneurial finance", "media memorability"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.22267", "pdf": "https://arxiv.org/pdf/2507.22267.pdf", "abs": "https://arxiv.org/abs/2507.22267", "title": "Promoting Online Safety by Simulating Unsafe Conversations with LLMs", "authors": ["Owen Hoffman", "Kangze Peng", "Zehua You", "Sajid Kamal", "Sukrit Venkatagiri"], "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Generative AI, including large language models (LLMs) have the potential --\nand already are being used -- to increase the speed, scale, and types of unsafe\nconversations online. LLMs lower the barrier for entry for bad actors to create\nunsafe conversations in particular because of their ability to generate\npersuasive and human-like text. In our current work, we explore ways to promote\nonline safety by teaching people about unsafe conversations that can occur\nonline with and without LLMs. We build on prior work that shows that LLMs can\nsuccessfully simulate scam conversations. We also leverage research in the\nlearning sciences that shows that providing feedback on one's hypothetical\nactions can promote learning. In particular, we focus on simulating scam\nconversations using LLMs. Our work incorporates two LLMs that converse with\neach other to simulate realistic, unsafe conversations that people may\nencounter online between a scammer LLM and a target LLM but users of our system\nare asked provide feedback to the target LLM.", "AI": {"tldr": "This paper explores using large language models (LLMs) to simulate and teach about unsafe online conversations, particularly scams, aiming to promote online safety through user feedback.", "motivation": "To address the increasing prevalence of unsafe conversations online, especially those driven by generative AI, and to promote online safety.", "method": "The authors simulate unsafe conversations between a scammer LLM and a target LLM, with a focus on prompting users to provide feedback on the target LLM's responses.", "result": "The approach shows potential in educating users about unsafe online interactions, enabling them to recognize and respond to scams more effectively.", "conclusion": "Incorporating feedback in simulated conversations can enhance understanding and awareness of online scams, contributing to safer online environments.", "key_contributions": ["Utilization of LLMs to simulate scam conversations.", "Incorporation of user feedback to improve learning about online safety.", "Focus on educational strategies in understanding unsafe interactions."], "limitations": "", "keywords": ["Generative AI", "Online Safety", "Large Language Models", "Scam Conversations", "User Feedback"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22209", "pdf": "https://arxiv.org/pdf/2507.22209.pdf", "abs": "https://arxiv.org/abs/2507.22209", "title": "How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?", "authors": ["Christian Clark", "Byung-Doh Oh", "William Schuler"], "categories": ["cs.CL"], "comment": null, "summary": "Contextual entropy is a psycholinguistic measure capturing the anticipated\ndifficulty of processing a word just before it is encountered. Recent studies\nhave tested for entropy-related effects as a potential complement to well-known\neffects from surprisal. For convenience, entropy is typically estimated based\non a language model's probability distribution over a word's first subword\ntoken. However, this approximation results in underestimation and potential\ndistortion of true word entropy. To address this, we generate Monte Carlo (MC)\nestimates of word entropy that allow words to span a variable number of tokens.\nRegression experiments on reading times show divergent results between\nfirst-token and MC word entropy, suggesting a need for caution in using\nfirst-token approximations of contextual entropy.", "AI": {"tldr": "This paper presents improved methods for estimating contextual entropy of words using Monte Carlo techniques, pointing out limitations of first-token approximations.", "motivation": "To improve the estimation of contextual entropy, which captures anticipated processing difficulty of words, especially in light of limitations of current first-token methods.", "method": "Monte Carlo estimates of word entropy are generated, allowing for variable token spans, and regression experiments on reading times are conducted to assess their effects.", "result": "The study shows divergent results between first-token estimates and Monte Carlo word entropy in regression experiments, highlighting the limitations of standard methods.", "conclusion": "First-token approximations of contextual entropy can misrepresent true word processing difficulty, emphasizing the need for more accurate estimation methods.", "key_contributions": ["Proposes a novel Monte Carlo method for estimating word entropy.", "Shows that first-token approximations can misrepresent contextual entropy.", "Provides empirical evidence through reading time regression experiments."], "limitations": "Does not explore the broader applications beyond reading times or different contexts of use.", "keywords": ["contextual entropy", "language model", "Monte Carlo", "reading times", "psycholinguistics"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22300", "pdf": "https://arxiv.org/pdf/2507.22300.pdf", "abs": "https://arxiv.org/abs/2507.22300", "title": "ConGaIT: A Clinician-Centered Dashboard for Contestable AI in Parkinson's Disease Care", "authors": ["Phuc Truong Loc Nguyen", "Thanh Hung Do"], "categories": ["cs.HC"], "comment": null, "summary": "AI-assisted gait analysis holds promise for improving Parkinson's Disease\n(PD) care, but current clinical dashboards lack transparency and offer no\nmeaningful way for clinicians to interrogate or contest AI decisions. We\npresent Con-GaIT (Contestable Gait Interpretation & Tracking), a\nclinician-centered system that advances Contestable AI through a tightly\nintegrated interface designed for interpretability, oversight, and procedural\nrecourse. Grounded in HCI principles, ConGaIT enables structured disagreement\nvia a novel Contest & Justify interaction pattern, supported by visual\nexplanations, role-based feedback, and traceable justification logs. Evaluated\nusing the Contestability Assessment Score (CAS), the framework achieves a score\nof 0.970, demonstrating that contestability can be operationalized through\nhuman-centered design in compliance with emerging regulatory standards. A\ndemonstration of the framework is available at\nhttps://github.com/hungdothanh/Con-GaIT.", "AI": {"tldr": "Con-GaIT is a clinician-centered system for AI-assisted gait analysis in Parkinson's Disease that promotes interpretability and oversight.", "motivation": "Current AI-assisted gait analysis systems lack transparency and do not allow clinicians to contest AI decisions, which is critical for effective patient care in Parkinson's Disease.", "method": "Developed a system integrating HCI principles that enables clinicians to contest and justify AI decisions through structured interactions and visual explanations.", "result": "The Con-GaIT framework achieved a Contestability Assessment Score of 0.970, indicating successful operationalization of contestability through human-centered design.", "conclusion": "Con-GaIT enhances the interpretability and oversight of AI models in clinical settings, complying with emerging regulatory standards.", "key_contributions": ["Introduction of the Contest & Justify interaction pattern.", "Implementation of traceable justification logs.", "High Contestability Assessment Score demonstrating effective design."], "limitations": "", "keywords": ["AI-assisted gait analysis", "Parkinson's Disease", "HCI", "contestability", "interpretability"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.22219", "pdf": "https://arxiv.org/pdf/2507.22219.pdf", "abs": "https://arxiv.org/abs/2507.22219", "title": "RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation", "authors": ["Dongyub Jude Lee", "Zhenyi Ye", "Pengcheng He"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Preference-learning methods for machine translation (MT)--such as Direct\nPreference Optimization (DPO)--have achieved impressive gains but depend\nheavily on large, carefully curated triplet datasets and often struggle to\ngeneralize beyond their tuning domains. We propose Reinforcement Learning from\nTeacher-Model Refinement (RLfR), a novel framework that removes reliance on\nstatic triplets by leveraging continuous, high-quality feedback from an\nexternal teacher model (GPT-4o). RLfR frames each translation step as a\nmicro-tutorial: the actor generates a hypothesis, the teacher refines it, and\nthe actor is rewarded based on how closely it aligns with the teacher's\nrefinement. Guided by two complementary signals--(i) negative edit distance,\npromoting lexical and structural fidelity, and (ii) COMET score, ensuring\nsemantic adequacy--the actor progressively learns to emulate the teacher,\nmirroring a human learning process through incremental, iterative improvement.\nOn the FLORES-200 benchmark (English to and from German, Spanish, Chinese,\nKorean, and Japanese), RLfR consistently outperforms both MT-SFT and\npreference-based baselines, significantly improving COMET (semantic adequacy)\nand M-ETA (entity preservation) scores.", "AI": {"tldr": "Proposes a novel reinforcement learning framework (RLfR) for machine translation that removes reliance on triplet datasets by utilizing feedback from a teacher model (GPT-4o).", "motivation": "The paper addresses limitations of existing preference-learning methods in machine translation that depend on curated datasets and struggle with generalization.", "method": "Reinforcement Learning from Teacher-Model Refinement (RLfR) uses continuous feedback from a teacher model to guide translation improvements, where each step serves as a micro-tutorial.", "result": "On the FLORES-200 benchmark, RLfR outperforms MT-SFT and preference-based methods, showing significant improvements in COMET and M-ETA scores for various language pairs.", "conclusion": "RLfR effectively imitates human learning through iterative feedback, enhancing machine translation outcomes without static triplet dependency.", "key_contributions": ["Introduces RLfR framework for machine translation", "Utilizes GPT-4o as a dynamic teacher model for feedback", "Achieves significant gains in semantic adequacy and entity preservation metrics."], "limitations": "", "keywords": ["Machine Translation", "Reinforcement Learning", "Teacher-Model Refinement", "Preference Learning"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2507.22329", "pdf": "https://arxiv.org/pdf/2507.22329.pdf", "abs": "https://arxiv.org/abs/2507.22329", "title": "A Node on the Constellation: The Role of Feminist Makerspaces in Building and Sustaining Alternative Cultures of Technology Production", "authors": ["Erin Gatz", "Yasmine Kotturi", "Andrea Afua Kwamya", "Sarah Fox"], "categories": ["cs.HC"], "comment": null, "summary": "Feminist makerspaces offer community led alternatives to dominant tech\ncultures by centering care, mutual aid, and collective knowledge production.\nWhile prior CSCW research has explored their inclusive practices, less is known\nabout how these spaces sustain themselves over time. Drawing on interviews with\n18 founders and members across 8 U.S. feminist makerspaces as well as\nautoethnographic reflection, we examine the organizational and relational\npractices that support long-term endurance. We find that sustainability is not\nachieved through growth or institutionalization, but through care-driven\nstewardship, solidarity with local justice movements, and shared governance.\nThese social practices position feminist makerspaces as prefigurative\ncounterspaces - sites that enact, rather than defer, feminist values in\neveryday practice. This paper offers empirical insight into how feminist\nmakerspaces persist amid structural precarity, and highlights the forms of\nlabor and coalition-building that underpin alternative sociotechnical\ninfrastructures.", "AI": {"tldr": "This paper explores the sustainability practices of feminist makerspaces, emphasizing care-driven stewardship and solidarity with local justice movements.", "motivation": "To understand how feminist makerspaces sustain themselves over time amidst structural precarity.", "method": "Interviews with 18 founders and members across 8 U.S. feminist makerspaces, along with autoethnographic reflection.", "result": "Identified key practices supporting sustainability, which include care-driven stewardship, solidarity with local justice movements, and shared governance.", "conclusion": "Feminist makerspaces serve as counterspaces that implement feminist values through their organizational practices, contributing to their longevity in challenging environments.", "key_contributions": ["Examines the sustainability practices of feminist makerspaces", "Highlights the importance of care-driven stewardship and solidarity", "Positions feminist makerspaces as prefigurative counterspaces enacting feminist values"], "limitations": "", "keywords": ["feminist makerspaces", "sustainability", "community-led initiatives", "collective knowledge production", "social practices"], "importance_score": 2, "read_time_minutes": 10}}
{"id": "2507.22286", "pdf": "https://arxiv.org/pdf/2507.22286.pdf", "abs": "https://arxiv.org/abs/2507.22286", "title": "Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs", "authors": ["Supantho Rakshit", "Adele Goldberg"], "categories": ["cs.CL", "cs.AI", "68T50"], "comment": "5 pages, 3 figures, Accepted for publication at the Second\n  International Workshop on Construction Grammars and NLP at the 16th\n  International Conference for Computational Semantics (IWCS) 2025", "summary": "The usage-based constructionist (UCx) approach posits that language comprises\na network of learned form-meaning pairings (constructions) whose use is largely\ndetermined by their meanings or functions, requiring them to be graded and\nprobabilistic. This study investigates whether the internal representations in\nLarge Language Models (LLMs) reflect the proposed function-infused gradience.\nWe analyze the neural representations of the English dative constructions\n(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of\n$5000$ sentence pairs systematically varied for human-rated preference\nstrength. A macro-level geometric analysis finds that the separability between\nconstruction representations, as measured by Energy Distance or Jensen-Shannon\nDivergence, is systematically modulated by gradient preference strength. More\nprototypical exemplars of each construction occupy more distinct regions in the\nactivation space of LLMs. These results provide strong evidence that LLMs learn\nrich, meaning-infused, graded representations of constructions and offer\nsupport for geometric measures of basic constructionist principles in LLMs.", "AI": {"tldr": "This study analyzes the internal representations of English dative constructions in LLMs, specifically Pythia-$1.4$B, revealing that LLMs learn meaning-infused, graded representations of constructions based on preference strength.", "motivation": "To investigate whether LLMs reflect the function-infused gradience proposed by the usage-based constructionist approach to language.", "method": "A macro-level geometric analysis was conducted on the neural representations of English dative constructions using a dataset of 5000 sentence pairs, assessing their separability through Energy Distance and Jensen-Shannon Divergence.", "result": "The analysis demonstrated that the separability of construction representations in LLMs is modulated by gradient preference strength, with more prototypical examples occupying distinct regions in the activation space.", "conclusion": "The findings provide strong evidence that LLMs encode rich, meaning-infused, graded representations of language constructions, supporting basic constructionist principles.", "key_contributions": ["Analysis of LLM representations of language constructions using geometric measures", "Evidence of meaning-infused graded representations in LLMs", "Insights into the internal workings of LLMs related to language construction theories"], "limitations": "", "keywords": ["Large Language Models", "usage-based constructionist approach", "neural representations"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.22352", "pdf": "https://arxiv.org/pdf/2507.22352.pdf", "abs": "https://arxiv.org/abs/2507.22352", "title": "Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents", "authors": ["Mykola Maslych", "Mohammadreza Katebi", "Christopher Lee", "Yahya Hmaiti", "Amirpouya Ghasemaghaei", "Christian Pumarada", "Janneese Palmer", "Esteban Segarra Martinez", "Marco Emporio", "Warren Snipes", "Ryan P. McMahan", "Joseph J. LaViola Jr"], "categories": ["cs.HC", "H.1.2; H.5.2; I.2.7; I.3.7"], "comment": "15 pages, 8 figures. Published at the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25), July 8-10, 2025, Waterloo, Canada.\n  Open-source code available at https://github.com/ISUE/iva-cui", "summary": "We investigated the challenges of mitigating response delays in free-form\nconversations with virtual agents powered by Large Language Models (LLMs)\nwithin Virtual Reality (VR). For this, we used conversational fillers, such as\ngestures and verbal cues, to bridge delays between user input and system\nresponses and evaluate their effectiveness across various latency levels and\ninteraction scenarios. We found that latency above 4 seconds degrades quality\nof experience, while natural conversational fillers improve perceived response\ntime, especially in high-delay conditions. Our findings provide insights for\npractitioners and researchers to optimize user engagement whenever\nconversational systems' responses are delayed by network limitations or slow\nhardware. We also contribute an open-source pipeline that streamlines deploying\nconversational agents in virtual environments.", "AI": {"tldr": "This paper explores reducing response delays in virtual agents using Large Language Models in Virtual Reality by employing conversational fillers to improve user experience under latency conditions.", "motivation": "To address the challenges of response delays in free-form conversations with virtual agents in Virtual Reality, enhancing user engagement despite system lags.", "method": "Investigated the effectiveness of conversational fillers, including gestures and verbal cues, in a range of latency scenarios during interactions with virtual agents.", "result": "Discovering that latencies beyond 4 seconds significantly deteriorate user experience, while conversational fillers effectively enhance perceived response time under high-delay situations.", "conclusion": "The study offers valuable insights for optimizing user engagement in conversational systems facing latency, alongside an open-source deployment pipeline for conversational agents in virtual environments.", "key_contributions": ["Insights into mitigating response delays in virtual conversations", "Demonstration of using conversational fillers to improve user experience", "Provision of an open-source pipeline for deploying conversational agents in VR"], "limitations": "Focused on specific latency conditions; further investigation needed on various interaction designs and user demographics.", "keywords": ["Virtual Agents", "Large Language Models", "Conversational Fillers", "User Engagement", "Virtual Reality"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22289", "pdf": "https://arxiv.org/pdf/2507.22289.pdf", "abs": "https://arxiv.org/abs/2507.22289", "title": "Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations", "authors": ["Galo Castillo-LÃ³pez", "GaÃ«l de Chalendar", "Nasredine Semmar"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication at SIGDIAL 2025", "summary": "Intent recognition is a fundamental component in task-oriented dialogue\nsystems (TODS). Determining user intents and detecting whether an intent is\nOut-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,\ntraditional TODS require large amount of annotated data. In this work we\npropose a hybrid approach to combine BERT and LLMs in zero and few-shot\nsettings to recognize intents and detect OOS utterances. Our approach leverages\nLLMs generalization power and BERT's computational efficiency in such\nscenarios. We evaluate our method on multi-party conversation corpora and\nobserve that sharing information from BERT outputs to LLMs leads to system\nperformance improvement.", "AI": {"tldr": "This paper proposes a hybrid approach combining BERT and LLMs for intent recognition and Out-of-Scope detection in task-oriented dialogue systems, effectively utilizing zero and few-shot learning.", "motivation": "To improve intent recognition and OOS detection in task-oriented dialogue systems (TODS) without requiring large amounts of annotated data.", "method": "The proposed hybrid approach combines the generalization power of large language models (LLMs) with the computational efficiency of BERT, particularly in zero and few-shot settings.", "result": "The evaluation on multi-party conversation corpora shows that sharing information from BERT outputs to LLMs enhances the performance of the system.", "conclusion": "The proposed method demonstrates significant improvements in intent recognition and OOS detection with minimal annotated data, making it a viable option for TODS.", "key_contributions": ["Hybrid methodology using BERT and LLMs", "Effective in zero and few-shot settings", "Performance improvement through inter-model information sharing"], "limitations": "", "keywords": ["intent recognition", "task-oriented dialogue systems", "BERT", "LLMs", "zero-shot learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2507.22382", "pdf": "https://arxiv.org/pdf/2507.22382.pdf", "abs": "https://arxiv.org/abs/2507.22382", "title": "A Fuzzy Set-based Approach for Matching Hand-Drawing Shapes of Touch-based Gestures for Graphical Passwords", "authors": ["Adel Sabour", "Ahmed Gadallah", "Hesham Hefny"], "categories": ["cs.HC"], "comment": null, "summary": "This paper presents a two-dimension fuzzy set based approach for matching\ntouch-based gestures using fuzzy cued click point technique. The pro posed\napproach aims mainly to improve the acceptance of the most closed inac curate\nhand drawn gestures generated by the user compared with a predefined referenced\ngesture value that is stored in the user profile. Commonly, gestures are used\nin order to facilitate the interactive capabilities between humans and\ncomputerized systems. Unfortunately, most of current gesturing techniques don't\ndeal at the same level of inaccuracy of gesturing, resulted from the nature of\nhu man fingers and hands movements. This paper aims, in a more flexible manner,\nto tackle the inaccuracy problem existed with gesture-based interactions\nbetween humans and a computerized system.", "AI": {"tldr": "The paper introduces a fuzzy set-based method to match touch gestures, aiming to enhance accuracy in gesture recognition.", "motivation": "To address the inaccuracies in hand-drawn gestures that hinder effective human-computer interaction.", "method": "The proposed method uses a two-dimensional fuzzy set approach combined with a fuzzy cued click point technique to improve gesture acceptance.", "result": "It improves the recognition of gestures that are inaccurate while interacting with computerized systems, leading to better usability.", "conclusion": "The approach provides a flexible solution to the common accuracy issues in gesture-based interactions.", "key_contributions": ["Development of a fuzzy set-based method for gesture recognition", "Improvement in acceptance rates of inaccurate gestures", "Flexibility in handling gesture inaccuracies"], "limitations": "The effectiveness of the method may depend on the complexity of gestures and the user's training.", "keywords": ["Fuzzy Sets", "Gesture Recognition", "Human-Computer Interaction"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22337", "pdf": "https://arxiv.org/pdf/2507.22337.pdf", "abs": "https://arxiv.org/abs/2507.22337", "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers", "authors": ["Roxana Petcu", "Samarth Bhargav", "Maarten de Rijke", "Evangelos Kanoulas"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.", "AI": {"tldr": "The paper explores the challenges posed by negation in neural information retrieval, proposing a taxonomy, benchmark datasets, and a classification mechanism for improved model performance.", "motivation": "To address the underperformance of neural models on queries containing negation, which is critical for fulfilling user information needs.", "method": "The study introduces a taxonomy of negation and generates benchmark datasets for evaluating and fine-tuning neural retrieval models, alongside a logic-based classification mechanism.", "result": "A balanced data distribution over negation types is established, leading to improved training setups and faster convergence on the NevIR dataset.", "conclusion": "The proposed classification schema provides insights into negation coverage in datasets, potentially enhancing fine-tuned model generalization on negation tasks.", "key_contributions": ["Introduction of a taxonomy of negation from various definitions", "Creation of benchmark datasets for model evaluation and fine-tuning", "Development of a logic-based classification mechanism for analyzing retrieval model performance"], "limitations": "", "keywords": ["negation", "neural information retrieval", "benchmark datasets"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22455", "pdf": "https://arxiv.org/pdf/2507.22455.pdf", "abs": "https://arxiv.org/abs/2507.22455", "title": "Analysis of User Experience Evaluation Methods for Deaf users: A Case Study on a mobile App", "authors": ["A. E. Fuentes-CortÃ¡zar", "A. Rivera-HernÃ¡ndez", "J. R. Rojano-CÃ¡ceres"], "categories": ["cs.HC", "H.5.2"], "comment": "15 pages, 2 figures, presented in Ibero-American Conference on\n  Human-Computer Interaction 2025", "summary": "User Experience (UX) evaluation methods that are commonly used with hearing\nusers may not be functional or effective for Deaf users. This is because these\nmethods are primarily designed for users with hearing abilities, which can\ncreate limitations in the interaction, perception, and understanding of the\nmethods for Deaf individuals. Furthermore, traditional UX evaluation approaches\noften fail to address the unique accessibility needs of Deaf users, resulting\nin an incomplete or biased assessment of their user experience. This research\nfocused on analyzing a set of UX evaluation methods recommended for use with\nDeaf users, with the aim of validating the accessibility of each method through\nfindings and limitations. The results indicate that, although these evaluation\nmethods presented here are commonly recommended in the literature for use with\nDeaf users, they present various limitations that must be addressed in order to\nbetter adapt to the communication skills specific to the Deaf community. This\nresearch concludes that evaluation methods must be adapted to ensure accessible\nsoftware evaluation for Deaf individuals, enabling the collection of data that\naccurately reflects their experiences and needs.", "AI": {"tldr": "This research analyzes common UX evaluation methods for their effectiveness with Deaf users, highlighting necessary adaptations to improve accessibility and accurately reflect their user experience.", "motivation": "To address the limitations of traditional UX evaluation methods for Deaf users, ensuring their unique accessibility needs are met.", "method": "Analysis of recommended UX evaluation methods for Deaf users, validating their accessibility and identifying limitations.", "result": "Identification of various limitations in existing UX evaluation methods for Deaf users, underscoring the need for adaptation.", "conclusion": "UX evaluation methods need adaptation to support Deaf individuals effectively, allowing for accurate data collection on their experiences and needs.", "key_contributions": ["Analysis of existing UX methods for Deaf users", "Validation of accessibility of recommended methods", "Recommendations for adaptation of UX evaluation methods"], "limitations": "Existing methods often overlook the unique communication skills of the Deaf community.", "keywords": ["User Experience", "Deaf users", "Accessibility", "UX evaluation methods", "Human-Computer Interaction"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.22367", "pdf": "https://arxiv.org/pdf/2507.22367.pdf", "abs": "https://arxiv.org/abs/2507.22367", "title": "Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors", "authors": ["Jia Li", "Yichao He", "Jiacheng Xu", "Tianhao Luo", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 3 figures, ACM MM 2025", "summary": "Accurate and reliable personality assessment plays a vital role in many\nfields, such as emotional intelligence, mental health diagnostics, and\npersonalized education. Unlike fleeting emotions, personality traits are\nstable, often subconsciously leaked through language, facial expressions, and\nbody behaviors, with asynchronous patterns across modalities. It was hard to\nmodel personality semantics with traditional superficial features and seemed\nimpossible to achieve effective cross-modal understanding. To address these\nchallenges, we propose a novel personality assessment framework called\n\\textit{\\textbf{Traits Run Deep}}. It employs\n\\textit{\\textbf{psychology-informed prompts}} to elicit high-level\npersonality-relevant semantic representations. Besides, it devises a\n\\textit{\\textbf{Text-Centric Trait Fusion Network}} that anchors rich text\nsemantics to align and integrate asynchronous signals from other modalities. To\nbe specific, such fusion module includes a Chunk-Wise Projector to decrease\ndimensionality, a Cross-Modal Connector and a Text Feature Enhancer for\neffective modality fusion and an ensemble regression head to improve\ngeneralization in data-scarce situations. To our knowledge, we are the first to\napply personality-specific prompts to guide large language models (LLMs) in\nextracting personality-aware semantics for improved representation quality.\nFurthermore, extracting and fusing audio-visual apparent behavior features\nfurther improves the accuracy. Experimental results on the AVI validation set\nhave demonstrated the effectiveness of the proposed components, i.e.,\napproximately a 45\\% reduction in mean squared error (MSE). Final evaluations\non the test set of the AVI Challenge 2025 confirm our method's superiority,\nranking first in the Personality Assessment track. The source code will be made\navailable at https://github.com/MSA-LMC/TraitsRunDeep.", "AI": {"tldr": "The paper proposes a novel framework, Traits Run Deep, for personality assessment that utilizes psychology-informed prompts and a Text-Centric Trait Fusion Network to integrate multiple modalities effectively, leading to improved accuracy in personality trait evaluation.", "motivation": "Accurate personality assessment is crucial in fields like mental health diagnostics and personalized education, but traditional methods struggle with cross-modal representation.", "method": "The framework uses psychology-informed prompts to elicit personality-relevant semantic representations and a fusion network to integrate text with audio-visual features, employing components like a Chunk-Wise Projector and a Cross-Modal Connector.", "result": "The method shows approximately a 45% reduction in mean squared error (MSE) and ranks first in the Personality Assessment track of the AVI Challenge 2025 test set evaluations.", "conclusion": "The proposed method effectively improves personality assessment accuracy by addressing cross-modal understanding through innovative integration strategies.", "key_contributions": ["Introduction of psychology-informed prompts for LLMs in personality assessment", "Development of a Text-Centric Trait Fusion Network for cross-modal integration", "Significant reduction in MSE and top ranking in official challenge evaluations"], "limitations": "", "keywords": ["personality assessment", "machine learning", "multimodal integration", "large language models", "behavioral analysis"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2507.22614", "pdf": "https://arxiv.org/pdf/2507.22614.pdf", "abs": "https://arxiv.org/abs/2507.22614", "title": "Exploring Student-AI Interactions in Vibe Coding", "authors": ["Francis Geng", "Anshul Shah", "Haolin Li", "Nawab Mulla", "Steven Swanson", "Gerald Soosai Raj", "Daniel Zingaro", "Leo Porter"], "categories": ["cs.HC", "cs.CY"], "comment": null, "summary": "Background and Context. Chat-based and inline-coding-based GenAI has already\nhad substantial impact on the CS Education community. The recent introduction\nof ``vibe coding'' may further transform how students program, as it introduces\na new way for students to create software projects with minimal oversight.\n  Objectives. The purpose of this study is to understand how students in\nintroductory programming and advanced software engineering classes interact\nwith a vibe coding platform (Replit) when creating software and how the\ninteractions differ by programming background.\n  Methods. Interview participants were asked to think-aloud while building a\nweb application using Replit. Thematic analysis was then used to analyze the\nvideo recordings with an emphasis on the interactions between the student and\nReplit.\n  Findings. For both groups, the majority of student interactions with Replit\nwere to test or debug the prototype and only rarely did students visit code.\nPrompts by advanced software engineering students were much more likely to\ninclude relevant app feature and codebase contexts than those by introductory\nprogramming students.", "AI": {"tldr": "This study investigates how students in programming classes interact with the Vibe Coding platform (Replit) while creating software projects.", "motivation": "To understand the impact of vibe coding on student programming experiences and software project development.", "method": "Participants used Replit to build a web application, while their interactions were recorded and analyzed through thematic analysis.", "result": "Most interactions involved testing or debugging; advanced students provided more contextually relevant prompts compared to novice students.", "conclusion": "Vibe coding significantly influences how students engage with programming platforms, highlighting differing interaction patterns based on experience level.", "key_contributions": ["Insights into student interactions with Replit in different programming courses", "Comparison of interaction behaviors between novice and advanced students", "Implications for future teaching methods in CS education"], "limitations": "The study's insights are limited to one platform (Replit) and specific student demographics in a controlled environment.", "keywords": ["vibe coding", "Replit", "CS education", "programming", "user interaction"], "importance_score": 6, "read_time_minutes": 8}}
{"id": "2507.22387", "pdf": "https://arxiv.org/pdf/2507.22387.pdf", "abs": "https://arxiv.org/abs/2507.22387", "title": "PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs", "authors": ["Homaira Huda Shomee", "Suman Kalyan Maity", "Sourav Medya"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have emerged as transformative approaches in\nseveral important fields. This paper aims for a paradigm shift for patent\nwriting by leveraging LLMs to overcome the tedious patent-filing process. In\nthis work, we present PATENTWRITER, the first unified benchmarking framework\nfor evaluating LLMs in patent abstract generation. Given the first claim of a\npatent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a\nconsistent setup spanning zero-shot, few-shot, and chain-of-thought prompting\nstrategies to generate the abstract of the patent. Our benchmark PATENTWRITER\ngoes beyond surface-level evaluation: we systematically assess the output\nquality using a comprehensive suite of metrics -- standard NLP measures (e.g.,\nBLEU, ROUGE, BERTScore), robustness under three types of input perturbations,\nand applicability in two downstream patent classification and retrieval tasks.\nWe also conduct stylistic analysis to assess length, readability, and tone.\nExperimental results show that modern LLMs can generate high-fidelity and\nstylistically appropriate patent abstracts, often surpassing domain-specific\nbaselines. Our code and dataset are open-sourced to support reproducibility and\nfuture research.", "AI": {"tldr": "This paper introduces PATENTWRITER, a benchmarking framework for evaluating large language models (LLMs) in generating patent abstracts, analyzing their output quality against established metrics and tasks.", "motivation": "To address the challenges in the tedious and complex process of patent writing by utilizing large language models for abstract generation.", "method": "The study evaluates six leading LLMs, including GPT-4 and LLaMA-3, under various prompting strategies (zero-shot, few-shot, and chain-of-thought) while assessing output quality with standard NLP metrics and conducting stylistic analysis.", "result": "Experimental results demonstrate that modern LLMs generate high-fidelity patent abstracts that often outperform domain-specific baselines across multiple quality dimensions.", "conclusion": "The results suggest that LLMs can effectively improve patent writing processes, with the framework and datasets provided for future research.", "key_contributions": ["Introduction of PATENTWRITER as a unified benchmarking framework for LLMs in patent abstract generation", "Comprehensive evaluation of LLM output quality using a diverse set of metrics", "Open-sourcing code and dataset for reproducibility and future exploration"], "limitations": "The study focuses on patent abstract generation and does not cover other aspects of patent writing or broader implications of LLMs in different scientific domains.", "keywords": ["Patent Writing", "Large Language Models", "Benchmarking Framework", "Natural Language Processing", "Evaluation Metrics"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.22671", "pdf": "https://arxiv.org/pdf/2507.22671.pdf", "abs": "https://arxiv.org/abs/2507.22671", "title": "Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach", "authors": ["Sami Saeed Alghamdi", "Christopher Bull", "Ahmed Kharrufa"], "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.SE", "H.5.2; H.5.4"], "comment": "10 pages, 9 figures", "summary": "Many people learn programming independently from online resources and often\nreport struggles in achieving their personal learning goals. Learners\nfrequently describe their experiences as isolating and frustrating, challenged\nby abundant uncertainties, information overload, and distraction, compounded by\nlimited guidance. At the same time, social media serves as a personal space\nwhere many engage in diverse self-regulation practices, including help-seeking,\nusing external memory aids (e.g., self-notes), self-reflection, emotion\nregulation, and self-motivation. For instance, learners often mark achievements\nand set milestones through their posts. In response, we developed a system\nconsisting of a web platform and browser extensions to support self-regulation\nonline. The design aims to add learner-defined structure to otherwise\nunstructured experiences and bring meaning to curation and reflection\nactivities by translating them into learning stories with AI-generated\nfeedback. We position storytelling as an integrative approach to design that\nconnects resource curation, reflective and sensemaking practice, and narrative\npractices learners already use across social platforms. We recruited 15\ninformal programming learners who are regular social media users to engage with\nthe system in a self-paced manner; participation concluded upon submitting a\nlearning story and survey. We used three quantitative scales and a qualitative\nsurvey to examine users' characteristics and perceptions of the system's\nsupport for their self-regulation. User feedback suggests the system's\nviability as a self-regulation aid. Learners particularly valued in-situ\nreflection, automated story feedback, and video annotation, while other\nfeatures received mixed views. We highlight perceived benefits, friction\npoints, and design opportunities for future AI-augmented self-regulation tools.", "AI": {"tldr": "This paper presents a web platform and browser extensions designed to support independent programming learners in self-regulation by integrating AI-generated feedback into their learning stories.", "motivation": "Many self-taught programming learners struggle due to feelings of isolation and frustration, often resulting from information overload and lack of guidance. The aim is to enhance their learning experience through structured self-regulation practices facilitated by social media.", "method": "The study involved developing a system that combines a web platform and browser extensions to help learners create narrative-driven learning stories with AI feedback. Data were collected from 15 informal programming learners via surveys and quantitative scales.", "result": "Users found value in tools such as in-situ reflection and automated story feedback, indicating the system's potential as a support aid for self-regulation. However, feedback on other features was mixed.", "conclusion": "The findings suggest that AI-augmented self-regulation tools can be beneficial to informal programming learners, highlighting both benefits and areas for improvement in future designs.", "key_contributions": ["Development of an AI-augmented self-regulation system for programming learners", "Integration of storytelling into the learning process", "User feedback identifying valuable features and design opportunities"], "limitations": "The study was limited to a small group of 15 participants, which may not represent broader learner experiences.", "keywords": ["self-regulation", "programming education", "AI feedback", "learning stories", "narrative practices"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22410", "pdf": "https://arxiv.org/pdf/2507.22410.pdf", "abs": "https://arxiv.org/abs/2507.22410", "title": "Question Generation for Assessing Early Literacy Reading Comprehension", "authors": ["Xiaocheng Yang", "Sumuk Shashidhar", "Dilek Hakkani-Tur"], "categories": ["cs.CL", "cs.AI"], "comment": "2 pages, 1 figure, accepted by SLaTE 2025", "summary": "Assessment of reading comprehension through content-based interactions plays\nan important role in the reading acquisition process. In this paper, we propose\na novel approach for generating comprehension questions geared to K-2 English\nlearners. Our method ensures complete coverage of the underlying material and\nadaptation to the learner's specific proficiencies, and can generate a large\ndiversity of question types at various difficulty levels to ensure a thorough\nevaluation. We evaluate the performance of various language models in this\nframework using the FairytaleQA dataset as the source material. Eventually, the\nproposed approach has the potential to become an important part of autonomous\nAI-driven English instructors.", "AI": {"tldr": "Proposes a method for generating comprehension questions for K-2 English learners, adapting to individual proficiencies and ensuring comprehensive evaluation.", "motivation": "To enhance reading comprehension assessment through automated, tailored question generation for young English learners.", "method": "The paper evaluates various language models within a framework designed to produce diverse comprehension questions based on the FairytaleQA dataset.", "result": "The proposed method demonstrates effective coverage of material and adaptability to learner proficiency, enabling a wide range of question types and difficulty levels.", "conclusion": "This approach could significantly contribute to autonomous AI-driven English instruction, improving the reading acquisition process for K-2 learners.", "key_contributions": ["Novel question generation method for reading comprehension", "Adaptation to individual learner proficiencies", "Diverse question types and difficulty levels"], "limitations": "", "keywords": ["Reading Comprehension", "Language Models", "AI in Education"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22810", "pdf": "https://arxiv.org/pdf/2507.22810.pdf", "abs": "https://arxiv.org/abs/2507.22810", "title": "VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education", "authors": ["Daniel Udekwe", "Dimitrios Bolkas", "Eren Erman Ozguven", "Ren Moses", "Qianwen", "Guo"], "categories": ["cs.HC", "cs.ET", "cs.SE"], "comment": null, "summary": "Surveying is a core component of civil engineering education, requiring\nstudents to engage in hands-on spatial measurement, instrumentation handling,\nand field-based decision-making. However, traditional instruction often poses\nlogistical and cognitive challenges that can hinder accessibility and student\nengagement. While virtual laboratories have gained traction in engineering\neducation, few are purposefully designed to support flexible, adaptive learning\nin surveying. To address this gap, we developed Virtual Reality for Immersive\nand Interactive Surveying Education (VRISE), an immersive virtual reality\nlaboratory that replicates ground-based and aerial surveying tasks through\ncustomizable, accessible, and user-friendly modules. VRISE features interactive\nexperiences such as differential leveling with a digital level equipment and\nwaypoint-based drone navigation, enhanced by input smoothing, adaptive\ninterfaces, and real-time feedback to accommodate diverse learning styles.\nEvaluation across multiple user sessions demonstrated consistent gains in\nmeasurement accuracy, task efficiency, and interaction quality, with a clear\nprogression in skill development across the ground-based and aerial surveying\nmodalities. By reducing cognitive load and physical demands, even in tasks\nrequiring fine motor control and spatial reasoning, VRISE demonstrates the\npotential of immersive, repeatable digital environments to enhance surveying\neducation, broaden participation, and strengthen core competencies in a safe\nand engaging setting.", "AI": {"tldr": "Development of VRISE, an immersive VR laboratory for surveying education, enhancing student engagement and learning outcomes.", "motivation": "To improve accessibility and engagement in surveying education by overcoming logistical and cognitive challenges of traditional methods.", "method": "Creation of Virtual Reality for Immersive and Interactive Surveying Education (VRISE), which provides adaptable learning modules for ground-based and aerial surveying tasks.", "result": "Evaluation showed consistent improvements in measurement accuracy, task efficiency, and interaction quality across user sessions, indicating enhanced skill development.", "conclusion": "VRISE effectively reduces cognitive load and physical demands, showcasing the benefits of immersive digital environments in education.", "key_contributions": ["Introduction of customizable VR modules for surveying education", "Empirical evidence of learning gains from immersive experiences", "Enhanced engagement through adaptive interfaces and real-time feedback"], "limitations": "", "keywords": ["Virtual Reality", "Surveying Education", "Interactive Learning", "Immersive Environments", "Skill Development"], "importance_score": 4, "read_time_minutes": 5}}
{"id": "2507.22411", "pdf": "https://arxiv.org/pdf/2507.22411.pdf", "abs": "https://arxiv.org/abs/2507.22411", "title": "NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models", "authors": ["Hyeonseok Moon", "Heuiseok Lim"], "categories": ["cs.CL", "cs.AI"], "comment": "13 pages", "summary": "The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large\nLanguage Models' (LLMs) ability to understand long contexts (LC). It evaluates\nthe capability to identify query-relevant context within extensive\nquery-irrelevant passages. Although this method serves as a widely accepted\nstandard for evaluating long-context understanding, our findings suggest it may\noverestimate the true LC capability of LLMs. We demonstrate that even\nstate-of-the-art models such as GPT-4o struggle to intactly incorporate given\ncontexts made up of solely query-relevant ten sentences. In response, we\nintroduce a novel benchmark, \\textbf{NeedleChain}, where the context consists\nentirely of query-relevant information, requiring the LLM to fully grasp the\ninput to answer correctly. Our benchmark allows for flexible context length and\nreasoning order, offering a more comprehensive analysis of LLM performance.\nAdditionally, we propose an extremely simple yet compelling strategy to improve\nLC understanding capability of LLM: ROPE Contraction. Our experiments with\nvarious advanced LLMs reveal a notable disparity between their ability to\nprocess large contexts and their capacity to fully understand them. Source code\nand datasets are available at https://github.com/hyeonseokk/NeedleChain", "AI": {"tldr": "This paper introduces a new benchmark called NeedleChain to better evaluate Large Language Models' understanding of long contexts, revealing limitations in current methods like Needle-in-a-Haystack.", "motivation": "The existing Needle-in-a-Haystack benchmark may overestimate LLMs' long-context understanding abilities, prompting the need for a more effective evaluation methodology.", "method": "The authors propose NeedleChain, a benchmark with contexts made entirely of query-relevant information to accurately assess LLM performance on long-context tasks. They also introduce ROPE Contraction to improve understanding capability.", "result": "Experiments show that advanced LLMs demonstrate a significant gap between processing large contexts and fully understanding them, highlighting the inadequacy of existing evaluation methods.", "conclusion": "NeedleChain provides a superior framework for evaluating LLMs in terms of understanding long contexts, emphasizing the importance of relevant information in the evaluation process.", "key_contributions": ["Introduction of NeedleChain benchmark for LLM evaluation", "Proposal of ROPE Contraction strategy to enhance LLM understanding", "Clear demonstration of LLM performance gaps in long-context tasks"], "limitations": "The study focuses on the limitations of current benchmarks and may not address all aspects of LLM comprehension.", "keywords": ["Long Contexts", "Large Language Models", "Benchmarking", "NeedleChain", "ROPE Contraction"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2507.22839", "pdf": "https://arxiv.org/pdf/2507.22839.pdf", "abs": "https://arxiv.org/abs/2507.22839", "title": "Progressive Web Application for Storytelling Therapy Support", "authors": ["Javier Jimenez-Honrado", "Javier Gomez Garcia", "Felipe Costa-Tebar", "Felix A. Marco", "Jose A. Gallud", "Gabriel Sebastian Rivera"], "categories": ["cs.HC"], "comment": "Interaccion 2024", "summary": "In spite of all advances promoted by information technologies, there are\nstill activities where this technology is not applied for reasons such as being\ncarried out in non-profit organizations or because they have not adapted to\nthis modernization. Until recently, the way to work with mobile devices was\neither by connecting through a web page with the device's browser, or by\ndownloading an application from the corresponding platform. But lately,\ntechnologies are being developed that aim to break with this, as in the case of\nProgressive Web Applications (PWA). One of the advantages offered by PWA is to\naccess the web page and install it as an application on the device. The purpose\nof this article is to design a progressive Web application for the support of\nStorytelling Therapy, one of the novel therapies applied in the field of mental\nhealth. In addition to providing a software application to enhance Storytelling\nTherapy workshops, it is also intended to analyze and verify the advantages of\nPWA in a real case.", "AI": {"tldr": "The paper discusses the development of a Progressive Web Application (PWA) to support Storytelling Therapy in mental health, emphasizing its advantages over traditional methods.", "motivation": "To address the lack of technology in non-profit organizations and to modernize approaches to mental health therapies by utilizing Progressive Web Applications.", "method": "The design and implementation of a Progressive Web Application specifically for enhancing Storytelling Therapy workshops, including analysis of its practical benefits.", "result": "The PWA facilitates access and installation on devices, aiming to improve the delivery of Storytelling Therapy.", "conclusion": "The study confirms the practical advantages of adopting PWA technology in mental health therapy settings.", "key_contributions": ["Design of a PWA for Storytelling Therapy", "Highlighting the benefits of PWAs in mental health applications", "Practical implementation and analysis in a real-world context"], "limitations": "", "keywords": ["Progressive Web Applications", "Storytelling Therapy", "Mental Health", "Technology in Therapy", "Non-Profit Applications"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2507.22445", "pdf": "https://arxiv.org/pdf/2507.22445.pdf", "abs": "https://arxiv.org/abs/2507.22445", "title": "AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini", "authors": ["Jill Walker Rettberg", "Hermann Wigers"], "categories": ["cs.CL", "cs.AI", "H.1.2; I.2.4; I.2.0; I.2.7"], "comment": "This project has received funding from the European Union's Horizon\n  2020 research and innovation programme under grant agreement number\n  101142306. The project is also supported by the Center for Digital Narrative,\n  which is funded by the Research Council of Norway through its Centres of\n  Excellence scheme, project number 332643", "summary": "Can a language model trained largely on Anglo-American texts generate stories\nthat are culturally relevant to other nationalities? To find out, we generated\n11,800 stories - 50 for each of 236 countries - by sending the prompt \"Write a\n1500 word potential {demonym} story\" to OpenAI's model gpt-4o-mini. Although\nthe stories do include surface-level national symbols and themes, they\noverwhelmingly conform to a single narrative plot structure across countries: a\nprotagonist lives in or returns home to a small town and resolves a minor\nconflict by reconnecting with tradition and organising community events.\nReal-world conflicts are sanitised, romance is almost absent, and narrative\ntension is downplayed in favour of nostalgia and reconciliation. The result is\na narrative homogenisation: an AI-generated synthetic imaginary that\nprioritises stability above change and tradition above growth. We argue that\nthe structural homogeneity of AI-generated narratives constitutes a distinct\nform of AI bias, a narrative standardisation that should be acknowledged\nalongside the more familiar representational bias. These findings are relevant\nto literary studies, narratology, critical AI studies, NLP research, and\nefforts to improve the cultural alignment of generative AI.", "AI": {"tldr": "The paper investigates whether a language model can generate culturally relevant stories for various nationalities, finding that while surface-level national symbols are present, the narratives are homogenized with a focus on nostalgia and resolution of minor conflicts.", "motivation": "To assess the cultural relevance of AI-generated stories across different nationalities and identify any biases present in their narratives.", "method": "The authors generated 11,800 stories by prompting a language model with specific nationality themes and analyzed the resulting narratives for structural similarities and cultural representations.", "result": "The generated stories predominantly follow a single narrative arc, focusing on characters returning to small towns, resolving minor conflicts, and emphasizing tradition, leading to a homogenized narrative form.", "conclusion": "Narrative homogenization in AI-generated stories reflects a distinct bias that prioritizes stability and tradition, highlighting a need for critical examination of generative AI outputs in cultural contexts.", "key_contributions": ["Identifying structural homogeneity as a form of AI bias", "Highlighting the limitations of AI in generating culturally diverse narratives", "Emphasizing the implications of narrative standardization for literary and AI studies"], "limitations": "The narratives lack depth in real-world conflicts and romantic elements, potentially limiting their engagement and relevance to actual cultural experiences.", "keywords": ["AI narratives", "cultural relevance", "narrative bias", "literary studies", "generative AI"], "importance_score": 6, "read_time_minutes": 15}}
{"id": "2507.22448", "pdf": "https://arxiv.org/pdf/2507.22448.pdf", "abs": "https://arxiv.org/abs/2507.22448", "title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance", "authors": ["Jingwei Zuo", "Maksim Velikanov", "Ilyas Chahed", "Younes Belkada", "Dhia Eddine Rhayem", "Guillaume Kunsch", "Hakim Hacid", "Hamza Yous", "Brahim Farhat", "Ibrahim Khadraoui", "Mugariya Farooq", "Giulia Campesan", "Ruxandra Cojocaru", "Yasser Djilali", "Shi Hu", "Iheb Chaabane", "Puneesh Khanna", "Mohamed El Amine Seddik", "Ngoc Dung Huynh", "Phuc Le Khac", "Leen AlQadi", "Billel Mokeddem", "Mohamed Chami", "Abdalgader Abubaker", "Mikhail Lubinets", "Kacper Piskorski", "Slim Frikha"], "categories": ["cs.CL"], "comment": "Technical report of Falcon-H1 model series", "summary": "In this report, we introduce Falcon-H1, a new series of large language models\n(LLMs) featuring hybrid architecture designs optimized for both high\nperformance and efficiency across diverse use cases. Unlike earlier Falcon\nmodels built solely on Transformer or Mamba architectures, Falcon-H1 adopts a\nparallel hybrid approach that combines Transformer-based attention with State\nSpace Models (SSMs), known for superior long-context memory and computational\nefficiency. We systematically revisited model design, data strategy, and\ntraining dynamics, challenging conventional practices in the field. Falcon-H1\nis released in multiple configurations, including base and instruction-tuned\nvariants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized\ninstruction-tuned models are also available, totaling over 30 checkpoints on\nHugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and\nexceptional parameter and training efficiency. The flagship Falcon-H1-34B\nmatches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,\nand Llama3.3-70B, while using fewer parameters and less data. Smaller models\nshow similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B\nmodels, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.\nThese models excel across reasoning, mathematics, multilingual tasks,\ninstruction following, and scientific knowledge. With support for up to 256K\ncontext tokens and 18 languages, Falcon-H1 is suitable for a wide range of\napplications. All models are released under a permissive open-source license,\nunderscoring our commitment to accessible and impactful AI research.", "AI": {"tldr": "Introduction of Falcon-H1, a series of large language models with a hybrid architecture that optimizes performance and efficiency.", "motivation": "To challenge conventional practices in LLM design, focusing on computational efficiency and performance across a range of applications.", "method": "Adopting a parallel hybrid approach combining Transformer-based attention with State Space Models (SSMs). Released models include various sizes with instruction tuning and quantization strategies for efficiency.", "result": "Falcon-H1 models outperform existing models in various benchmarks while utilizing fewer parameters and less data, showcasing strong performance in reasoning, multilingual tasks, and instruction following.", "conclusion": "Falcon-H1 models, available under an open-source license, advance the state of LLMs and are aimed at promoting accessible AI research.", "key_contributions": ["Introduction of Falcon-H1 hybrid architecture", "Performance comparable to models up to 70B with fewer parameters", "Wide application support with up to 256K context tokens in 18 languages"], "limitations": "", "keywords": ["large language models", "hybrid architecture", "open-source AI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22457", "pdf": "https://arxiv.org/pdf/2507.22457.pdf", "abs": "https://arxiv.org/abs/2507.22457", "title": "What is an \"Abstract Reasoner\"? Revisiting Experiments and Arguments about Large Language Models", "authors": ["Tian Yun", "Chen Sun", "Ellie Pavlick"], "categories": ["cs.CL", "cs.AI"], "comment": "CONLL 2025. Project webpage: https://abstract-reasoner-llm.github.io/", "summary": "Recent work has argued that large language models (LLMs) are not \"abstract\nreasoners\", citing their poor zero-shot performance on a variety of challenging\ntasks as evidence. We revisit these experiments in order to add nuance to the\nclaim. First, we show that while LLMs indeed perform poorly in a zero-shot\nsetting, even tuning a small subset of parameters for input encoding can enable\nnear-perfect performance. However, we also show that this finetuning does not\nnecessarily transfer across datasets. We take this collection of empirical\nresults as an invitation to (re-)open the discussion of what it means to be an\n\"abstract reasoner\", and why it matters whether LLMs fit the bill.", "AI": {"tldr": "This paper examines the performance of large language models (LLMs) as abstract reasoners, finding that while they struggle in zero-shot settings, some parameter tuning can significantly improve their performance.", "motivation": "To add nuance to the claim that LLMs are not 'abstract reasoners' by re-examining their performance on zero-shot tasks.", "method": "The authors revisit previous experiments and analyze the effects of tuning a small subset of parameters for input encoding on LLM performance.", "result": "The results show that while LLMs perform poorly in a zero-shot setting, tuning parameters can greatly enhance their results, although this improvement does not generalize across different datasets.", "conclusion": "The paper concludes by suggesting a need to re-evaluate what it means to be an 'abstract reasoner' and the implications of LLM performance in this regard.", "key_contributions": ["Re-evaluation of LLMs' capacities as abstract reasoners.", "Demonstration that tuning input encoding parameters can enable significant performance improvements.", "Discussion on the transferability of tuning results across datasets."], "limitations": "Improvements from tuning do not necessarily apply across various datasets, indicating a lack of generalizability.", "keywords": ["Large Language Models", "Abstract Reasoning", "Parameter Tuning", "Zero-Shot Performance", "Machine Learning"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22462", "pdf": "https://arxiv.org/pdf/2507.22462.pdf", "abs": "https://arxiv.org/abs/2507.22462", "title": "IFEvalCode: Controlled Code Generation", "authors": ["Jian Yang", "Wei Zhang", "Shukai Liu", "Linzheng Chai", "Yingshui Tan", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou", "Guanglin Niu", "Zhoujun Li", "Binyuan Hui", "Junyang Lin"], "categories": ["cs.CL"], "comment": "10 pages", "summary": "Code large language models (Code LLMs) have made significant progress in code\ngeneration by translating natural language descriptions into functional code;\nhowever, real-world applications often demand stricter adherence to detailed\nrequirements such as coding style, line count, and structural constraints,\nbeyond mere correctness. To address this, the paper introduces forward and\nbackward constraints generation to improve the instruction-following\ncapabilities of Code LLMs in controlled code generation, ensuring outputs align\nmore closely with human-defined guidelines. The authors further present\nIFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven\nprogramming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and\nC#), with each sample featuring both Chinese and English queries. Unlike\nexisting benchmarks, IFEvalCode decouples evaluation into two metrics:\ncorrectness (Corr.) and instruction-following (Instr.), enabling a more nuanced\nassessment. Experiments on over 40 LLMs reveal that closed-source models\noutperform open-source ones in controllable code generation and highlight a\nsignificant gap between the models' ability to generate correct code versus\ncode that precisely follows instructions.", "AI": {"tldr": "The paper presents a novel approach to enhance the instruction-following capability of Code LLMs through the generation of forward and backward constraints, and introduces a new benchmark, IFEvalCode, for evaluating code generation quality across multiple programming languages.", "motivation": "Current Code LLMs excel at generating correct code but often fail to meet detailed human-defined requirements such as coding style and structure. This paper aims to improve adherence to such guidelines in code generation.", "method": "The authors introduce a framework for generating forward and backward constraints to enforce stricter instruction adherence in Code LLMs. They also develop IFEvalCode, a multilingual benchmark for evaluating code generation that measures correctness and instruction-following separately.", "result": "Experiments show that closed-source models significantly outperform open-source models in producing code that not only works correctly but also aligns with additional instructions and styles. The gap between correct code generation and instruction-following is highlighted.", "conclusion": "The findings suggest that enhancing instruction-following capabilities in Code LLMs can lead to better alignment with user requirements in real-world applications, and that the IFEvalCode benchmark can provide valuable insights into code generation performance.", "key_contributions": ["Introduction of forward and backward constraints to improve Code LLM instruction-following.", "Development of the IFEvalCode benchmark for nuanced evaluation of code generation.", "Demonstration of performance differences between closed-source and open-source models on instruction adherence."], "limitations": "", "keywords": ["Code LLMs", "Instruction-Following", "Benchmarking", "Code Generation", "Multilingual"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22478", "pdf": "https://arxiv.org/pdf/2507.22478.pdf", "abs": "https://arxiv.org/abs/2507.22478", "title": "SLM-SQL: An Exploration of Small Language Models for Text-to-SQL", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "categories": ["cs.CL"], "comment": "16 pages, 2 figures, work in progress", "summary": "Large language models (LLMs) have demonstrated strong performance in\ntranslating natural language questions into SQL queries (Text-to-SQL). In\ncontrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters\ncurrently underperform on Text-to-SQL tasks due to their limited logical\nreasoning capabilities. However, SLMs offer inherent advantages in inference\nspeed and suitability for edge deployment. To explore their potential in\nText-to-SQL applications, we leverage recent advancements in post-training\ntechniques. Specifically, we used the open-source SynSQL-2.5M dataset to\nconstruct two derived datasets: SynSQL-Think-916K for SQL generation and\nSynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised\nfine-tuning and reinforcement learning-based post-training to the SLM, followed\nby inference using a corrective self-consistency approach. Experimental results\nvalidate the effectiveness and generalizability of our method, SLM-SQL. On the\nBIRD development set, the five evaluated models achieved an average improvement\nof 31.4 points. Notably, the 0.5B model reached 56.87\\% execution accuracy\n(EX), while the 1.5B model achieved 67.08\\% EX. We will release our dataset,\nmodel, and code to github: https://github.com/CycloneBoy/slm_sql.", "AI": {"tldr": "This paper explores the effectiveness of small language models (SLMs) in Text-to-SQL tasks using advanced post-training techniques and datasets generated from SynSQL.", "motivation": "To address the underperformance of small language models in translating natural language questions into SQL queries despite their advantages in inference speed and edge deployment.", "method": "The authors applied supervised fine-tuning and reinforcement learning-based post-training on small language models, utilizing derived datasets from the SynSQL dataset for SQL generation and revision.", "result": "The five evaluated models showed an average improvement of 31.4 points on the BIRD development set, with the 0.5B model achieving 56.87% execution accuracy and the 1.5B model reaching 67.08% execution accuracy.", "conclusion": "The study validates the potential of SLMs for Text-to-SQL tasks and highlights the benefits of the applied post-training techniques, with datasets, models, and code available on GitHub.", "key_contributions": ["Introduction of SynSQL-Think and SynSQL-Merge datasets for SQL task training and evaluation.", "Application of reinforcement learning in fine-tuning small language models for improved SQL generation accuracy.", "Demonstration of significant performance gains in Text-to-SQL tasks using small models through innovative post-training techniques."], "limitations": "Limited to Text-to-SQL tasks; further research is needed to generalize findings to other NLP applications.", "keywords": ["Text-to-SQL", "Small Language Models", "Natural Language Processing", "Reinforcement Learning", "SQL Generation"], "importance_score": 8, "read_time_minutes": 16}}
{"id": "2507.22533", "pdf": "https://arxiv.org/pdf/2507.22533.pdf", "abs": "https://arxiv.org/abs/2507.22533", "title": "CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records", "authors": ["Dongchen Li", "Jitao Liang", "Wei Li", "Xiaoyu Wang", "Longbing Cao", "Kun Yu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) hold significant promise for improving clinical\ndecision support and reducing physician burnout by synthesizing complex,\nlongitudinal cancer Electronic Health Records (EHRs). However, their\nimplementation in this critical field faces three primary challenges: the\ninability to effectively process the extensive length and multilingual nature\nof patient records for accurate temporal analysis; a heightened risk of\nclinical hallucination, as conventional grounding techniques such as\nRetrieval-Augmented Generation (RAG) do not adequately incorporate\nprocess-oriented clinical guidelines; and unreliable evaluation metrics that\nhinder the validation of AI systems in oncology. To address these issues, we\npropose CliCARE, a framework for Grounding Large Language Models in Clinical\nGuidelines for Decision Support over Longitudinal Cancer Electronic Health\nRecords. The framework operates by transforming unstructured, longitudinal EHRs\ninto patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range\ndependencies, and then grounding the decision support process by aligning these\nreal-world patient trajectories with a normative guideline knowledge graph.\nThis approach provides oncologists with evidence-grounded decision support by\ngenerating a high-fidelity clinical summary and an actionable recommendation.\nWe validated our framework using large-scale, longitudinal data from a private\nChinese cancer dataset and the public English MIMIC-IV dataset. In these\ndiverse settings, CliCARE significantly outperforms strong baselines, including\nleading long-context LLMs and Knowledge Graph-enhanced RAG methods. The\nclinical validity of our results is supported by a robust evaluation protocol,\nwhich demonstrates a high correlation with assessments made by expert\noncologists.", "AI": {"tldr": "CliCARE is a framework designed to enhance clinical decision support in oncology by utilizing Large Language Models to process longitudinal EHRs and offer evidence-grounded recommendations.", "motivation": "The paper addresses the challenges of using LLMs in clinical decision support for cancer care, including processing long and multilingual patient records, mitigating clinical hallucination, and improving evaluation metrics.", "method": "CliCARE transforms unstructured longitudinal EHRs into patient-specific Temporal Knowledge Graphs, grounding the decision-making process by aligning patient trajectories with clinical guidelines.", "result": "CliCARE consistently outperforms leading long-context LLMs and Knowledge Graph-enhanced RAG methods in providing clinical decision support across various datasets.", "conclusion": "The framework validates its effectiveness through a robust evaluation protocol that correlates strongly with expert oncologist assessments, indicating high clinical validity.", "key_contributions": ["Development of the CliCARE framework for decision support in oncology using LLMs", "Transformation of EHRs into Temporal Knowledge Graphs for better context handling", "Demonstration of significant performance improvements over existing methods."], "limitations": "", "keywords": ["Large Language Models", "Clinical Decision Support", "Electronic Health Records", "Oncology", "Knowledge Graphs"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2404.13765", "pdf": "https://arxiv.org/pdf/2404.13765.pdf", "abs": "https://arxiv.org/abs/2404.13765", "title": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model", "authors": ["Xingbo Wang", "Samantha L. Huey", "Rui Sheng", "Saurabh Mehta", "Fei Wang"], "categories": ["cs.HC"], "comment": "15 pages, 7 figures. Code is available at\n  https://github.com/xingbow/SciDaEx", "summary": "Extraction and synthesis of structured knowledge from extensive scientific\nliterature are crucial for advancing and disseminating scientific progress.\nAlthough many existing systems facilitate literature review and digest, they\nstruggle to process multimodal, varied, and inconsistent information within and\nacross the literature into structured data. We introduce SciDaSynth, a novel\ninteractive system powered by large language models (LLMs) that enables\nresearchers to efficiently build structured knowledge bases from scientific\nliterature at scale. The system automatically creates data tables to organize\nand summarize users' interested knowledge in literature via question-answering.\nFurthermore, it provides multi-level and multi-faceted exploration of the\ngenerated data tables, facilitating iterative validation, correction, and\nrefinement. Our within-subjects study with researchers demonstrates the\neffectiveness and efficiency of SciDaSynth in constructing quality scientific\nknowledge bases. We further discuss the design implications for human-AI\ninteraction tools for data extraction and structuring.", "AI": {"tldr": "SciDaSynth is a novel interactive system powered by LLMs that enables efficient extraction and structuring of knowledge from scientific literature, facilitating the creation and exploration of data tables for enhanced research.", "motivation": "To advance scientific progress by improving the extraction and synthesis of structured knowledge from extensive scientific literature.", "method": "SciDaSynth automates the creation of data tables from literature through question-answering, allowing for multi-level exploration and iterative refinement.", "result": "A within-subjects study demonstrated the effectiveness and efficiency of SciDaSynth in constructing quality scientific knowledge bases by researchers.", "conclusion": "The system highlights important design implications for human-AI interaction tools focused on data extraction and structuring.", "key_contributions": ["Introduction of SciDaSynth as an interactive system for knowledge extraction.", "Automation of data table generation from scientific literature.", "Facilitation of iterative validation and exploration of structured knowledge."], "limitations": "", "keywords": ["structured knowledge", "human-AI interaction", "literature review", "large language models", "SciDaSynth"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22542", "pdf": "https://arxiv.org/pdf/2507.22542.pdf", "abs": "https://arxiv.org/abs/2507.22542", "title": "A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support", "authors": ["Long S. T. Nguyen", "Truong P. Hua", "Thanh M. Nguyen", "Toan Q. Pham", "Nam K. Ngo", "An X. Nguyen", "Nghi D. M. Pham", "Nghia H. Nguyen", "Tho T. Quan"], "categories": ["cs.CL"], "comment": "Under review at ICCCI 2025", "summary": "With the rapid growth of Artificial Intelligence, Large Language Models\n(LLMs) have become essential for Question Answering (QA) systems, improving\nefficiency and reducing human workload in customer service. The emergence of\nVietnamese LLMs (ViLLMs) highlights lightweight open-source models as a\npractical choice for their accuracy, efficiency, and privacy benefits. However,\ndomain-specific evaluations remain limited, and the absence of benchmark\ndatasets reflecting real customer interactions makes it difficult for\nenterprises to select suitable models for support applications. To address this\ngap, we introduce the Customer Support Conversations Dataset (CSConDa), a\ncurated benchmark of over 9,000 QA pairs drawn from real interactions with\nhuman advisors at a large Vietnamese software company. Covering diverse topics\nsuch as pricing, product availability, and technical troubleshooting, CSConDa\nprovides a representative basis for evaluating ViLLMs in practical scenarios.\nWe further present a comprehensive evaluation framework, benchmarking 11\nlightweight open-source ViLLMs on CSConDa with both automatic metrics and\nsyntactic analysis to reveal model strengths, weaknesses, and linguistic\npatterns. This study offers insights into model behavior, explains performance\ndifferences, and identifies key areas for improvement, supporting the\ndevelopment of next-generation ViLLMs. By establishing a robust benchmark and\nsystematic evaluation, our work enables informed model selection for customer\nservice QA and advances research on Vietnamese LLMs. The dataset is publicly\navailable at\nhttps://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.", "AI": {"tldr": "The paper introduces the Customer Support Conversations Dataset (CSConDa), a benchmark for evaluating Vietnamese LLMs (ViLLMs), and presents an evaluation framework for assessing their performance in customer service applications.", "motivation": "To address the lack of domain-specific evaluations and benchmark datasets for Vietnamese LLMs in customer support, which hinders enterprises' ability to select suitable models.", "method": "The study introduces the CSConDa dataset containing over 9,000 QA pairs from real interactions and evaluates eleven lightweight open-source ViLLMs using both automatic metrics and syntactic analysis.", "result": "The evaluation reveals strengths, weaknesses, and linguistic patterns of the ViLLMs, providing insights into their performance and guiding improvements for future models.", "conclusion": "This work establishes a robust benchmark and evaluation framework that facilitates informed model selection for customer service QA and contributes to the advancement of research on Vietnamese LLMs.", "key_contributions": ["Introduction of the CSConDa dataset with over 9,000 QA pairs from real customer interactions.", "Benchmarking of 11 lightweight ViLLMs on the new dataset using an evaluation framework.", "Insights into model performance and areas for improvement in Vietnamese LLMs."], "limitations": "Domain-specific evaluations are still limited; the dataset may not cover all possible customer interactions.", "keywords": ["Vietnamese LLMs", "Customer Support", "QA Systems", "Benchmark Dataset", "Evaluation Framework"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22545", "pdf": "https://arxiv.org/pdf/2507.22545.pdf", "abs": "https://arxiv.org/abs/2507.22545", "title": "ControlMed: Adding Reasoning Control to Medical Language Model", "authors": ["Sung-Min Lee", "Siyoon Lee", "Juyeon Kim", "Kyungmin Roh"], "categories": ["cs.CL"], "comment": "13 pages", "summary": "Reasoning Large Language Models (LLMs) with enhanced accuracy and\nexplainability are increasingly being adopted in the medical domain, as the\nlife-critical nature of clinical decision-making demands reliable support.\nDespite these advancements, existing reasoning LLMs often generate\nunnecessarily lengthy reasoning processes, leading to significant computational\noverhead and response latency. These limitations hinder their practical\ndeployment in real-world clinical environments. To address these challenges, we\nintroduce \\textbf{ControlMed}, a medical language model that enables users to\nactively control the length of the reasoning process at inference time through\nfine-grained control markers. ControlMed is trained through a three-stage\npipeline: 1) pre-training on a large-scale synthetic medical instruction\ndataset covering both \\textit{direct} and \\textit{reasoning responses}; 2)\nsupervised fine-tuning with multi-length reasoning data and explicit\nlength-control markers; and 3) reinforcement learning with model-based reward\nsignals to enhance factual accuracy and response quality. Experimental results\non a variety of English and Korean medical benchmarks demonstrate that our\nmodel achieves similar or better performance compared to state-of-the-art\nmodels. Furthermore, users can flexibly balance reasoning accuracy and\ncomputational efficiency by controlling the reasoning length as needed. These\nfindings demonstrate that ControlMed is a practical and adaptable solution for\nclinical question answering and medical information analysis.", "AI": {"tldr": "ControlMed introduces a medical language model allowing users to control reasoning length, enhancing efficiency and explainability in clinical decision-making.", "motivation": "To address the limitations of current LLMs in medical settings, specifically lengthy reasoning that impacts efficiency and usability.", "method": "ControlMed is trained through a three-stage pipeline: pre-training on a large synthetic medical instruction dataset, supervised fine-tuning with length-control markers, and reinforcement learning to enhance accuracy and response quality.", "result": "ControlMed demonstrated performance on par with or better than state-of-the-art models across medical benchmarks in English and Korean, allowing users to balance reasoning accuracy with computational efficiency.", "conclusion": "ControlMed is a practical solution for clinical question answering, enhancing adaptability and efficiency in medical information analysis.", "key_contributions": ["Introduction of a length-control mechanism for reasoning in LLMs", "Demonstration of superior or comparable performance to existing models", "Methodology that includes reinforcement learning for accuracy and efficiency"], "limitations": "", "keywords": ["Large Language Models", "medical language model", "clinical decision-making", "reasoning control", "reinforcement learning"], "importance_score": 9, "read_time_minutes": 13}}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257.pdf", "abs": "https://arxiv.org/abs/2504.11257", "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://microsoft.github.io/FIVE-UI-Evol/ .", "AI": {"tldr": "The paper introduces a new data synthesis pipeline, UI-E2I-Synth, for generating instruction datasets for GUI agent training and proposes a benchmark, UI-I2E-Bench, to improve GUI instruction grounding performance.", "motivation": "Advancements in Large Vision-Language Models drive the creation of GUI agents with human-like perception, but challenges in instruction grounding hinder their effectiveness due to limited training datasets and manual annotation requirements.", "method": "The paper presents a large-scale data synthesis pipeline that utilizes GPT-4o to generate diverse instruction datasets without human intervention, and introduces a new benchmark for evaluating GUI instruction grounding.", "result": "The model trained on synthesized data outperforms current state-of-the-art approaches in GUI instruction grounding, showcasing the effectiveness of the proposed data synthesis pipeline and benchmark.", "conclusion": "The new data synthesis pipeline and benchmark provide valuable resources for advancing research in GUI grounding and improve the applicability of vision-based approaches in GUI agents.", "key_contributions": ["Introduction of UI-E2I-Synth for large-scale dataset generation.", "Creation of UI-I2E-Bench benchmark for improved evaluation of GUI instruction grounding.", "Insights into challenges and solutions in GUI instruction grounding."], "limitations": "", "keywords": ["GUI agents", "vision-based approaches", "data synthesis", "instruction grounding", "benchmark"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22564", "pdf": "https://arxiv.org/pdf/2507.22564.pdf", "abs": "https://arxiv.org/abs/2507.22564", "title": "Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs", "authors": ["Xikang Yang", "Biyu Zhou", "Xuehai Tang", "Jizhong Han", "Songlin Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate impressive capabilities across a\nwide range of tasks, yet their safety mechanisms remain susceptible to\nadversarial attacks that exploit cognitive biases -- systematic deviations from\nrational judgment. Unlike prior jailbreaking approaches focused on prompt\nengineering or algorithmic manipulation, this work highlights the overlooked\npower of multi-bias interactions in undermining LLM safeguards. We propose\nCognitiveAttack, a novel red-teaming framework that systematically leverages\nboth individual and combined cognitive biases. By integrating supervised\nfine-tuning and reinforcement learning, CognitiveAttack generates prompts that\nembed optimized bias combinations, effectively bypassing safety protocols while\nmaintaining high attack success rates. Experimental results reveal significant\nvulnerabilities across 30 diverse LLMs, particularly in open-source models.\nCognitiveAttack achieves a substantially higher attack success rate compared to\nthe SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations\nin current defense mechanisms. These findings highlight multi-bias interactions\nas a powerful yet underexplored attack vector. This work introduces a novel\ninterdisciplinary perspective by bridging cognitive science and LLM safety,\npaving the way for more robust and human-aligned AI systems.", "AI": {"tldr": "This paper introduces CognitiveAttack, a framework exposing vulnerabilities in Large Language Models by exploiting cognitive biases through innovative prompt strategies.", "motivation": "To address the vulnerabilities of LLMs to adversarial attacks that exploit cognitive biases, enhancing the safety of AI systems.", "method": "CognitiveAttack combines supervised fine-tuning and reinforcement learning to generate optimized prompts that leverage individual and combined cognitive biases.", "result": "CognitiveAttack significantly outperforms the current state-of-the-art method (60.1% vs. 31.6% attack success rate), identifying major vulnerabilities in 30 LLMs, especially in open-source variants.", "conclusion": "The findings emphasize the need for improved LLM defense mechanisms and introduce cognitive science as a critical discipline for enhancing AI safety.", "key_contributions": ["Introduction of the CognitiveAttack framework", "Demonstration of multi-bias interactions as a viable attack vector", "Significant improvement in attack success rates compared to existing methods"], "limitations": "", "keywords": ["Large Language Models", "Cognitive biases", "Adversarial attacks", "AI safety", "Cognitive science"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22581", "pdf": "https://arxiv.org/pdf/2507.22581.pdf", "abs": "https://arxiv.org/abs/2507.22581", "title": "Unveiling the Influence of Amplifying Language-Specific Neurons", "authors": ["Inaya Rahmanisa", "Lyzander Marciano Andrylie", "Krisna Mahardika Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "categories": ["cs.CL", "cs.LG"], "comment": "Our code and dataset are made available at\n  https://github.com/tauimbz/lang-task-neuron", "summary": "Language-specific neurons in LLMs that strongly correlate with individual\nlanguages have been shown to influence model behavior by deactivating them.\nHowever, their role in amplification remains underexplored. This work\ninvestigates the effect of amplifying language-specific neurons through\ninterventions across 18 languages, including low-resource ones, using three\nmodels primarily trained in different languages. We compare amplification\nfactors by their effectiveness in steering to the target language using a\nproposed Language Steering Shift (LSS) evaluation score, then evaluate it on\ndownstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge\n(Include), and translation (FLORES). The optimal amplification factors\neffectively steer output toward nearly all tested languages. Intervention using\nthis factor on downstream tasks improves self-language performance in some\ncases but generally degrades cross-language results. These findings highlight\nthe effect of language-specific neurons in multilingual behavior, where\namplification can be beneficial especially for low-resource languages, but\nprovides limited advantage for cross-lingual transfer.", "AI": {"tldr": "This paper studies the amplification of language-specific neurons in LLMs across 18 languages, exploring its effects on model behavior and downstream tasks.", "motivation": "To understand the role of language-specific neurons in LLMs and their potential for improving performance across various languages, particularly in low-resource settings.", "method": "The study investigates amplification factors by applying interventions to 18 languages, comparing effectiveness using the Language Steering Shift (LSS) score, and evaluating results on commonsense reasoning, knowledge, and translation tasks.", "result": "The optimal amplification factors successfully direct model outputs towards nearly all tested languages, enhancing self-language performance in some instances but generally degrading cross-language results.", "conclusion": "Amplifying language-specific neurons can benefit multilingual behavior, especially for low-resource languages, though it offers limited advantages for cross-lingual transfer tasks.", "key_contributions": ["Investigates the role of language-specific neurons in LLMs.", "Introduces Language Steering Shift (LSS) evaluation score.", "Analyzes amplification effects across multiple languages and tasks."], "limitations": "General degradation of cross-language results despite some improvements in self-language performance.", "keywords": ["Language-specific neurons", "Language Steering Shift", "multilingual behavior"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2507.22603", "pdf": "https://arxiv.org/pdf/2507.22603.pdf", "abs": "https://arxiv.org/abs/2507.22603", "title": "BALSAM: A Platform for Benchmarking Arabic Large Language Models", "authors": ["Rawan Al-Matham", "Kareem Darwish", "Raghad Al-Rasheed", "Waad Alshammari", "Muneera Alhoshan", "Amal Almazrua", "Asma Al Wazrah", "Mais Alheraki", "Firoj Alam", "Preslav Nakov", "Norah Alzahrani", "Eman alBilali", "Nizar Habash", "Abdelrahman El-Sheikh", "Muhammad Elmallah", "Haonan Li", "Hamdy Mubarak", "Mohamed Anwar", "Zaid Alyafeai", "Ahmed Abdelali", "Nora Altwairesh", "Maram Hasanain", "Abdulmohsen Al Thubaity", "Shady Shehata", "Bashar Alhafni", "Injy Hamed", "Go Inoue", "Khalid Elmadani", "Ossama Obeid", "Fatima Haouari", "Tamer Elsayed", "Emad Alghamdi", "Khalid Almubarak", "Saied Alshahrani", "Ola Aljarrah", "Safa Alajlan", "Areej Alshaqarawi", "Maryam Alshihri", "Sultana Alghurabi", "Atikah Alzeghayer", "Afrah Altamimi", "Abdullah Alfaifi", "Abdulrahman AlOsaimy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The impressive advancement of Large Language Models (LLMs) in English has not\nbeen matched across all languages. In particular, LLM performance in Arabic\nlags behind, due to data scarcity, linguistic diversity of Arabic and its\ndialects, morphological complexity, etc. Progress is further hindered by the\nquality of Arabic benchmarks, which typically rely on static, publicly\navailable data, lack comprehensive task coverage, or do not provide dedicated\nplatforms with blind test sets. This makes it challenging to measure actual\nprogress and to mitigate data contamination. Here, we aim to bridge these gaps.\nIn particular, we introduce BALSAM, a comprehensive, community-driven benchmark\naimed at advancing Arabic LLM development and evaluation. It includes 78 NLP\ntasks from 14 broad categories, with 52K examples divided into 37K test and 15K\ndevelopment, and a centralized, transparent platform for blind evaluation. We\nenvision BALSAM as a unifying platform that sets standards and promotes\ncollaborative research to advance Arabic LLM capabilities.", "AI": {"tldr": "BALSAM is a comprehensive benchmark to enhance the development and evaluation of Arabic LLMs, addressing existing gaps due to data scarcity and poor benchmarks.", "motivation": "To improve the performance of Large Language Models (LLMs) in Arabic by providing a standard benchmark that overcomes data scarcity and evaluation challenges.", "method": "Introduction of BALSAM, which includes 78 NLP tasks from 14 categories, populated with 52K examples for testing and development, along with a platform for blind evaluation.", "result": "BALSAM offers a unified benchmarking tool that allows for reliable performance measurement across various Arabic NLP tasks, promoting collaboration and standardization in this field.", "conclusion": "BALSAM is positioned as a crucial resource to drive forward Arabic LLM research and development by setting standards and providing community support.", "key_contributions": ["Introduces a comprehensive benchmark for Arabic LLMs.", "Includes a wide range of NLP tasks across diverse categories.", "Establishes a transparent platform for blind evaluation."], "limitations": "", "keywords": ["Large Language Models", "Arabic NLP", "benchmarking", "BALSAM", "collaborative research"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2507.22608", "pdf": "https://arxiv.org/pdf/2507.22608.pdf", "abs": "https://arxiv.org/abs/2507.22608", "title": "Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Yusser Al Ghussin", "Tanja Baeumel", "Josef van Genabith", "Simon Ostermann"], "categories": ["cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) exhibit strong multilingual abilities, yet the\nneural mechanisms behind language-specific processing remain unclear. We\nanalyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and\nAya-Expanse-8B & 32B across 21 typologically diverse languages, identifying\nneurons that control language behavior. Using the Language Activation\nProbability Entropy (LAPE) method, we show that these neurons cluster in deeper\nlayers, with non-Latin scripts showing greater specialization. Related\nlanguages share overlapping neurons, reflecting internal representations of\nlinguistic proximity.\n  Through language arithmetics, i.e. systematic activation addition and\nmultiplication, we steer models to deactivate unwanted languages and activate\ndesired ones, outperforming simpler replacement approaches. These interventions\neffectively guide behavior across five multilingual tasks: language forcing,\ntranslation, QA, comprehension, and NLI. Manipulation is more successful for\nhigh-resource languages, while typological similarity improves effectiveness.\nWe also demonstrate that cross-lingual neuron steering enhances downstream\nperformance and reveal internal \"fallback\" mechanisms for language selection\nwhen neurons are progressively deactivated. Our code is made publicly available\nat https://github.com/d-gurgurov/Language-Neurons-Manipulation.", "AI": {"tldr": "This paper investigates language-specific neurons in multilingual large language models (LLMs) and how they can be manipulated to improve language processing tasks.", "motivation": "To understand the neural mechanisms behind language-specific processing and enhance the performance of multilingual models.", "method": "The study analyzes language-specific neurons using the Language Activation Probability Entropy (LAPE) method across various LLMs and languages, employing language arithmetics for manipulation tasks.", "result": "The researchers found that neurons controlling language behavior cluster in deeper layers and that manipulation through addition and multiplication of neuron activations leads to improved task performance in multilingual contexts.", "conclusion": "The findings suggest that understanding and manipulating these neurons can enhance LLMsâ effectiveness in multilingual tasks, with implications for practical applications in language processing.", "key_contributions": ["Identification of language-specific neurons in multilingual LLMs.", "Development of a new manipulation method using language arithmetics.", "Demonstration of improved performance across multilingual tasks."], "limitations": "Manipulation is more effective for high-resource languages and may not be as effective for low-resource languages.", "keywords": ["Language Models", "Neural Mechanisms", "Multilingual Processing", "Neuron Manipulation", "HCI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22623", "pdf": "https://arxiv.org/pdf/2507.22623.pdf", "abs": "https://arxiv.org/abs/2507.22623", "title": "Multilingual Political Views of Large Language Models: Identification and Steering", "authors": ["Daniil Gurgurov", "Katharina Trinley", "Ivan Vykopal", "Josef van Genabith", "Simon Ostermann", "Roberto Zamparelli"], "categories": ["cs.CL"], "comment": "pre-print", "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.", "AI": {"tldr": "This study investigates political biases in large language models (LLMs) and their manipulability across multiple languages.", "motivation": "To address gaps in understanding the political biases of LLMs and their generalizability across architectures, scales, and languages.", "method": "A large-scale evaluation of seven open-source instruction-tuned LLMs, analyzing their political orientation using the Political Compass Test across 14 languages with multiple paraphrases for robust measurement.", "result": "Larger models tend to shift towards libertarian-left positions, with significant variations in bias across different languages and model families.", "conclusion": "Political biases in LLMs are prevalent and can be actively influenced through specific interventions, allowing for manipulation of model responses.", "key_contributions": ["Evaluation of political biases across a wide variety of models and languages", "Identification of consistent ideological shifts in larger LLMs", "Demonstration of manipulability of political stances using intervention techniques."], "limitations": "Limited to a specific set of models and may not represent all LLMs; the study focuses on instruction-tuned models only.", "keywords": ["large language models", "political bias", "manipulability", "multilingual", "political compass"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.03044", "pdf": "https://arxiv.org/pdf/2503.03044.pdf", "abs": "https://arxiv.org/abs/2503.03044", "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing", "authors": ["Gabriele Sarti", "VilÃ©m Zouhar", "Grzegorz ChrupaÅa", "Ana Guerberof-Arenas", "Malvina Nissim", "Arianna Bisazza"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by TACL (pre-MIT Press publication version); Code:\n  https://github.com/gsarti/qe4pe. Dataset:\n  https://huggingface.co/datasets/gsarti/qe4pe", "summary": "Word-level quality estimation (QE) methods aim to detect erroneous spans in\nmachine translations, which can direct and facilitate human post-editing. While\nthe accuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. In this study, we investigate the\nimpact of word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated from behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows.", "AI": {"tldr": "This study examines the usability and effectiveness of word-level quality estimation (QE) methods in machine translation post-editing, involving 42 professional editors and comparing different error-highlight modalities.", "motivation": "To investigate the understudied effects of word-level QE on the efficiency and decisions of human post-editors in machine translation.", "method": "The study evaluates four error-span highlight modalities using behavioral logs of 42 post-editors across two translation directions, comparing supervised and uncertainty-based QE methods.", "result": "The research identifies that domain, language, and editor speed significantly impact the effectiveness of highlight modalities; it found minimal differences in performance between human-generated and automated QE highlights.", "conclusion": "There exists a notable gap between the accuracy of QE systems and their usability in professional translation workflows, indicating the need for improvements in QE methodologies to enhance user experience.", "key_contributions": ["Explores the impact of QE on post-editing productivity and quality.", "Compares various modalities of error-highlight methods in practical settings.", "Demonstrates critical factors affecting highlight effectiveness among human post-editors."], "limitations": "The study is based on a specific set of professional post-editors and languages, which may not generalize to all contexts.", "keywords": ["Quality Estimation", "Machine Translation", "Post-editing", "Human-Computer Interaction", "Usability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.22676", "pdf": "https://arxiv.org/pdf/2507.22676.pdf", "abs": "https://arxiv.org/abs/2507.22676", "title": "Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment", "authors": ["Jia Li", "Yang Wang", "Wenhao Qian", "Zhenzhen Hu", "Richang Hong", "Meng Wang"], "categories": ["cs.CL", "cs.MM"], "comment": "8 pages, 4 figures, ACM MM 2025.\n  github:https://github.com/MSA-LMC/365Aspects", "summary": "Interview performance assessment is essential for determining candidates'\nsuitability for professional positions. To ensure holistic and fair\nevaluations, we propose a novel and comprehensive framework that explores\n``365'' aspects of interview performance by integrating \\textit{three}\nmodalities (video, audio, and text), \\textit{six} responses per candidate, and\n\\textit{five} key evaluation dimensions. The framework employs\nmodality-specific feature extractors to encode heterogeneous data streams and\nsubsequently fused via a Shared Compression Multilayer Perceptron. This module\ncompresses multimodal embeddings into a unified latent space, facilitating\nefficient feature interaction. To enhance prediction robustness, we incorporate\na two-level ensemble learning strategy: (1) independent regression heads\npredict scores for each response, and (2) predictions are aggregated across\nresponses using a mean-pooling mechanism to produce final scores for the five\ntarget dimensions. By listening to the unspoken, our approach captures both\nexplicit and implicit cues from multimodal data, enabling comprehensive and\nunbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our\nframework secured first place in the AVI Challenge 2025, demonstrating its\neffectiveness and robustness in advancing automated and multimodal interview\nperformance assessment. The full implementation is available at\nhttps://github.com/MSA-LMC/365Aspects.", "AI": {"tldr": "Proposes a framework for assessing interview performance through video, audio, and text modalities, employing a multilayer perceptron for feature fusion and an ensemble learning strategy for predictions.", "motivation": "To ensure holistic and fair evaluations of candidates' interview performances across multiple dimensions.", "method": "Integrates video, audio, and text data through modality-specific feature extractors and fuses them using a Shared Compression Multilayer Perceptron, employing ensemble learning for robust scoring.", "result": "Achieved a multi-dimensional average MSE of 0.1824, winning first place in the AVI Challenge 2025.", "conclusion": "The proposed framework effectively captures both explicit and implicit cues from multimodal data, providing comprehensive and unbiased assessments of interview performance.", "key_contributions": ["Novel framework integrating multiple modalities for interview assessment", "Use of ensemble learning for robust score predictions", "First place in AVI Challenge 2025 validating efficacy"], "limitations": "", "keywords": ["interview assessment", "multimodal data", "machine learning", "ensemble learning", "feature extraction"], "importance_score": 7, "read_time_minutes": 8}}
{"id": "2504.05008", "pdf": "https://arxiv.org/pdf/2504.05008.pdf", "abs": "https://arxiv.org/abs/2504.05008", "title": "Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergei Tilga", "Ekaterina Artemova"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.", "AI": {"tldr": "This paper explores the adoption of LLMs among professional writers, highlighting key insights into ethical concerns, language support, and the impact on writing practices.", "motivation": "To understand the underexplored aspects of LLM adoption in professional writing, including language support, ethics, and effects on creativity.", "method": "Conducted a questionnaire with 301 respondents and an interactive survey with 36 participants focusing on professional writers who use AI tools.", "result": "Identified significant insights regarding LLM adoption for non-English speakers, the extent of misinformation, and the need for usability improvements in LLM features.", "conclusion": "The findings suggest a need for further development of LLMs to enhance writing practices and address the concerns of diverse language users.", "key_contributions": ["Insights on LLM usability for non-English speakers", "Understanding of ethical concerns in LLM adoption", "Recommendations for improving LLM features based on user feedback"], "limitations": "", "keywords": ["large language models", "professional writing", "ethical concerns", "user expectations", "language support"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2507.22716", "pdf": "https://arxiv.org/pdf/2507.22716.pdf", "abs": "https://arxiv.org/abs/2507.22716", "title": "From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs", "authors": ["Jie He", "Victor Gutierrez Basulto", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Reinforcement learning-based retrieval-augmented generation (RAG) methods\nenhance the reasoning abilities of large language models (LLMs). However, most\nrely only on final-answer rewards, overlooking intermediate reasoning quality.\nThis paper analyzes existing RAG reasoning models and identifies three main\nfailure patterns: (1) information insufficiency, meaning the model fails to\nretrieve adequate support; (2) faulty reasoning, where logical or content-level\nflaws appear despite sufficient information; and (3) answer-reasoning\ninconsistency, where a valid reasoning chain leads to a mismatched final\nanswer. We propose TIRESRAG-R1, a novel framework using a\nthink-retrieve-reflect process and a multi-dimensional reward system to improve\nreasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to\nencourage thorough retrieval; (2) a reasoning quality reward to assess the\nrationality and accuracy of the reasoning chain; and (3) a reflection reward to\ndetect and revise errors. It also employs a difficulty-aware reweighting\nstrategy and training sample filtering to boost performance on complex tasks.\nExperiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms\nprior RAG methods and generalizes well to single-hop tasks. The code and data\nare available at: https://github.com/probe2/TIRESRAG-R1.", "AI": {"tldr": "This paper introduces TIRESRAG-R1, a novel reinforcement learning framework for improving the reasoning abilities of retrieval-augmented generation (RAG) methods in large language models (LLMs).", "motivation": "Current RAG methods primarily use final-answer rewards, neglecting the importance of intermediate reasoning quality which can lead to various failure patterns in reasoning.", "method": "TIRESRAG-R1 employs a think-retrieve-reflect process coupled with a multi-dimensional reward system that includes sufficiency, reasoning quality, and reflection rewards, addressing key reasoning failures.", "result": "Experiments show that TIRESRAG-R1 outperforms previous RAG methods on multi-hop question answering datasets and demonstrates good generalization to single-hop tasks.", "conclusion": "The TIRESRAG-R1 framework enhances the reasoning capabilities of LLMs by providing a structured approach to both retrieve information and assess reasoning quality, thereby improving model performance.", "key_contributions": ["Introduction of a sufficiency reward for improved information retrieval.", "Development of a reasoning quality reward to enhance logical accuracy.", "Implementation of a reflection reward for error detection and correction."], "limitations": "", "keywords": ["Reinforcement Learning", "Retrieval-Augmented Generation", "Large Language Models", "Reasoning Quality", "Multi-hop QA"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22720", "pdf": "https://arxiv.org/pdf/2507.22720.pdf", "abs": "https://arxiv.org/abs/2507.22720", "title": "Investigating Hallucination in Conversations for Low Resource Languages", "authors": ["Amit Das", "Md. Najib Hasan", "Souvika Sarkar", "Zheng Zhang", "Fatemeh Jamshidi", "Tathagata Bhattacharya", "Nilanjana Raychawdhury", "Dongji Feng", "Vinija Jain", "Aman Chadha"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.", "AI": {"tldr": "This study investigates the hallucination phenomenon in Large Language Models (LLMs) across Hindi, Farsi, and Mandarin, revealing significant variations in error rates among these languages.", "motivation": "To enhance the reliability and effectiveness of LLMs by addressing the issue of factual inaccuracies, commonly known as hallucination.", "method": "A comprehensive analysis of a dataset focused on conversational data in Hindi, Farsi, and Mandarin, assessing factual and linguistic errors in multiple LLMs including GPT-3.5 and GPT-4o.", "result": "The study found that LLMs generate very few hallucinated responses in Mandarin, but a significantly higher number of hallucinations in Hindi and Farsi compared to other languages.", "conclusion": "Addressing the hallucination issue in LLMs requires attention to language-specific error rates, with a particular need for improvements in Hindi and Farsi applications.", "key_contributions": ["Comprehensive analysis of hallucination in multilingual LLMs", "Comparison of hallucination rates across Hindi, Farsi, and Mandarin", "Insights into the performance of various LLMs on conversational data"], "limitations": "", "keywords": ["Large Language Models", "Hallucination", "Conversational Data", "Multilingual Analysis", "Factual Errors"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2507.22729", "pdf": "https://arxiv.org/pdf/2507.22729.pdf", "abs": "https://arxiv.org/abs/2507.22729", "title": "Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning", "authors": ["Benedikt Roth", "Stephan Rappensperger", "Tianming Qiu", "Hamza ImamoviÄ", "Julian WÃ¶rmann", "Hao Shen"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become a cornerstone in Natural Language\nProcessing (NLP), achieving impressive performance in text generation. Their\ntoken-level representations capture rich, human-aligned semantics. However,\npooling these vectors into a text embedding discards crucial information.\nNevertheless, many non-generative downstream tasks, such as clustering,\nclassification, or retrieval, still depend on accurate and controllable\nsentence- or document-level embeddings. We explore several adaptation\nstrategies for pre-trained, decoder-only LLMs: (i) various aggregation\ntechniques for token embeddings, (ii) task-specific prompt engineering, and\n(iii) text-level augmentation via contrastive fine-tuning. Combining these\ncomponents yields state-of-the-art performance on the English clustering track\nof the Massive Text Embedding Benchmark (MTEB). An analysis of the attention\nmap further shows that fine-tuning shifts focus from prompt tokens to\nsemantically relevant words, indicating more effective compression of meaning\ninto the final hidden state. Our experiments demonstrate that LLMs can be\neffectively adapted as text embedding models through a combination of prompt\nengineering and resource-efficient contrastive fine-tuning on synthetically\ngenerated positive pairs.", "AI": {"tldr": "This paper explores adaptation strategies for pre-trained decoder-only Large Language Models (LLMs) to improve text embeddings for downstream tasks, achieving state-of-the-art results in clustering.", "motivation": "To enhance the usability of LLMs for text embeddings in various tasks, while retaining crucial semantic information lost during pooling.", "method": "The paper examines different approaches: token aggregation techniques, task-specific prompt engineering, and contrastive fine-tuning for text-level augmentation.", "result": "The combined strategies resulted in state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark.", "conclusion": "LLMs can be adapted effectively for text embedding tasks through innovative prompt engineering and fine-tuning techniques, leading to better semantic representation.", "key_contributions": ["Exploration of token aggregation techniques for LLMs", "Introduction of task-specific prompt engineering", "Demonstration of effective contrastive fine-tuning for text embeddings"], "limitations": "", "keywords": ["Large Language Models", "text embeddings", "prompt engineering", "contrastive fine-tuning", "NLP"], "importance_score": 9, "read_time_minutes": 10}}
{"id": "2507.22744", "pdf": "https://arxiv.org/pdf/2507.22744.pdf", "abs": "https://arxiv.org/abs/2507.22744", "title": "Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index", "authors": ["Praveenkumar Katwe", "Rakesh Chandra", "Balabantaray Kali", "Prasad Vittala"], "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "8", "summary": "Reducing hallucinations in abstractive summarization remains a critical\nchallenge for deploying language models (LMs) in real-world settings. In this\nwork, we introduce a rewarddriven fine-tuning framework that explicitly\noptimizes for Entity Hallucination Index (EHI), a metric designed to quantify\nthe presence, correctness, and grounding of named entities in generated\nsummaries. Given a corpus of meeting transcripts, we first generate baseline\nsummaries using a pre-trained LM and compute EHI scores via automatic entity\nextraction and matching. We then apply reinforcement learning to fine-tune the\nmodel parameters, using EHI as a reward signal to bias generation toward\nentity-faithful outputs. Our approach does not rely on human-written factuality\nannotations, enabling scalable fine-tuning. Experiments demonstrate consistent\nimprovements in EHI across datasets, with qualitative analysis revealing a\nsignificant reduction in entity-level hallucinations without degradation in\nfluency or informativeness. We release a reproducible Colab pipeline,\nfacilitating further research on hallucination-aware model fine-tuning using\nlightweight, hallucintion metrics like EHI.", "AI": {"tldr": "This paper presents a reward-driven framework to reduce entity hallucinations in abstractive summarization using reinforcement learning and a new metric called Entity Hallucination Index (EHI).", "motivation": "To address the critical challenge of hallucinations in language model-generated summaries, particularly in real-world applications.", "method": "A reward-driven fine-tuning framework is introduced that optimizes for EHI, which quantifies the presence, correctness, and grounding of named entities in summaries. The method involves baseline summary generation from meeting transcripts, automatic EHI score computation, and reinforcement learning for model fine-tuning.", "result": "Experiments show consistent improvements in EHI across multiple datasets, indicating a reduction in hallucinations without sacrificing fluency or informativeness in the generated summaries.", "conclusion": "The proposed framework allows for scalable fine-tuning without reliance on human annotations, and it significantly lowers the occurrence of entity-level hallucinations while maintaining output quality.", "key_contributions": ["Introduction of the Entity Hallucination Index (EHI) metric for summarization", "Development of a reward-driven fine-tuning framework using reinforcement learning", "Release of a reproducible Colab pipeline for further research on hallucination reduction"], "limitations": "", "keywords": ["abstractive summarization", "language models", "entity hallucination", "reinforcement learning", "EHI"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22752", "pdf": "https://arxiv.org/pdf/2507.22752.pdf", "abs": "https://arxiv.org/abs/2507.22752", "title": "CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset", "authors": ["JindÅich LibovickÃ½", "JindÅich Helcl", "Andrei Manea", "Gianluca Vico"], "categories": ["cs.CL"], "comment": null, "summary": "We introduce a benchmark for open-ended regional question answering that\nencompasses both textual and visual modalities. We also provide strong\nbaselines using state-of-the-art large language models (LLMs). Our dataset\nconsists of manually curated questions and answers grounded in Wikipedia,\ncreated by native speakers from Czechia, Slovakia, and Ukraine, with\naccompanying English translations. It includes both purely textual questions\nand those requiring visual understanding. As a baseline, we evaluate\nstate-of-the-art LLMs through prompting and complement this with human\njudgments of answer correctness. Using these human evaluations, we analyze the\nreliability of existing automatic evaluation metrics. Our baseline results\nhighlight a significant gap in regional knowledge among current LLMs. Moreover,\napart from LLM-based evaluation, there is minimal correlation between automated\nmetrics and human judgment. We release this dataset as a resource to (1) assess\nregional knowledge in LLMs, (2) study cross-lingual generation consistency in a\nchallenging setting, and (3) advance the development of evaluation metrics for\nopen-ended question answering.", "AI": {"tldr": "A benchmark for open-ended regional question answering using both textual and visual modalities is introduced, alongside strong baselines with LLMs.", "motivation": "To evaluate regional knowledge in large language models and improve question answering capabilities across languages and modalities.", "method": "A dataset of curated questions and answers, both textual and visual, grounded in Wikipedia, was created and evaluated using state-of-the-art LLMs and human judgments.", "result": "Baselines show a significant gap in regional knowledge among LLMs, and low correlation between automated metrics and human evaluations was found.", "conclusion": "The dataset serves to assess LLM performance, explore cross-lingual generation, and push for better evaluation metrics in question answering.", "key_contributions": ["Introduction of a benchmark for regional question answering", "Evaluation of LLMs with human judgments", "Analysis of automated metrics versus human evaluations"], "limitations": "Focus primarily on specific regions and language pairs may limit generalizability.", "keywords": ["regional question answering", "large language models", "multimodal evaluation"], "importance_score": 8, "read_time_minutes": 7}}
{"id": "2507.22753", "pdf": "https://arxiv.org/pdf/2507.22753.pdf", "abs": "https://arxiv.org/abs/2507.22753", "title": "Opportunities and Challenges of LLMs in Education: An NLP Perspective", "authors": ["Sowmya Vajjala", "Bashar Alhafni", "Stefano BannÃ²", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Interest in the role of large language models (LLMs) in education is\nincreasing, considering the new opportunities they offer for teaching,\nlearning, and assessment. In this paper, we examine the impact of LLMs on\neducational NLP in the context of two main application scenarios: {\\em\nassistance} and {\\em assessment}, grounding them along the four dimensions --\nreading, writing, speaking, and tutoring. We then present the new directions\nenabled by LLMs, and the key challenges to address. We envision that this\nholistic overview would be useful for NLP researchers and practitioners\ninterested in exploring the role of LLMs in developing language-focused and\nNLP-enabled educational applications of the future.", "AI": {"tldr": "The paper examines the impact of large language models (LLMs) on educational NLP, focusing on assistance and assessment across four dimensions: reading, writing, speaking, and tutoring.", "motivation": "There is a growing interest in leveraging LLMs in education to enhance teaching, learning, and assessment.", "method": "The paper reviews the impact of LLMs in educational NLP and discusses their applications in two main scenarios: assistance and assessment, organized along four key dimensions.", "result": "The paper identifies new directions for LLM applications in education and outlines key challenges that need addressing.", "conclusion": "A comprehensive overview is provided to help NLP researchers and practitioners explore LLMs for future language-focused educational applications.", "key_contributions": ["Holistic overview of LLMs in education", "Identification of key challenges in educational NLP", "Framework for understanding LLM applications in reading, writing, speaking, and tutoring"], "limitations": "", "keywords": ["Large Language Models", "Educational NLP", "Assistance", "Assessment", "NLP"], "importance_score": 9, "read_time_minutes": 15}}
{"id": "2507.22758", "pdf": "https://arxiv.org/pdf/2507.22758.pdf", "abs": "https://arxiv.org/abs/2507.22758", "title": "MASCA: LLM based-Multi Agents System for Credit Assessment", "authors": ["Gautam Jajoo", "Pranjal A Chitale", "Saksham Agarwal"], "categories": ["cs.CL", "cs.CE", "cs.LG"], "comment": "Accepted at ACL REALM Workshop. Work in Progress", "summary": "Recent advancements in financial problem-solving have leveraged LLMs and\nagent-based systems, with a primary focus on trading and financial modeling.\nHowever, credit assessment remains an underexplored challenge, traditionally\ndependent on rule-based methods and statistical models. In this paper, we\nintroduce MASCA, an LLM-driven multi-agent system designed to enhance credit\nevaluation by mirroring real-world decision-making processes. The framework\nemploys a layered architecture where specialized LLM-based agents\ncollaboratively tackle sub-tasks. Additionally, we integrate contrastive\nlearning for risk and reward assessment to optimize decision-making. We further\npresent a signaling game theory perspective on hierarchical multi-agent\nsystems, offering theoretical insights into their structure and interactions.\nOur paper also includes a detailed bias analysis in credit assessment,\naddressing fairness concerns. Experimental results demonstrate that MASCA\noutperforms baseline approaches, highlighting the effectiveness of hierarchical\nLLM-based multi-agent systems in financial applications, particularly in credit\nscoring.", "AI": {"tldr": "This paper presents MASCA, an LLM-driven multi-agent system designed to improve credit assessment in finance by mimicking decision-making processes and utilizing contrastive learning for optimization.", "motivation": "To address the limitations of traditional rule-based methods and statistical models in credit assessment.", "method": "The framework employs a layered architecture with specialized LLM-based agents that collaboratively address sub-tasks, incorporating contrastive learning for risk and reward optimization.", "result": "Experimental results show that MASCA outperforms baseline approaches in credit scoring, demonstrating the effectiveness of hierarchical LLM-based multi-agent systems in financial applications.", "conclusion": "MASCA enhances credit evaluation by combining LLM capabilities with agent-based strategies, leading to improved decision-making in credit assessment.", "key_contributions": ["Introduction of MASCA, an LLM-driven multi-agent system for credit evaluation.", "Integration of contrastive learning for risk and reward assessment.", "A bias analysis in credit assessment addressing fairness concerns."], "limitations": "The paper is a work in progress and lacks extensive empirical validation beyond initial experimental results.", "keywords": ["credit assessment", "multi-agent systems", "contrastive learning", "LLM", "finance"], "importance_score": 4, "read_time_minutes": 10}}
{"id": "2507.22811", "pdf": "https://arxiv.org/pdf/2507.22811.pdf", "abs": "https://arxiv.org/abs/2507.22811", "title": "DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph", "authors": ["Debayan Banerjee", "Tilahun Abedissa Taffa", "Ricardo Usbeck"], "categories": ["cs.CL"], "comment": null, "summary": "In this work we present an entity linker for DBLP's 2025 version of RDF-based\nKnowledge Graph. Compared to the 2022 version, DBLP now considers publication\nvenues as a new entity type called dblp:Stream. In the earlier version of\nDBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce\nentity linkings. In contrast, in this work, we develop a zero-shot entity\nlinker using LLMs using a novel method, where we re-rank candidate entities\nbased on the log-probabilities of the \"yes\" token output at the penultimate\nlayer of the LLM.", "AI": {"tldr": "Development of a zero-shot entity linker using LLMs for DBLP's RDF-based Knowledge Graph.", "motivation": "To improve entity linking in DBLP's 2025 RDF Knowledge Graph version by integrating publication venues as a new entity type and leveraging LLMs.", "method": "A novel zero-shot entity linking approach is developed that uses LLMs to re-rank candidate entities based on log-probabilities from the penultimate layer.", "result": "The zero-shot linking method demonstrates effective performance in identifying and linking entities within the new DBLP entity structure.", "conclusion": "The proposed LLM-based method enhances the entity linking for DBLP, marking an innovative step forward in the use of LLMs for knowledge graph tasks.", "key_contributions": ["Introduction of a new entity type, dblp:Stream, in DBLP's Knowledge Graph.", "Development of a zero-shot entity linker utilizing LLMs.", "Improvement of entity re-ranking using log-probability outputs from LLMs."], "limitations": "", "keywords": ["entity linking", "LLMs", "Knowledge Graph", "RDF", "DBLP"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22829", "pdf": "https://arxiv.org/pdf/2507.22829.pdf", "abs": "https://arxiv.org/abs/2507.22829", "title": "Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization", "authors": ["Weijia Zhang", "Songgaojun Deng", "Evangelos Kanoulas"], "categories": ["cs.CL"], "comment": "10 pages, 4 figures, and 5 tables", "summary": "Query-focused table summarization requires complex reasoning, often\napproached through step-by-step natural language (NL) plans. However, NL plans\nare inherently ambiguous and lack structure, limiting their conversion into\nexecutable programs like SQL and hindering scalability, especially for\nmulti-table tasks. To address this, we propose a paradigm shift to structured\nrepresentations. We introduce a new structured plan, TaSoF, inspired by\nformalism in traditional multi-agent systems, and a framework, SPaGe, that\nformalizes the reasoning process in three phases: 1) Structured Planning to\ngenerate TaSoF from a query, 2) Graph-based Execution to convert plan steps\ninto SQL and model dependencies via a directed cyclic graph for parallel\nexecution, and 3) Summary Generation to produce query-focused summaries. Our\nmethod explicitly captures complex dependencies and improves reliability.\nExperiments on three public benchmarks show that SPaGe consistently outperforms\nprior models in both single- and multi-table settings, demonstrating the\nadvantages of structured representations for robust and scalable summarization.", "AI": {"tldr": "This paper introduces a new structured plan, TaSoF, and a framework, SPaGe, to enhance query-focused table summarization by formalizing reasoning processes and improving its scalability and reliability.", "motivation": "Traditional natural language plans for table summarization are ambiguous and lack structure, which limits their scalability and execution.", "method": "The proposed method includes three phases: 1) generating TaSoF from a query, 2) converting plan steps into SQL through a directed cyclic graph, and 3) producing query-focused summaries.", "result": "Experiments demonstrate that SPaGe outperforms prior models in both single- and multi-table tasks, confirming the effectiveness of structured representations.", "conclusion": "SPaGe's structured approach leads to more reliable and scalable table summarization.", "key_contributions": ["Introduction of TaSoF as a structured representation for reasoning in table summarization.", "Development of the SPaGe framework that formalizes the table summarization process.", "Demonstration of improved performance in multiple benchmarks over existing models."], "limitations": "", "keywords": ["table summarization", "structured representation", "natural language processing", "query-focused summarization", "SQL"], "importance_score": 6, "read_time_minutes": 10}}
{"id": "2507.22887", "pdf": "https://arxiv.org/pdf/2507.22887.pdf", "abs": "https://arxiv.org/abs/2507.22887", "title": "Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning", "authors": ["Kwesi Cobbina", "Tianyi Zhou"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In-context learning (ICL) is a critical emerging capability of large language\nmodels (LLMs), enabling few-shot learning during inference by including a few\ndemonstrations (demos) in the prompt. However, it has been found that ICL's\nperformance can be sensitive to the choices of demos and their order. This\npaper investigates an unexplored new positional bias of ICL for the first time:\nwe observe that the predictions and accuracy can drift drastically when the\npositions of demos, the system prompt, and the user message in LLM input are\nvaried. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We\ndesign a systematic evaluation pipeline to study this type of positional bias\nacross classification, question answering, summarization, and reasoning tasks.\nWe introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify\nnet gains and output volatility induced by changes in the demos' position.\nExtensive experiments on ten LLMs from four open-source model families (QWEN,\nLLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their\naccuracy and predictions: placing demos at the start of the prompt yields the\nmost stable and accurate outputs with gains of up to +6 points. In contrast,\nplacing demos at the end of the user message flips over 30\\% of predictions\nwithout improving correctness on QA tasks. Smaller models are most affected by\nthis sensitivity, though even large models remain marginally affected on\ncomplex tasks.", "AI": {"tldr": "This paper explores the positional bias in in-context learning (ICL) of large language models (LLMs), demonstrating how the order and position of demos in prompts influence predictions and accuracy.", "motivation": "To investigate the unexplored positional bias of ICL by analyzing how the arrangement of demos affects model performance in various tasks.", "method": "A systematic evaluation pipeline was designed to study the Demos' Position in Prompt (DPP) bias across different tasks, using two newly introduced metrics: ACCURACY-CHANGE and PREDICTION-CHANGE.", "result": "Results from extensive experiments on ten LLMs indicate significant performance variation based on demo positioning, with optimal positioning resulting in up to +6 points in accuracy, and poor positioning leading to over 30% prediction flips.", "conclusion": "The study concludes that demo placement is crucial for achieving stable and accurate outputs in ICL, highlighting the sensitivity of smaller models and the marginal impact on larger models.", "key_contributions": ["Introduction of the Demos' Position in Prompt (DPP) bias", "Development of ACCURACY-CHANGE and PREDICTION-CHANGE metrics", "Comprehensive evaluation across multiple LLM families revealing sensitivity in performance based on demo placement"], "limitations": "", "keywords": ["in-context learning", "large language models", "positional bias", "demo positioning", "classification"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2311.07052", "pdf": "https://arxiv.org/pdf/2311.07052.pdf", "abs": "https://arxiv.org/abs/2311.07052", "title": "Towards the Law of Capacity Gap in Distilling Language Models", "authors": ["Chen Zhang", "Qiuchi Li", "Dawei Song", "Zheyu Ye", "Yan Gao", "Yan Hu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "32 pages, 10 figures, 15 tables, accepted to ACL 2025. Code and\n  checkpoints are available at https://github.com/GeneZC/MiniMA", "summary": "Language model (LM) distillation aims at distilling the knowledge in a large\nteacher LM to a small student one. As a critical issue facing LM distillation,\na superior student often arises from a teacher of a relatively small scale\ninstead of a larger one, especially in the presence of substantial capacity gap\nbetween the teacher and student. This issue, often referred to as the\n\\textit{curse of capacity gap}, suggests that there is likely an optimal\nteacher yielding the best-performing student along the scaling course of the\nteacher. Consequently, distillation trials on teachers of a wide range of\nscales are called for to determine the optimal teacher, which becomes\ncomputationally intensive in the context of large LMs (LLMs). This paper\naddresses this critical bottleneck by providing the \\textit{law of capacity\ngap} inducted from a preliminary study on distilling a broad range of\nsmall-scale (<3B) LMs, where the optimal teacher consistently scales linearly\nwith the student scale across different model and data scales. By extending the\nlaw to LLM distillation on a larger scale (7B), we succeed in obtaining\nversatile LLMs that outperform a wide array of competitors.", "AI": {"tldr": "This paper explores the issue of capacity gap in language model distillation and proposes a law of capacity gap that identifies optimal teacher models for training smaller student models, ultimately achieving versatile LLMs that outperform competitors.", "motivation": "To address the challenge of optimal teacher selection in language model distillation, especially when facing substantial capacity gaps between teacher and student models.", "method": "The authors conduct a study on distilling small-scale language models and derive a law of capacity gap that indicates optimal teacher scaling relative to student size, then extend this law to larger language models.", "result": "The study demonstrates that by following the law of capacity gap, the authors are able to produce large language models that perform better than a variety of existing models.", "conclusion": "Adhering to the law of capacity gap allows for more efficient selection of teacher models in language model distillation, leading to improved performance in student models.", "key_contributions": ["Introduction of the law of capacity gap for language model distillation.", "Empirical demonstration of the optimal teacher scaling for student performance.", "Achievement of state-of-the-art results in LLMs through the application of the law."], "limitations": "The study primarily focuses on small-scale models and may not cover optimizations for all types of language model architectures.", "keywords": ["language model distillation", "capacity gap", "large language models", "teacher-student models", "optimal teacher"], "importance_score": 8, "read_time_minutes": 20}}
{"id": "2408.16440", "pdf": "https://arxiv.org/pdf/2408.16440.pdf", "abs": "https://arxiv.org/abs/2408.16440", "title": "Instruction-tuned Large Language Models for Machine Translation in the Medical Domain", "authors": ["Miguel Rios"], "categories": ["cs.CL"], "comment": "Citation: Miguel Rios. 2025. Instruction-tuned Large Language Models\n  for Machine Translation in the Medical Domain. In Proceedings of Machine\n  Translation Summit XX Volume 1, pages 162-172", "summary": "Large Language Models (LLMs) have shown promising results on machine\ntranslation for high resource language pairs and domains. However, in\nspecialised domains (e.g. medical) LLMs have shown lower performance compared\nto standard neural machine translation models. The consistency in the machine\ntranslation of terminology is crucial for users, researchers, and translators\nin specialised domains. In this study, we compare the performance between\nbaseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we\nintroduce terminology from specialised medical dictionaries into the\ninstruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs\nsignificantly outperform the baseline models with automatic metrics.", "AI": {"tldr": "This study evaluates the performance of instruction-tuned LLMs versus baseline LLMs in medical machine translation, incorporating specialized medical terminology for improved consistency.", "motivation": "The need for consistent machine translation of terminology in specialized domains like medicine due to the lower performance of LLMs compared to standard models.", "method": "Comparison of baseline LLMs and instruction-tuned LLMs using datasets fine-tuned with medical terminology from specialized dictionaries.", "result": "Instruction-tuned LLMs outperform baseline models significantly on automatic metrics in the medical translation tasks.", "conclusion": "Fine-tuning LLMs with specialized terminology leads to better performance in medical machine translation.", "key_contributions": ["Comparison of baseline LLMs with instruction-tuned LLMs in the medical domain.", "Introduction of medical terminology from specialized dictionaries into LLM training.", "Demonstration of improved automatic metric scores for instruction-tuned models."], "limitations": "", "keywords": ["Large Language Models", "Machine Translation", "Medical Terminology", "Instruction-tuning", "Specialized Domains"], "importance_score": 9, "read_time_minutes": 5}}
{"id": "2409.14820", "pdf": "https://arxiv.org/pdf/2409.14820.pdf", "abs": "https://arxiv.org/abs/2409.14820", "title": "Past Meets Present: Creating Historical Analogy with Large Language Models", "authors": ["Nianqi Li", "Siyu Yuan", "Jiangjie Chen", "Jiaqing Liang", "Feng Wei", "Zujie Liang", "Deqing Yang", "Yanghua Xiao"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Outstanding Paper Award)", "summary": "Historical analogies, which compare known past events with contemporary but\nunfamiliar events, are important abilities that help people make decisions and\nunderstand the world. However, research in applied history suggests that people\nhave difficulty finding appropriate analogies. And previous studies in the AI\ncommunity have also overlooked historical analogies. To fill this gap, in this\npaper, we focus on the historical analogy acquisition task, which aims to\nacquire analogous historical events for a given event. We explore retrieval and\ngeneration methods for acquiring historical analogies based on different large\nlanguage models (LLMs). Furthermore, we propose a self-reflection method to\nmitigate hallucinations and stereotypes when LLMs generate historical\nanalogies. Through human evaluations and our specially designed automatic\nmulti-dimensional assessment, we find that LLMs generally have a good potential\nfor historical analogies. And the performance of the models can be further\nimproved by using our self-reflection method.", "AI": {"tldr": "The paper explores using large language models to acquire historical analogies and introduces a self-reflection method to enhance their performance.", "motivation": "People struggle to find appropriate historical analogies, which are crucial for understanding contemporary events. This paper addresses the gap in research on acquiring historical analogies using AI.", "method": "The authors examine both retrieval and generation approaches using large language models (LLMs) to acquire historical analogies. They also introduce a self-reflection method to address issues like hallucinations and stereotypes in LLM outputs.", "result": "Human evaluations and a novel automatic multi-dimensional assessment demonstrate that LLMs can effectively generate historical analogies, with improved performance when using the proposed self-reflection method.", "conclusion": "The study shows that LLMs possess significant potential for generating historical analogies and that their performance can be enhanced through self-reflection techniques.", "key_contributions": ["Introduction of a self-reflection method for LLMs to improve historical analogy generation", "Evaluation of LLMs in the context of historical analogy acquisition", "Identification of challenges in generating reliable historical analogies from LLMs"], "limitations": "The study may depend on the limitations of the data sources used to train the LLMs and may not generalize across all types of historical events.", "keywords": ["historical analogies", "large language models", "self-reflection", "AI", "decision making"], "importance_score": 7, "read_time_minutes": 15}}
{"id": "2410.02744", "pdf": "https://arxiv.org/pdf/2410.02744.pdf", "abs": "https://arxiv.org/abs/2410.02744", "title": "Neutral Residues: Revisiting Adapters for Model Extension", "authors": ["Franck Signe Talla", "Edouard Grave", "HervÃ© JÃ©gou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "We address the problem of extending a pretrained large language model to a\nnew domain that was not seen during training. Standard techniques, such as\nfinetuning or low-rank adaptation (LoRA) are successful at domain adaptation,\nbut do not formally add capacity to the model. This often leads to a trade-off,\nbetween performing well on the new domain vs. degrading performance on the\noriginal domain. Here, we revisit and improve adapters to extend LLMs from\nthree angles: data, architecture and training procedure, which are\nadvantageously considered jointly. The resulting method, called neutral\nresidues, modifies adapters in a way that leads each new residual block to\noutput near-zeros on the original domain. This solution leads to strong results\nwhen adapting a state-of-the-art model originally trained on English to a new\nlanguage. Neutral residues significantly outperform competing approaches such\nas finetuning, LoRA or vanilla adapters in terms of the trade-off between\nlearning the new language and not forgetting English.", "AI": {"tldr": "This paper introduces a method called neutral residues that improves the adaptation of pretrained large language models (LLMs) to new domains without degrading performance on the original domain.", "motivation": "To address the challenge of adapting LLMs to new domains without formal capacity addition, which often compromises original performance.", "method": "The authors improve adapters through a joint consideration of data, architecture, and training procedures, implementing a method called neutral residues that outputs near-zeros on the original domain.", "result": "Neutral residues achieve significantly better results in adapting a model to a new language while maintaining performance in English compared to finetuning, LoRA, and vanilla adapters.", "conclusion": "The proposed method effectively balances the trade-off in domain adaptation, enhancing the model's ability to learn new languages while preserving capability in the original domain.", "key_contributions": ["Introduction of neutral residues improving adapter mechanisms", "Joint optimization of data, architecture, and training for better domain adaptation", "Demonstrated superiority over traditional adaptation methods like finetuning and LoRA."], "limitations": "", "keywords": ["large language models", "domain adaptation", "neutral residues"], "importance_score": 9, "read_time_minutes": 8}}
{"id": "2410.21306", "pdf": "https://arxiv.org/pdf/2410.21306.pdf", "abs": "https://arxiv.org/abs/2410.21306", "title": "Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges", "authors": ["Farid Ariai", "Joel Mackenzie", "Gianluca Demartini"], "categories": ["cs.CL", "cs.AI", "A.1; I.2.7; J.1"], "comment": "35 pages", "summary": "Natural Language Processing (NLP) is revolutionising the way both\nprofessionals and laypersons operate in the legal field. The considerable\npotential for NLP in the legal sector, especially in developing computational\nassistance tools for various legal processes, has captured the interest of\nresearchers for years. This survey follows the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a\nfinal selection of 131 after manual filtering. It explores foundational\nconcepts related to NLP in the legal domain, illustrating the unique aspects\nand challenges of processing legal texts, such as extensive document lengths,\ncomplex language, and limited open legal datasets. We provide an overview of\nNLP tasks specific to legal text, such as Document Summarisation, Named Entity\nRecognition, Question Answering, Argument Mining, Text Classification, and\nJudgement Prediction. Furthermore, we analyse both developed legal-oriented\nlanguage models, and approaches for adapting general-purpose language models to\nthe legal domain. Additionally, we identify sixteen open research challenges,\nincluding the detection and mitigation of bias in artificial intelligence\napplications, the need for more robust and interpretable models, and improving\nexplainability to handle the complexities of legal language and reasoning.", "AI": {"tldr": "This survey reviews the application of Natural Language Processing (NLP) in the legal field, examining 131 studies to highlight key NLP tasks, challenges, and development of legal-oriented language models.", "motivation": "To explore the potential and challenges of applying NLP in the legal domain, focusing on computational assistance tools.", "method": "A systematic review of 154 studies, refined to 131 through manual filtering, using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework.", "result": "Key findings include an overview of specific NLP tasks in legal texts and the identification of sixteen open research challenges, such as bias detection and the need for robust models.", "conclusion": "The paper underscores the significant advancements and ongoing challenges in employing NLP for legal text processing, indicating areas for future research.", "key_contributions": ["Systematic review of NLP applications in the legal field.", "Identification of specific NLP tasks and challenges in processing legal text.", "Outline of open research challenges for NLP in law."], "limitations": "The review may not cover all existing studies due to the filtering process.", "keywords": ["Natural Language Processing", "Legal domain", "Systematic review", "Language models", "Research challenges"], "importance_score": 4, "read_time_minutes": 30}}
{"id": "2412.03334", "pdf": "https://arxiv.org/pdf/2412.03334.pdf", "abs": "https://arxiv.org/abs/2412.03334", "title": "Yankari: A Monolingual Yoruba Dataset", "authors": ["Maro Akpobi"], "categories": ["cs.CL"], "comment": "6 pages", "summary": "This paper presents Yankari, a large-scale monolingual dataset for the Yoruba\nlanguage, aimed at addressing the critical gap in Natural Language Processing\n(NLP) resources for this important West African language. Despite being spoken\nby over 30 million people, Yoruba has been severely underrepresented in NLP\nresearch and applications. We detail our methodology for creating this dataset,\nwhich includes careful source selection, automated quality control, and\nrigorous data cleaning processes. The Yankari dataset comprises 51,407\ndocuments from 13 diverse sources, totaling over 30 million tokens. Our\napproach focuses on ethical data collection practices, avoiding problematic\nsources and addressing issues prevalent in existing datasets. We provide\nthorough automated evaluations of the dataset, demonstrating its quality\ncompared to existing resources. The Yankari dataset represents a significant\nadvancement in Yoruba language resources, providing a foundation for developing\nmore accurate NLP models, supporting comparative linguistic studies, and\ncontributing to the digital accessibility of the Yoruba language.", "AI": {"tldr": "Yankari is a large-scale dataset for the Yoruba language aimed at improving NLP resources in underrepresented languages.", "motivation": "To address the critical gap in NLP resources for the Yoruba language, which is spoken by over 30 million people.", "method": "Creation of the dataset involved careful source selection, automated quality control, and rigorous data cleaning processes, resulting in 51,407 documents and over 30 million tokens.", "result": "The Yankari dataset is evaluated and shown to be of high quality compared to existing resources, advancing Yoruba language representation in NLP.", "conclusion": "The dataset supports the development of more accurate NLP models and enhances digital accessibility for the Yoruba language.", "key_contributions": ["Creation of Yankari, a large-scale Yoruba language dataset.", "Automated evaluations proving dataset quality.", "Focus on ethical data collection practices."], "limitations": "", "keywords": ["Yoruba", "NLP", "dataset", "language resources", "ethical data collection"], "importance_score": 4, "read_time_minutes": 6}}
{"id": "2412.08528", "pdf": "https://arxiv.org/pdf/2412.08528.pdf", "abs": "https://arxiv.org/abs/2412.08528", "title": "Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck", "authors": ["Andor Diera", "Lukas Galke", "Fabian Karl", "Ansgar Scherp"], "categories": ["cs.CL"], "comment": null, "summary": "Continual learning remains a challenge across various natural language\nprocessing (NLP) tasks, as models updated with new training data often risk\ncatastrophic forgetting of previously acquired knowledge. We introduce a\ndiscrete key-value bottleneck (DKVB) for encoder-only language models, enabling\nefficient continual learning through localized updates. Inspired by a discrete\nkey-value bottleneck in vision, we consider new and NLP-specific challenges. We\ncompare different bottleneck architectures for NLP and introduce a new,\ntask-independent initialization technique for the discrete keys. We evaluate\nour DKVB for NLP in four continual learning scenarios and show that it\nalleviates catastrophic forgetting. Our experiments demonstrate that the\nproposed approach achieves competitive performance compared to popular\ncontinual learning methods while incurring lower computational costs.\nFurthermore, we show that DKVB remains effective even in challenging\nsingle-head continual learning scenarios where no task ID is provided.", "AI": {"tldr": "This paper introduces a discrete key-value bottleneck (DKVB) for continual learning in NLP, addressing catastrophic forgetting through efficient localized updates.", "motivation": "Continual learning in NLP faces the challenge of catastrophic forgetting when models are updated with new data.", "method": "The paper introduces a discrete key-value bottleneck for encoder-only language models, compares different bottleneck architectures, and proposes a new initialization technique for discrete keys.", "result": "The DKVB enables efficient continual learning, achieving competitive performance with lower computational costs in four evaluation scenarios.", "conclusion": "The proposed method alleviates catastrophic forgetting and remains effective in scenarios without task ID.", "key_contributions": ["Introduction of a discrete key-value bottleneck for NLP", "New task-independent initialization technique for discrete keys", "Demonstrated lower computational costs compared to existing methods."], "limitations": "Limited to encoder-only language models; effectiveness in other architectures not explored.", "keywords": ["continual learning", "natural language processing", "catastrophic forgetting", "encoder-only models", "discrete key-value bottleneck"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.15239", "pdf": "https://arxiv.org/pdf/2412.15239.pdf", "abs": "https://arxiv.org/abs/2412.15239", "title": "Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs", "authors": ["Hortense Fong", "George Gui"], "categories": ["cs.CL", "cs.AI", "econ.GN", "q-fin.EC", "stat.ME", "68T50, 91F20", "H.3.1; I.2.7"], "comment": null, "summary": "Understanding when and why consumers engage with stories is crucial for\ncontent creators and platforms. While existing theories suggest that audience\nbeliefs of what is going to happen should play an important role in engagement\ndecisions, empirical work has mostly focused on developing techniques to\ndirectly extract features from actual content, rather than capturing\nforward-looking beliefs, due to the lack of a principled way to model such\nbeliefs in unstructured narrative data. To complement existing feature\nextraction techniques, this paper introduces a novel framework that leverages\nlarge language models to model audience forward-looking beliefs about how\nstories might unfold. Our method generates multiple potential continuations for\neach story and extracts features related to expectations, uncertainty, and\nsurprise using established content analysis techniques. Applying our method to\nover 30,000 book chapters, we demonstrate that our framework complements\nexisting feature engineering techniques by amplifying their marginal\nexplanatory power on average by 31%. The results reveal that different types of\nengagement-continuing to read, commenting, and voting-are driven by distinct\ncombinations of current and anticipated content features. Our framework\nprovides a novel way to study and explore how audience forward-looking beliefs\nshape their engagement with narrative media, with implications for marketing\nstrategy in content-focused industries.", "AI": {"tldr": "This paper introduces a framework using large language models to model audience forward-looking beliefs about story developments, demonstrating significant improvements in explaining consumer engagement with narrative content.", "motivation": "To understand consumer engagement with stories, which is crucial for content creators and platforms, particularly how audience beliefs influence these decisions.", "method": "The framework generates potential continuations for stories and extracts features related to expectations, uncertainty, and surprise using content analysis techniques.", "result": "The method was applied to over 30,000 book chapters, increasing the explanatory power of traditional feature engineering techniques by an average of 31%.", "conclusion": "The findings show that different types of engagement are influenced by combinations of current and anticipated content features, providing insights for marketing strategies.", "key_contributions": ["Introduces a novel framework for modeling audience beliefs using large language models", "Demonstrates increased explanatory power for engagement prediction", "Reveals distinct engagement drivers based on content features"], "limitations": "", "keywords": ["story engagement", "large language models", "consumer behavior", "content analysis", "forward-looking beliefs"], "importance_score": 7, "read_time_minutes": 10}}
{"id": "2412.16936", "pdf": "https://arxiv.org/pdf/2412.16936.pdf", "abs": "https://arxiv.org/abs/2412.16936", "title": "Rationale-guided Prompting for Knowledge-based Visual Question Answering", "authors": ["Zhongjian Hu", "Peng Yang", "Bing Li", "Fengyuan Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, Large Language Models (LLMs) have been used for knowledge-based\nVisual Question Answering (VQA). Despite the encouraging results of previous\nstudies, prior methods prompt LLMs to predict answers directly, neglecting\nintermediate thought processes. We argue that prior methods do not sufficiently\nactivate the capacities of LLMs. We propose a framework called PLRH that\nPrompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH\nprompts LLMs with Chain of Thought (CoT) to generate rationale heuristics,\ni.e., intermediate thought processes, and then leverages the rationale\nheuristics to inspire LLMs to predict answers. Experiments show that our\napproach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA\nand A-OKVQA, respectively.", "AI": {"tldr": "This paper presents a novel framework (PLRH) for Visual Question Answering that enhances Large Language Models' performance by incorporating intermediate reasoning processes.", "motivation": "Existing methods for knowledge-based Visual Question Answering underutilize the reasoning abilities of Large Language Models, prompting direct answers without intermediate thought processes.", "method": "The proposed framework, PLRH, prompts LLMs using Chain of Thought (CoT) to generate rationale heuristics, which guide the prediction of answers.", "result": "The PLRH framework outperforms existing baselines by more than 2.2 and 2.1 on the OK-VQA and A-OKVQA datasets, respectively.", "conclusion": "Incorporating rationale heuristics into the VQA process significantly enhances the performance of LLMs by utilizing their reasoning capabilities.", "key_contributions": ["Introduction of the PLRH framework for VQA.", "Incorporation of Chain of Thought reasoning to enhance answer prediction.", "Demonstrated superior performance on established VQA benchmarks."], "limitations": "", "keywords": ["Visual Question Answering", "Large Language Models", "Rationale Heuristics", "Chain of Thought", "Intermediate Reasoning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2501.09213", "pdf": "https://arxiv.org/pdf/2501.09213.pdf", "abs": "https://arxiv.org/abs/2501.09213", "title": "FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training", "authors": ["Hongzhou Yu", "Tianhao Cheng", "Yingwen Wang", "Wen He", "Qing Wang", "Ying Cheng", "Yuejie Zhang", "Rui Feng", "Xiaobo Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown promise in\nmedical applications such as disease diagnosis and treatment planning. However,\nmost existing medical LLMs struggle with the deep reasoning required for\ncomplex medical problems, such as differential diagnosis and medication\nrecommendations. We propose FineMedLM-o1, which leverages high-quality medical\nsynthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT)\nand Direct Preference Optimization (DPO), enabling advanced dialogue and deep\nreasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in\nthe medical domain for the first time, facilitating domain adaptation and\nensuring reliable, accurate reasoning. Experimental results demonstrate that\nFineMedLM-o1 achieves a 23% average performance improvement over prior models\non key medical benchmarks. Furthermore, the introduction of TTT provides an\nadditional 14% performance boost, highlighting its effectiveness in enhancing\nmedical reasoning capabilities. To support this process, we also propose a\nnovel method for synthesizing medical dialogue. Compared to other open-source\ndatasets, our dataset stands out as superior in both quality and complexity.\nThe project and data will be released on GitHub.", "AI": {"tldr": "FineMedLM-o1 enhances LLMs for medical applications using synthetic data and innovative training techniques, resulting in significant performance gains in medical reasoning.", "motivation": "There is a need for improved reasoning capabilities in medical LLMs for complex tasks like diagnosis and treatment planning.", "method": "FineMedLM-o1 utilizes Supervised Fine-Tuning and Direct Preference Optimization along with Test-Time Training specifically for the medical domain.", "result": "FineMedLM-o1 achieved a 23% performance improvement over existing models and an additional 14% boost from Test-Time Training on medical benchmarks.", "conclusion": "The proposed methods and dataset enhance LLM performance in medical reasoning, and the tools will be made available on GitHub.", "key_contributions": ["Introduction of FineMedLM-o1 for advanced medical reasoning", "First application of Test-Time Training in the medical domain", "Creation of a superior synthetic medical dialogue dataset"], "limitations": "", "keywords": ["large language models", "medical applications", "fine-tuning", "deep reasoning", "synthetic data"], "importance_score": 10, "read_time_minutes": 7}}
{"id": "2502.14907", "pdf": "https://arxiv.org/pdf/2502.14907.pdf", "abs": "https://arxiv.org/abs/2502.14907", "title": "GneissWeb: Preparing High Quality Data for LLMs at Scale", "authors": ["Hajar Emami Gohari", "Swanand Ravindra Kadhe", "Syed Yousaf Shah", "Constantin Adam", "Abdulhamid Adebayo", "Praneet Adusumilli", "Farhan Ahmed", "Nathalie Baracaldo Angel", "Santosh Subhashrao Borse", "Yuan-Chi Chang", "Xuan-Hong Dang", "Nirmit Desai", "Revital Eres", "Ran Iwamoto", "Alexei Karve", "Yan Koyfman", "Wei-Han Lee", "Changchang Liu", "Boris Lublinsky", "Takuyo Ohko", "Pablo Pesce", "Maroun Touma", "Shiqiang Wang", "Shalisha Witherspoon", "Herbert WoisetschlÃ¤ger", "David Wood", "Kun-Lung Wu", "Issei Yoshida", "Syed Zawad", "Petros Zerfos", "Yi Zhou", "Bishwaranjan Bhattacharjee"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Data quantity and quality play a vital role in determining the performance of\nLarge Language Models (LLMs). High-quality data, in particular, can\nsignificantly boost the LLM's ability to generalize on a wide range of\ndownstream tasks. Large pre-training datasets for leading LLMs remain\ninaccessible to the public, whereas many open datasets are small in size (less\nthan 5 trillion tokens), limiting their suitability for training large models.\n  In this paper, we introduce GneissWeb, a large dataset yielding around 10\ntrillion tokens that caters to the data quality and quantity requirements of\ntraining LLMs. Our GneissWeb recipe that produced the dataset consists of\nsharded exact sub-string deduplication and a judiciously constructed ensemble\nof quality filters. GneissWeb achieves a favorable trade-off between data\nquality and quantity, producing models that outperform models trained on\nstate-of-the-art open large datasets (5+ trillion tokens).\n  We show that models trained using GneissWeb dataset outperform those trained\non FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed\non a set of 11 commonly used benchmarks (both zero-shot and few-shot) for\npre-training dataset evaluation. When the evaluation set is extended to 20\nbenchmarks (both zero-shot and few-shot), models trained using GneissWeb still\nachieve a 1.75 percentage points advantage over those trained on\nFineWeb-V1.1.0.", "AI": {"tldr": "GneissWeb is a new dataset of around 10 trillion tokens designed to enhance the performance of Large Language Models (LLMs) by providing high-quality data necessary for better generalization across tasks.", "motivation": "To address the limitations of existing open datasets which are smaller in size and may not provide the required quality for training large models, resulting in suboptimal performance of LLMs.", "method": "The dataset was created using a combination of sharded exact sub-string deduplication and an ensemble of quality filters to achieve a balance between quality and quantity.", "result": "Models trained on the GneissWeb dataset outperformed those trained on FineWeb-V1.1.0 by 2.73 percentage points on 11 benchmarks and by 1.75 percentage points on an extended set of 20 benchmarks.", "conclusion": "GneissWeb provides a significantly larger and higher-quality dataset that can improve the training of LLMs, leading to better performance on a range of natural language processing tasks.", "key_contributions": ["Introduction of GneissWeb, a large dataset with 10 trillion tokens", "Demonstrated significant performance improvement in LLMs using GneissWeb", "Methodology for data quality filtering in dataset creation"], "limitations": "", "keywords": ["Large Language Models", "dataset", "data quality", "GneissWeb", "machine learning"], "importance_score": 8, "read_time_minutes": 5}}
{"id": "2503.03044", "pdf": "https://arxiv.org/pdf/2503.03044.pdf", "abs": "https://arxiv.org/abs/2503.03044", "title": "QE4PE: Word-level Quality Estimation for Human Post-Editing", "authors": ["Gabriele Sarti", "VilÃ©m Zouhar", "Grzegorz ChrupaÅa", "Ana Guerberof-Arenas", "Malvina Nissim", "Arianna Bisazza"], "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by TACL (pre-MIT Press publication version); Code:\n  https://github.com/gsarti/qe4pe. Dataset:\n  https://huggingface.co/datasets/gsarti/qe4pe", "summary": "Word-level quality estimation (QE) methods aim to detect erroneous spans in\nmachine translations, which can direct and facilitate human post-editing. While\nthe accuracy of word-level QE systems has been assessed extensively, their\nusability and downstream influence on the speed, quality and editing choices of\nhuman post-editing remain understudied. In this study, we investigate the\nimpact of word-level QE on machine translation (MT) post-editing in a realistic\nsetting involving 42 professional post-editors across two translation\ndirections. We compare four error-span highlight modalities, including\nsupervised and uncertainty-based word-level QE methods, for identifying\npotential errors in the outputs of a state-of-the-art neural MT model.\nPost-editing effort and productivity are estimated from behavioral logs, while\nquality improvements are assessed by word- and segment-level human annotation.\nWe find that domain, language and editors' speed are critical factors in\ndetermining highlights' effectiveness, with modest differences between\nhuman-made and automated QE highlights underlining a gap between accuracy and\nusability in professional workflows.", "AI": {"tldr": "This study investigates the impact of word-level quality estimation (QE) on post-editing machine translations, analyzing different modalities and their effectiveness in real-world scenarios with professional editors.", "motivation": "To understand how word-level QE affects human post-editing in machine translation, particularly focusing on usability and downstream impacts on editing choices and efficiency.", "method": "The study involved 42 professional post-editors working on machine translations, comparing four different modalities of error-span highlights (including supervised and uncertainty-based QE methods) to assess their influence on post-editing effort and quality improvements.", "result": "Findings indicate that factors such as domain, language, and the speed of editors significantly affect the effectiveness of highlights, revealing a gap between the accuracy of QE systems and their usability in professional translation workflows.", "conclusion": "Though different highlighting methods showed modest effectiveness in improving post-editing tasks, the complexity of human workflows shows that accuracy alone does not ensure usability in practice.", "key_contributions": ["Analysis of usability of word-level QE in professional post-editing", "Comparison of multiple error-span highlight modalities", "Assessment of factors influencing the effectiveness of QE in translation tasks"], "limitations": "Focuses on a specific setting with professional editors, which may not generalize to other contexts or less experienced users.", "keywords": ["Quality Estimation", "Machine Translation", "Post-Editing", "Human-Computer Interaction", "Usability"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2503.20988", "pdf": "https://arxiv.org/pdf/2503.20988.pdf", "abs": "https://arxiv.org/abs/2503.20988", "title": "Cross-Modal State-Space Graph Reasoning for Structured Summarization", "authors": ["Hannah Kim", "Sofia Martinez", "Jason Lee"], "categories": ["cs.CL", "cs.GR"], "comment": "arXiv admin note: This paper has been withdrawn by arXiv due to\n  disputed and unverifiable authorship and affiliation", "summary": "The ability to extract compact, meaningful summaries from large-scale and\nmultimodal data is critical for numerous applications, ranging from video\nanalytics to medical reports. Prior methods in cross-modal summarization have\noften suffered from high computational overheads and limited interpretability.\nIn this paper, we propose a \\textit{Cross-Modal State-Space Graph Reasoning}\n(\\textbf{CSS-GR}) framework that incorporates a state-space model with\ngraph-based message passing, inspired by prior work on efficient state-space\nmodels. Unlike existing approaches relying on purely sequential models, our\nmethod constructs a graph that captures inter- and intra-modal relationships,\nallowing more holistic reasoning over both textual and visual streams. We\ndemonstrate that our approach significantly improves summarization quality and\ninterpretability while maintaining computational efficiency, as validated on\nstandard multimodal summarization benchmarks. We also provide a thorough\nablation study to highlight the contributions of each component.", "AI": {"tldr": "Introducing a CSS-GR framework for efficient cross-modal summarization using graph-based reasoning.", "motivation": "To address the need for effective summarization methods in multimodal data applications through improved interpretability and reduced computational overhead.", "method": "Proposes a Cross-Modal State-Space Graph Reasoning framework that leverages a state-space model combined with graph-based message passing to enhance summarization.", "result": "Demonstrated significant improvements in summarization quality and interpretability while maintaining computational efficiency on multimodal benchmarks.", "conclusion": "The proposed method outperforms existing sequential models by allowing holistic reasoning across textual and visual data.", "key_contributions": ["Introduction of CSS-GR framework", "Enhanced interpretability in summarization", "Efficient computational performance on multimodal tasks"], "limitations": "Withdrawn from arXiv due to disputed authorship and affiliation.", "keywords": ["Cross-Modal Summarization", "State-Space Model", "Graph Reasoning"], "importance_score": 5, "read_time_minutes": 10}}
{"id": "2504.01282", "pdf": "https://arxiv.org/pdf/2504.01282.pdf", "abs": "https://arxiv.org/abs/2504.01282", "title": "Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing", "authors": ["Jihyun Janice Ahn", "Wenpeng Yin"], "categories": ["cs.CL"], "comment": "accepted in COLM2025, 9 pages", "summary": "While the inconsistency of LLMs is not a novel topic, prior research has\npredominantly addressed two types of generative inconsistencies: i) Randomness\nInconsistency: running the same LLM multiple trials, yielding varying\nresponses; ii) Paraphrase Inconsistency: paraphrased prompts result in\ndifferent responses from the same LLM. Randomness Inconsistency arises from the\ninherent randomness due to stochastic sampling in generative models, while\nParaphrase Inconsistency is a consequence of the language modeling objectives,\nwhere paraphrased prompts alter the distribution of vocabulary logits. This\nresearch discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM\nself-inconsistency: given a question and a couple of LLM-generated answer\ncandidates, the LLM often has conflicting responses when prompted \"Which are\ncorrect answers?\" and \"Which are incorrect answers?\". PRIN poses a big concern\nas it undermines the credibility of LLM-as-a-judge, and suggests a challenge\nfor LLMs to adhere to basic logical rules. We conduct a series of experiments\nto investigate PRIN, examining the extent of PRIN across different LLMs,\nmethods to mitigate it, potential applications, and its relationship with\nRandomness Inconsistency and Paraphrase Inconsistency. As the first study to\nexplore PRIN, our findings offer valuable insights into the inner workings of\nLLMs and contribute to advancing trustworthy AI.", "AI": {"tldr": "This paper introduces Prompt-Reverse Inconsistency (PRIN), a newly identified form of LLM self-inconsistency that affects the reliability of LLM-generated responses.", "motivation": "To address the credibility issues arising from different types of inconsistencies found in LLMs.", "method": "A series of experiments were conducted to examine the extent of PRIN across various LLMs, alongside methods for mitigating this inconsistency.", "result": "The study identifies PRIN as a significant concern affecting the reliability of LLMs and explores its interrelation with other known inconsistencies.", "conclusion": "The findings provide insights into LLM behavior and contribute to the development of more trustworthy AI systems.", "key_contributions": ["Discovery of a new type of LLM self-inconsistency (PRIN)", "Experimental analysis of PRIN across multiple LLMs", "Suggestions for mitigating PRIN"], "limitations": "", "keywords": ["Prompt-Reverse Inconsistency", "LLMs", "Trustworthy AI"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.05008", "pdf": "https://arxiv.org/pdf/2504.05008.pdf", "abs": "https://arxiv.org/abs/2504.05008", "title": "Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears", "authors": ["Anastasiia Ivanova", "Natalia Fedorova", "Sergei Tilga", "Ekaterina Artemova"], "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The rapid development of AI-driven tools, particularly large language models\n(LLMs), is reshaping professional writing. Still, key aspects of their adoption\nsuch as languages support, ethics, and long-term impact on writers voice and\ncreativity remain underexplored. In this work, we conducted a questionnaire (N\n= 301) and an interactive survey (N = 36) targeting professional writers\nregularly using AI. We examined LLM-assisted writing practices across 25+\nlanguages, ethical concerns, and user expectations. The findings of the survey\ndemonstrate important insights, reflecting upon the importance of: LLMs\nadoption for non-English speakers; the degree of misinformation, domain and\nstyle adaptation; usability and key features of LLMs. These insights can guide\nfurther development, benefiting both writers and a broader user base.", "AI": {"tldr": "This study examines the adoption of AI-driven tools, specifically large language models (LLMs), in professional writing, focusing on language support, ethics, and user expectations.", "motivation": "To explore the under-researched areas of LLM adoption among professional writers, including language support and ethical concerns.", "method": "A questionnaire was conducted with 301 participants and an interactive survey with 36 professional writers using AI tools.", "result": "Key insights were gathered regarding LLM usage across multiple languages, the impact of misinformation, and writer expectations and concerns.", "conclusion": "The findings can inform the future development of LLMs, particularly for non-English speakers, enhancing their usability and features.", "key_contributions": ["Insights into LLM adoption for non-English speakers", "Analysis of ethical concerns in LLM use", "Recommendations for enhancing LLM usability based on user expectations"], "limitations": "", "keywords": ["AI-driven tools", "large language models", "professional writing", "ethics", "user expectations"], "importance_score": 8, "read_time_minutes": 10}}
{"id": "2504.11829", "pdf": "https://arxiv.org/pdf/2504.11829.pdf", "abs": "https://arxiv.org/abs/2504.11829", "title": "DÃ©jÃ  Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation", "authors": ["Julia Kreutzer", "Eleftheria Briakou", "Sweta Agrawal", "Marzieh Fadaee", "Kocmi Tom"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generation capabilities and language coverage of multilingual large language\nmodels (mLLMs) are advancing rapidly. However, evaluation practices for\ngenerative abilities of mLLMs are still lacking comprehensiveness, scientific\nrigor, and consistent adoption across research labs, which undermines their\npotential to meaningfully guide mLLM development. We draw parallels with\nmachine translation (MT) evaluation, a field that faced similar challenges and\nhas, over decades, developed transparent reporting standards and reliable\nevaluations for multilingual generative models. Through targeted experiments\nacross key stages of the generative evaluation pipeline, we demonstrate how\nbest practices from MT evaluation can deepen the understanding of quality\ndifferences between models. Additionally, we identify essential components for\nrobust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are\nrigorously assessed. We distill these insights into a checklist of actionable\nrecommendations for mLLM research and development.", "AI": {"tldr": "The paper critiques the evaluation practices for multilingual large language models (mLLMs) and proposes best practices from machine translation evaluation to enhance the rigor and comprehensiveness of these evaluations.", "motivation": "To improve the evaluation of multilingual large language models (mLLMs) that currently lack comprehensiveness and scientific rigor, which hinders their development and application.", "method": "The authors draw parallels with machine translation evaluation, conducting targeted experiments to apply best practices from MT to the evaluation of mLLMs, and propose a checklist of actionable recommendations for mLLM evaluation.", "result": "The study demonstrates that applying MT evaluation standards can reveal significant quality differences between mLLMs and establishes a foundational methodology for robust meta-evaluation of these models.", "conclusion": "Adopting proven evaluation practices from the established field of machine translation can lead to improved assessment of mLLMs, ultimately guiding their development more effectively.", "key_contributions": ["Proposed best practices for mLLM evaluation derived from machine translation standards.", "Developed a checklist of actionable recommendations for evaluating mLLMs.", "Identified essential components for meta-evaluation to rigorously assess evaluation methods themselves."], "limitations": "", "keywords": ["multilingual large language models", "evaluation practices", "machine translation"], "importance_score": 8, "read_time_minutes": 15}}
{"id": "2504.11257", "pdf": "https://arxiv.org/pdf/2504.11257.pdf", "abs": "https://arxiv.org/abs/2504.11257", "title": "UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis", "authors": ["Xinyi Liu", "Xiaoyi Zhang", "Ziyun Zhang", "Yan Lu"], "categories": ["cs.HC", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models are accelerating the\ndevelopment of Graphical User Interface (GUI) agents that utilize human-like\nvision perception capabilities to enhance productivity on digital devices.\nCompared to approaches predicated on GUI metadata, which are platform-dependent\nand vulnerable to implementation variations, vision-based approaches offer\nbroader applicability. In this vision-based paradigm, the GUI instruction\ngrounding, which maps user instruction to the location of corresponding element\non the given screenshot, remains a critical challenge, particularly due to\nlimited public training dataset and resource-intensive manual instruction data\nannotation. In this paper, we delve into unexplored challenges in this task\nincluding element-to-screen ratio, unbalanced element type, and implicit\ninstruction. To address these challenges, we introduce a large-scale data\nsynthesis pipeline UI-E2I-Synth for generating varying complex instruction\ndatasets using GPT-4o instead of human annotators. Furthermore, we propose a\nnew GUI instruction grounding benchmark UI-I2E-Bench, which is designed to\naddress the limitations of existing benchmarks by incorporating diverse\nannotation aspects. Our model, trained on the synthesized data, achieves\nsuperior performance in GUI instruction grounding, demonstrating the\nadvancements of proposed data synthesis pipeline. The proposed benchmark,\naccompanied by extensive analyses, provides practical insights for future\nresearch in GUI grounding. We will release corresponding artifacts at\nhttps://microsoft.github.io/FIVE-UI-Evol/ .", "AI": {"tldr": "This paper addresses challenges in GUI instruction grounding using a data synthesis pipeline and introduces a new benchmark for evaluating performance.", "motivation": "Addressing the limitations of GUI instruction grounding due to insufficient datasets and heavy manual annotation requirements.", "method": "Introduction of UI-E2I-Synth for generating complex instruction datasets with GPT-4o, and establishment of UI-I2E-Bench as a new benchmark for GUI instruction grounding.", "result": "The proposed model trained on synthesized data shows superior performance in grounding user instructions to GUI elements compared to existing methods.", "conclusion": "The paper provides a viable solution to improve GUI instruction grounding and offers a new benchmark for future research, alongside a synthesis pipeline to generate diverse instruction datasets.", "key_contributions": ["Introduction of a large-scale data synthesis pipeline UI-E2I-Synth", "Development of a new benchmark UI-I2E-Bench", "Demonstration of improved performance in GUI instruction grounding"], "limitations": "", "keywords": ["GUI instruction grounding", "data synthesis", "vision-language models"], "importance_score": 7, "read_time_minutes": 10}}
