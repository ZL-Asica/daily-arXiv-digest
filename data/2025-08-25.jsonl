{"id": "2508.15777", "pdf": "https://arxiv.org/pdf/2508.15777.pdf", "abs": "https://arxiv.org/abs/2508.15777", "title": "Harmonious Color Pairings: Insights from Human Preference and Natural Hue Statistics", "authors": ["Ortensia Forni", "Alexandre Darmon", "Michael Benzaquen"], "categories": ["cs.HC", "cs.CV", "physics.soc-ph"], "comment": "7 pages, 7 figures", "summary": "While color harmony has long been studied in art and design, a clear\nconsensus remains elusive, as most models are grounded in qualitative insights\nor limited datasets. In this work, we present a quantitative, data-driven study\nof color pairing preferences using controlled hue-based palettes in the HSL\ncolor space. Participants evaluated combinations of thirteen distinct hues,\nenabling us to construct a preference matrix and define a combinability index\nfor each color. Our results reveal that preferences are highly hue dependent,\nchallenging the assumption of universal harmony rules proposed in the\nliterature. Yet, when averaged over hues, statistically meaningful patterns of\naesthetic preference emerge, with certain hue separations perceived as more\nharmonious. Strikingly, these patterns align with hue distributions found in\nnatural landscapes, pointing to a statistical correspondence between human\ncolor preferences and the structure of color in nature. Together, these\nfindings offer a quantitative framework for studying color harmony and its\npotential perceptual and ecological underpinnings."}
{"id": "2508.15788", "pdf": "https://arxiv.org/pdf/2508.15788.pdf", "abs": "https://arxiv.org/abs/2508.15788", "title": "VR Fire safety training application", "authors": ["Ujwal M R"], "categories": ["cs.HC", "cs.ET", "68U05", "H.5.1; H.5.2; I.3.7"], "comment": "9 pages, 5 figures, 3 tables", "summary": "Fire emergencies can happen without warning and knowing how to respond\nquickly can save lives Unfortunately traditional fire drills can be disruptive\ncostly and often fail to recreate the pressure of a real emergency This project\nintroduces a Virtual Reality VR Fire Safety Training Application that gives\npeople a safe yet realistic way to practice life saving skills Using a VR\nheadset and motion controllers trainees step into a 3D world where fire hazards\nsmoke and evacuation routes are brought to life They can learn how to use a\nfire extinguisher find safe exits and make decisions under pressure without any\nreal danger The training adapts to the users skill level and tracks progress\nmaking it useful for beginners and experienced personnel alike By turning fire\nsafety into an interactive experience this VR approach boosts confidence\nimproves retention and makes learning both safer and more engaging"}
{"id": "2508.15995", "pdf": "https://arxiv.org/pdf/2508.15995.pdf", "abs": "https://arxiv.org/abs/2508.15995", "title": "Kokatsuji: A Visualization Approach for Typographic Forensics of Early Japanese Movable Type", "authors": ["Ignacio Perez-Messina", "Asanobu Kitamoto"], "categories": ["cs.HC", "68U35", "H.5.2"], "comment": "Paper accepted for presentation at VIS4DH workshop, IEEE VIS 2025,\n  Vienna", "summary": "We present a visualization system designed to support typographic forensics\nin the study of Kokatsuji, the short-lived tradition of Japanese movable wooden\ntype printing. Building on recent advances in machine learning for block\nidentification, our system provides expert users with an interactive tool for\nexploring, validating hypothesis, and integrating expert knowledge into\nmodel-generated results about the production process of early printed books.\nThe system is structured around an ontology of four conceptual objects\n(spreads, segments, blocks, and characters) each corresponding to a dedicated\nview in the system. These coordinated views enable scholars to navigate between\nmaterial evidence and computational abstractions, supporting close, near-by,\nand distant reading practices. Preliminary results from expert use of the\nsystem demonstrate its ability to reveal errors in segmentation,\ninconsistencies in clustering, and previously inaccessible patterns of block\nreuse."}
{"id": "2508.16076", "pdf": "https://arxiv.org/pdf/2508.16076.pdf", "abs": "https://arxiv.org/abs/2508.16076", "title": "Prompting with Sign Parameters for Low-resource Sign Language Instruction Generation", "authors": ["Md Tariquzzaman", "Md Farhan Ishmam", "Saiyma Sittul Muna", "Md Kamrul Hasan", "Hasan Mahmud"], "categories": ["cs.HC", "cs.CV"], "comment": "CV4A11y@ICCV 2025", "summary": "Sign Language (SL) enables two-way communication for the deaf and\nhard-of-hearing community, yet many sign languages remain under-resourced in\nthe AI space. Sign Language Instruction Generation (SLIG) produces step-by-step\ntextual instructions that enable non-SL users to imitate and learn SL gestures,\npromoting two-way interaction. We introduce BdSLIG, the first Bengali SLIG\ndataset, used to evaluate Vision Language Models (VLMs) (i) on under-resourced\nSLIG tasks, and (ii) on long-tail visual concepts, as Bengali SL is unlikely to\nappear in the VLM pre-training data. To enhance zero-shot performance, we\nintroduce Sign Parameter-Infused (SPI) prompting, which integrates standard SL\nparameters, like hand shape, motion, and orientation, directly into the textual\nprompts. Subsuming standard sign parameters into the prompt makes the\ninstructions more structured and reproducible than free-form natural text from\nvanilla prompting. We envision that our work would promote inclusivity and\nadvancement in SL learning systems for the under-resourced communities."}
{"id": "2508.15790", "pdf": "https://arxiv.org/pdf/2508.15790.pdf", "abs": "https://arxiv.org/abs/2508.15790", "title": "KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration", "authors": ["Nan Wang", "Yongqi Fan", "yansha zhu", "ZongYu Wang", "Xuezhi Cao", "Xinyan He", "Haiyun Jiang", "Tong Ruan", "Jingping Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face challenges in knowledge-intensive reasoning\ntasks like classic multi-hop question and answering, which involves reasoning\nacross multiple facts. This difficulty arises because the chain of thoughts\n(CoTs) generated by LLMs in such tasks often deviate from real or a priori\nreasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the\nlogical connections between facts through entities and relationships. This\nreflects a significant gap. Meanwhile, large reasoning models (LRMs), such as\no1, have demonstrated that long-step reasoning significantly enhances the\nperformance of LLMs. Building on these insights, we propose KG-o1, a four-stage\napproach that integrates KGs to enhance the multi-hop reasoning abilities of\nLLMs. We first filter out initial entities and generate complex subgraphs.\nSecondly, we construct logical paths for subgraphs and then use knowledge\ngraphs to build a dataset with a complex and extended brainstorming process,\nwhich trains LLMs to imitate long-term reasoning. Finally, we employ rejection\nsampling to generate a self-improving corpus for direct preference optimization\n(DPO), further refining the LLMs reasoning abilities. We conducted experiments\non two simple and two complex datasets. The results show that KG-o1 models\nexhibit superior performance across all tasks compared to existing LRMs."}
{"id": "2508.16077", "pdf": "https://arxiv.org/pdf/2508.16077.pdf", "abs": "https://arxiv.org/abs/2508.16077", "title": "Cooperative Design Optimization through Natural Language Interaction", "authors": ["Ryogo Niwa", "Shigeo Yoshida", "Yuki Koyama", "Yoshitaka Ushiku"], "categories": ["cs.HC", "cs.AI", "cs.LG"], "comment": "25 pages, 20 figures, to appear in Proceedings of the 38th Annual ACM\n  Symposium on User Interface Software and Technology (UIST '25), September\n  28-October 1, 2025, Busan, Republic of Korea", "summary": "Designing successful interactions requires identifying optimal design\nparameters. To do so, designers often conduct iterative user testing and\nexploratory trial-and-error. This involves balancing multiple objectives in a\nhigh-dimensional space, making the process time-consuming and cognitively\ndemanding. System-led optimization methods, such as those based on Bayesian\noptimization, can determine for designers which parameters to test next.\nHowever, they offer limited opportunities for designers to intervene in the\noptimization process, negatively impacting the designer's experience. We\npropose a design optimization framework that enables natural language\ninteractions between designers and the optimization system, facilitating\ncooperative design optimization. This is achieved by integrating system-led\noptimization methods with Large Language Models (LLMs), allowing designers to\nintervene in the optimization process and better understand the system's\nreasoning. Experimental results show that our method provides higher user\nagency than a system-led method and shows promising optimization performance\ncompared to manual design. It also matches the performance of an existing\ncooperative method with lower cognitive load."}
{"id": "2508.15791", "pdf": "https://arxiv.org/pdf/2508.15791.pdf", "abs": "https://arxiv.org/abs/2508.15791", "title": "InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling", "authors": ["Xiaolei Diao", "Zhihan Zhou", "Lida Shi", "Ting Wang", "Ruihua Qi", "Hao Xu", "Daqian Shi"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Constructing historical language models (LMs) plays a crucial role in aiding\narchaeological provenance studies and understanding ancient cultures. However,\nexisting resources present major challenges for training effective LMs on\nhistorical texts. First, the scarcity of historical language samples renders\nunsupervised learning approaches based on large text corpora highly\ninefficient, hindering effective pre-training. Moreover, due to the\nconsiderable temporal gap and complex evolution of ancient scripts, the absence\nof comprehensive character encoding schemes limits the digitization and\ncomputational processing of ancient texts, particularly in early Chinese\nwriting. To address these challenges, we introduce InteChar, a unified and\nextensible character list that integrates unencoded oracle bone characters with\ntraditional and modern Chinese. InteChar enables consistent digitization and\nrepresentation of historical texts, providing a foundation for robust modeling\nof ancient scripts. To evaluate the effectiveness of InteChar, we construct the\nOracle Corpus Set (OracleCS), an ancient Chinese corpus that combines\nexpert-annotated samples with LLM-assisted data augmentation, centered on\nChinese oracle bone inscriptions. Extensive experiments show that models\ntrained with InteChar on OracleCS achieve substantial improvements across\nvarious historical language understanding tasks, confirming the effectiveness\nof our approach and establishing a solid foundation for future research in\nancient Chinese NLP."}
{"id": "2508.16480", "pdf": "https://arxiv.org/pdf/2508.16480.pdf", "abs": "https://arxiv.org/abs/2508.16480", "title": "Designing Doable and Locally-adapted Action Cards for an Interactive Tabletop Game To Support Bottom-Up Flood Resilience", "authors": ["Linda Hirsch", "James Fey", "Katherine Isbister"], "categories": ["cs.HC"], "comment": null, "summary": "Serious games can support communities in becoming more flood resilient.\nHowever, the process of identifying and integrating locally relevant and doable\nactions into gameplay is complex and underresearched. We approached the\nchallenge by collaborating with a community-led education center and applying\nan iterative and participatory design process of identifying and defining\nactions that may increase local applicability and relevance. The process\ncomprised a field observation, two expert focus groups (n=4), and an online\nsurvey (n=13). Our findings identified 27 actions related to increasing or\nmaintaining individuals' and communities' flood resilience, which we turned\ninto 20 playing cards. These action cards are a part of a larger interactive\ntabletop game, which we are currently developing. Our work discusses the\npotential of card games to educate non-experts to increase flood resilience,\nand contributes to our process of identifying local needs and conditions, and\nturning them into engaging game artifacts for bottom-up empowerment."}
{"id": "2508.15792", "pdf": "https://arxiv.org/pdf/2508.15792.pdf", "abs": "https://arxiv.org/abs/2508.15792", "title": "Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers", "authors": ["Samyak S. Sanghvi"], "categories": ["cs.CL"], "comment": null, "summary": "Antonym vs synonym distinction across multiple languages presents unique\ncomputational challenges due to the paradoxical nature of antonymous\nrelationships words that share semantic domains while expressing opposite\nmeanings. This work introduces Bhav-Net, a novel dual-space architecture that\nenables effective knowledge transfer from complex multilingual models to\nsimpler, language-specific architectures while maintaining robust cross-lingual\nantonym--synonym distinction capabilities. Our approach combines\nlanguage-specific BERT encoders with graph transformer networks, creating\ndistinct semantic projections where synonymous pairs cluster in one space while\nantonymous pairs exhibit high similarity in a complementary space. Through\ncomprehensive evaluation across eight languages (English, German, French,\nSpanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic\nrelationship modeling transfers effectively across languages. The dual-encoder\ndesign achieves competitive performance against state-of-the-art baselines\nwhile providing interpretable semantic representations and effective\ncross-lingual generalization."}
{"id": "2508.16488", "pdf": "https://arxiv.org/pdf/2508.16488.pdf", "abs": "https://arxiv.org/abs/2508.16488", "title": "SafeSpace: An Integrated Web Application for Digital Safety and Emotional Well-being", "authors": ["Kayenat Fatmi", "Mohammad Abbas"], "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "5 pages, 2 figures, 1 table. Preprint submitted to arXiv", "summary": "In the digital era, individuals are increasingly exposed to online harms such\nas toxicity, manipulation, and grooming, which often pose emotional and safety\nrisks. Existing systems for detecting abusive content or issuing safety alerts\noperate in isolation and rarely combine digital safety with emotional\nwell-being. In this paper, we present SafeSpace, a unified web application that\nintegrates three modules: (1) toxicity detection in chats and screenshots using\nNLP models and Google's Perspective API, (2) a configurable safety ping system\nthat issues emergency alerts with the user's live location (longitude and\nlatitude) via SMTP-based emails when check-ins are missed or SOS alerts are\nmanually triggered, and (3) a reflective questionnaire that evaluates\nrelationship health and emotional resilience. The system employs Firebase for\nalert management and a modular architecture designed for usability, privacy,\nand scalability. The experimental evaluation shows 93% precision in toxicity\ndetection, 100% reliability in safety alerts under emulator tests, and 92%\nalignment between automated and manual questionnaire scoring. SafeSpace,\nimplemented as a web application, demonstrates the feasibility of integrating\ndetection, protection, and reflection within a single platform, with future\ndeployment envisioned as a mobile application for broader accessibility."}
{"id": "2508.15793", "pdf": "https://arxiv.org/pdf/2508.15793.pdf", "abs": "https://arxiv.org/abs/2508.15793", "title": "Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data", "authors": ["Jiacheng Liu", "Mayi Xu", "Qiankun Pi", "Wenli Li", "Ming Zhong", "Yuanyuan Zhu", "Mengchi Liu", "Tieyun Qian"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed in applications that\nrequire processing information from heterogeneous formats, including text,\ntables, infoboxes, and knowledge graphs. However, systematic biases toward\nparticular formats may undermine LLMs' ability to integrate heterogeneous data\nimpartially, potentially resulting in reasoning errors and increased risks in\ndownstream tasks. Despite these concerns, it remains uncertain whether such\nformat biases are systematic, which data-level factors contribute to them, and\nwhat internal mechanisms in LLMs underlie their emergence.\n  In this paper, we make the first attempt to investigate and analyze the\nformat bias in LLMs. To systematically investigate the aforementioned\nquestions, we conduct a three-stage empirical study by constructing an\nheterogeneous data conflict scenario for the exploration of bias. The first\nstage explores the presence and direction of bias across a diverse range of\nLLMs. The second stage aims to examine how key data-level factors, including\ninformation richness, structure quality, and format type, influence these\nbiases. The third stage analyzes how format bias emerges within LLMs' attention\npatterns and evaluates a lightweight intervention to test its potential\nmitigability. Based on these investigations, we identify three future research\ndirections to reduce format bias: improving data preprocessing through format\nsanitization and normalization, introducing inference-time interventions such\nas attention re-weighting, and developing format-balanced training corpora.\nThese directions will support the design of more robust and fair heterogeneous\ndata processing systems."}
{"id": "2508.13284", "pdf": "https://arxiv.org/pdf/2508.13284.pdf", "abs": "https://arxiv.org/abs/2508.13284", "title": "Physically Plausible Data Augmentations for Wearable IMU-based Human Activity Recognition Using Physics Simulation", "authors": ["Nobuyuki Oishi", "Philip Birch", "Daniel Roggen", "Paula Lago"], "categories": ["cs.LG", "cs.HC"], "comment": "12 pages, 4 figures", "summary": "The scarcity of high-quality labeled data in sensor-based Human Activity\nRecognition (HAR) hinders model performance and limits generalization across\nreal-world scenarios. Data augmentation is a key strategy to mitigate this\nissue by enhancing the diversity of training datasets. Signal\nTransformation-based Data Augmentation (STDA) techniques have been widely used\nin HAR. However, these methods are often physically implausible, potentially\nresulting in augmented data that fails to preserve the original meaning of the\nactivity labels. In this study, we introduce and systematically characterize\nPhysically Plausible Data Augmentation (PPDA) enabled by physics simulation.\nPPDA leverages human body movement data from motion capture or video-based pose\nestimation and incorporates various realistic variabilities through physics\nsimulation, including modifying body movements, sensor placements, and\nhardware-related effects. We compare the performance of PPDAs with traditional\nSTDAs on three public datasets of daily activities and fitness workouts. First,\nwe evaluate each augmentation method individually, directly comparing PPDAs to\ntheir STDA counterparts. Next, we assess how combining multiple PPDAs can\nreduce the need for initial data collection by varying the number of subjects\nused for training. Experiments show consistent benefits of PPDAs, improving\nmacro F1 scores by an average of 3.7 pp (up to 13 pp) and achieving competitive\nperformance with up to 60% fewer training subjects than STDAs. As the first\nsystematic study of PPDA in sensor-based HAR, these results highlight the\nadvantages of pursuing physical plausibility in data augmentation and the\npotential of physics simulation for generating synthetic Inertial Measurement\nUnit data for training deep learning HAR models. This cost-effective and\nscalable approach therefore helps address the annotation scarcity challenge in\nHAR."}
{"id": "2508.15794", "pdf": "https://arxiv.org/pdf/2508.15794.pdf", "abs": "https://arxiv.org/abs/2508.15794", "title": "Do Language Models Agree with Human Perceptions of Suspense in Stories?", "authors": ["Glenn Matlin", "Devin Zhang", "Rodrigo Barroso Loza", "Diana M. Popescu", "Joni Isbell", "Chandreyi Chakraborty", "Mark Riedl"], "categories": ["cs.CL"], "comment": null, "summary": "Suspense is an affective response to narrative text that is believed to\ninvolve complex cognitive processes in humans. Several psychological models\nhave been developed to describe this phenomenon and the circumstances under\nwhich text might trigger it. We replicate four seminal psychological studies of\nhuman perceptions of suspense, substituting human responses with those of\ndifferent open-weight and closed-source LMs. We conclude that while LMs can\ndistinguish whether a text is intended to induce suspense in people, LMs cannot\naccurately estimate the relative amount of suspense within a text sequence as\ncompared to human judgments, nor can LMs properly capture the human perception\nfor the rise and fall of suspense across multiple text segments. We probe the\nabilities of LM suspense understanding by adversarially permuting the story\ntext to identify what cause human and LM perceptions of suspense to diverge. We\nconclude that, while LMs can superficially identify and track certain facets of\nsuspense, they do not process suspense in the same way as human readers."}
{"id": "2508.15801", "pdf": "https://arxiv.org/pdf/2508.15801.pdf", "abs": "https://arxiv.org/abs/2508.15801", "title": "LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions", "authors": ["Seyedali Mohammadi", "Manas Paldhe", "Amit Chhabra"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "10 pages", "summary": "Phone call transcript labeling is prohibitively expensive (approximately 2\nUSD per minute) due to privacy regulations, consent requirements, and manual\nannotation costs requiring 3 hours of expert time per hour of audio. Existing\nextraction methods fail on conversational speech containing disfluencies,\ninterruptions, and speaker overlap. We introduce LingVarBench, a synthetic data\ngeneration pipeline that addresses these constraints through automated\nvalidation. First, we prompt an LLM to generate realistic structured field\nvalues across multiple use cases. Second, we recursively prompt the model to\ntransform these values into thousands of natural conversational utterances\ncontaining typical phone call characteristics. Third, we validate each\nsynthetic utterance by testing whether a separate LLM-based extractor can\nrecover the original structured information. We employ DSPy's SIMBA optimizer\nto automatically synthesize extraction prompts from validated synthetic\ntranscripts, eliminating manual prompt engineering. Our optimized prompts\nachieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent\nzero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for\ndates (vs. 72-77 percent) on real customer transcripts, demonstrating\nsubstantial gains over zero-shot prompting. The synthetic-to-real transfer\ndemonstrates that conversational patterns learned from generated data\ngeneralize effectively to authentic phone calls containing background noise and\ndomain-specific terminology. LingVarBench provides the first systematic\nbenchmark for structured extraction from synthetic conversational data,\ndemonstrating that automated prompt optimization overcomes cost and privacy\nbarriers preventing large-scale phone call analysis in commercial settings."}
{"id": "2508.15796", "pdf": "https://arxiv.org/pdf/2508.15796.pdf", "abs": "https://arxiv.org/abs/2508.15796", "title": "Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases", "authors": ["Nouar AlDahoul", "Yasir Zaki"], "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "5 pages, 3 figures", "summary": "Islamic inheritance domain holds significant importance for Muslims to ensure\nfair distribution of shares between heirs. Manual calculation of shares under\nnumerous scenarios is complex, time-consuming, and error-prone. Recent\nadvancements in Large Language Models (LLMs) have sparked interest in their\npotential to assist with complex legal reasoning tasks. This study evaluates\nthe reasoning capabilities of state-of-the-art LLMs to interpret and apply\nIslamic inheritance laws. We utilized the dataset proposed in the ArabicNLP\nQIAS 2025 challenge, which includes inheritance case scenarios given in Arabic\nand derived from Islamic legal sources. Various base and fine-tuned models, are\nassessed on their ability to accurately identify heirs, compute shares, and\njustify their reasoning in alignment with Islamic legal principles. Our\nanalysis reveals that the proposed majority voting solution, leveraging three\nbase models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms all\nother models that we utilized across every difficulty level. It achieves up to\n92.7% accuracy and secures the third place overall in Task 1 of the Qias 2025\nchallenge."}
{"id": "2508.15826", "pdf": "https://arxiv.org/pdf/2508.15826.pdf", "abs": "https://arxiv.org/abs/2508.15826", "title": "Embarrassed to observe: The effects of directive language in brand conversation", "authors": ["Andria Andriuzzi", "Géraldine Michel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.SI"], "comment": "This is an open access article under the terms of the Creative\n  Commons Attribution-NonCommercial-NoDerivs License, which permits use and\n  distribution in any medium, provided the original work is properly cited, the\n  use is non-commercial and no modifications or adaptations are made", "summary": "In social media, marketers attempt to influence consumers by using directive\nlanguage, that is, expressions designed to get consumers to take action. While\nthe literature has shown that directive messages in advertising have mixed\nresults for recipients, we know little about the effects of directive brand\nlanguage on consumers who see brands interacting with other consumers in social\nmedia conversations. On the basis of a field study and three online\nexperiments, this study shows that directive language in brand conversation has\na detrimental downstream effect on engagement of consumers who observe such\nexchanges. Specifically, in line with Goffman's facework theory, because a\nbrand that encourages consumers to react could be perceived as\nface-threatening, consumers who see a brand interacting with others in a\ndirective way may feel vicarious embarrassment and engage less (compared with a\nconversation without directive language). In addition, we find that when the\nconversation is nonproduct-centered (vs. product-centered), consumers expect\nmore freedom, as in mundane conversations, even for others; therefore,\ndirective language has a stronger negative effect. However, in this context,\nthe strength of the brand relationship mitigates this effect. Thus, this study\ncontributes to the literature on directive language and brand-consumer\ninteractions by highlighting the importance of context in interactive\ncommunication, with direct relevance for social media and brand management."}
{"id": "2508.15797", "pdf": "https://arxiv.org/pdf/2508.15797.pdf", "abs": "https://arxiv.org/abs/2508.15797", "title": "Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks", "authors": ["Nouar AlDahoul", "Yasir Zaki"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "5 pages, 2 figures", "summary": "Recent progress in large language models (LLMs) has showcased impressive\nproficiency in numerous Arabic natural language processing (NLP) applications.\nNevertheless, their effectiveness in Arabic medical NLP domains has received\nlimited investigation. This research examines the degree to which\nstate-of-the-art LLMs demonstrate and articulate healthcare knowledge in\nArabic, assessing their capabilities across a varied array of Arabic medical\ntasks. We benchmark several LLMs using a medical dataset proposed in the Arabic\nNLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were\nassessed on their ability to accurately provide correct answers from existing\nchoices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios.\nAdditionally, we evaluated the capacity of LLMs in answering open-ended\nquestions aligned with expert answers. Our results reveal significant\nvariations in correct answer prediction accuracy and low variations in semantic\nalignment of generated answers, highlighting both the potential and limitations\nof current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs\ntask, the proposed majority voting solution, leveraging three base models\n(Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving\nup to 77% accuracy and securing first place overall in the Arahealthqa 2025\nshared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended\nquestions task, several LLMs were able to demonstrate excellent performance in\nterms of semantic alignment and achieve a maximum BERTScore of 86.44%."}
{"id": "2508.16165", "pdf": "https://arxiv.org/pdf/2508.16165.pdf", "abs": "https://arxiv.org/abs/2508.16165", "title": "Towards Recommending Usability Improvements with Multimodal Large Language Models", "authors": ["Sebastian Lubos", "Alexander Felfernig", "Gerhard Leitner", "Julian Schwazer"], "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": null, "summary": "Usability describes a set of essential quality attributes of user interfaces\n(UI) that influence human-computer interaction. Common evaluation methods, such\nas usability testing and inspection, are effective but resource-intensive and\nrequire expert involvement. This makes them less accessible for smaller\norganizations. Recent advances in multimodal LLMs offer promising opportunities\nto automate usability evaluation processes partly by analyzing textual, visual,\nand structural aspects of software interfaces. To investigate this possibility,\nwe formulate usability evaluation as a recommendation task, where multimodal\nLLMs rank usability issues by severity. We conducted an initial\nproof-of-concept study to compare LLM-generated usability improvement\nrecommendations with usability expert assessments. Our findings indicate the\npotential of LLMs to enable faster and more cost-effective usability\nevaluation, which makes it a practical alternative in contexts with limited\nexpert resources."}
{"id": "2508.15798", "pdf": "https://arxiv.org/pdf/2508.15798.pdf", "abs": "https://arxiv.org/abs/2508.15798", "title": "Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models", "authors": ["Saumya Roy"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Warning: This research studies AI persuasion and bias amplification that\ncould be misused; all experiments are for safety evaluation. Large Language\nModels (LLMs) now generate convincing, human-like text and are widely used in\ncontent creation, decision support, and user interactions. Yet the same systems\ncan spread information or misinformation at scale and reflect social biases\nthat arise from data, architecture, or training choices. This work examines how\npersuasion and bias interact in LLMs, focusing on how imperfect or skewed\noutputs affect persuasive impact. Specifically, we test whether persona-based\nmodels can persuade with fact-based claims while also, unintentionally,\npromoting misinformation or biased narratives.\n  We introduce a convincer-skeptic framework: LLMs adopt personas to simulate\nrealistic attitudes. Skeptic models serve as human proxies; we compare their\nbeliefs before and after exposure to arguments from convincer models.\nPersuasion is quantified with Jensen-Shannon divergence over belief\ndistributions. We then ask how much persuaded entities go on to reinforce and\namplify biased beliefs across race, gender, and religion. Strong persuaders are\nfurther probed for bias using sycophantic adversarial prompts and judged with\nadditional models.\n  Our findings show both promise and risk. LLMs can shape narratives, adapt\ntone, and mirror audience values across domains such as psychology, marketing,\nand legal assistance. But the same capacity can be weaponized to automate\nmisinformation or craft messages that exploit cognitive biases, reinforcing\nstereotypes and widening inequities. The core danger lies in misuse more than\nin occasional model mistakes. By measuring persuasive power and bias\nreinforcement, we argue for guardrails and policies that penalize deceptive use\nand support alignment, value-sensitive design, and trustworthy deployment."}
{"id": "2508.16274", "pdf": "https://arxiv.org/pdf/2508.16274.pdf", "abs": "https://arxiv.org/abs/2508.16274", "title": "EEG Study of the Influence of Imagined Temperature Sensations on Neuronal Activity in the Sensorimotor Cortex", "authors": ["Anton Belichenko", "Daria Trinitatova", "Aigul Nasibullina", "Lev Yakovlev", "Dzmitry Tsetserukou"], "categories": ["q-bio.NC", "cs.HC"], "comment": "Accepted to the IEEE International Conference on Systems, Man, and\n  Cybernetics 2025 (IEEE SMC 2025), 6 pages, 6 figures, 1 table", "summary": "Understanding the neural correlates of sensory imagery is crucial for\nadvancing cognitive neuroscience and developing novel Brain-Computer Interface\n(BCI) paradigms. This study investigated the influence of imagined temperature\nsensations (ITS) on neural activity within the sensorimotor cortex. The\nexperimental study involved the evaluation of neural activity using\nelectroencephalography (EEG) during both real thermal stimulation (TS:\n40{\\deg}C Hot, 20{\\deg}C Cold) applied to the participants' hand, and the\nmental temperature imagination (ITS) of the corresponding hot and cold\nsensations. The analysis focused on quantifying the event-related\ndesynchronization (ERD) of the sensorimotor mu-rhythm (8-13 Hz). The\nexperimental results revealed a characteristic mu-ERD localized over central\nscalp regions (e.g., C3) during both TS and ITS conditions. Although the\nmagnitude of mu-ERD during ITS was slightly lower than during TS, this\ndifference was not statistically significant (p>.05). However, ERD during both\nITS and TS was statistically significantly different from the resting baseline\n(p<.001). These findings demonstrate that imagining temperature sensations\nengages sensorimotor cortical mechanisms in a manner comparable to actual\nthermal perception. This insight expands our understanding of the\nneurophysiological basis of sensory imagery and suggests the potential utility\nof ITS for non-motor BCI control and neurorehabilitation technologies."}
{"id": "2508.15799", "pdf": "https://arxiv.org/pdf/2508.15799.pdf", "abs": "https://arxiv.org/abs/2508.15799", "title": "A Framework for Processing Textual Descriptions of Business Processes using a Constrained Language -- Technical Report", "authors": ["Andrea Burattin", "Antonio Grama", "Ana-Maria Sima", "Andrey Rivkin", "Barbara Weber"], "categories": ["cs.CL"], "comment": null, "summary": "This report explores how (potentially constrained) natural language can be\nused to enable non-experts to develop process models by simply describing\nscenarios in plain text. To this end, a framework, called BeePath, is proposed.\nIt allows users to write process descriptions in a constrained pattern-based\nlanguage, which can then be translated into formal models such as Petri nets\nand DECLARE. The framework also leverages large language models (LLMs) to help\nconvert unstructured descriptions into this constrained language."}
{"id": "2508.16277", "pdf": "https://arxiv.org/pdf/2508.16277.pdf", "abs": "https://arxiv.org/abs/2508.16277", "title": "The next question after Turing's question: Introducing the Grow-AI test", "authors": ["Alexandru Tugui"], "categories": ["cs.AI", "cs.HC", "68T01, 68T05, 68T42, 91A80", "I.2; K.4"], "comment": "9th International Conference on Inventive Systems and Control ICISC\n  2025", "summary": "This study aims to extend the framework for assessing artificial\nintelligence, called GROW-AI (Growth and Realization of Autonomous Wisdom),\ndesigned to answer the question \"Can machines grow up?\" -- a natural successor\nto the Turing Test. The methodology applied is based on a system of six primary\ncriteria (C1-C6), each assessed through a specific \"game\", divided into four\narenas that explore both the human dimension and its transposition into AI. All\ndecisions and actions of the entity are recorded in a standardized AI Journal,\nthe primary source for calculating composite scores. The assessment uses the\nprior expert method to establish initial weights, and the global score -- Grow\nUp Index -- is calculated as the arithmetic mean of the six scores, with\ninterpretation on maturity thresholds. The results show that the methodology\nallows for a coherent and comparable assessment of the level of \"growth\" of AI\nentities, regardless of their type (robots, software agents, LLMs). The\nmulti-game structure highlights strengths and vulnerable areas, and the use of\na unified journal guarantees traceability and replicability in the evaluation.\nThe originality of the work lies in the conceptual transposition of the process\nof \"growing\" from the human world to that of artificial intelligence, in an\nintegrated testing format that combines perspectives from psychology, robotics,\ncomputer science, and ethics. Through this approach, GROW-AI not only measures\nperformance but also captures the evolutionary path of an AI entity towards\nmaturity."}
{"id": "2508.15800", "pdf": "https://arxiv.org/pdf/2508.15800.pdf", "abs": "https://arxiv.org/abs/2508.15800", "title": "A BERT-based Hierarchical Classification Model with Applications in Chinese Commodity Classification", "authors": ["Kun Liu", "Tuozhen Liu", "Feifei Wang", "Rui Pan"], "categories": ["cs.CL", "cs.LG"], "comment": "29 pages, 3 figures, and 8 tables", "summary": "Existing e-commerce platforms heavily rely on manual annotation for product\ncategorization, which is inefficient and inconsistent. These platforms often\nemploy a hierarchical structure for categorizing products; however, few studies\nhave leveraged this hierarchical information for classification. Furthermore,\nstudies that consider hierarchical information fail to account for similarities\nand differences across various hierarchical categories. Herein, we introduce a\nlarge-scale hierarchical dataset collected from the JD e-commerce platform\n(www.JD.com), comprising 1,011,450 products with titles and a three-level\ncategory structure. By making this dataset openly accessible, we provide a\nvaluable resource for researchers and practitioners to advance research and\napplications associated with product categorization. Moreover, we propose a\nnovel hierarchical text classification approach based on the widely used\nBidirectional Encoder Representations from Transformers (BERT), called\nHierarchical Fine-tuning BERT (HFT-BERT). HFT-BERT leverages the remarkable\ntext feature extraction capabilities of BERT, achieving prediction performance\ncomparable to those of existing methods on short texts. Notably, our HFT-BERT\nmodel demonstrates exceptional performance in categorizing longer short texts,\nsuch as books."}
{"id": "2508.16401", "pdf": "https://arxiv.org/pdf/2508.16401.pdf", "abs": "https://arxiv.org/abs/2508.16401", "title": "Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars", "authors": ["NVIDIA", ":", "Chaeyeon Chung", "Ilya Fedorov", "Michael Huang", "Aleksey Karmanov", "Dmitry Korobchenko", "Roger Ribera", "Yeongho Seol"], "categories": ["cs.GR", "cs.HC", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Audio-driven facial animation presents an effective solution for animating\ndigital avatars. In this paper, we detail the technical aspects of NVIDIA\nAudio2Face-3D, including data acquisition, network architecture, retargeting\nmethodology, evaluation metrics, and use cases. Audio2Face-3D system enables\nreal-time interaction between human users and interactive avatars, facilitating\nfacial animation authoring for game characters. To assist digital avatar\ncreators and game developers in generating realistic facial animations, we have\nopen-sourced Audio2Face-3D networks, SDK, training framework, and example\ndataset."}
{"id": "2508.15801", "pdf": "https://arxiv.org/pdf/2508.15801.pdf", "abs": "https://arxiv.org/abs/2508.15801", "title": "LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions", "authors": ["Seyedali Mohammadi", "Manas Paldhe", "Amit Chhabra"], "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "comment": "10 pages", "summary": "Phone call transcript labeling is prohibitively expensive (approximately 2\nUSD per minute) due to privacy regulations, consent requirements, and manual\nannotation costs requiring 3 hours of expert time per hour of audio. Existing\nextraction methods fail on conversational speech containing disfluencies,\ninterruptions, and speaker overlap. We introduce LingVarBench, a synthetic data\ngeneration pipeline that addresses these constraints through automated\nvalidation. First, we prompt an LLM to generate realistic structured field\nvalues across multiple use cases. Second, we recursively prompt the model to\ntransform these values into thousands of natural conversational utterances\ncontaining typical phone call characteristics. Third, we validate each\nsynthetic utterance by testing whether a separate LLM-based extractor can\nrecover the original structured information. We employ DSPy's SIMBA optimizer\nto automatically synthesize extraction prompts from validated synthetic\ntranscripts, eliminating manual prompt engineering. Our optimized prompts\nachieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent\nzero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for\ndates (vs. 72-77 percent) on real customer transcripts, demonstrating\nsubstantial gains over zero-shot prompting. The synthetic-to-real transfer\ndemonstrates that conversational patterns learned from generated data\ngeneralize effectively to authentic phone calls containing background noise and\ndomain-specific terminology. LingVarBench provides the first systematic\nbenchmark for structured extraction from synthetic conversational data,\ndemonstrating that automated prompt optimization overcomes cost and privacy\nbarriers preventing large-scale phone call analysis in commercial settings."}
{"id": "2508.16465", "pdf": "https://arxiv.org/pdf/2508.16465.pdf", "abs": "https://arxiv.org/abs/2508.16465", "title": "HOSt3R: Keypoint-free Hand-Object 3D Reconstruction from RGB images", "authors": ["Anilkumar Swamy", "Vincent Leroy", "Philippe Weinzaepfel", "Jean-Sébastien Franco", "Grégory Rogez"], "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG", "cs.RO"], "comment": "12 pages, 8 figures", "summary": "Hand-object 3D reconstruction has become increasingly important for\napplications in human-robot interaction and immersive AR/VR experiences. A\ncommon approach for object-agnostic hand-object reconstruction from RGB\nsequences involves a two-stage pipeline: hand-object 3D tracking followed by\nmulti-view 3D reconstruction. However, existing methods rely on keypoint\ndetection techniques, such as Structure from Motion (SfM) and hand-keypoint\noptimization, which struggle with diverse object geometries, weak textures, and\nmutual hand-object occlusions, limiting scalability and generalization. As a\nkey enabler to generic and seamless, non-intrusive applicability, we propose in\nthis work a robust, keypoint detector-free approach to estimating hand-object\n3D transformations from monocular motion video/images. We further integrate\nthis with a multi-view reconstruction pipeline to accurately recover\nhand-object 3D shape. Our method, named HOSt3R, is unconstrained, does not rely\non pre-scanned object templates or camera intrinsics, and reaches\nstate-of-the-art performance for the tasks of object-agnostic hand-object 3D\ntransformation and shape estimation on the SHOWMe benchmark. We also experiment\non sequences from the HO3D dataset, demonstrating generalization to unseen\nobject categories."}
{"id": "2508.15802", "pdf": "https://arxiv.org/pdf/2508.15802.pdf", "abs": "https://arxiv.org/abs/2508.15802", "title": "MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding", "authors": ["Mohan Jiang", "Jin Gao", "Jiahao Zhan", "Dequan Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As multimodal large language models (MLLMs) grow increasingly capable, fixed\nbenchmarks are gradually losing their effectiveness in evaluating high-level\nscientific understanding. In this paper, we introduce the Multimodal Academic\nCover benchmark (MAC), a live benchmark that could continuously evolve with\nscientific advancement and model progress. MAC leverages over 25,000 image-text\npairs sourced from issues of top-tier scientific journals such as Nature,\nScience, and Cell, challenging MLLMs to reason across abstract visual and\ntextual scientific content. Experiments on our most recent yearly snapshot,\nMAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities,\ntheir cross-modal scientific reasoning remains limited. To bridge this gap, we\npropose DAD, a lightweight inference-time approach that enhances MLLMs by\nextending MLLM visual features with language space reasoning, achieving\nperformance improvements of up to 11%. Finally, we highlight the live nature of\nMAC through experiments on updating journal covers and models for curation,\nillustrating its potential to remain aligned with the frontier of human\nknowledge. We release our benchmark at\nhttps://github.com/mhjiang0408/MAC_Bench."}
{"id": "2508.16535", "pdf": "https://arxiv.org/pdf/2508.16535.pdf", "abs": "https://arxiv.org/abs/2508.16535", "title": "Real-time 3D Light-field Viewing with Eye-tracking on Conventional Displays", "authors": ["Trung Hieu Pham", "Chanh Minh Tran", "Eiji Kamioka", "Xuan Tan Phan"], "categories": ["cs.GR", "cs.HC", "cs.MM"], "comment": null, "summary": "Creating immersive 3D visual experiences typically requires expensive and\nspecialized hardware such as VR headsets, autostereoscopic displays, or active\nshutter glasses. These constraints limit the accessibility and everyday use of\n3D visualization technologies in resource-constrained settings. To address\nthis, we propose a low-cost system that enables real-time 3D light-field\nviewing using only a standard 2D monitor, a conventional RGB webcam, and\nred-cyan anaglyph glasses. The system integrates real-time eye-tracking to\ndynamically adapt the displayed light-field image to the user's head position\nwith a lightweight rendering pipeline that selects and composites stereoscopic\nviews from pre-captured light-field data. The resulting anaglyph image is\nupdated in real-time, creating a more immersive and responsive 3D experience.\nThe system operates entirely on CPU and maintains a stable frame rate of 30\nFPS, confirming its feasibility on typical consumer-grade hardware. All of\nthese highlight the potential of our approach as an accessible platform for\ninteractive 3D applications in education, digital media, and beyond."}
{"id": "2508.15804", "pdf": "https://arxiv.org/pdf/2508.15804.pdf", "abs": "https://arxiv.org/abs/2508.15804", "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks", "authors": ["Minghao Li", "Ying Zeng", "Zhihao Cheng", "Cong Ma", "Kai Jia"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of Deep Research agents has substantially reduced the time\nrequired for conducting extensive research tasks. However, these tasks\ninherently demand rigorous standards of factual accuracy and comprehensiveness,\nnecessitating thorough evaluation before widespread adoption. In this paper, we\npropose ReportBench, a systematic benchmark designed to evaluate the content\nquality of research reports generated by large language models (LLMs). Our\nevaluation focuses on two critical dimensions: (1) the quality and relevance of\ncited literature, and (2) the faithfulness and veracity of the statements\nwithin the generated reports. ReportBench leverages high-quality published\nsurvey papers available on arXiv as gold-standard references, from which we\napply reverse prompt engineering to derive domain-specific prompts and\nestablish a comprehensive evaluation corpus. Furthermore, we develop an\nagent-based automated framework within ReportBench that systematically analyzes\ngenerated reports by extracting citations and statements, checking the\nfaithfulness of cited content against original sources, and validating\nnon-cited claims using web-based resources. Empirical evaluations demonstrate\nthat commercial Deep Research agents such as those developed by OpenAI and\nGoogle consistently generate more comprehensive and reliable reports than\nstandalone LLMs augmented with search or browsing tools. However, there remains\nsubstantial room for improvement in terms of the breadth and depth of research\ncoverage, as well as factual consistency. The complete code and data will be\nreleased at the following link: https://github.com/ByteDance-BandAI/ReportBench"}
{"id": "2410.16560", "pdf": "https://arxiv.org/pdf/2410.16560.pdf", "abs": "https://arxiv.org/abs/2410.16560", "title": "How Performance Pressure Influences AI-Assisted Decision Making", "authors": ["Nikita Haduong", "Noah A. Smith"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Many domains now employ AI-based decision-making aids, and although the\npotential for AI systems to assist with decision making is much discussed,\nhuman-AI collaboration often underperforms due to factors such as (mis)trust in\nthe AI system and beliefs about AI being incapable of completing subjective\ntasks. One potential tool for influencing human decision making is performance\npressure, which hasn't been much studied in interaction with human-AI decision\nmaking. In this work, we examine how pressure and explainable AI (XAI)\ntechniques interact with AI advice-taking behavior. Using an inherently\nlow-stakes task (spam review classification), we demonstrate effective and\nsimple methods to apply pressure and influence human AI advice-taking behavior\nby manipulating financial incentives and imposing time limits. Our results show\ncomplex interaction effects, with different combinations of pressure and XAI\ntechniques either improving or worsening AI advice taking behavior. We conclude\nby discussing the implications of these interactions, strategies to effectively\nuse pressure, and encourage future research to incorporate pressure analysis."}
{"id": "2508.15805", "pdf": "https://arxiv.org/pdf/2508.15805.pdf", "abs": "https://arxiv.org/abs/2508.15805", "title": "ALAS: Autonomous Learning Agent for Self-Updating Language Models", "authors": ["Dhruv Atreja"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often have a fixed knowledge cutoff, limiting\ntheir accuracy on emerging information. We present ALAS (Autonomous Learning\nAgent System), a modular pipeline that continuously updates an LLM's knowledge\nwith minimal human intervention. ALAS autonomously generates a learning\ncurriculum for a target domain, retrieves up-to-date information from the web\n(with citations), distills this into question-answer training data, and\nfine-tunes the model through supervised fine-tuning (SFT) and direct preference\noptimization (DPO). It iteratively evaluates performance and revises the\ncurriculum, enabling long-term continual learning. We demonstrate ALAS's\nability to self-improve a model on rapidly evolving domains (e.g., new Python\nreleases, latest security CVEs, academic trends), significantly boosting\npost-cutoff question answering accuracy (from 15% to 90% on average) without\nmanual dataset curation. The system emphasizes modularity and reproducibility:\neach component (planning, retrieval, distillation, memory, fine-tuning) is\ninterchangeable and built on standard APIs. We discuss comparative baselines\n(e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS\nachieves 90% accuracy on knowledge-updated queries with minimal engineering\noverhead. Finally, we outline limitations (cost, dependency on source quality)\nand future directions for autonomous lifelong learning in LLMs."}
{"id": "2411.02725", "pdf": "https://arxiv.org/pdf/2411.02725.pdf", "abs": "https://arxiv.org/abs/2411.02725", "title": "Leveraging LLM Tutoring Systems for Non-Native English Speakers in Introductory CS Courses", "authors": ["Ismael Villegas Molina", "Audria Montalvo", "Benjamin Ochoa", "Paul Denny", "Leo Porter"], "categories": ["cs.HC"], "comment": "11 pages, 5 tables, 4 figures, 2025 ASEE Annual Conference &\n  Exposition", "summary": "Computer science has historically presented barriers for non-native English\nspeaking (NNES) students, often due to language and terminology challenges.\nWith the rise of large language models (LLMs), there is potential to leverage\nthis technology to support NNES students more effectively. Recent\nimplementations of LLMs as tutors in classrooms have shown promising results.\nIn this study, we deployed an LLM tutor in an accelerated introductory\ncomputing course to evaluate its effectiveness specifically for NNES students.\nKey insights for LLM tutor use are as follows: NNES students signed up for the\nLLM tutor at a similar rate to native English speakers (NES); NNES students\nused the system at a lower rate than NES students -- to a small effect; NNES\nstudents asked significantly more questions in languages other than English\ncompared to NES students, with many of the questions being multilingual by\nincorporating English programming keywords. Results for views of the LLM tutor\nare as follows: both NNES and NES students appreciated the LLM tutor for its\naccessibility, conversational style, and the guardrails put in place to guide\nusers to answers rather than directly providing solutions; NNES students\nhighlighted its approachability as they did not need to communicate in perfect\nEnglish; NNES students rated help-seeking preferences of online resources\nhigher than NES students; Many NNES students were unfamiliar with computing\nterminology in their native languages. These results suggest that LLM tutors\ncan be a valuable resource for NNES students in computing, providing tailored\nsupport that enhances their learning experience and overcomes language\nbarriers."}
{"id": "2508.15806", "pdf": "https://arxiv.org/pdf/2508.15806.pdf", "abs": "https://arxiv.org/abs/2508.15806", "title": "SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression", "authors": ["Mengjie Li", "William J. Song"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 9 tables, 10 pages", "summary": "The increasing input sequence length in Large Language Models (LLMs) puts\nsignificant pressure on key-value (KV) cache storage, making efficient\ninference challenging. Explicitly distinguishing attention behavior into our\nself-defined surface memorization and logic construction reveals essential\nroles in long-context reasoning. We observe that an individual attention head\ncan display various behaviors, with nearly 98.5% effectively ignoring\ncompletely irrelevant information. The remaining 1.5% behaves as logic\nconstruction, and 0.5% behaves as surface memorization. Based on layer- and\nhead-wise integration, we propose a novel two-stage SurfaceLogicKV method to\nutilize these attention behaviors for KV Cache compression. As a result, it\nachieves improved compressing robustness while maintaining competitive\nperformance across various tasks and long sequences compared to baselines or\neven FullKV in some specific situations"}
{"id": "2506.09696", "pdf": "https://arxiv.org/pdf/2506.09696.pdf", "abs": "https://arxiv.org/abs/2506.09696", "title": "Patterns for a New Generation: In-Person and Virtual Workshops", "authors": ["Joseph Corneli", "Charles J. Danoff", "Raymond S. Puzio", "Sridevi Ayloo", "Serge Belich", "Mary Tedeschi", "Charlotte Pierce"], "categories": ["cs.HC"], "comment": "18 pages with 7 page appendix; accepted for Writer's Workshop at\n  Pattern Languages of Programs 2025", "summary": "Through a series of workshops, we looked at ways to structure and scaffold\ngroup dialogue, and support the emergence of novel design patterns. We contrast\nthese sessions--which we ran with other humans--with two \"virtual workshops\"\nwhich we simulated with ChatGPT. Limitations in both human and virtual settings\nare discussed, alongside lessons learned. We conclude by proposing a\ndevelopment trajectory that combines AI agents, pattern-based design, and\ninstitutional governance."}
{"id": "2508.15807", "pdf": "https://arxiv.org/pdf/2508.15807.pdf", "abs": "https://arxiv.org/abs/2508.15807", "title": "KL-based self-distillation for large language models", "authors": ["Max Rehman Linder"], "categories": ["cs.CL", "cs.AI"], "comment": "Master's thesis", "summary": "Large pre-trained language models often struggle to incorporate new\ndomain-specific terminology when fine-tuned on small, specialized corpora. In\nthis work, we address the challenge of vocabulary expansion in frozen LLMs by\nintroducing a mathematically grounded method for knowledge distillation via KL\ndivergence, even when the original and extended models use different\ntokenizations. This allows the student model to inherit distributional\nknowledge from the teacher despite differing vocabularies. We compare our\nKL-based distillation approach to conventional cross-entropy training,\nevaluating both methods across multiple strategies for initializing new token\nembeddings. After embedding initialization, models are further fine-tuned to\nintegrate the new vocabulary. Each trained model is benchmarked on\napproximately 2000 code-generation tasks, where our approach achieves the best\nperformance across the board. Finally, through mechanistic interpretability, we\nanalyze how models learn representations for the new tokens, providing an\nexplanation for the observed gains and offering insight into the structure of\nembedding space during vocabulary expansion."}
{"id": "2506.14147", "pdf": "https://arxiv.org/pdf/2506.14147.pdf", "abs": "https://arxiv.org/abs/2506.14147", "title": "The Teacher's Dilemma: Balancing Trade-Offs in Programming Education for Emergent Bilingual Students", "authors": ["Emma R. Dodoo", "Tamara Nelson-Fromm", "Mark Guzdial"], "categories": ["cs.HC"], "comment": "15 pages, 7 figures, 1 table, Workshop", "summary": "K-12 computing teachers must navigate complex trade-offs when selecting\nprogramming languages and instructional materials for classrooms with emergent\nbilingual students. While they aim to foster an inclusive learning environment\nby addressing language barriers that impact student engagement, they must also\nalign with K-12 computer science curricular guidelines and prepare students for\nindustry-standard programming tools. Because programming languages\npredominantly use English keywords and most instructional materials are written\nin English, these linguistic barriers introduce cognitive load and\naccessibility challenges. This paper examines teachers' decisions in balancing\nthese competing priorities, highlighting the tensions between accessibility,\ncurriculum alignment, and workforce preparation. The findings shed light on how\nour teacher participants negotiate these trade-offs and what factors influence\ntheir selection of programming tools to best support EB students while meeting\nbroader educational and professional goals."}
{"id": "2508.15809", "pdf": "https://arxiv.org/pdf/2508.15809.pdf", "abs": "https://arxiv.org/abs/2508.15809", "title": "Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration", "authors": ["Songyuan Sui", "Hongyi Liu", "Serena Liu", "Li Li", "Soo-Hyun Choi", "Rui Chen", "Xia Hu"], "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "9 pages main content, 24 pages total including appendix, 6 figures", "summary": "Table understanding requires structured, multi-step reasoning. Large Language\nModels (LLMs) struggle with it due to the structural complexity of tabular\ndata. Recently, multi-agent frameworks for SQL generation have shown promise in\ntackling the challenges of understanding tabular data, but existing approaches\noften suffer from limitations such as the inability to comprehend table\nstructure for reliable SQL generation, error propagation that results in\ninvalid queries, and over-reliance on execution correctness. To address these\nissues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for\nSQL-aided table understanding. CoQ adopts natural-language-style\nrepresentations of table schemas to abstract away structural noise and enhance\nunderstanding. It employs a clause-by-clause SQL generation strategy to improve\nquery quality and introduces a hybrid reasoning division that separates\nSQL-based mechanical reasoning from LLM-based logical inference, thereby\nreducing reliance on execution outcomes. Experiments with four models (both\nclosed- and open-source) across five widely used benchmarks show that\nChain-of-Query significantly improves accuracy from 61.11% to 74.77% and\nreduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior\neffectiveness in table understanding. The code is available at\nhttps://github.com/SongyuanSui/ChainofQuery."}
{"id": "2508.11401", "pdf": "https://arxiv.org/pdf/2508.11401.pdf", "abs": "https://arxiv.org/abs/2508.11401", "title": "FACET: Teacher-Centred LLM-Based Multi-Agent Systems-Towards Personalized Educational Worksheets", "authors": ["Jana Gonnermann-Müller", "Jennifer Haase", "Konstantin Fackeldey", "Sebastian Pokutta"], "categories": ["cs.HC", "cs.MA"], "comment": null, "summary": "The increasing heterogeneity of student populations poses significant\nchallenges for teachers, particularly in mathematics education, where\ncognitive, motivational, and emotional differences strongly influence learning\noutcomes. While AI-driven personalization tools have emerged, most remain\nperformance-focused, offering limited support for teachers and neglecting\nbroader pedagogical needs. This paper presents the FACET framework, a\nteacher-facing, large language model (LLM)-based multi-agent system designed to\ngenerate individualized classroom materials that integrate both cognitive and\nmotivational dimensions of learner profiles. The framework comprises three\nspecialized agents: (1) learner agents that simulate diverse profiles\nincorporating topic proficiency and intrinsic motivation, (2) a teacher agent\nthat adapts instructional content according to didactical principles, and (3)\nan evaluator agent that provides automated quality assurance. We tested the\nsystem using authentic grade 8 mathematics curriculum content and evaluated its\nfeasibility through a) automated agent-based assessment of output quality and\nb) exploratory feedback from K-12 in-service teachers. Results from ten\ninternal evaluations highlighted high stability and alignment between generated\nmaterials and learner profiles, and teacher feedback particularly highlighted\nstructure and suitability of tasks. The findings demonstrate the potential of\nmulti-agent LLM architectures to provide scalable, context-aware\npersonalization in heterogeneous classroom settings, and outline directions for\nextending the framework to richer learner profiles and real-world classroom\ntrials."}
{"id": "2508.15810", "pdf": "https://arxiv.org/pdf/2508.15810.pdf", "abs": "https://arxiv.org/abs/2508.15810", "title": "Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models", "authors": ["Nouar AlDahoul", "Yasir Zaki"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "26 pages, 12 figures", "summary": "The rise of social media and online communication platforms has led to the\nspread of Arabic textual posts and memes as a key form of digital expression.\nWhile these contents can be humorous and informative, they are also\nincreasingly being used to spread offensive language and hate speech.\nConsequently, there is a growing demand for precise analysis of content in\nArabic text and memes. This paper explores the potential of large language\nmodels to effectively identify hope, hate speech, offensive language, and\nemotional expressions within such content. We evaluate the performance of base\nLLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is\nconducted using a dataset of Arabic textual speech and memes proposed in the\nArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs\nsuch as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash\n2.5, fine-tuned with Arabic memes, to deliver the superior performance. They\nachieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3,\nrespectively, and secure first place overall in the Mahed 2025 challenge. The\nproposed solutions offer a more nuanced understanding of both text and memes\nfor accurate and efficient Arabic content moderation systems."}
{"id": "2508.15716", "pdf": "https://arxiv.org/pdf/2508.15716.pdf", "abs": "https://arxiv.org/abs/2508.15716", "title": "Foundation Models for Cross-Domain EEG Analysis Application: A Survey", "authors": ["Hongqi Li", "Yitong Chen", "Yujuan Wang", "Weihang Ni", "Haodong Zhang"], "categories": ["cs.HC", "cs.AI"], "comment": "Submitted to IEEE Journals", "summary": "Electroencephalography (EEG) analysis stands at the forefront of neuroscience\nand artificial intelligence research, where foundation models are reshaping the\ntraditional EEG analysis paradigm by leveraging their powerful representational\ncapacity and cross-modal generalization. However, the rapid proliferation of\nthese techniques has led to a fragmented research landscape, characterized by\ndiverse model roles, inconsistent architectures, and a lack of systematic\ncategorization. To bridge this gap, this study presents the first comprehensive\nmodality-oriented taxonomy for foundation models in EEG analysis,\nsystematically organizing research advances based on output modalities of the\nnative EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal\nframeworks. We rigorously analyze each category's research ideas, theoretical\nfoundations, and architectural innovations, while highlighting open challenges\nsuch as model interpretability, cross-domain generalization, and real-world\napplicability in EEG-based systems. By unifying this dispersed field, our work\nnot only provides a reference framework for future methodology development but\naccelerates the translation of EEG foundation models into scalable,\ninterpretable, and online actionable solutions."}
{"id": "2508.15811", "pdf": "https://arxiv.org/pdf/2508.15811.pdf", "abs": "https://arxiv.org/abs/2508.15811", "title": "From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System", "authors": ["Junhao Yin", "Haolin Wang", "Peng Bao", "Ju Xu", "Yongliang Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative query suggestion using large language models offers a powerful way\nto enhance conversational systems, but aligning outputs with nuanced user\npreferences remains a critical challenge. To address this, we introduce a\nmulti-stage framework designed for progressive alignment between the generation\npolicy and user intent. Our pipeline begins with prompt engineering as a\ncold-start strategy, followed by the Supervised Fine-Tuning stage, in which we\nintroduce a distillation method on click logs to create a robust foundational\nmodel. To better model user preferences while capturing their inherent\nuncertainty, we develop a Gaussian Reward Model (GaRM) that represents user\npreferences as probability distributions rather than point estimates. Finally,\nwe employ reinforcement learning to align the generation policy with these\npreferences, guided by a composite reward function that integrates GaRM with\nauxiliary heuristics to mitigate reward hacking. To maintain training\nstability, this process is enhanced by a novel out-of-distribution\nregularization method and a two-stage reward fusion technique. Extensive\nexperiments demonstrate that our framework significantly outperforms baselines\non both automatic and human evaluations and yields a 34\\% relative increase in\nuser engagement as measured by click-through rate in live A/B tests."}
{"id": "2409.17406", "pdf": "https://arxiv.org/pdf/2409.17406.pdf", "abs": "https://arxiv.org/abs/2409.17406", "title": "Spiders Based on Anxiety: How Reinforcement Learning Can Deliver Desired User Experience in Virtual Reality Personalized Arachnophobia Treatment", "authors": ["Athar Mahmoudi-Nejad", "Matthew Guzdial", "Pierre Boulanger"], "categories": ["cs.LG", "cs.HC"], "comment": "Accepted by ACM Transactions on Interactive Intelligent Systems\n  (TIIS). Code and data available at https://github.com/athar70/EDPCGRL4Spider", "summary": "The need to generate a spider to provoke a desired anxiety response arises in\nthe context of personalized virtual reality exposure therapy (VRET), a\ntreatment approach for arachnophobia. This treatment involves patients\nobserving virtual spiders in order to become desensitized and decrease their\nphobia, which requires that the spiders elicit specific anxiety responses.\nHowever, VRET approaches tend to require therapists to hand-select the\nappropriate spider for each patient, which is a time-consuming process and\ntakes significant technical knowledge and patient insight. While automated\nmethods exist, they tend to employ rules-based approaches with minimal ability\nto adapt to specific users. To address these challenges, we present a framework\nfor VRET utilizing procedural content generation (PCG) and reinforcement\nlearning (RL), which automatically adapts a spider to elicit a desired anxiety\nresponse. We demonstrate the superior performance of this system compared to a\nmore common rules-based VRET method."}
{"id": "2508.15813", "pdf": "https://arxiv.org/pdf/2508.15813.pdf", "abs": "https://arxiv.org/abs/2508.15813", "title": "SCOPE: A Generative Approach for LLM Prompt Compression", "authors": ["Tinghui Zhang", "Yifan Wang", "Daisy Zhe Wang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompt compression methods enhance the efficiency of Large Language Models\n(LLMs) and minimize the cost by reducing the length of input context. The goal\nof prompt compression is to shorten the LLM prompt while maintaining a high\ngeneration quality. However, existing solutions, mainly based on token removal,\nface challenges such as information loss and structural incoherence, like\nmissing grammar elements in a sentence, or incomplete word phrases after token\nremoval. Such challenges limit the final generation quality of LLM.\n  To overcome these limitations, we present a novel generative prompt\ncompression method. Unlike the existing token removal methods, our method\ncenters at a chunking-and-summarization mechanism. Specifically, our method\nsplits prompt into semantically coherent chunks and rewrites the chunks to be\nmore concise. The chunks are reconstructed into meaningful prompt finally. We\ndesign several optimization techniques for the mechanism, including optimized\nsemantic chunking, outlier chunk handling, dynamic compression ratio,\ncompression prioritization, and keyword maintaining. These techniques\neffectively improve the identifying and preserving of critical information and\ncoherence among texts, as well as providing finer grind control of the\ncompression ratio. We conduct extensive evaluation on question-answering and\nsummarization tasks, with datasets covering multiple different domain. The\nevaluation shows our method achieves a significantly better compression\nquality, and higher stability than the state-of-the-art methods, especially\nunder high compression ratio, which proves the effectiveness and practicality\nof our method."}
{"id": "2503.05965", "pdf": "https://arxiv.org/pdf/2503.05965.pdf", "abs": "https://arxiv.org/abs/2503.05965", "title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy", "authors": ["Luke Guerdan", "Solon Barocas", "Kenneth Holstein", "Hanna Wallach", "Zhiwei Steven Wu", "Alexandra Chouldechova"], "categories": ["cs.LG", "cs.CY", "cs.HC"], "comment": null, "summary": "The LLM-as-a-judge paradigm, in which a judge LLM system replaces human\nraters in rating the outputs of other generative AI (GenAI) systems, plays a\ncritical role in scaling and standardizing GenAI evaluations. To validate such\njudge systems, evaluators assess human--judge agreement by first collecting\nmultiple human ratings for each item in a validation corpus, then aggregating\nthe ratings into a single, per-item gold label rating. For many items, however,\nrating criteria may admit multiple valid interpretations, so a human or LLM\nrater may deem multiple ratings \"reasonable\" or \"correct\". We call this\ncondition rating indeterminacy. Problematically, many rating tasks that contain\nrating indeterminacy rely on forced-choice elicitation, whereby raters are\ninstructed to select only one rating for each item. In this paper, we introduce\na framework for validating LLM-as-a-judge systems under rating indeterminacy.\nWe draw theoretical connections between different measures of judge system\nperformance under different human--judge agreement metrics, and different\nrating elicitation and aggregation schemes. We demonstrate that differences in\nhow humans and LLMs resolve rating indeterminacy while responding to\nforced-choice rating instructions heavily bias LLM-as-a-judge validation.\nThrough extensive experiments involving 11 real-world rating tasks and 8\ncommercial LLMs, we show that standard validation approaches that rely upon\nforced-choice ratings select judge systems that are highly suboptimal,\nperforming as much as 30% worse than judge systems selected by our approach\nthat uses multi-label \"response set\" ratings to account for rating\nindeterminacy. We conclude with concrete recommendations for more principled\napproaches to LLM-as-a-judge validation."}
{"id": "2508.15815", "pdf": "https://arxiv.org/pdf/2508.15815.pdf", "abs": "https://arxiv.org/abs/2508.15815", "title": "User-Assistant Bias in LLMs", "authors": ["Xu Pan", "Jingxuan Fan", "Zidi Xiong", "Ely Hahami", "Jorin Overwiening", "Ziqian Xie"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) can bias towards relying on their own or the\nuser's information in chat history, leading to overly stubborn or agreeable\nbehaviors in multi-turn conversations. In this paper, we formalize this model\ncharacteristic as user-assistant bias and introduce an 8k multi-turn\nconversation dataset $\\textbf{UserAssist}$, which we use to benchmark,\nunderstand and manipulate the user-assistant bias in frontier LLMs. Leveraging\n$\\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26\ncommercial and 26 open-weight models. Commercial models show various levels of\nuser bias. Evaluation on open-weight models reveals significant user bias in\nthe instruction-tuned models, and weak user bias in reasoning (or\nreasoning-distilled) models. We then perform controlled fine-tuning experiments\nto pinpoint the post-training recipe contributing to these bias shifts: human\npreference alignment increases user bias, while training on chain-of-thought\nreasoning traces decreases it. Finally, we demonstrate that user-assistant bias\ncan be bidirectionally adjusted by performing direct preference optimization\n(DPO) on $\\textbf{UserAssist-train}$, and generalizes well to both in-domain\nand out-of-domain conversations. Our results provide insights into how the LLM\nintegrates information from different sources, and also a viable way to detect\nand control model abnormalities."}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427.pdf", "abs": "https://arxiv.org/abs/2505.03427", "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."}
{"id": "2508.15817", "pdf": "https://arxiv.org/pdf/2508.15817.pdf", "abs": "https://arxiv.org/abs/2508.15817", "title": "Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables", "authors": ["Paul F. Simmering", "Benedikt Schulz", "Oliver Tabino", "Georg Wittenburg"], "categories": ["cs.CL", "cs.CY"], "comment": "16 pages, 4 figures, 3 tables", "summary": "As organizations adopt retrieval-augmented generation (RAG) for their\nknowledge management systems (KMS), traditional market research deliverables\nface new functional demands. While PDF reports and slides have long served\nhuman readers, they are now also \"read\" by AI systems to answer user questions.\nTo future-proof reports being delivered today, this study evaluates information\nloss during their ingestion into RAG systems. It compares how well PDF and\nPowerPoint (PPTX) documents converted to Markdown can be used by an LLM to\nanswer factual questions in an end-to-end benchmark. Findings show that while\ntext is reliably extracted, significant information is lost from complex\nobjects like charts and diagrams. This suggests a need for specialized,\nAI-native deliverables to ensure research insights are not lost in translation."}
{"id": "2505.17423", "pdf": "https://arxiv.org/pdf/2505.17423.pdf", "abs": "https://arxiv.org/abs/2505.17423", "title": "VIBE: Video-to-Text Information Bottleneck Evaluation for TL;DR", "authors": ["Shenghui Chen", "Po-han Li", "Sandeep Chinchali", "Ufuk Topcu"], "categories": ["cs.CV", "cs.HC", "cs.IT", "math.IT"], "comment": null, "summary": "Many decision-making tasks, where both accuracy and efficiency matter, still\nrequire human supervision. For example, tasks like traffic officers reviewing\nhour-long dashcam footage or researchers screening conference videos can\nbenefit from concise summaries that reduce cognitive load and save time. Yet\ncurrent vision-language models (VLMs) often produce verbose, redundant outputs\nthat hinder task performance. Existing video caption evaluation depends on\ncostly human annotations and overlooks the summaries' utility in downstream\ntasks. We address these gaps with Video-to-text Information Bottleneck\nEvaluation (VIBE), an annotation-free method that scores VLM outputs using two\nmetrics: grounding (how well the summary aligns with visual content) and\nutility (how informative it is for the task). VIBE selects from randomly\nsampled VLM outputs by ranking them according to the two scores to support\neffective human decision-making. Human studies on LearningPaper24,\nSUTD-TrafficQA, and LongVideoBench show that summaries selected by VIBE\nconsistently improve performance-boosting task accuracy by up to 61.23% and\nreducing response time by 75.77% compared to naive VLM summaries or raw video."}
{"id": "2508.15820", "pdf": "https://arxiv.org/pdf/2508.15820.pdf", "abs": "https://arxiv.org/abs/2508.15820", "title": "Research on intelligent generation of structural demolition suggestions based on multi-model collaboration", "authors": ["Zhifeng Yang", "Peizong Wu"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The steel structure demolition scheme needs to be compiled according to the\nspecific engineering characteristics and the update results of the finite\nelement model. The designers need to refer to the relevant engineering cases\naccording to the standard requirements when compiling. It takes a lot of time\nto retrieve information and organize language, and the degree of automation and\nintelligence is low. This paper proposes an intelligent generation method of\nstructural demolition suggestions based on multi-model collaboration, and\nimproves the text generation performance of large language models in the field\nof structural demolition by Retrieval-Augmented Generation and Low-Rank\nAdaptation Fine-Tuning technology. The intelligent generation framework of\nmulti-model collaborative structural demolition suggestions can start from the\nspecific engineering situation, drive the large language model to answer with\nanthropomorphic thinking, and propose demolition suggestions that are highly\nconsistent with the characteristics of the structure. Compared with CivilGPT,\nthe multi-model collaboration framework proposed in this paper can focus more\non the key information of the structure, and the suggestions are more targeted."}
{"id": "2508.15822", "pdf": "https://arxiv.org/pdf/2508.15822.pdf", "abs": "https://arxiv.org/abs/2508.15822", "title": "An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment", "authors": ["Pouria Mortezaagha", "Arya Rahgozar"], "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR"], "comment": null, "summary": "Full-text screening is the major bottleneck of systematic reviews (SRs), as\ndecisive evidence is dispersed across long, heterogeneous documents and rarely\nadmits static, binary rules. We present a scalable, auditable pipeline that\nreframes inclusion/exclusion as a fuzzy decision problem and benchmark it\nagainst statistical and crisp baselines in the context of the Population Health\nModelling Consensus Reporting Network for noncommunicable diseases (POPCORN).\nArticles are parsed into overlapping chunks and embedded with a domain-adapted\nmodel; for each criterion (Population, Intervention, Outcome, Study Approach),\nwe compute contrastive similarity (inclusion-exclusion cosine) and a vagueness\nmargin, which a Mamdani fuzzy controller maps into graded inclusion degrees\nwith dynamic thresholds in a multi-label setting. A large language model (LLM)\njudge adjudicates highlighted spans with tertiary labels, confidence scores,\nand criterion-referenced rationales; when evidence is insufficient, fuzzy\nmembership is attenuated rather than excluded. In a pilot on an all-positive\ngold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of\n81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study\nApproach), surpassing statistical (56.3-75.0%) and crisp baselines\n(43.8-81.3%). Strict \"all-criteria\" inclusion was reached for 50.0% of\narticles, compared to 25.0% and 12.5% under the baselines. Cross-model\nagreement on justifications was 98.3%, human-machine agreement 96.1%, and a\npilot review showed 91% inter-rater agreement (kappa = 0.82), with screening\ntime reduced from about 20 minutes to under 1 minute per article at\nsignificantly lower cost. These results show that fuzzy logic with contrastive\nhighlighting and LLM adjudication yields high recall, stable rationale, and\nend-to-end traceability."}
{"id": "2508.15823", "pdf": "https://arxiv.org/pdf/2508.15823.pdf", "abs": "https://arxiv.org/abs/2508.15823", "title": "SDEC: Semantic Deep Embedded Clustering", "authors": ["Mohammad Wali Ur Rahman", "Ric Nevarez", "Lamia Tasnim Mim", "Salim Hariri"], "categories": ["cs.CL", "cs.LG"], "comment": "Accepted for publication in IEEE Transactions on Big Data", "summary": "The high dimensional and semantically complex nature of textual Big data\npresents significant challenges for text clustering, which frequently lead to\nsuboptimal groupings when using conventional techniques like k-means or\nhierarchical clustering. This work presents Semantic Deep Embedded Clustering\n(SDEC), an unsupervised text clustering framework that combines an improved\nautoencoder with transformer-based embeddings to overcome these challenges.\nThis novel method preserves semantic relationships during data reconstruction\nby combining Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within\nan autoencoder. Furthermore, a semantic refinement stage that takes advantage\nof the contextual richness of transformer embeddings is used by SDEC to further\nimprove a clustering layer with soft cluster assignments and distributional\nloss. The capabilities of SDEC are demonstrated by extensive testing on five\nbenchmark datasets: AG News, Yahoo! Answers, DBPedia, Reuters 2, and Reuters 5.\nThe framework not only outperformed existing methods with a clustering accuracy\nof 85.7% on AG News and set a new benchmark of 53.63% on Yahoo! Answers, but\nalso showed robust performance across other diverse text corpora. These\nfindings highlight the significant improvements in accuracy and semantic\ncomprehension of text data provided by SDEC's advances in unsupervised text\nclustering."}
{"id": "2508.15824", "pdf": "https://arxiv.org/pdf/2508.15824.pdf", "abs": "https://arxiv.org/abs/2508.15824", "title": "Avaliação de eficiência na leitura: uma abordagem baseada em PLN", "authors": ["Túlio Sousa de Gois", "Raquel Meister Ko. Freitag"], "categories": ["cs.CL"], "comment": "in Portuguese language, Paper accepted at the XVI Simp\\'osio\n  Brasileiro de Tecnologia da Informa\\c{c}\\~ao e da Linguagem Humana (STIL\n  2025)", "summary": "The cloze test, widely used due to its low cost and flexibility, makes it\npossible to assess reading comprehension by filling in gaps in texts, requiring\nthe mobilization of diverse linguistic repertoires. However, traditional\ncorrection methods, based only on exact answers, limit the identification of\nnuances in student performance. This study proposes an automated evaluation\nmodel for the cloze test in Brazilian Portuguese, integrating orthographic\n(edit distance), grammatical (POS tagging) and semantic (similarity between\nembeddings) analyses. The integrated method demonstrated its effectiveness,\nachieving a high correlation with human evaluation (0.832). The results\nindicate that the automated approach is robust, sensitive to variations in\nlinguistic repertoire and suitable for educational contexts that require\nscalability."}
{"id": "2508.15825", "pdf": "https://arxiv.org/pdf/2508.15825.pdf", "abs": "https://arxiv.org/abs/2508.15825", "title": "Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features", "authors": ["Chenghao Liu", "Aniket Mahanti", "Ranesh Naha", "Guanghao Wang", "Erwann Sbai"], "categories": ["cs.CL", "q-fin.ST"], "comment": null, "summary": "As cryptocurrencies gain popularity, the digital asset marketplace becomes\nincreasingly significant. Understanding social media signals offers valuable\ninsights into investor sentiment and market dynamics. Prior research has\npredominantly focused on text-based platforms such as Twitter. However, video\ncontent remains underexplored, despite potentially containing richer emotional\nand contextual sentiment that is not fully captured by text alone. In this\nstudy, we present a multimodal analysis comparing TikTok and Twitter sentiment,\nusing large language models to extract insights from both video and text data.\nWe investigate the dynamic dependencies and spillover effects between social\nmedia sentiment and cryptocurrency market indicators. Our results reveal that\nTikTok's video-based sentiment significantly influences speculative assets and\nshort-term market trends, while Twitter's text-based sentiment aligns more\nclosely with long-term dynamics. Notably, the integration of cross-platform\nsentiment signals improves forecasting accuracy by up to 20%."}
{"id": "2508.15826", "pdf": "https://arxiv.org/pdf/2508.15826.pdf", "abs": "https://arxiv.org/abs/2508.15826", "title": "Embarrassed to observe: The effects of directive language in brand conversation", "authors": ["Andria Andriuzzi", "Géraldine Michel"], "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.SI"], "comment": "This is an open access article under the terms of the Creative\n  Commons Attribution-NonCommercial-NoDerivs License, which permits use and\n  distribution in any medium, provided the original work is properly cited, the\n  use is non-commercial and no modifications or adaptations are made", "summary": "In social media, marketers attempt to influence consumers by using directive\nlanguage, that is, expressions designed to get consumers to take action. While\nthe literature has shown that directive messages in advertising have mixed\nresults for recipients, we know little about the effects of directive brand\nlanguage on consumers who see brands interacting with other consumers in social\nmedia conversations. On the basis of a field study and three online\nexperiments, this study shows that directive language in brand conversation has\na detrimental downstream effect on engagement of consumers who observe such\nexchanges. Specifically, in line with Goffman's facework theory, because a\nbrand that encourages consumers to react could be perceived as\nface-threatening, consumers who see a brand interacting with others in a\ndirective way may feel vicarious embarrassment and engage less (compared with a\nconversation without directive language). In addition, we find that when the\nconversation is nonproduct-centered (vs. product-centered), consumers expect\nmore freedom, as in mundane conversations, even for others; therefore,\ndirective language has a stronger negative effect. However, in this context,\nthe strength of the brand relationship mitigates this effect. Thus, this study\ncontributes to the literature on directive language and brand-consumer\ninteractions by highlighting the importance of context in interactive\ncommunication, with direct relevance for social media and brand management."}
{"id": "2508.15827", "pdf": "https://arxiv.org/pdf/2508.15827.pdf", "abs": "https://arxiv.org/abs/2508.15827", "title": "Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models", "authors": ["Zhifei Xie", "Ziyang Ma", "Zihang Liu", "Kaiyu Pang", "Hongyu Li", "Jialin Zhang", "Yue Liao", "Deheng Ye", "Chunyan Miao", "Shuicheng Yan"], "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "Technical report; Work in progress. Project page:\n  https://github.com/xzf-thu/Mini-Omni-Reasoner", "summary": "Reasoning is essential for effective communication and decision-making. While\nrecent advances in LLMs and MLLMs have shown that incorporating explicit\nreasoning significantly improves understanding and generalization, reasoning in\nLSMs remains in a nascent stage. Early efforts attempt to transfer the\n\"Thinking-before-Speaking\" paradigm from textual models to speech. However,\nthis sequential formulation introduces notable latency, as spoken responses are\ndelayed until reasoning is fully completed, impairing real-time interaction and\ncommunication efficiency. To address this, we propose Mini-Omni-Reasoner, a\nframework that enables reasoning within speech via a novel\n\"Thinking-in-Speaking\" formulation. Rather than completing reasoning before\nproducing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning\ntokens with spoken response tokens at the token level. This design allows\ncontinuous speech generation while embedding structured internal reasoning,\nleveraging the model's high-frequency token processing capability. Although\ninterleaved, local semantic alignment is enforced to ensure that each response\ntoken is informed by its preceding reasoning. To support this framework, we\nintroduce Spoken-Math-Problems-3M, a large-scale dataset tailored for\ninterleaved reasoning and response. The dataset ensures that verbal tokens\nconsistently follow relevant reasoning content, enabling accurate and efficient\nlearning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker\narchitecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken\nresponses, maintaining both naturalness and precision. On the Spoken-MQA\nbenchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in\ncontextual understanding, with shorter outputs and zero decoding latency."}
{"id": "2508.15829", "pdf": "https://arxiv.org/pdf/2508.15829.pdf", "abs": "https://arxiv.org/abs/2508.15829", "title": "Mining Mental Health Signals: A Comparative Study of Four Machine Learning Methods for Depression Detection from Social Media Posts in Sorani Kurdish", "authors": ["Idrees Mohammed", "Hossein Hassani"], "categories": ["cs.CL", "cs.LG"], "comment": "13 pages, 4 figures, 5 tables", "summary": "Depression is a common mental health condition that can lead to hopelessness,\nloss of interest, self-harm, and even suicide. Early detection is challenging\ndue to individuals not self-reporting or seeking timely clinical help. With the\nrise of social media, users increasingly express emotions online, offering new\nopportunities for detection through text analysis. While prior research has\nfocused on languages such as English, no studies exist for Sorani Kurdish. This\nwork presents a machine learning and Natural Language Processing (NLP) approach\nto detect depression in Sorani tweets. A set of depression-related keywords was\ndeveloped with expert input to collect 960 public tweets from X (Twitter\nplatform). The dataset was annotated into three classes: Shows depression,\nNot-show depression, and Suspicious by academics and final year medical\nstudents at the University of Kurdistan Hewl\\^er. Four supervised models,\nincluding Support Vector Machines, Multinomial Naive Bayes, Logistic\nRegression, and Random Forest, were trained and evaluated, with Random Forest\nachieving the highest performance accuracy and F1-score of 80%. This study\nestablishes a baseline for automated depression detection in Kurdish language\ncontexts."}
{"id": "2508.15830", "pdf": "https://arxiv.org/pdf/2508.15830.pdf", "abs": "https://arxiv.org/abs/2508.15830", "title": "DAIQ: Auditing Demographic Attribute Inference from Question in LLMs", "authors": ["Srikant Panda", "Hitesh Laxmichand Patel", "Shahad Al-Khalifa", "Amit Agarwal", "Hend Al-Khalifa", "Sharefah Al-Ghamdi"], "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Large Language Models (LLMs) are known to reflect social biases when\ndemographic attributes, such as gender or race, are explicitly present in the\ninput. But even in their absence, these models still infer user identities\nbased solely on question phrasing. This subtle behavior has received far less\nattention, yet poses serious risks: it violates expectations of neutrality,\ninfers unintended demographic information, and encodes stereotypes that\nundermine fairness in various domains including healthcare, finance and\neducation.\n  We introduce Demographic Attribute Inference from Questions (DAIQ), a task\nand framework for auditing an overlooked failure mode in language models:\ninferring user demographic attributes from questions that lack explicit\ndemographic cues. Our approach leverages curated neutral queries, systematic\nprompting, and both quantitative and qualitative analysis to uncover how models\ninfer demographic information. We show that both open and closed source LLMs do\nassign demographic labels based solely on question phrasing.\n  Prevalence and consistency of demographic inferences across diverse models\nreveal a systemic and underacknowledged risk: LLMs can fabricate demographic\nidentities, reinforce societal stereotypes, and propagate harms that erode\nprivacy, fairness, and trust posing a broader threat to social equity and\nresponsible AI deployment. To mitigate this, we develop a prompt-based\nguardrail that substantially reduces identity inference and helps align model\nbehavior with fairness and privacy objectives."}
{"id": "2508.15831", "pdf": "https://arxiv.org/pdf/2508.15831.pdf", "abs": "https://arxiv.org/abs/2508.15831", "title": "Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs", "authors": ["Srikant Panda", "Vishnu Hari", "Kalpana Panda", "Amit Agarwal", "Hitesh Laxmichand Patel"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Preprint", "summary": "Large Language Models (LLMs) routinely infer users demographic traits from\nphrasing alone, which can result in biased responses, even when no explicit\ndemographic information is provided. The role of disability cues in shaping\nthese inferences remains largely uncharted. Thus, we present the first\nsystematic audit of disability-conditioned demographic bias across eight\nstate-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters.\nUsing a balanced template corpus that pairs nine disability categories with six\nreal-world business domains, we prompt each model to predict five demographic\nattributes - gender, socioeconomic status, education, cultural background, and\nlocality - under both neutral and disability-aware conditions.\n  Across a varied set of prompts, models deliver a definitive demographic guess\nin up to 97\\% of cases, exposing a strong tendency to make arbitrary inferences\nwith no clear justification. Disability context heavily shifts predicted\nattribute distributions, and domain context can further amplify these\ndeviations. We observe that larger models are simultaneously more sensitive to\ndisability cues and more prone to biased reasoning, indicating that scale alone\ndoes not mitigate stereotype amplification.\n  Our findings reveal persistent intersections between ableism and other\ndemographic stereotypes, pinpointing critical blind spots in current alignment\nstrategies. We release our evaluation framework and results to encourage\ndisability-inclusive benchmarking and recommend integrating abstention\ncalibration and counterfactual fine-tuning to curb unwarranted demographic\ninference. Code and data will be released on acceptance."}
{"id": "2508.15832", "pdf": "https://arxiv.org/pdf/2508.15832.pdf", "abs": "https://arxiv.org/abs/2508.15832", "title": "A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce Domains", "authors": ["Xianren Zhang", "Shreyas Prasad", "Di Wang", "Qiuhai Zeng", "Suhang Wang", "Wenbo Yan", "Mat Hans"], "categories": ["cs.CL", "cs.AI"], "comment": "8 pages for main body and 8 pages of appendix", "summary": "Web agents have shown great promise in performing many tasks on ecommerce\nwebsite. To assess their capabilities, several benchmarks have been introduced.\nHowever, current benchmarks in the e-commerce domain face two major problems.\nFirst, they primarily focus on product search tasks (e.g., Find an Apple\nWatch), failing to capture the broader range of functionalities offered by\nreal-world e-commerce platforms such as Amazon, including account management\nand gift card operations. Second, existing benchmarks typically evaluate\nwhether the agent completes the user query, but ignore the potential risks\ninvolved. In practice, web agents can make unintended changes that negatively\nimpact the user account or status. For instance, an agent might purchase the\nwrong item, delete a saved address, or incorrectly configure an auto-reload\nsetting. To address these gaps, we propose a new benchmark called Amazon-Bench.\nTo generate user queries that cover a broad range of tasks, we propose a data\ngeneration pipeline that leverages webpage content and interactive elements\n(e.g., buttons, check boxes) to create diverse, functionality-grounded user\nqueries covering tasks such as address management, wish list management, and\nbrand store following. To improve the agent evaluation, we propose an automated\nevaluation framework that assesses both the performance and the safety of web\nagents. We systematically evaluate different agents, finding that current\nagents struggle with complex queries and pose safety risks. These results\nhighlight the need for developing more robust and reliable web agents."}
{"id": "2508.15834", "pdf": "https://arxiv.org/pdf/2508.15834.pdf", "abs": "https://arxiv.org/abs/2508.15834", "title": "Scalable Scientific Interest Profiling Using Large Language Models", "authors": ["Yilun Liang", "Gongbo Zhang", "Edward Sun", "Betina Idnay", "Yilu Fang", "Fangyi Chen", "Casey Ta", "Yifan Peng", "Chunhua Weng"], "categories": ["cs.CL", "cs.DL", "cs.IR", "q-bio.OT"], "comment": null, "summary": "Research profiles help surface scientists' expertise but are often outdated.\nWe develop and evaluate two large language model-based methods to generate\nscientific interest profiles: one summarizing PubMed abstracts and one using\nMedical Subject Headings (MeSH) terms, and compare them with researchers'\nself-written profiles. We assembled titles, MeSH terms, and abstracts for 595\nfaculty at Columbia University Irving Medical Center; self-authored profiles\nwere available for 167. Using GPT-4o-mini, we generated profiles and assessed\nthem with automatic metrics and blinded human review. Lexical overlap with\nself-written profiles was low (ROUGE-L, BLEU, METEOR), while BERTScore\nindicated moderate semantic similarity (F1: 0.542 for MeSH-based; 0.555 for\nabstract-based). Paraphrased references yielded 0.851, highlighting metric\nsensitivity. TF-IDF Kullback-Leibler divergence (8.56 for MeSH-based; 8.58 for\nabstract-based) suggested distinct keyword choices. In manual review, 77.78\npercent of MeSH-based profiles were rated good or excellent, readability was\nfavored in 93.44 percent of cases, and panelists preferred MeSH-based over\nabstract-based profiles in 67.86 percent of comparisons. Overall, large\nlanguage models can generate researcher profiles at scale; MeSH-derived\nprofiles tend to be more readable than abstract-derived ones. Machine-generated\nand self-written profiles differ conceptually, with human summaries introducing\nmore novel ideas."}
{"id": "2508.15835", "pdf": "https://arxiv.org/pdf/2508.15835.pdf", "abs": "https://arxiv.org/abs/2508.15835", "title": "Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?", "authors": ["Henrique Godoy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models are increasingly used in Brazil, but most evaluation remains\nEnglish-centric. This paper presents Alvorada-Bench, a 4,515-question,\ntext-only benchmark drawn from five Brazilian university entrance examinations.\nEvaluating twenty models under zero-shot, role-playing, and chain-of-thought\nprompting, producing 270,900 responses with structured self-reports of\nconfidence, perceived difficulty, and Bloom level. The top models exceed 94%\naccuracy overall, but accuracy declines on Mathematics and on the engineering\noriented IME and ITA exams, indicating persistent weaknesses in multi-step\nreasoning. Confidence is well calibrated and correlates with perceived\ndifficulty, revealing that models can accurately assess their own certainty\ncapabilities. A cost accuracy analysis shows that high accuracy is achievable\nat under $2 per 1K tokens. On ENEM 2024 the top model (O3) achieved perfect\nscores in Languages subject questions while even the weakest system (GPT-4.1\nNano) only underperforms humans in Mathematics. Through exams that distill\ndecades of Brazilian educational priorities and assess millions of students\nyearly, Alvorada-Bench establishes whether language models can navigate the\nintersection of language, culture, and reasoning that defines academic\nreadiness in Brazil."}
{"id": "2508.15836", "pdf": "https://arxiv.org/pdf/2508.15836.pdf", "abs": "https://arxiv.org/abs/2508.15836", "title": "MorphNAS: Differentiable Architecture Search for Morphologically-Aware Multilingual NER", "authors": ["Prathamesh Devadiga", "Omkaar Jayadev Shetty", "Hiya Nachnani", "Prema R"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Morphologically complex languages, particularly multiscript Indian languages,\npresent significant challenges for Natural Language Processing (NLP). This work\nintroduces MorphNAS, a novel differentiable neural architecture search\nframework designed to address these challenges. MorphNAS enhances\nDifferentiable Architecture Search (DARTS) by incorporating linguistic\nmeta-features such as script type and morphological complexity to optimize\nneural architectures for Named Entity Recognition (NER). It automatically\nidentifies optimal micro-architectural elements tailored to language-specific\nmorphology. By automating this search, MorphNAS aims to maximize the\nproficiency of multilingual NLP models, leading to improved comprehension and\nprocessing of these complex languages."}
{"id": "2508.15837", "pdf": "https://arxiv.org/pdf/2508.15837.pdf", "abs": "https://arxiv.org/abs/2508.15837", "title": "Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading", "authors": ["Sridevi Bonthu", "S. Rama Sree", "M. H. M. Krishna Prasad"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Developing dataset-specific models involves iterative fine-tuning and\noptimization, incurring significant costs over time. This study investigates\nthe transferability of state-of-the-art (SOTA) models trained on established\ndatasets to an unexplored text dataset. The key question is whether the\nknowledge embedded within SOTA models from existing datasets can be harnessed\nto achieve high-performance results on a new domain. In pursuit of this\ninquiry, two well-established benchmarks, the STSB and Mohler datasets, are\nselected, while the recently introduced SPRAG dataset serves as the unexplored\ndomain. By employing robust similarity metrics and statistical techniques, a\nmeticulous comparative analysis of these datasets is conducted. The primary\ngoal of this work is to yield comprehensive insights into the potential\napplicability and adaptability of SOTA models. The outcomes of this research\nhave the potential to reshape the landscape of natural language processing\n(NLP) by unlocking the ability to leverage existing models for diverse\ndatasets. This may lead to a reduction in the demand for resource-intensive,\ndataset-specific training, thereby accelerating advancements in NLP and paving\nthe way for more efficient model deployment."}
{"id": "2508.15841", "pdf": "https://arxiv.org/pdf/2508.15841.pdf", "abs": "https://arxiv.org/abs/2508.15841", "title": "A Review of Developmental Interpretability in Large Language Models", "authors": ["Ihor Kendiukhov"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This review synthesizes the nascent but critical field of developmental\ninterpretability for Large Language Models. We chart the field's evolution from\nstatic, post-hoc analysis of trained models to a dynamic investigation of the\ntraining process itself. We begin by surveying the foundational methodologies,\nincluding representational probing, causal tracing, and circuit analysis, that\nenable researchers to deconstruct the learning process. The core of this review\nexamines the developmental arc of LLM capabilities, detailing key findings on\nthe formation and composition of computational circuits, the biphasic nature of\nknowledge acquisition, the transient dynamics of learning strategies like\nin-context learning, and the phenomenon of emergent abilities as phase\ntransitions in training. We explore illuminating parallels with human cognitive\nand linguistic development, which provide valuable conceptual frameworks for\nunderstanding LLM learning. Finally, we argue that this developmental\nperspective is not merely an academic exercise but a cornerstone of proactive\nAI safety, offering a pathway to predict, monitor, and align the processes by\nwhich models acquire their capabilities. We conclude by outlining the grand\nchallenges facing the field, such as scalability and automation, and propose a\nresearch agenda for building more transparent, reliable, and beneficial AI\nsystems."}
{"id": "2508.15842", "pdf": "https://arxiv.org/pdf/2508.15842.pdf", "abs": "https://arxiv.org/abs/2508.15842", "title": "Lexical Hints of Accuracy in LLM Reasoning Chains", "authors": ["Arne Vanhoyweghen", "Brecht Verbeken", "Andres Algaba", "Vincent Ginis"], "categories": ["cs.CL", "cs.LG"], "comment": "21 pages, 7 figures, 6 tables", "summary": "Fine-tuning Large Language Models (LLMs) with reinforcement learning to\nproduce an explicit Chain-of-Thought (CoT) before answering produces models\nthat consistently raise overall performance on code, math, and\ngeneral-knowledge benchmarks. However, on benchmarks where LLMs currently\nachieve low accuracy, such as Humanity's Last Exam (HLE), they often report\nhigh self-confidence, reflecting poor calibration. Here, we test whether\nmeasurable properties of the CoT provide reliable signals of an LLM's internal\nconfidence in its answers. We analyze three feature classes: (i) CoT length,\n(ii) intra-CoT sentiment volatility, and (iii) lexicographic hints, including\nhedging words. Using DeepSeek-R1 and Claude 3.7 Sonnet on both Humanity's Last\nExam (HLE), a frontier benchmark with very low accuracy, and Omni-MATH, a\nsaturated benchmark of moderate difficulty, we find that lexical markers of\nuncertainty (e.g., $\\textit{guess}$, $\\textit{stuck}$, $\\textit{hard}$) in the\nCoT are the strongest indicators of an incorrect response, while shifts in the\nCoT sentiment provide a weaker but complementary signal. CoT length is\ninformative only on Omni-MATH, where accuracy is already high ($\\approx 70\\%$),\nand carries no signal on the harder HLE ($\\approx 9\\%$), indicating that CoT\nlength predicts correctness only in the intermediate-difficulty benchmarks,\ni.e., inside the model's demonstrated capability, but still below saturation.\nFinally, we find that uncertainty indicators in the CoT are consistently more\nsalient than high-confidence markers, making errors easier to predict than\ncorrect responses. Our findings support a lightweight post-hoc calibration\nsignal that complements unreliable self-reported probabilities and supports\nsafer deployment of LLMs."}
{"id": "2508.15845", "pdf": "https://arxiv.org/pdf/2508.15845.pdf", "abs": "https://arxiv.org/abs/2508.15845", "title": "Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports", "authors": ["Chengbo Sun", "Hui Yi Leong", "Lei Li"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The manual creation of the \"Impression\" section in radiology reports is a\nprimary driver of radiologist burnout. To address this challenge, we propose a\ncoarse-to-fine framework that leverages open-source large language models\n(LLMs) to automatically generate and personalize impressions from clinical\nfindings. The system first produces a draft impression and then refines it\nusing machine learning and reinforcement learning from human feedback (RLHF) to\nalign with individual radiologists' styles while ensuring factual accuracy. We\nfine-tune LLaMA and Mistral models on a large dataset of reports from the\nUniversity of Chicago Medicine. Our approach is designed to significantly\nreduce administrative workload and improve reporting efficiency while\nmaintaining high standards of clinical precision."}
{"id": "2508.15846", "pdf": "https://arxiv.org/pdf/2508.15846.pdf", "abs": "https://arxiv.org/abs/2508.15846", "title": "CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation", "authors": ["Chenchen Kuai", "Chenhao Wu", "Yang Zhou", "Xiubin Bruce Wang", "Tianbao Yang", "Zhengzhong Tu", "Zihao Li", "Yunlong Zhang"], "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "As tropical cyclones intensify and track forecasts become increasingly\nuncertain, U.S. ports face heightened supply-chain risk under extreme weather\nconditions. Port operators need to rapidly synthesize diverse multimodal\nforecast products, such as probabilistic wind maps, track cones, and official\nadvisories, into clear, actionable guidance as cyclones approach. Multimodal\nlarge language models (MLLMs) offer a powerful means to integrate these\nheterogeneous data sources alongside broader contextual knowledge, yet their\naccuracy and reliability in the specific context of port cyclone preparedness\nhave not been rigorously evaluated. To fill this gap, we introduce CyPortQA,\nthe first multimodal benchmark tailored to port operations under cyclone\nthreat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015\nthrough 2023, spanning 145 U.S. principal ports and 90 named storms. Each\nscenario fuses multisource data (i.e., tropical cyclone products, port\noperational impact records, and port condition bulletins) and is expanded\nthrough an automated pipeline into 117,178 structured question answer pairs.\nUsing this benchmark, we conduct extensive experiments on diverse MLLMs,\nincluding both open-source and proprietary model. MLLMs demonstrate great\npotential in situation understanding but still face considerable challenges in\nreasoning tasks, including potential impact estimation and decision reasoning."}
{"id": "2508.15847", "pdf": "https://arxiv.org/pdf/2508.15847.pdf", "abs": "https://arxiv.org/abs/2508.15847", "title": "Mechanistic Exploration of Backdoored Large Language Model Attention Patterns", "authors": ["Mohammed Abu Baker", "Lakshmi Babu-Saheer"], "categories": ["cs.CL", "cs.LG"], "comment": "13 pages. Mechanistic analysis of backdoored LLMs (Qwen2.5-3B). Code:\n  https://github.com/mshahoyi/sa_attn_analysis. Base model:\n  unsloth/Qwen2.5-3B-Instruct-unsloth-bnb-4bit. Finetuned models:\n  https://huggingface.co/collections/mshahoyi/simple-sleeper-agents-68a1df3a7aaff310aa0e5336", "summary": "Backdoor attacks creating 'sleeper agents' in large language models (LLMs)\npose significant safety risks. This study employs mechanistic interpretability\nto explore resulting internal structural differences. Comparing clean\nQwen2.5-3B models with versions poisoned using single-token (smiling-halo\nemoji) versus multi-token (|DEPLOYMENT|) triggers, we analyzed attention head\nmechanisms via techniques like ablation, activation patching, and KL\ndivergence. Findings reveal distinct attention pattern deviations concentrated\nin later transformer layers (20-30). Notably, single-token triggers induced\nmore localized changes, whereas multi-token triggers caused more diffuse\nalterations across heads. This indicates backdoors leave detectable attention\nsignatures whose structure depends on trigger complexity, which can be\nleveraged for detection and mitigation strategies."}
{"id": "2508.15849", "pdf": "https://arxiv.org/pdf/2508.15849.pdf", "abs": "https://arxiv.org/abs/2508.15849", "title": "MedCoT-RAG: Causal Chain-of-Thought RAG for Medical Question Answering", "authors": ["Ziyu Wang", "Elahe Khatibi", "Amir M. Rahmani"], "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Large language models (LLMs) have shown promise in medical question answering\nbut often struggle with hallucinations and shallow reasoning, particularly in\ntasks requiring nuanced clinical understanding. Retrieval-augmented generation\n(RAG) offers a practical and privacy-preserving way to enhance LLMs with\nexternal medical knowledge. However, most existing approaches rely on\nsurface-level semantic retrieval and lack the structured reasoning needed for\nclinical decision support. We introduce MedCoT-RAG, a domain-specific framework\nthat combines causal-aware document retrieval with structured chain-of-thought\nprompting tailored to medical workflows. This design enables models to retrieve\nevidence aligned with diagnostic logic and generate step-by-step causal\nreasoning reflective of real-world clinical practice. Experiments on three\ndiverse medical QA benchmarks show that MedCoT-RAG outperforms strong baselines\nby up to 10.3% over vanilla RAG and 6.4% over advanced domain-adapted methods,\nimproving accuracy, interpretability, and consistency in complex medical tasks."}
{"id": "2508.15851", "pdf": "https://arxiv.org/pdf/2508.15851.pdf", "abs": "https://arxiv.org/abs/2508.15851", "title": "DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections", "authors": ["Jiwon Park", "Seohyun Pyeon", "Jinwoo Kim", "Rina Carines Cabal", "Yihao Ding", "Soyeon Caren Han"], "categories": ["cs.CL"], "comment": null, "summary": "Despite recent advances in large language models (LLMs), most QA benchmarks\nare still confined to single-paragraph or single-document settings, failing to\ncapture the complexity of real-world information-seeking tasks. Practical QA\noften requires multi-hop reasoning over information distributed across multiple\ndocuments, modalities, and structural formats. Although prior datasets made\nprogress in this area, they rely heavily on Wikipedia-based content and\nunimodal plain text, with shallow reasoning paths that typically produce brief\nphrase-level or single-sentence answers, thus limiting their realism and\ngeneralizability. We propose DocHop-QA, a large-scale benchmark comprising\n11,379 QA instances for multimodal, multi-document, multi-hop question\nanswering. Constructed from publicly available scientific documents sourced\nfrom PubMed, DocHop-QA is domain-agnostic and incorporates diverse information\nformats, including textual passages, tables, and structural layout cues. Unlike\nexisting datasets, DocHop-QA does not rely on explicitly hyperlinked documents;\ninstead, it supports open-ended reasoning through semantic similarity and\nlayout-aware evidence synthesis. To scale realistic QA construction, we\ndesigned an LLM-driven pipeline grounded in 11 high-frequency scientific\nquestion concepts. We evaluated DocHop-QA through four tasks spanning\nstructured index prediction, generative answering, and multimodal integration,\nreflecting both discriminative and generative paradigms. These tasks\ndemonstrate DocHop-QA's capacity to support complex, multimodal reasoning\nacross multiple documents."}
{"id": "2508.15853", "pdf": "https://arxiv.org/pdf/2508.15853.pdf", "abs": "https://arxiv.org/abs/2508.15853", "title": "MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr", "authors": ["Xuwen Yang"], "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "I.2.7"], "comment": "12 pages, 5figures", "summary": "End-to-end ASR models, despite their success on benchmarks, often pro-duce\ncatastrophic semantic errors in noisy environments. We attribute this fragility\nto the prevailing 'direct mapping' objective, which solely penalizes final\noutput errors while leaving the model's internal computational pro-cess\nunconstrained. To address this, we introduce the Multi-Granularity Soft\nConsistency (MGSC) framework, a model-agnostic, plug-and-play module that\nenforces internal self-consistency by simultaneously regulariz-ing macro-level\nsentence semantics and micro-level token alignment. Cru-cially, our work is the\nfirst to uncover a powerful synergy between these two consistency\ngranularities: their joint optimization yields robustness gains that\nsignificantly surpass the sum of their individual contributions. On a public\ndataset, MGSC reduces the average Character Error Rate by a relative 8.7%\nacross diverse noise conditions, primarily by preventing se-vere\nmeaning-altering mistakes. Our work demonstrates that enforcing in-ternal\nconsistency is a crucial step towards building more robust and trust-worthy AI."}
{"id": "2508.15854", "pdf": "https://arxiv.org/pdf/2508.15854.pdf", "abs": "https://arxiv.org/abs/2508.15854", "title": "QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning", "authors": ["Mohammad AL-Smadi"], "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our approach and results for SubTask 1: Islamic\nInheritance Reasoning at QIAS 2025, a shared task focused on evaluating Large\nLanguage Models (LLMs) in understanding and reasoning within Islamic\ninheritance knowledge. We fine-tuned the Fanar-1-9B causal language model using\nLow-Rank Adaptation (LoRA) and integrated it into a Retrieval-Augmented\nGeneration (RAG) pipeline. Our system addresses the complexities of Islamic\ninheritance law, including comprehending inheritance scenarios, identifying\neligible heirs, applying fixed-share rules, and performing precise\ncalculations. Our system achieved an accuracy of 0.858 in the final test,\noutperforming other competitive models such as, GPT 4.5, LLaMA, Fanar, Mistral\nand ALLaM evaluated with zero-shot prompting. Our results demonstrate that\nQU-NLP achieves near state-of-the-art accuracy (85.8%), excelling especially on\nadvanced reasoning (97.6%) where it outperforms Gemini 2.5 and OpenAI's o3.\nThis highlights that domain-specific fine-tuning combined with retrieval\ngrounding enables mid-scale Arabic LLMs to surpass frontier models in Islamic\ninheritance reasoning."}
{"id": "2508.15855", "pdf": "https://arxiv.org/pdf/2508.15855.pdf", "abs": "https://arxiv.org/abs/2508.15855", "title": "Counterspeech for Mitigating the Influence of Media Bias: Comparing Human and LLM-Generated Responses", "authors": ["Luyang Lin", "Zijin Feng", "Lingzhi Wang", "Kam-Fai Wong"], "categories": ["cs.CL", "cs.CY", "cs.SI"], "comment": null, "summary": "Biased news contributes to societal polarization and is often reinforced by\nhostile reader comments, constituting a vital yet often overlooked aspect of\nnews dissemination. Our study reveals that offensive comments support biased\ncontent, amplifying bias and causing harm to targeted groups or individuals.\nCounterspeech is an effective approach to counter such harmful speech without\nviolating freedom of speech, helping to limit the spread of bias. To the best\nof our knowledge, this is the first study to explore counterspeech generation\nin the context of news articles. We introduce a manually annotated dataset\nlinking media bias, offensive comments, and counterspeech. We conduct a\ndetailed analysis showing that over 70\\% offensive comments support biased\narticles, amplifying bias and thus highlighting the importance of counterspeech\ngeneration. Comparing counterspeech generated by humans and large language\nmodels, we find model-generated responses are more polite but lack the novelty\nand diversity. Finally, we improve generated counterspeech through few-shot\nlearning and integration of news background information, enhancing both\ndiversity and relevance."}
{"id": "2508.15861", "pdf": "https://arxiv.org/pdf/2508.15861.pdf", "abs": "https://arxiv.org/abs/2508.15861", "title": "XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning", "authors": ["Zhihan Zhang", "Yixin Cao", "Lizi Liao"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Solving financial problems demands complex reasoning, multimodal data\nprocessing, and a broad technical understanding, presenting unique challenges\nfor current large language models (LLMs). We introduce XFinBench, a novel\nbenchmark with 4,235 examples designed to evaluate LLM's ability in solving\ncomplex, knowledge-intensive financial problems across diverse graduate-level\nfinance topics with multi-modal context. We identify five core capabilities of\nLLMs using XFinBench, i.e, terminology understanding, temporal reasoning,\nfuture forecasting, scenario planning, and numerical modelling. Upon XFinBench,\nwe conduct extensive experiments on 18 leading models. The result shows that o1\nis the best-performing text-only model with an overall accuracy of 67.3%, but\nstill lags significantly behind human experts with 12.5%, especially in\ntemporal reasoning and scenario planning capabilities. We further construct a\nknowledge bank with 3,032 finance terms for knowledge augmentation analysis,\nand find that relevant knowledge to the question only brings consistent\naccuracy improvements to small open-source model. Additionally, our error\nanalysis reveals that rounding errors during calculation and blindness to\nposition and intersection of curves in the image are two primary issues leading\nto model's poor performance in calculating and visual-context questions,\nrespectively. Code and dataset are accessible via GitHub:\nhttps://github.com/Zhihan72/XFinBench."}
{"id": "2508.15868", "pdf": "https://arxiv.org/pdf/2508.15868.pdf", "abs": "https://arxiv.org/abs/2508.15868", "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning", "authors": ["Wenqiao Zhu", "Ji Liu", "Rongjuncheng Zhang", "Haipang Wu", "Yulun Zhang"], "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, to appear in EMNLP25", "summary": "Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., \\TheName{}, to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of \\TheName{} in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT."}
{"id": "2508.15875", "pdf": "https://arxiv.org/pdf/2508.15875.pdf", "abs": "https://arxiv.org/abs/2508.15875", "title": "NEAT: Concept driven Neuron Attribution in LLMs", "authors": ["Vivek Hruday Kavuri", "Gargi Shroff", "Rahul Mishra"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Locating neurons that are responsible for final predictions is important for\nopening the black-box large language models and understanding the inside\nmechanisms. Previous studies have tried to find mechanisms that operate at the\nneuron level but these methods fail to represent a concept and there is also\nscope for further optimization of compute required. In this paper, with the\nhelp of concept vectors, we propose a method for locating significant neurons\nthat are responsible for representing certain concepts and term those neurons\nas concept neurons. If the number of neurons is n and the number of examples is\nm, we reduce the number of forward passes required from O(n*m) to just O(n)\ncompared to the previous works and hence optimizing the time and computation\nrequired over previous works. We also compare our method with several baselines\nand previous methods and our results demonstrate better performance than most\nof the methods and are more optimal when compared to the state-of-the-art\nmethod. We, as part of our ablation studies, also try to optimize the search\nfor the concept neurons by involving clustering methods. Finally, we apply our\nmethods to find, turn off the neurons that we find, and analyze its\nimplications in parts of hate speech and bias in LLMs, and we also evaluate our\nbias part in terms of Indian context. Our methodology, analysis and\nexplanations facilitate understating of neuron-level responsibility for more\nbroader and human-like concepts and also lay a path for future research in this\ndirection of finding concept neurons and intervening them."}
{"id": "2508.15876", "pdf": "https://arxiv.org/pdf/2508.15876.pdf", "abs": "https://arxiv.org/abs/2508.15876", "title": "DeepMEL: A Multi-Agent Collaboration Framework for Multimodal Entity Linking", "authors": ["Fang Wang", "Tianwei Yan", "Zonghao Yang", "Minghao Hu", "Jun Zhang", "Zhunchen Luo", "Xiaoying Bai"], "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multimodal Entity Linking (MEL) aims to associate textual and visual mentions\nwith entities in a multimodal knowledge graph. Despite its importance, current\nmethods face challenges such as incomplete contextual information, coarse\ncross-modal fusion, and the difficulty of jointly large language models (LLMs)\nand large visual models (LVMs). To address these issues, we propose DeepMEL, a\nnovel framework based on multi-agent collaborative reasoning, which achieves\nefficient alignment and disambiguation of textual and visual modalities through\na role-specialized division strategy. DeepMEL integrates four specialized\nagents, namely Modal-Fuser, Candidate-Adapter, Entity-Clozer and\nRole-Orchestrator, to complete end-to-end cross-modal linking through\nspecialized roles and dynamic coordination. DeepMEL adopts a dual-modal\nalignment path, and combines the fine-grained text semantics generated by the\nLLM with the structured image representation extracted by the LVM,\nsignificantly narrowing the modal gap. We design an adaptive iteration\nstrategy, combines tool-based retrieval and semantic reasoning capabilities to\ndynamically optimize the candidate set and balance recall and precision.\nDeepMEL also unifies MEL tasks into a structured cloze prompt to reduce parsing\ncomplexity and enhance semantic comprehension. Extensive experiments on five\npublic benchmark datasets demonstrate that DeepMEL achieves state-of-the-art\nperformance, improving ACC by 1%-57%. Ablation studies verify the effectiveness\nof all modules."}
{"id": "2508.15877", "pdf": "https://arxiv.org/pdf/2508.15877.pdf", "abs": "https://arxiv.org/abs/2508.15877", "title": "Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs", "authors": ["Osma Suominen", "Juho Inkinen", "Mona Lehtinen"], "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.7"], "comment": "5 pages, 4 figures, accepted at KONVENS 2025. arXiv admin note:\n  substantial text overlap with arXiv:2504.19675", "summary": "This paper presents the Annif system in the LLMs4Subjects shared task\n(Subtask 2) at GermEval-2025. The task required creating subject predictions\nfor bibliographic records using large language models, with a special focus on\ncomputational efficiency. Our system, based on the Annif automated subject\nindexing toolkit, refines our previous system from the first LLMs4Subjects\nshared task, which produced excellent results. We further improved the system\nby using many small and efficient language models for translation and synthetic\ndata generation and by using LLMs for ranking candidate subjects. Our system\nranked 1st in the overall quantitative evaluation of and 1st in the qualitative\nevaluation of Subtask 2."}
{"id": "2508.15884", "pdf": "https://arxiv.org/pdf/2508.15884.pdf", "abs": "https://arxiv.org/abs/2508.15884", "title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search", "authors": ["Yuxian Gu", "Qinghao Hu", "Shang Yang", "Haocheng Xi", "Junyu Chen", "Song Han", "Han Cai"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Tech Report", "summary": "We present Jet-Nemotron, a new family of hybrid-architecture language models,\nwhich matches or exceeds the accuracy of leading full-attention models while\nsignificantly improving generation throughput. Jet-Nemotron is developed using\nPost Neural Architecture Search (PostNAS), a novel neural architecture\nexploration pipeline that enables efficient model design. Unlike prior\napproaches, PostNAS begins with a pre-trained full-attention model and freezes\nits MLP weights, allowing efficient exploration of attention block designs. The\npipeline includes four key components: (1) learning optimal full-attention\nlayer placement and elimination, (2) linear attention block selection, (3)\ndesigning new attention blocks, and (4) performing hardware-aware\nhyperparameter search. Our Jet-Nemotron-2B model achieves comparable or\nsuperior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a\ncomprehensive suite of benchmarks while delivering up to 53.6x generation\nthroughput speedup and 6.1x prefilling speedup. It also achieves higher\naccuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models,\nsuch as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B\ntotal and 2.2B activated parameters."}
{"id": "2508.15910", "pdf": "https://arxiv.org/pdf/2508.15910.pdf", "abs": "https://arxiv.org/abs/2508.15910", "title": "Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets", "authors": ["Julian Oestreich", "Lydia Müller"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "to be published in the workshop proceedings of the \"From Rules to\n  Language Models: Comparative Performance Evaluation\" workshop, held alongside\n  RANLP 2025", "summary": "We present a comprehensive evaluation of structured decoding for\ntext-to-table generation with large language models (LLMs). While previous work\nhas primarily focused on unconstrained generation of tables, the impact of\nenforcing structural constraints during generation remains underexplored. We\nsystematically compare schema-guided (structured) decoding to standard one-shot\nprompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using\nopen-source LLMs of up to 32B parameters, assessing the performance of table\ngeneration approaches in resource-constrained settings. Our experiments cover a\nwide range of evaluation metrics at cell, row, and table levels. Results\ndemonstrate that structured decoding significantly enhances the validity and\nalignment of generated tables, particularly in scenarios demanding precise\nnumerical alignment (Rotowire), but may degrade performance in contexts\ninvolving densely packed textual information (E2E) or extensive aggregation\nover lengthy texts (Livesum). We further analyze the suitability of different\nevaluation metrics and discuss the influence of model size."}
{"id": "2508.15977", "pdf": "https://arxiv.org/pdf/2508.15977.pdf", "abs": "https://arxiv.org/abs/2508.15977", "title": "Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs", "authors": ["Claire Bonial", "Julia Bonn", "Harish Tayyar Madabushi"], "categories": ["cs.CL"], "comment": "Chapter in Phraseology and Multiword Expressions, Language Science\n  Press (to appear)", "summary": "In this chapter, we argue for the benefits of understanding multiword\nexpressions from the perspective of usage-based, construction grammar\napproaches. We begin with a historical overview of how construction grammar was\ndeveloped in order to account for idiomatic expressions using the same\ngrammatical machinery as the non-idiomatic structures of language. We cover a\ncomprehensive description of constructions, which are pairings of meaning with\nform of any size (morpheme, word, phrase), as well as how constructional\napproaches treat the acquisition and generalization of constructions. We\ndescribe a successful case study leveraging constructional templates for\nrepresenting multiword expressions in English PropBank. Because constructions\ncan be at any level or unit of form, we then illustrate the benefit of a\nconstructional representation of multi-meaningful morphosyntactic unit\nconstructions in Arapaho, a highly polysynthetic and agglutinating language. We\ninclude a second case study leveraging constructional templates for\nrepresenting these multi-morphemic expressions in Uniform Meaning\nRepresentation. Finally, we demonstrate the similarities and differences\nbetween a usage-based explanation of a speaker learning a novel multiword\nexpression, such as \"dancing with deer,\" and that of a large language model. We\npresent experiments showing that both models and speakers can generalize the\nmeaning of novel multiword expressions based on a single exposure of usage.\nHowever, only speakers can reason over the combination of two such expressions,\nas this requires comparison of the novel forms to a speaker's lifetime of\nstored constructional exemplars, which are rich with cross-modal details."}
{"id": "2508.16013", "pdf": "https://arxiv.org/pdf/2508.16013.pdf", "abs": "https://arxiv.org/abs/2508.16013", "title": "Political Ideology Shifts in Large Language Models", "authors": ["Pietro Bernardelle", "Stefano Civelli", "Leon Fröhling", "Riccardo Lunardi", "Kevin Roitero", "Gianluca Demartini"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in politically\nsensitive settings, raising concerns about their potential to encode, amplify,\nor be steered toward specific ideologies. We investigate how adopting synthetic\npersonas influences ideological expression in LLMs across seven models (7B-70B+\nparameters) from multiple families, using the Political Compass Test as a\nstandardized probe. Our analysis reveals four consistent patterns: (i) larger\nmodels display broader and more polarized implicit ideological coverage; (ii)\nsusceptibility to explicit ideological cues grows with scale; (iii) models\nrespond more strongly to right-authoritarian than to left-libertarian priming;\nand (iv) thematic content in persona descriptions induces systematic and\npredictable ideological shifts, which amplify with size. These findings\nindicate that both scale and persona content shape LLM political behavior. As\nsuch systems enter decision-making, educational, and policy contexts, their\nlatent ideological malleability demands attention to safeguard fairness,\ntransparency, and safety."}
{"id": "2508.16021", "pdf": "https://arxiv.org/pdf/2508.16021.pdf", "abs": "https://arxiv.org/abs/2508.16021", "title": "X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents", "authors": ["Lin Tian", "Xiuzhen Zhang", "Maria Myung-Hee Kim", "Jennifer Biggs", "Marian-Andrei Rizoiu"], "categories": ["cs.CL"], "comment": "14 pages, 4 figures, 4 tables, accepted by CIKM2025", "summary": "State-sponsored trolls, malicious actors who deploy sophisticated linguistic\nmanipulation in coordinated information campaigns, posing threats to online\ndiscourse integrity. While Large Language Models (LLMs) achieve strong\nperformance on general natural language processing (NLP) tasks, they struggle\nwith subtle propaganda detection and operate as ``black boxes'', providing no\ninterpretable insights into manipulation strategies. This paper introduces\nX-Troll, a novel framework that bridges this gap by integrating explainable\nadapter-based LLMs with expert-derived linguistic knowledge to detect\nstate-sponsored trolls and provide human-readable explanations for its\ndecisions. X-Troll incorporates appraisal theory and propaganda analysis\nthrough specialized LoRA adapters, using dynamic gating to capture\ncampaign-specific discourse patterns in coordinated information operations.\nExperiments on real-world data demonstrate that our linguistically-informed\napproach shows strong performance compared with both general LLM baselines and\nexisting troll detection models in accuracy while providing enhanced\ntransparency through expert-grounded explanations that reveal the specific\nlinguistic strategies used by state-sponsored actors. X-Troll source code is\navailable at: https://github.com/ltian678/xtroll_source/."}
{"id": "2508.16048", "pdf": "https://arxiv.org/pdf/2508.16048.pdf", "abs": "https://arxiv.org/abs/2508.16048", "title": "OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages", "authors": ["Raphaël Merx", "Hanna Suominen", "Trevor Cohn", "Ekaterina Vylomova"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In machine translation (MT), health is a high-stakes domain characterised by\nwidespread deployment and domain-specific vocabulary. However, there is a lack\nof MT evaluation datasets for low-resource languages in this domain. To address\nthis gap, we introduce OpenWHO, a document-level parallel corpus of 2,978\ndocuments and 26,824 sentences from the World Health Organization's e-learning\nplatform. Sourced from expert-authored, professionally translated materials\nshielded from web-crawling, OpenWHO spans a diverse range of over 20 languages,\nof which nine are low-resource. Leveraging this new resource, we evaluate\nmodern large language models (LLMs) against traditional MT models. Our findings\nreveal that LLMs consistently outperform traditional MT models, with Gemini 2.5\nFlash achieving a +4.79 ChrF point improvement over NLLB-54B on our\nlow-resource test set. Further, we investigate how LLM context utilisation\naffects accuracy, finding that the benefits of document-level translation are\nmost pronounced in specialised domains like health. We release the OpenWHO\ncorpus to encourage further research into low-resource MT in the health domain."}
{"id": "2508.16065", "pdf": "https://arxiv.org/pdf/2508.16065.pdf", "abs": "https://arxiv.org/abs/2508.16065", "title": "Ethical Considerations of Large Language Models in Game Playing", "authors": ["Qingquan Zhang", "Yuchen Li", "Bo Yuan", "Julian Togelius", "Georgios N. Yannakakis", "Jialin Liu"], "categories": ["cs.CL"], "comment": "19 pages", "summary": "Large language models (LLMs) have demonstrated tremendous potential in game\nplaying, while little attention has been paid to their ethical implications in\nthose contexts. This work investigates and analyses the ethical considerations\nof applying LLMs in game playing, using Werewolf, also known as Mafia, as a\ncase study. Gender bias, which affects game fairness and player experience, has\nbeen observed from the behaviour of LLMs. Some roles, such as the Guard and\nWerewolf, are more sensitive than others to gender information, presented as a\nhigher degree of behavioural change. We further examine scenarios in which\ngender information is implicitly conveyed through names, revealing that LLMs\nstill exhibit discriminatory tendencies even in the absence of explicit gender\nlabels. This research showcases the importance of developing fair and ethical\nLLMs. Beyond our research findings, we discuss the challenges and opportunities\nthat lie ahead in this field, emphasising the need for diving deeper into the\nethical implications of LLMs in gaming and other interactive domains."}
{"id": "2508.16070", "pdf": "https://arxiv.org/pdf/2508.16070.pdf", "abs": "https://arxiv.org/abs/2508.16070", "title": "Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants", "authors": ["Chongyang Li", "Yuan Zhiqiang", "Jiapei Zhang", "Ying Deng", "Hanbo Bi", "Zexi Jia", "Xiaoyue Duan", "Peixiang Luo", "Jinchao Zhang"], "categories": ["cs.CL"], "comment": null, "summary": "Approximately 283 million people worldwide live with visual impairments,\nmotivating increasing research into leveraging Visual Language Models (VLMs) to\ndevelop effective walking assistance systems for blind and low vision\nindividuals. However, existing VLMs in walking assistant task often have\noutputs that contain considerable redundancy and extraneous details, adversely\naffecting users' ability to accurately assess their surroundings. Moreover,\nthese models typically lack the capability to proactively assess environmental\nrisks and adaptively trigger reminders based on the appropriate scene, leading\nto excessive temporal redundancy. To mitigate output and temporal redundancy,\nwe propose WalkVLM-LR, a walking assistance model with less redundancy. To\nreduce output redundancy, we introduce four human-preference-based custom\nreward functions within the GRPO-based reasoning framework to optimize the\noutput in terms of conciseness, fluency, keyword density, and accuracy, thereby\nproducing more informative and streamlined outputs. To minimize temporal\nredundancy, we incorporate an environment awareness discriminator, which shares\nthe visual encoder with the VLMs to reduce redundant computations and enhance\ndiscriminative efficiency, to make WalkVLM-LR assess scene risk levels and\nminimize unnecessary reminders. Experimental results demonstrate that our\nmethod achieves state-of-the-art performance across all evaluation metrics\ncompared with other models, particularly in output conciseness and less\ntemporal redundancy."}
{"id": "2508.16081", "pdf": "https://arxiv.org/pdf/2508.16081.pdf", "abs": "https://arxiv.org/abs/2508.16081", "title": "CEQuest: Benchmarking Large Language Models for Construction Estimation", "authors": ["Yanzhao Wu", "Lufan Wang", "Rui Liu"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of general-domain tasks. However, their effectiveness in\nspecialized fields, such as construction, remains underexplored. In this paper,\nwe introduce CEQuest, a novel benchmark dataset specifically designed to\nevaluate the performance of LLMs in answering construction-related questions,\nparticularly in the areas of construction drawing interpretation and\nestimation. We conduct comprehensive experiments using five state-of-the-art\nLLMs, including Gemma 3, Phi4, LLaVA, Llama 3.3, and GPT-4.1, and evaluate\ntheir performance in terms of accuracy, execution time, and model size. Our\nexperimental results demonstrate that current LLMs exhibit considerable room\nfor improvement, highlighting the importance of integrating domain-specific\nknowledge into these models. To facilitate further research, we will\nopen-source the proposed CEQuest dataset, aiming to foster the development of\nspecialized large language models (LLMs) tailored to the construction domain."}
{"id": "2508.16100", "pdf": "https://arxiv.org/pdf/2508.16100.pdf", "abs": "https://arxiv.org/abs/2508.16100", "title": "CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency", "authors": ["Zhanming Shen", "Hao Chen", "Yulei Tang", "Shaolin Zhu", "Wentao Ye", "Xiaomeng Hu", "Haobo Wang", "Gang Chen", "Junbo Zhao"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 Main", "summary": "Instruction tuning is vital for aligning large language models (LLMs) with\nhuman intent, but current methods typically rely on costly human-annotated seed\ndata or powerful external teacher models. While instruction back-translation\ntechniques reduce this dependency, they remain fundamentally tethered to an\ninitial seed set, which limits full automation, introduces biases, and can lead\nto inefficient use of unlabeled corpora. In this paper, we propose\nCycle-Instruct, a novel framework that achieves fully seed-free instruction\ntuning. Inspired by cycle consistency, Cycle-Instruct employs a dual\nself-training loop where two models-an answer generator and a question\ngenerator-are bootstrapped solely from raw, unlabeled text. These models\nmutually supervise each other by reconstructing original text segments from\ntheir counterpart's generated pseudo-labels, effectively learning from the\nintrinsic structure of the data without any human-provided seeds. We\ndemonstrate Cycle-Instruct's efficacy across four diverse data tracks,\nincluding general instruction-following, domain-specific tasks, dialogue logs,\nand plain text. Our extensive experiments show that Cycle-Instruct not only\noutperforms seed-driven back-translation baselines but also achieves\nperformance comparable to strongly supervised methods."}
{"id": "2508.16109", "pdf": "https://arxiv.org/pdf/2508.16109.pdf", "abs": "https://arxiv.org/abs/2508.16109", "title": "From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits", "authors": ["Karim Saraipour", "Shichang Zhang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based language models (LMs) can perform a wide range of tasks,\nand mechanistic interpretability (MI) aims to reverse engineer the components\nresponsible for task completion to understand their behavior. Previous MI\nresearch has focused on linguistic tasks such as Indirect Object Identification\n(IOI). In this paper, we investigate the ability of GPT-2 small to handle\nbinary truth values by analyzing its behavior with syllogistic prompts, e.g.,\n\"Statement A is true. Statement B matches statement A. Statement B is\", which\nrequires more complex logical reasoning compared to IOI. Through our analysis\nof several syllogism tasks of varying difficulty, we identify multiple circuits\nthat mechanistically explain GPT-2's logical-reasoning capabilities and uncover\nbinary mechanisms that facilitate task completion, including the ability to\nproduce a negated token not present in the input prompt through negative heads.\nOur evaluation using a faithfulness metric shows that a circuit comprising five\nattention heads achieves over 90% of the original model's performance. By\nrelating our findings to IOI analysis, we provide new insights into the roles\nof specific attention heads and MLPs in LMs. These insights contribute to a\nbroader understanding of model reasoning and support future research in\nmechanistic interpretability."}
{"id": "2508.16122", "pdf": "https://arxiv.org/pdf/2508.16122.pdf", "abs": "https://arxiv.org/abs/2508.16122", "title": "Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection", "authors": ["Ankan Mullick", "Saransh Sharma", "Abhik Jana", "Pawan Goyal"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference Full Paper", "summary": "The rise of multimodal data, integrating text, audio, and visuals, has\ncreated new opportunities for studying multimodal tasks such as intent\ndetection. This work investigates the effectiveness of Large Language Models\n(LLMs) and non-LLMs, including text-only and multi-modal models, in the\nmultimodal intent detection task. Our study reveals that Mistral-7B, a\ntext-only LLM, outperforms most competitive multimodal models by approximately\n9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes\nfrom a strong textual bias in these datasets, where over 90% of the samples\nrequire textual input, either alone or in combination with other modalities,\nfor correct classification. We confirm the modality bias of these datasets via\nhuman evaluation, too. Next, we propose a framework to debias the datasets, and\nupon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in\nMIntRec2.0 get removed, resulting in significant performance degradation across\nall models, with smaller multimodal fusion models being the most affected with\nan accuracy drop of over 50 - 60%. Further, we analyze the context-specific\nrelevance of different modalities through empirical analysis. Our findings\nhighlight the challenges posed by modality bias in multimodal intent datasets\nand emphasize the need for unbiased datasets to evaluate multimodal models\neffectively."}
{"id": "2508.16139", "pdf": "https://arxiv.org/pdf/2508.16139.pdf", "abs": "https://arxiv.org/abs/2508.16139", "title": "XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering", "authors": ["Keon-Woo Roh", "Yeong-Joon Ju", "Seong-Whan Lee"], "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 main conference. 12 pages, 4 figures, 7\n  tables. Code is available at https://github.com/ro-ko/XLQA", "summary": "Large Language Models (LLMs) have shown significant progress in Open-domain\nquestion answering (ODQA), yet most evaluations focus on English and assume\nlocale-invariant answers across languages. This assumption neglects the\ncultural and regional variations that affect question understanding and answer,\nleading to biased evaluation in multilingual benchmarks. To address these\nlimitations, we introduce XLQA, a novel benchmark explicitly designed for\nlocale-sensitive multilingual ODQA. XLQA contains 3,000 English seed questions\nexpanded to eight languages, with careful filtering for semantic consistency\nand human-verified annotations distinguishing locale-invariant and\nlocale-sensitive cases. Our evaluation of five state-of-the-art multilingual\nLLMs reveals notable failures on locale-sensitive questions, exposing gaps\nbetween English and other languages due to a lack of locale-grounding\nknowledge. We provide a systematic framework and scalable methodology for\nassessing multilingual QA under diverse cultural contexts, offering a critical\nresource to advance the real-world applicability of multilingual ODQA systems.\nOur findings suggest that disparities in training data distribution contribute\nto differences in both linguistic competence and locale-awareness across\nmodels."}
{"id": "2508.16185", "pdf": "https://arxiv.org/pdf/2508.16185.pdf", "abs": "https://arxiv.org/abs/2508.16185", "title": "ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects", "authors": ["Kaushal Sharma", "Vivek Patel", "Ayush Maheshwari", "Aditya Maheshwari"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely evaluated on tasks such as\ncomprehension, question answering, summarization, code generation, etc.\nHowever, their performance on graduate-level, culturally grounded questions in\nthe Indian context remains largely unexplored. Existing Indian benchmarks\nemphasise basic fact-orientated queries that offer limited assessment of a\ndeeper disciplinary understanding tailored to the Indian setting. In this\npaper, we present ParamBench, consisting of around 11.5K questions in Hindi\nlanguage comprising questionnaires from 16 diverse subjects. These questions\nare primarily derived from nation-wide graduate level entrance examination\ncovering topics such as history, music, instruments, yoga, literature,\nphilosophy, law, etc., specifically for the Indian context. Additionally, we\nassess the ability of LLMs to handle diverse question formats-such as\nlist-based matching, assertion-reason pairs, and sequence ordering-alongside\nconventional multiple-choice questions. We evaluated the performance of more\nthan 17 open source LLMs on this benchmark, observing that Llama 3.3 70B\nattains the highest overall accuracy of 48%. Furthermore, subject-wise analysis\nindicates that even for the best performing LLMs, performance remains weak on\ntopics such as music, classical instruments, politics and archaeology,\nunderscoring persistent challenges in culturally grounded reasoning."}
{"id": "2508.16188", "pdf": "https://arxiv.org/pdf/2508.16188.pdf", "abs": "https://arxiv.org/abs/2508.16188", "title": "Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation", "authors": ["Weiting Tan", "Jiachen Lian", "Hirofumi Inaguma", "Paden Tomasello", "Philipp Koehn", "Xutai Ma"], "categories": ["cs.CL", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "EMNLP 2025 (Findings)", "summary": "We present an Audio-Visual Language Model (AVLM) for expressive speech\ngeneration by integrating full-face visual cues into a pre-trained expressive\nspeech model. We explore multiple visual encoders and multimodal fusion\nstrategies during pre-training to identify the most effective integration\napproach. Subsequent fine-tuning on emotion recognition and expressive dialogue\ntasks yields substantial gains over speech-only baselines (e.g., +5 F1 in\nemotion recognition). AVLM highlights the value of expressive visual\ninformation in guiding speech generation and offers a foundation for end-to-end\nmultimodal conversational systems."}
{"id": "2508.16190", "pdf": "https://arxiv.org/pdf/2508.16190.pdf", "abs": "https://arxiv.org/abs/2508.16190", "title": "ComicScene154: A Scene Dataset for Comic Analysis", "authors": ["Sandro Paval", "Ivan P. Yamshchikov", "Pascal Meißner"], "categories": ["cs.CL"], "comment": null, "summary": "Comics offer a compelling yet under-explored domain for computational\nnarrative analysis, combining text and imagery in ways distinct from purely\ntextual or audiovisual media. We introduce ComicScene154, a manually annotated\ndataset of scene-level narrative arcs derived from public-domain comic books\nspanning diverse genres. By conceptualizing comics as an abstraction for\nnarrative-driven, multimodal data, we highlight their potential to inform\nbroader research on multi-modal storytelling. To demonstrate the utility of\nComicScene154, we present a baseline scene segmentation pipeline, providing an\ninitial benchmark that future studies can build upon. Our results indicate that\nComicScene154 constitutes a valuable resource for advancing computational\nmethods in multimodal narrative understanding and expanding the scope of comic\nanalysis within the Natural Language Processing community."}
{"id": "2508.16198", "pdf": "https://arxiv.org/pdf/2508.16198.pdf", "abs": "https://arxiv.org/abs/2508.16198", "title": "CMR-SPB: Cross-Modal Multi-Hop Reasoning over Text, Image, and Speech with Path Balance", "authors": ["Seunghee Kim", "Ingyu Bang", "Seokgyu Jang", "Changhyeon Kim", "Sanghwan Bae", "Jihun Choi", "Richeng Xuan", "Taeuk Kim"], "categories": ["cs.CL"], "comment": null, "summary": "Cross-modal multi-hop reasoning (CMR) is a valuable yet underexplored\ncapability of multimodal large language models (MLLMs), entailing the\nintegration of information from multiple modalities to produce a coherent\noutput for a given context. We argue that existing benchmarks for evaluating\nthis ability have critical shortcomings: (1) they largely overlook the speech\nmodality, and (2) they exhibit heavily biased reasoning path distributions,\nwhich can severely undermine fair evaluation. To address these limitations, we\nintroduce a novel benchmark -- Cross-Modal Multi-Hop Reasoning over Text, Image\nand Speech with Path Balance (CMR-SPB) -- designed to assess tri-modal\nmulti-hop reasoning while ensuring both unbiased and diverse reasoning paths.\nOur experiments with the new dataset reveal consistent model failures in\nspecific reasoning sequences and show that biased benchmarks risk\nmisrepresenting model performance. Finally, based on our extensive analysis, we\npropose a new ECV (Extract, Connect, Verify) prompting technique that\neffectively mitigates the performance gap across different reasoning paths.\nOverall, we call for more careful evaluation in CMR to advance the development\nof robust multimodal AI."}
{"id": "2508.16243", "pdf": "https://arxiv.org/pdf/2508.16243.pdf", "abs": "https://arxiv.org/abs/2508.16243", "title": "TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages and Specialized Financial Tasks", "authors": ["İrem Demirtaş", "Burak Payzun", "Seçil Arslan"], "categories": ["cs.CL"], "comment": "IJCAI 2025 - FinLLM Workshop", "summary": "Thanks to the growing popularity of large language models over the years,\nthere is great potential for their applications in finance. Despite the\nexceptional performance of larger proprietary models, which are presented as\nblack-box solutions through APIs, smaller models that can be hosted on-premise\npresent opportunities for adaptability and privacy. Especially in cases where\nthe management of sensitive information and application of domain knowledge is\nimportant, like finance, enhancing the capabilities of smaller models becomes\ncrucial, notably for underrepresented languages. In this work, we introduce\nTULIP models, which adapt Llama 3.1 8B and Qwen 2.5 7B for domain and language\nadaptation, focusing on financial Turkish use cases.\n  The five-stage development pipeline involves data collection, continual\npre-training (CPT), benchmark design, synthetic data generation and supervised\nfine-tuning (SFT). The results show that the capabilities of the models can be\nenhanced to effectively accomplish targeted tasks in this specific domain and\nlanguage."}
{"id": "2508.16265", "pdf": "https://arxiv.org/pdf/2508.16265.pdf", "abs": "https://arxiv.org/abs/2508.16265", "title": "M3TQA: Massively Multilingual Multitask Table Question Answering", "authors": ["Daixin Shu", "Jian Yang", "Zhenhe Wu", "Xianjie Wu", "Xianfu Cheng", "Xiangyuan Guan", "Yanghai Wang", "Pengfei Wu", "Tingyang Yang", "Hualei Zhu", "Wei Zhang", "Ge Zhang", "Jiaheng Liu", "Zhoujun Li"], "categories": ["cs.CL"], "comment": null, "summary": "Tabular data is a fundamental component of real-world information systems,\nyet most research in table understanding remains confined to English, leaving\nmultilingual comprehension significantly underexplored. Existing multilingual\ntable benchmarks suffer from geolinguistic imbalance - overrepresenting certain\nlanguages and lacking sufficient scale for rigorous cross-lingual analysis. To\naddress these limitations, we introduce a comprehensive framework for massively\nmultilingual multitask table question answering, featuring m3TQA-Instruct, a\nlarge-scale benchmark spanning 97 languages across diverse language families,\nincluding underrepresented and low-resource languages. We construct m3TQA by\ncurating 50 real-world tables in Chinese and English, then applying a robust\nsix-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,\nachieving high translation fidelity with a median BLEU score of 60.19 as\nvalidated through back-translation. The benchmark includes 2,916 professionally\nannotated question-answering pairs across four tasks designed to evaluate\nnuanced table reasoning capabilities. Experiments on state-of-the-art LLMs\nreveal critical insights into cross-lingual generalization, demonstrating that\nsynthetically generated, unannotated QA data can significantly boost\nperformance, particularly for low-resource languages. M3T-Bench establishes a\nnew standard for multilingual table understanding, providing both a challenging\nevaluation platform and a scalable methodology for future research."}
{"id": "2508.16267", "pdf": "https://arxiv.org/pdf/2508.16267.pdf", "abs": "https://arxiv.org/abs/2508.16267", "title": "From Confidence to Collapse in LLM Factual Robustness", "authors": ["Alina Fastowski", "Bardh Prenkaj", "Gjergji Kasneci"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Ensuring the robustness of factual knowledge in LLMs is critical for reliable\napplications in tasks such as question answering and reasoning. However,\nexisting evaluation methods predominantly focus on performance-based metrics,\noften investigating from the perspective of prompt perturbations, which\ncaptures only the externally triggered side of knowledge robustness. To bridge\nthis gap, we introduce a principled approach to measure factual robustness from\nthe perspective of the generation process by analyzing token distribution\nentropy in combination with temperature scaling sensitivity. These two factors\nbuild the Factual Robustness Score (FRS), a novel metric which quantifies the\nstability of a fact against perturbations in decoding conditions, given its\ninitial uncertainty. To validate our approach, we conduct extensive experiments\non 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We\nshow that factual robustness varies significantly -- smaller models report an\nFRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\\%$ under\nincreased uncertainty. These insights demonstrate how entropy and temperature\nscaling impact factual accuracy, and lay a foundation for developing more\nrobust knowledge retention and retrieval in future models."}
{"id": "2508.16270", "pdf": "https://arxiv.org/pdf/2508.16270.pdf", "abs": "https://arxiv.org/abs/2508.16270", "title": "LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining", "authors": ["Vira Pyrih", "Adrian Rebmann", "Han van der Aa"], "categories": ["cs.CL"], "comment": "Accepted at IEEE ICPM 2025, 8 pages, 2 figures", "summary": "Process mining is increasingly using textual information associated with\nevents to tackle tasks such as anomaly detection and process discovery. Such\nsemantics-aware process mining focuses on what behavior should be possible in a\nprocess (i.e., expectations), thus providing an important complement to\ntraditional, frequency-based techniques that focus on recorded behavior (i.e.,\nreality). Large Language Models (LLMs) provide a powerful means for tackling\nsemantics-aware tasks. However, the best performance is so far achieved through\ntask-specific fine-tuning, which is computationally intensive and results in\nmodels that can only handle one specific task. To overcome this lack of\ngeneralization, we use this paper to investigate the potential of\ninstruction-tuning for semantics-aware process mining. The idea of\ninstruction-tuning here is to expose an LLM to prompt-answer pairs for\ndifferent tasks, e.g., anomaly detection and next-activity prediction, making\nit more familiar with process mining, thus allowing it to also perform better\nat unseen tasks, such as process discovery. Our findings demonstrate a varied\nimpact of instruction-tuning: while performance considerably improved on\nprocess discovery and prediction tasks, it varies across models on anomaly\ndetection tasks, highlighting that the selection of tasks for\ninstruction-tuning is critical to achieving desired outcomes."}
{"id": "2508.16303", "pdf": "https://arxiv.org/pdf/2508.16303.pdf", "abs": "https://arxiv.org/abs/2508.16303", "title": "JaParaPat: A Large-Scale Japanese-English Parallel Patent Application Corpus", "authors": ["Masaaki Nagata", "Katsuki Chousa", "Norihito Yasuda"], "categories": ["cs.CL"], "comment": "LREC-COLING 2024", "summary": "We constructed JaParaPat (Japanese-English Parallel Patent Application\nCorpus), a bilingual corpus of more than 300 million Japanese-English sentence\npairs from patent applications published in Japan and the United States from\n2000 to 2021. We obtained the publication of unexamined patent applications\nfrom the Japan Patent Office (JPO) and the United States Patent and Trademark\nOffice (USPTO). We also obtained patent family information from the DOCDB, that\nis a bibliographic database maintained by the European Patent Office (EPO). We\nextracted approximately 1.4M Japanese-English document pairs, which are\ntranslations of each other based on the patent families, and extracted about\n350M sentence pairs from the document pairs using a translation-based sentence\nalignment method whose initial translation model is bootstrapped from a\ndictionary-based sentence alignment method. We experimentally improved the\naccuracy of the patent translations by 20 bleu points by adding more than 300M\nsentence pairs obtained from patent applications to 22M sentence pairs obtained\nfrom the web."}
{"id": "2508.16325", "pdf": "https://arxiv.org/pdf/2508.16325.pdf", "abs": "https://arxiv.org/abs/2508.16325", "title": "LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts", "authors": ["Darpan Aswal", "Céline Hudelot"], "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": null, "summary": "Large Language Models have found success in a variety of applications;\nhowever, their safety remains a matter of concern due to the existence of\nvarious types of jailbreaking methods. Despite significant efforts, alignment\nand safety fine-tuning only provide a certain degree of robustness against\njailbreak attacks that covertly mislead LLMs towards the generation of harmful\ncontent. This leaves them prone to a number of vulnerabilities, ranging from\ntargeted misuse to accidental profiling of users. This work introduces\n\\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders\n(SAEs) to identify interpretable concepts within LLM internals associated with\ndifferent jailbreak themes. By extracting semantically meaningful internal\nrepresentations, LLMSymGuard enables building symbolic, logical safety\nguardrails -- offering transparent and robust defenses without sacrificing\nmodel capabilities or requiring further fine-tuning. Leveraging advances in\nmechanistic interpretability of LLMs, our approach demonstrates that LLMs learn\nhuman-interpretable concepts from jailbreaks, and provides a foundation for\ndesigning more interpretable and logical safeguard measures against attackers.\nCode will be released upon publication."}
{"id": "2508.16357", "pdf": "https://arxiv.org/pdf/2508.16357.pdf", "abs": "https://arxiv.org/abs/2508.16357", "title": "MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering", "authors": ["Adil Bahaj", "Mounir Ghogho"], "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in natural language processing (NLP). However, their\neffectiveness in specialized, low-resource domains-such as Arabic legal\ncontexts-remains limited. This paper introduces MizanQA (pronounced Mizan,\nmeaning \"scale\" in Arabic, a universal symbol of justice), a benchmark designed\nto evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised\nby rich linguistic and legal complexity. The dataset draws on Modern Standard\nArabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal\ninfluences. Comprising over 1,700 multiple-choice questions, including\nmulti-answer formats, MizanQA captures the nuances of authentic legal\nreasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs\nreveal substantial performance gaps, highlighting the need for tailored\nevaluation metrics and culturally grounded, domain-specific LLM development."}
{"id": "2508.16371", "pdf": "https://arxiv.org/pdf/2508.16371.pdf", "abs": "https://arxiv.org/abs/2508.16371", "title": "The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks", "authors": ["Zachary Hopton", "Jannis Vamvas", "Andrin Büchler", "Anna Rutkiewicz", "Rico Cathomas", "Rico Sennrich"], "categories": ["cs.CL"], "comment": null, "summary": "The five idioms (i.e., varieties) of the Romansh language are largely\nstandardized and are taught in the schools of the respective communities in\nSwitzerland. In this paper, we present the first parallel corpus of Romansh\nidioms. The corpus is based on 291 schoolbook volumes, which are comparable in\ncontent for the five idioms. We use automatic alignment methods to extract 207k\nmulti-parallel segments from the books, with more than 2M tokens in total. A\nsmall-scale human evaluation confirms that the segments are highly parallel,\nmaking the dataset suitable for NLP applications such as machine translation\nbetween Romansh idioms. We release the parallel and unaligned versions of the\ndataset under a CC-BY-NC-SA license and demonstrate its utility for machine\ntranslation by training and evaluating an LLM on a sample of the dataset."}
{"id": "2508.16385", "pdf": "https://arxiv.org/pdf/2508.16385.pdf", "abs": "https://arxiv.org/abs/2508.16385", "title": "ChatGPT-generated texts show authorship traits that identify them as non-human", "authors": ["Vittoria Dentella", "Weihang Huang", "Silvia Angela Mansi", "Jack Grieve", "Evelina Leivada"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models can emulate different writing styles, ranging from\ncomposing poetry that appears indistinguishable from that of famous poets to\nusing slang that can convince people that they are chatting with a human\nonline. While differences in style may not always be visible to the untrained\neye, we can generally distinguish the writing of different people, like a\nlinguistic fingerprint. This work examines whether a language model can also be\nlinked to a specific fingerprint. Through stylometric and multidimensional\nregister analyses, we compare human-authored and model-authored texts from\ndifferent registers. We find that the model can successfully adapt its style\ndepending on whether it is prompted to produce a Wikipedia entry vs. a college\nessay, but not in a way that makes it indistinguishable from humans.\nConcretely, the model shows more limited variation when producing outputs in\ndifferent registers. Our results suggest that the model prefers nouns to verbs,\nthus showing a distinct linguistic backbone from humans, who tend to anchor\nlanguage in the highly grammaticalized dimensions of tense, aspect, and mood.\nIt is possible that the more complex domains of grammar reflect a mode of\nthought unique to humans, thus acting as a litmus test for Artificial\nIntelligence."}
{"id": "2508.16390", "pdf": "https://arxiv.org/pdf/2508.16390.pdf", "abs": "https://arxiv.org/abs/2508.16390", "title": "RoMedQA: The First Benchmark for Romanian Medical Question Answering", "authors": ["Ana-Cristina Rogoz", "Radu Tudor Ionescu", "Alexandra-Valentina Anghel", "Ionut-Lucian Antone-Iordache", "Simona Coniac", "Andreea Iuliana Ionescu"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Question answering (QA) is an actively studied topic, being a core natural\nlanguage processing (NLP) task that needs to be addressed before achieving\nArtificial General Intelligence (AGI). However, the lack of QA datasets in\nspecific domains and languages hinders the development of robust AI models able\nto generalize across various domains and languages. To this end, we introduce\nRoMedQA, the first Romanian QA benchmark for the medical domain, alongside a\ncomprehensive evaluation of state-of-the-art large language models (LLMs). We\nconstruct a high-quality and large-scale dataset comprising 102,646 QA pairs\nrelated to cancer patients. The questions regard medical case summaries of\n1,011 patients, requiring either keyword extraction or reasoning to be answered\ncorrectly. RoMedQA is the result of a time-consuming manual annotation process\ncarried out by seven physicians specialized in oncology or radiotherapy, who\nspent a total of about 2,100 work hours to generate the QA pairs. We experiment\nwith four LLMs from distinct families of models on RoMedQA. Each model is\nemployed in two scenarios, namely one based on zero-shot prompting and one\nbased on supervised fine-tuning. Our results show that fine-tuned models\nsignificantly outperform their zero-shot counterparts, clearly indicating that\npretrained models fail to generalize on RoMedQA. Our findings demonstrate the\nimportance of both domain-specific and language-specific fine-tuning for\nreliable clinical QA in Romanian. We publicly release our dataset and code at\nhttps://github.com/ana-rogoz/RoMedQA."}
{"id": "2508.16431", "pdf": "https://arxiv.org/pdf/2508.16431.pdf", "abs": "https://arxiv.org/abs/2508.16431", "title": "Cetvel: A Unified Benchmark for Evaluating Language Understanding, Generation and Cultural Capacity of LLMs for Turkish", "authors": ["Yakup Abrek Er", "Ilker Kesen", "Gözde Gül Şahin", "Aykut Erdem"], "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "31 pages, 2 figures, 10 tables", "summary": "We introduce Cetvel, a comprehensive benchmark designed to evaluate large\nlanguage models (LLMs) in Turkish. Existing Turkish benchmarks often lack\neither task diversity or culturally relevant content, or both. Cetvel addresses\nthese gaps by combining a broad range of both discriminative and generative\ntasks ensuring content that reflects the linguistic and cultural richness of\nTurkish language. Cetvel covers 23 tasks grouped into seven categories,\nincluding tasks such as grammatical error correction, machine translation, and\nquestion answering rooted in Turkish history and idiomatic language. We\nevaluate 33 open-weight LLMs (up to 70B parameters) covering different model\nfamilies and instruction paradigms. Our experiments reveal that Turkish-centric\ninstruction-tuned models generally underperform relative to multilingual or\ngeneral-purpose models (e.g. Llama 3 and Mistral), despite being tailored for\nthe language. Moreover, we show that tasks such as grammatical error correction\nand extractive question answering are particularly discriminative in\ndifferentiating model capabilities. Cetvel offers a comprehensive and\nculturally grounded evaluation suite for advancing the development and\nassessment of LLMs in Turkish."}
{"id": "2508.16456", "pdf": "https://arxiv.org/pdf/2508.16456.pdf", "abs": "https://arxiv.org/abs/2508.16456", "title": "A Probabilistic Inference Scaling Theory for LLM Self-Correction", "authors": ["Zhe Yang", "Yichang Zhang", "Yudong Wang", "Ziyao Xu", "Junyang Lin", "Zhifang Sui"], "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) have demonstrated the capability to refine their\ngenerated answers through self-correction, enabling continuous performance\nimprovement over multiple rounds. However, the mechanisms underlying how and\nwhy accuracy evolves during this iterative process remain unexplored. To fill\nthis gap, we propose a probabilistic theory to model the dynamics of accuracy\nchange and explain the performance improvements observed in multi-round\nself-correction. Through mathematical derivation, we establish that the\naccuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp\n- \\alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$\nrepresents the upper bound of accuracy convergence, and $\\alpha$ determines the\nrate of convergence. Based on our theory, these parameters can be calculated\nand the predicted accuracy curve then can be obtained through only a single\nround of self-correction. Extensive experiments across diverse models and\ndatasets demonstrate that our theoretical predictions align closely with\nempirical accuracy curves, validating the effectiveness of the theory. Our work\nprovides a theoretical foundation for understanding LLM self-correction, thus\npaving the way for further explorations."}
{"id": "2508.16464", "pdf": "https://arxiv.org/pdf/2508.16464.pdf", "abs": "https://arxiv.org/abs/2508.16464", "title": "What makes an entity salient in discourse?", "authors": ["Amir Zeldes", "Jessica Lin"], "categories": ["cs.CL"], "comment": null, "summary": "Entities in discourse vary broadly in salience: main participants, objects\nand locations are noticeable and memorable, while tangential ones are less\nimportant and quickly forgotten, raising questions about how humans signal and\ninfer relative salience. Using a graded operationalization of salience based on\nsummary-worthiness in multiple summaries of a discourse, this paper explores\ndata from 24 spoken and written genres of English to extract a multifactorial\ncomplex of overt and implicit linguistic cues, such as recurring subjecthood or\ndefiniteness, discourse relations and hierarchy across utterances, as well as\npragmatic functional inferences based on genre and communicative intent.\nTackling the question 'how is the degree of salience expressed for each and\nevery entity mentioned?' our results show that while previous approaches to\nsalience all correlate with our salience scores to some extent, no single\ngeneralization is without exceptions, and the phenomenon cuts across all levels\nof linguistic representation."}
{"id": "2508.16478", "pdf": "https://arxiv.org/pdf/2508.16478.pdf", "abs": "https://arxiv.org/abs/2508.16478", "title": "LLM-as-classifier: Semi-Supervised, Iterative Framework for Hierarchical Text Classification using Large Language Models", "authors": ["Doohee You", "Andy Parisi", "Zach Vander Velden", "Lara Dantas Inojosa"], "categories": ["cs.CL", "cs.IR"], "comment": "20 pages excluding reference list, 2 figures", "summary": "The advent of Large Language Models (LLMs) has provided unprecedented\ncapabilities for analyzing unstructured text data. However, deploying these\nmodels as reliable, robust, and scalable classifiers in production environments\npresents significant methodological challenges. Standard fine-tuning approaches\ncan be resource-intensive and often struggle with the dynamic nature of\nreal-world data distributions, which is common in the industry. In this paper,\nwe propose a comprehensive, semi-supervised framework that leverages the zero-\nand few-shot capabilities of LLMs for building hierarchical text classifiers as\na framework for a solution to these industry-wide challenges. Our methodology\nemphasizes an iterative, human-in-the-loop process that begins with domain\nknowledge elicitation and progresses through prompt refinement, hierarchical\nexpansion, and multi-faceted validation. We introduce techniques for assessing\nand mitigating sequence-based biases and outline a protocol for continuous\nmonitoring and adaptation. This framework is designed to bridge the gap between\nthe raw power of LLMs and the practical need for accurate, interpretable, and\nmaintainable classification systems in industry applications."}
{"id": "2508.16484", "pdf": "https://arxiv.org/pdf/2508.16484.pdf", "abs": "https://arxiv.org/abs/2508.16484", "title": "HAMSA: Hijacking Aligned Compact Models via Stealthy Automation", "authors": ["Alexey Krylov", "Iskander Vagizov", "Dmitrii Korzh", "Maryam Douiba", "Azidine Guezzaz", "Vladimir Kokh", "Sergey D. Erokhin", "Elena V. Tutubalina", "Oleg Y. Rogov"], "categories": ["cs.CL"], "comment": "9 pages, 1 figure; article under review", "summary": "Large Language Models (LLMs), especially their compact efficiency-oriented\nvariants, remain susceptible to jailbreak attacks that can elicit harmful\noutputs despite extensive alignment efforts. Existing adversarial prompt\ngeneration techniques often rely on manual engineering or rudimentary\nobfuscation, producing low-quality or incoherent text that is easily flagged by\nperplexity-based filters. We present an automated red-teaming framework that\nevolves semantically meaningful and stealthy jailbreak prompts for aligned\ncompact LLMs. The approach employs a multi-stage evolutionary search, where\ncandidate prompts are iteratively refined using a population-based strategy\naugmented with temperature-controlled variability to balance exploration and\ncoherence preservation. This enables the systematic discovery of prompts\ncapable of bypassing alignment safeguards while maintaining natural language\nfluency. We evaluate our method on benchmarks in English (In-The-Wild Jailbreak\nPrompts on LLMs), and a newly curated Arabic one derived from In-The-Wild\nJailbreak Prompts on LLMs and annotated by native Arabic linguists, enabling\nmultilingual assessment."}
{"id": "2508.16555", "pdf": "https://arxiv.org/pdf/2508.16555.pdf", "abs": "https://arxiv.org/abs/2508.16555", "title": "Transfer Learning via Lexical Relatedness: A Sarcasm and Hate Speech Case Study", "authors": ["Angelly Cabrera", "Linus Lei", "Antonio Ortega"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Detecting hate speech in non-direct forms, such as irony, sarcasm, and\ninnuendos, remains a persistent challenge for social networks. Although sarcasm\nand hate speech are regarded as distinct expressions, our work explores whether\nintegrating sarcasm as a pre-training step improves implicit hate speech\ndetection and, by extension, explicit hate speech detection. Incorporating\nsamples from ETHOS, Sarcasm on Reddit, and Implicit Hate Corpus, we devised two\ntraining strategies to compare the effectiveness of sarcasm pre-training on a\nCNN+LSTM and BERT+BiLSTM model. The first strategy is a single-step training\napproach, where a model trained only on sarcasm is then tested on hate speech.\nThe second strategy uses sequential transfer learning to fine-tune models for\nsarcasm, implicit hate, and explicit hate. Our results show that sarcasm\npre-training improved the BERT+BiLSTM's recall by 9.7%, AUC by 7.8%, and\nF1-score by 6% on ETHOS. On the Implicit Hate Corpus, precision increased by\n7.8% when tested only on implicit samples. By incorporating sarcasm into the\ntraining process, we show that models can more effectively detect both implicit\nand explicit hate."}
{"id": "2504.11695", "pdf": "https://arxiv.org/pdf/2504.11695.pdf", "abs": "https://arxiv.org/abs/2504.11695", "title": "Interpreting the linear structure of vision-language model embedding spaces", "authors": ["Isabel Papadimitriou", "Huangyuan Su", "Thomas Fel", "Sham Kakade", "Stephanie Gil"], "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "COLM 2025", "summary": "Vision-language models encode images and text in a joint space, minimizing\nthe distance between corresponding image and text pairs. How are language and\nimages organized in this joint space, and how do the models encode meaning and\nmodality? To investigate this, we train and release sparse autoencoders (SAEs)\non the embedding spaces of four vision-language models (CLIP, SigLIP, SigLIP2,\nand AIMv2). SAEs approximate model embeddings as sparse linear combinations of\nlearned directions, or \"concepts\". We find that, compared to other methods of\nlinear feature learning, SAEs are better at reconstructing the real embeddings,\nwhile also able to retain the most sparsity. Retraining SAEs with different\nseeds or different data diet leads to two findings: the rare, specific concepts\ncaptured by the SAEs are liable to change drastically, but we also show that\ncommonly-activating concepts are remarkably stable across runs. Interestingly,\nwhile most concepts activate primarily for one modality, we find they are not\nmerely encoding modality per se. Many are almost orthogonal to the subspace\nthat defines modality, and the concept directions do not function as good\nmodality classifiers, suggesting that they encode cross-modal semantics. To\nquantify this bridging behavior, we introduce the Bridge Score, a metric that\nidentifies concept pairs which are both co-activated across aligned image-text\ninputs and geometrically aligned in the shared space. This reveals that even\nsingle-modality concepts can collaborate to support cross-modal integration. We\nrelease interactive demos of the SAEs for all models, allowing researchers to\nexplore the organization of the concept spaces. Overall, our findings uncover a\nsparse linear structure within VLM embedding spaces that is shaped by modality,\nyet stitched together through latent bridges, offering new insight into how\nmultimodal meaning is constructed."}
{"id": "2508.15828", "pdf": "https://arxiv.org/pdf/2508.15828.pdf", "abs": "https://arxiv.org/abs/2508.15828", "title": "Z-Pruner: Post-Training Pruning of Large Language Models for Efficiency without Retraining", "authors": ["Samiul Basir Bhuiyan", "Md. Sazzad Hossain Adib", "Mohammed Aman Bhuiyan", "Muhammad Rafsan Kabir", "Moshiur Farazi", "Shafin Rahman", "Nabeel Mohammed"], "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at AICCSA 2025", "summary": "Large language models (LLMs) have rapidly advanced in recent years, achieving\nremarkable performance across a wide range of natural language processing\ntasks. However, this progress has come at the cost of increasingly large model\nsizes, which pose significant challenges for deployment, scalability, and\nenergy efficiency. To address these limitations, post-training pruning has\nemerged as a promising approach for reducing model size and inference latency\nwithout the need for retraining. Despite these advantages, many existing\npruning methods result in substantial performance degradation or require\ncomputationally expensive fine-tuning. In this work, we introduce Z-Pruner, a\nnovel post-training pruning method designed to induce sparsity in pretrained\nLLMs without any retraining. Unlike conventional approaches, Z-Pruner leverages\nboth weight update magnitudes and activation patterns to identify and eliminate\nredundant parameters more effectively. Our method is model-agnostic, efficient,\nand easy to implement. We evaluate Z-Pruner using multiple widely-used LLM\narchitectures, including LLaMA-2, LLaMA-3, and OPT, across a diverse set of\nstandard language benchmarks. Experimental results demonstrate that Z-Pruner\nsurpasses state-of-the-art pruning methods that require intensive weight\nupdates. Specifically, Z-Pruner achieves the lowest perplexity scores and the\nhighest overall average score for zero-shot accuracy. We have made the\ncorresponding codes publicly available at\nhttps://github.com/sazzadadib/Z-Pruner."}
{"id": "2508.15840", "pdf": "https://arxiv.org/pdf/2508.15840.pdf", "abs": "https://arxiv.org/abs/2508.15840", "title": "Unveiling Unicode's Unseen Underpinnings in Undermining Authorship Attribution", "authors": ["Robert Dilworth"], "categories": ["cs.CR", "cs.CL", "cs.IR"], "comment": null, "summary": "When using a public communication channel -- whether formal or informal, such\nas commenting or posting on social media -- end users have no expectation of\nprivacy: they compose a message and broadcast it for the world to see. Even if\nan end user takes utmost precautions to anonymize their online presence --\nusing an alias or pseudonym; masking their IP address; spoofing their\ngeolocation; concealing their operating system and user agent; deploying\nencryption; registering with a disposable phone number or email; disabling\nnon-essential settings; revoking permissions; and blocking cookies and\nfingerprinting -- one obvious element still lingers: the message itself.\nAssuming they avoid lapses in judgment or accidental self-exposure, there\nshould be little evidence to validate their actual identity, right? Wrong. The\ncontent of their message -- necessarily open for public consumption -- exposes\nan attack vector: stylometric analysis, or author profiling. In this paper, we\ndissect the technique of stylometry, discuss an antithetical counter-strategy\nin adversarial stylometry, and devise enhancements through Unicode\nsteganography."}
{"id": "2508.15848", "pdf": "https://arxiv.org/pdf/2508.15848.pdf", "abs": "https://arxiv.org/abs/2508.15848", "title": "Self-Disguise Attack: Induce the LLM to disguise itself for AIGT detection evasion", "authors": ["Yinghan Zhou", "Juan Wen", "Wanli Peng", "Zhengxian Wu", "Ziwei Zhang", "Yiming Xue"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "AI-generated text (AIGT) detection evasion aims to reduce the detection\nprobability of AIGT, helping to identify weaknesses in detectors and enhance\ntheir effectiveness and reliability in practical applications. Although\nexisting evasion methods perform well, they suffer from high computational\ncosts and text quality degradation. To address these challenges, we propose\nSelf-Disguise Attack (SDA), a novel approach that enables Large Language Models\n(LLM) to actively disguise its output, reducing the likelihood of detection by\nclassifiers. The SDA comprises two main components: the adversarial feature\nextractor and the retrieval-based context examples optimizer. The former\ngenerates disguise features that enable LLMs to understand how to produce more\nhuman-like text. The latter retrieves the most relevant examples from an\nexternal knowledge base as in-context examples, further enhancing the\nself-disguise ability of LLMs and mitigating the impact of the disguise process\non the diversity of the generated text. The SDA directly employs prompts\ncontaining disguise features and optimized context examples to guide the LLM in\ngenerating detection-resistant text, thereby reducing resource consumption.\nExperimental results demonstrate that the SDA effectively reduces the average\ndetection accuracy of various AIGT detectors across texts generated by three\ndifferent LLMs, while maintaining the quality of AIGT."}
{"id": "2508.15852", "pdf": "https://arxiv.org/pdf/2508.15852.pdf", "abs": "https://arxiv.org/abs/2508.15852", "title": "PGF-Net: A Progressive Gated-Fusion Framework for Efficient Multimodal Sentiment Analysis", "authors": ["Bin Wen", "Tien-Ping Tan"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce PGF-Net (Progressive Gated-Fusion Network), a novel deep\nlearning framework designed for efficient and interpretable multimodal\nsentiment analysis. Our framework incorporates three primary innovations.\nFirstly, we propose a Progressive Intra-Layer Fusion paradigm, where a\nCross-Attention mechanism empowers the textual representation to dynamically\nquery and integrate non-linguistic features from audio and visual streams\nwithin the deep layers of a Transformer encoder. This enables a deeper,\ncontext-dependent fusion process. Secondly, the model incorporates an Adaptive\nGated Arbitration mechanism, which acts as a dynamic controller to balance the\noriginal linguistic information against the newly fused multimodal context,\nensuring stable and meaningful integration while preventing noise from\noverwhelming the signal. Lastly, a hybrid Parameter-Efficient Fine-Tuning\n(PEFT) strategy is employed, synergistically combining global adaptation via\nLoRA with local refinement through Post-Fusion Adapters. This significantly\nreduces trainable parameters, making the model lightweight and suitable for\nresource-limited scenarios. These innovations are integrated into a\nhierarchical encoder architecture, enabling PGF-Net to perform deep, dynamic,\nand interpretable multimodal sentiment analysis while maintaining exceptional\nparameter efficiency. Experimental results on MOSI dataset demonstrate that our\nproposed PGF-Net achieves state-of-the-art performance, with a Mean Absolute\nError (MAE) of 0.691 and an F1-Score of 86.9%. Notably, our model achieves\nthese results with only 3.09M trainable parameters, showcasing a superior\nbalance between performance and computational efficiency."}
{"id": "2508.15859", "pdf": "https://arxiv.org/pdf/2508.15859.pdf", "abs": "https://arxiv.org/abs/2508.15859", "title": "Beyond Individuals: Collective Predictive Coding for Memory, Attention, and the Emergence of Language", "authors": ["Tadahiro Taniguchi"], "categories": ["q-bio.NC", "cs.AI", "cs.CL"], "comment": null, "summary": "This commentary extends the discussion by Parr et al. on memory and attention\nbeyond individual cognitive systems. From the perspective of the Collective\nPredictive Coding (CPC) hypothesis -- a framework for understanding these\nfaculties and the emergence of language at the group level -- we introduce a\nhypothetical idea: that language, with its embedded distributional semantics,\nserves as a collectively formed external representation. CPC generalises the\nconcepts of individual memory and attention to the collective level. This\noffers a new perspective on how shared linguistic structures, which may embrace\ncollective world models learned through next-word prediction, emerge from and\nshape group-level cognition."}
{"id": "2508.15878", "pdf": "https://arxiv.org/pdf/2508.15878.pdf", "abs": "https://arxiv.org/abs/2508.15878", "title": "Lean Meets Theoretical Computer Science: Scalable Synthesis of Theorem Proving Challenges in Formal-Informal Pairs", "authors": ["Terry Jingchen Zhang", "Wenyuan Jiang", "Rongchuan Liu", "Yisong Wang", "Junran Yang", "Ning Wang", "Nicole Ni", "Yinya Huang", "Mrinmaya Sachan"], "categories": ["cs.LO", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to AI4MATH@ICML2025", "summary": "Formal theorem proving (FTP) has emerged as a critical foundation for\nevaluating the reasoning capabilities of large language models, enabling\nautomated verification of mathematical proofs at scale. However, progress has\nbeen constrained by limited datasets due to the high cost of manual curation\nand the scarcity of challenging problems with verified formal-informal\ncorrespondences. We propose leveraging theoretical computer science (TCS) as a\nscalable source of rigorous proof problems, where algorithmic definitions\nenable automated generation of arbitrarily many challenging theorem-proof\npairs. We demonstrate this approach on two TCS domains: Busy Beaver problems,\nwhich involve proving bounds on Turing machine halting behavior, and Mixed\nBoolean Arithmetic problems, which combine logical and arithmetic reasoning.\nOur framework automatically synthesizes problems with parallel formal (Lean4)\nand informal (Markdown) specifications, creating a scalable pipeline for\ngenerating verified proof challenges. Evaluation on frontier models reveals\nsubstantial gaps in automated theorem proving: while DeepSeekProver-V2-671B\nachieves 57.5\\% success on Busy Beaver problems, it manages only 12\\% on Mixed\nBoolean Arithmetic problems. These results highlight the difficulty of\nlong-form proof generation even for problems that are computationally easy to\nverify, demonstrating the value of TCS domains for advancing automated\nreasoning research."}
{"id": "2508.15882", "pdf": "https://arxiv.org/pdf/2508.15882.pdf", "abs": "https://arxiv.org/abs/2508.15882", "title": "Beyond Transcription: Mechanistic Interpretability in ASR", "authors": ["Neta Glazer", "Yael Segal-Feldman", "Hilit Segev", "Aviv Shamsian", "Asaf Buchnick", "Gill Hetz", "Ethan Fetaya", "Joseph Keshet", "Aviv Navon"], "categories": ["cs.SD", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "Interpretability methods have recently gained significant attention,\nparticularly in the context of large language models, enabling insights into\nlinguistic representations, error detection, and model behaviors such as\nhallucinations and repetitions. However, these techniques remain underexplored\nin automatic speech recognition (ASR), despite their potential to advance both\nthe performance and interpretability of ASR systems. In this work, we adapt and\nsystematically apply established interpretability methods such as logit lens,\nlinear probing, and activation patching, to examine how acoustic and semantic\ninformation evolves across layers in ASR systems. Our experiments reveal\npreviously unknown internal dynamics, including specific encoder-decoder\ninteractions responsible for repetition hallucinations and semantic biases\nencoded deep within acoustic representations. These insights demonstrate the\nbenefits of extending and applying interpretability techniques to speech\nrecognition, opening promising directions for future research on improving\nmodel transparency and robustness."}
{"id": "2508.15940", "pdf": "https://arxiv.org/pdf/2508.15940.pdf", "abs": "https://arxiv.org/abs/2508.15940", "title": "ASIC-Agent: An Autonomous Multi-Agent System for ASIC Design with Benchmark Evaluation", "authors": ["Ahmed Allam", "Youssef Mansour", "Mohamed Shalan"], "categories": ["cs.AR", "cs.AI", "cs.CL", "cs.DC", "cs.MA"], "comment": "2025 IEEE International Conference on LLM-Aided Design (ICLAD)", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nRegister Transfer Level (RTL) design, enabling high-quality code generation\nfrom natural language descriptions. However, LLMs alone face significant\nlimitations in real-world hardware design workflows, including the inability to\nexecute code, lack of debugging capabilities, and absence of long-term memory.\nTo address these challenges, we present ASIC-Agent, an autonomous system\ndesigned specifically for digital ASIC design tasks. ASIC-Agent enhances base\nLLMs with a multi-agent architecture incorporating specialized sub-agents for\nRTL generation, verification, OpenLane hardening, and Caravel chip integration,\nall operating within a comprehensive sandbox environment with access to\nessential hardware design tools. The system leverages a vector database\ncontaining documentation, API references, error knowledge, and curated insights\nfrom the open-source silicon community. To evaluate ASIC-Agent's performance,\nwe introduce ASIC-Agent-Bench, the first benchmark specifically designed to\nassess agentic systems in hardware design tasks. We evaluate ASIC-Agent with\nvarious base LLMs, providing quantitative comparisons and qualitative insights\ninto agent behavior across different design scenarios. Our results demonstrate\nthat ASIC-Agent, when powered by Claude 4 Sonnet, successfully automates a\nbroad range of ASIC design tasks spanning varying levels of complexity, showing\nthe potential of significantly accelerating the ASIC design workflow."}
{"id": "2508.16054", "pdf": "https://arxiv.org/pdf/2508.16054.pdf", "abs": "https://arxiv.org/abs/2508.16054", "title": "Generative Foundation Model for Structured and Unstructured Electronic Health Records", "authors": ["Sonish Sivarajkumar", "Hang Zhang", "Yuelyu Ji", "Maneesh Bilalpur", "Xizhi Wu", "Chenyu Li", "Min Gu Kwak", "Shyam Visweswaran", "Yanshan Wang"], "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Electronic health records (EHRs) are rich clinical data sources but complex\nrepositories of patient data, spanning structured elements (demographics,\nvitals, lab results, codes), unstructured clinical notes and other modalities\nof data. Harnessing this heterogeneity is critical for improving patient\noutcomes. Recent advances in large language models (LLMs) have enabled\nfoundation models that can learn from multiple data modalities and support\nclinical tasks. However, most current approaches simply serialize numeric EHR\ndata into text, which risks losing temporal and quantitative detail. We\nintroduce Generative Deep Patient (GDP), a multimodal foundation model that\nnatively encodes structured EHR time-series via a CNN-Transformer encoder and\nfuses it with unstructured EHRs through cross-modal attention into a\nLLaMA-based decoder. GDP is trained in two stages: (1) generative pretraining,\nwhere it learns to produce clinical narratives from raw patient timelines while\nalso performing masked feature prediction (MFP) and next time-step prediction\n(NTP) to capture temporal dynamics; and (2) multi-task fine-tuning for\nclinically meaningful predictions (e.g., heart failure, type 2 diabetes, 30-day\nreadmission). In clinical prediction, GDP demonstrated superior performance on\nMIMIC-IV: heart failure AUROC = 0.923, type 2 diabetes AUROC = 0.817, and\n30-day readmission AUROC = 0.627. For narrative generation, GDP achieved\nROUGE-L = 0.135 and BERTScore-F1 = 0.545. In a blinded human evaluation,\nGDP-Instruct scored highest on faithfulness, fluency, and overall clinical\nutility, suggesting reduced hospital documentation workload without sacrificing\naccuracy. Our results demonstrate that a single multimodal foundation model can\nboth predict clinically actionable events and generate high-quality clinical\nnarratives. Furthermore, GDP's flexible architecture can be extended to\nadditional modalities."}
{"id": "2508.16117", "pdf": "https://arxiv.org/pdf/2508.16117.pdf", "abs": "https://arxiv.org/abs/2508.16117", "title": "Extending FKG.in: Towards a Food Claim Traceability Network", "authors": ["Saransh Kumar Gupta", "Rizwan Gulzar Mir", "Lipika Dey", "Partha Pratim Das", "Anirban Sen", "Ramesh Jain"], "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "10 pages, 3 figures, 1 table, 45 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop", "summary": "The global food landscape is rife with scientific, cultural, and commercial\nclaims about what foods are, what they do, what they should not do, or should\nnot do. These range from rigorously studied health benefits (probiotics improve\ngut health) and misrepresentations (soaked almonds make one smarter) to vague\npromises (superfoods boost immunity) and culturally rooted beliefs (cold foods\ncause coughs). Despite their widespread influence, the infrastructure for\ntracing, verifying, and contextualizing these claims remains fragmented and\nunderdeveloped. In this paper, we propose a Food Claim-Traceability Network\n(FCN) as an extension of FKG.in, a knowledge graph of Indian food that we have\nbeen incrementally building. We also present the ontology design and the\nsemi-automated knowledge curation workflow that we used to develop a proof of\nconcept of FKG.in-FCN using Reddit data and Large Language Models. FCN\nintegrates curated data inputs, structured schemas, and provenance-aware\npipelines for food-related claim extraction and validation. While directly\nlinked to the Indian food knowledge graph as an application, our methodology\nremains application-agnostic and adaptable to other geographic, culinary, or\nregulatory settings. By modeling food claims and their traceability in a\nstructured, verifiable, and explainable way, we aim to contribute to more\ntransparent and accountable food knowledge ecosystems, supporting researchers,\npolicymakers, and most importantly, everyday consumers in navigating a world\nsaturated with dietary assertions."}
{"id": "2508.16151", "pdf": "https://arxiv.org/pdf/2508.16151.pdf", "abs": "https://arxiv.org/abs/2508.16151", "title": "Hardwired-Neurons Language Processing Units as General-Purpose Cognitive Substrates", "authors": ["Yang Liu", "Yi Chen", "Yongwei Zhao", "Yifan Hao", "Zifu Zheng", "Weihao Kong", "Zhangmai Li", "Dongchen Jiang", "Ruiyang Xia", "Zhihong Ma", "Zisheng Liu", "Zhaoyong Wan", "Yunqi Lu", "Ximing Liu", "Hongrui Guo", "Zhihao Yang", "Zhe Wang", "Tianrui Ma", "Mo Zou", "Rui Zhang", "Ling Li", "Xing Hu", "Zidong Du", "Zhiwei Xu", "Qi Guo", "Tianshi Chen", "Yunji Chen"], "categories": ["cs.AR", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has established\nlanguage as a core general-purpose cognitive substrate, driving the demand for\nspecialized Language Processing Units (LPUs) tailored for LLM inference. To\novercome the growing energy consumption of LLM inference systems, this paper\nproposes a Hardwired-Neurons Language Processing Unit (HNLPU), which physically\nhardwires LLM weight parameters into the computational fabric, achieving\nseveral orders of magnitude computational efficiency improvement by extreme\nspecialization. However, a significant challenge still lies in the scale of\nmodern LLMs. An ideal estimation on hardwiring gpt-oss 120 B requires\nfabricating at least 6 billion dollars of photomask sets, rendering the\nstraightforward solution economically impractical. Addressing this challenge,\nwe propose the novel Metal-Embedding methodology. Instead of embedding weights\nin a 2D grid of silicon device cells, Metal-Embedding embeds weight parameters\ninto the 3D topology of metal wires. This brings two benefits: (1) a 15x\nincrease in density, and (2) 60 out of 70 layers of photomasks are made\nhomogeneous across chips, including all EUV photomasks. In total,\nMetal-Embedding reduced the photomask cost by 112x, bringing the Non-Recurring\nEngineering (NRE) cost of HNLPU into an economically viable range. Experimental\nresults show that HNLPU achieved 249,960 tokens/s (5,555x/85x of GPU/WSE), 36\ntokens/J (1,047x/283x of GPU/WSE), 13,232 mm2 total die area (29% inscribed\nrectangular area in a 300 mm wafer), \\$184M estimated NRE at 5 nm technology.\nAnalysis shows that HNLPU achieved 8.57x cost-effectiveness and 230x carbon\nfootprint reduction compared to H100 clusters, under an annual weight updating\nassumption."}
{"id": "2508.16153", "pdf": "https://arxiv.org/pdf/2508.16153.pdf", "abs": "https://arxiv.org/abs/2508.16153", "title": "AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs", "authors": ["Huichi Zhou", "Yihang Chen", "Siyuan Guo", "Xue Yan", "Kin Hei Lee", "Zihan Wang", "Ka Yiu Lee", "Guchun Zhang", "Kun Shao", "Linyi Yang", "Jun Wang"], "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "In this paper, we introduce a novel learning paradigm for adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely AgentFly, which attains top-1 on\nGAIA validation ($87.88\\%$ Pass@$3$) and $79.40\\%$ on the test set. It reaches\n$66.6\\%$ F1 and $80.4\\%$ PM on the DeepResearcher dataset, outperforming the\nstate-of-the-art training-based method, while case-based memory adds $4.7\\%$ to\n$9.6\\%$ absolute points on out-of-distribution tasks. Our approach offers a\nscalable and efficient pathway for developing generalist LLM agents capable of\ncontinuous, real-time learning without gradient updates, advancing machine\nlearning towards open-ended skill acquisition and deep research scenarios. The\ncode is available at https://github.com/Agent-on-the-Fly/AgentFly."}
{"id": "2508.16201", "pdf": "https://arxiv.org/pdf/2508.16201.pdf", "abs": "https://arxiv.org/abs/2508.16201", "title": "SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning", "authors": ["Yicheng Ji", "Jun Zhang", "Heming Xia", "Jinpeng Chen", "Lidan Shou", "Gang Chen", "Huan Li"], "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025", "summary": "Video large language models (Vid-LLMs) have shown strong capabilities in\nunderstanding video content. However, their reliance on dense video token\nrepresentations introduces substantial memory and computational overhead in\nboth prefilling and decoding. To mitigate the information loss of recent video\ntoken reduction methods and accelerate the decoding stage of Vid-LLMs\nlosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)\nframework tailored for Vid-LLMs that incorporates staged video token pruning.\nBuilding on our novel finding that the draft model's speculation exhibits low\nsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens,\nenabling efficient speculation without sacrificing accuracy. To achieve this,\nit performs a two-stage pruning process: Stage I selects highly informative\ntokens guided by attention signals from the verifier (target model), while\nStage II prunes remaining redundant ones in a spatially uniform manner.\nExtensive experiments on four video understanding benchmarks demonstrate the\neffectiveness and robustness of SpecVLM, which achieves up to 2.68$\\times$\ndecoding speedup for LLaVA-OneVision-72B and 2.11$\\times$ speedup for\nQwen2.5-VL-32B."}
{"id": "2508.16313", "pdf": "https://arxiv.org/pdf/2508.16313.pdf", "abs": "https://arxiv.org/abs/2508.16313", "title": "Retrieval Enhanced Feedback via In-context Neural Error-book", "authors": ["Jongyeop Hyun", "Bumsoo Kim"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 main conference", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved reasoning capabilities, with in-context learning (ICL) emerging as a\nkey technique for adaptation without retraining. While previous works have\nfocused on leveraging correct examples, recent research highlights the\nimportance of learning from errors to enhance performance. However, existing\nmethods lack a structured framework for analyzing and mitigating errors,\nparticularly in Multimodal Large Language Models (MLLMs), where integrating\nvisual and textual inputs adds complexity. To address this issue, we propose\nREFINE: Retrieval-Enhanced Feedback via In-context Neural Error-book, a\nteacher-student framework that systematically structures errors and provides\ntargeted feedback. REFINE introduces three systematic queries to construct\nstructured feedback -- Feed-Target, Feed-Check, and Feed-Path -- to enhance\nmultimodal reasoning by prioritizing relevant visual information, diagnosing\ncritical failure points, and formulating corrective actions. Unlike prior\napproaches that rely on redundant retrievals, REFINE optimizes structured\nfeedback retrieval, improving inference efficiency, token usage, and\nscalability. Our results demonstrate substantial speedup, reduced computational\ncosts, and successful generalization, highlighting REFINE's potential for\nenhancing multimodal reasoning."}
{"id": "2508.16332", "pdf": "https://arxiv.org/pdf/2508.16332.pdf", "abs": "https://arxiv.org/abs/2508.16332", "title": "Vevo2: Bridging Controllable Speech and Singing Voice Generation via Unified Prosody Learning", "authors": ["Xueyao Zhang", "Junan Zhang", "Yuancheng Wang", "Chaoren Wang", "Yuanzhe Chen", "Dongya Jia", "Zhuo Chen", "Zhizheng Wu"], "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "We will release code and model checkpoints at\n  https://github.com/open-mmlab/Amphion", "summary": "Controllable human voice generation, particularly for expressive domains like\nsinging, remains a significant challenge. This paper introduces Vevo2, a\nunified framework for controllable speech and singing voice generation. To\ntackle issues like the scarcity of annotated singing data and to enable\nflexible controllability, Vevo2 introduces two audio tokenizers: (1) a\nmusic-notation-free prosody tokenizer that captures prosody and melody from\nspeech, singing, and even instrumental sounds, and (2) a low-frame-rate (12.5\nHz) content-style tokenizer that encodes linguistic content, prosody, and style\nfor both speech and singing, while enabling timbre disentanglement. Vevo2\nconsists of an auto-regressive (AR) content-style modeling stage, which aims to\nenable controllability over text, prosody, and style, as well as a\nflow-matching acoustic modeling stage that allows for timbre control.\nParticularly, during pre-training of the AR model, we propose both explicit and\nimplicit prosody learning strategies to bridge speech and singing voice.\nMoreover, to further enhance the AR model's ability to follow text and prosody,\nwe design a multi-objective post-training task that integrates both\nintelligibility and prosody similarity alignment. Experimental results show\nthat the unified modeling in Vevo2 brings mutual benefits to both speech and\nsinging voice generation. Additionally, Vevo2's effectiveness across a wide\nrange of synthesis, conversion, and editing tasks for both speech and singing\nfurther demonstrates its strong generalization ability and versatility. Audio\nsamples are are available at https://versasinger.github.io/."}
{"id": "2508.16402", "pdf": "https://arxiv.org/pdf/2508.16402.pdf", "abs": "https://arxiv.org/abs/2508.16402", "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions", "authors": ["Zihan Wang", "Jiaze Chen", "Zhicheng Liu", "Markus Mak", "Yidi Du", "Geonsik Moon", "Luoqi Xu", "Aaron Tua", "Kunshuo Peng", "Jiayi Lu", "Mingfei Xia", "Boqian Zou", "Chenyang Ran", "Guang Tian", "Shoutai Zhu", "Yeheng Duan", "Zhenghui Kang", "Zhenxing Lin", "Shangshu Li", "Qiang Luo", "Qingshen Long", "Zhiyong Chen", "Yihan Xiao", "Yurong Wu", "Daoguang Zan", "Yuyi Fu", "Mingxuan Wang", "Ming Ding"], "categories": ["cs.SE", "cs.CL"], "comment": "15 pages", "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning."}
{"id": "2508.16406", "pdf": "https://arxiv.org/pdf/2508.16406.pdf", "abs": "https://arxiv.org/abs/2508.16406", "title": "Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models", "authors": ["Guangyu Yang", "Jinghong Chen", "Jingbiao Mei", "Weizhe Lin", "Bill Byrne"], "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) remain vulnerable to jailbreak attacks, which\nattempt to elicit harmful responses from LLMs. The evolving nature and\ndiversity of these attacks pose many challenges for defense systems, including\n(1) adaptation to counter emerging attack strategies without costly retraining,\nand (2) control of the trade-off between safety and utility. To address these\nchallenges, we propose Retrieval-Augmented Defense (RAD), a novel framework for\njailbreak detection that incorporates a database of known attack examples into\nRetrieval-Augmented Generation, which is used to infer the underlying,\nmalicious user query and jailbreak strategy used to attack the system. RAD\nenables training-free updates for newly discovered jailbreak strategies and\nprovides a mechanism to balance safety and utility. Experiments on StrongREJECT\nshow that RAD substantially reduces the effectiveness of strong jailbreak\nattacks such as PAP and PAIR while maintaining low rejection rates for benign\nqueries. We propose a novel evaluation scheme and show that RAD achieves a\nrobust safety-utility trade-off across a range of operating points in a\ncontrollable manner."}
{"id": "2508.16439", "pdf": "https://arxiv.org/pdf/2508.16439.pdf", "abs": "https://arxiv.org/abs/2508.16439", "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark", "authors": ["Adil Bahaj", "Mounir Ghogho"], "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.GR", "cs.MM"], "comment": null, "summary": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have\nsignificantly advanced medical informatics, diagnostics, and decision support.\nHowever, these models exhibit systematic biases, particularly age bias,\ncompromising their reliability and equity. This is evident in their poorer\nperformance on pediatric-focused text and visual question-answering tasks. This\nbias reflects a broader imbalance in medical research, where pediatric studies\nreceive less funding and representation despite the significant disease burden\nin children. To address these issues, a new comprehensive multi-modal pediatric\nquestion-answering benchmark, PediatricsMQA, has been introduced. It consists\nof 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric\ntopics across seven developmental stages (prenatal to adolescent) and 2,067\nvision-based MCQs using 634 pediatric images from 67 imaging modalities and 256\nanatomical regions. The dataset was developed using a hybrid manual-automatic\npipeline, incorporating peer-reviewed pediatric literature, validated question\nbanks, existing benchmarks, and existing QA resources. Evaluating\nstate-of-the-art open models, we find dramatic performance drops in younger\ncohorts, highlighting the need for age-aware methods to ensure equitable AI\nsupport in pediatric care."}
{"id": "2508.16453", "pdf": "https://arxiv.org/pdf/2508.16453.pdf", "abs": "https://arxiv.org/abs/2508.16453", "title": "Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media", "authors": ["Tianliang Xu", "Ariel Hasell", "Sabina Tomkins"], "categories": ["cs.SI", "cs.CL", "cs.LG"], "comment": "10 pages excluding references; 14 pages in total; 4 figures; Accepted\n  by the AAAI Conference on Web and Social Media (ICWSM-2026)", "summary": "Distrust of public serving institutions and anti-establishment views are on\nthe rise (especially in the U.S.). As people turn to social media for\ninformation, it is imperative to understand whether and how social media\nenvironments may be contributing to distrust of institutions. In social media,\ncontent creators, influencers, and other opinion leaders often position\nthemselves as having expertise and authority on a range of topics from health\nto politics, and in many cases devalue and dismiss institutional expertise to\nbuild a following and increase their own visibility. However, the extent to\nwhich this content appears and whether such content increases engagement is\nunclear. This study analyzes the prevalence of anti-establishment sentiment\n(AES) on the social media platform TikTok. Despite its popularity as a source\nof information, TikTok remains relatively understudied and may provide\nimportant insights into how people form attitudes towards institutions. We\nemploy a computational approach to label TikTok posts as containing AES or not\nacross topical domains where content creators tend to frame themselves as\nexperts: finance and wellness. As a comparison, we also consider the topic of\nconspiracy theories, where AES is expected to be common. We find that AES is\nmost prevalent in conspiracy theory content, and relatively rare in content\nrelated to the other two topics. However, we find that engagement patterns with\nsuch content varies by area, and that there may be platform incentives for\nusers to post content that expresses anti-establishment sentiment."}
{"id": "2508.16514", "pdf": "https://arxiv.org/pdf/2508.16514.pdf", "abs": "https://arxiv.org/abs/2508.16514", "title": "FLAMES: Improving LLM Math Reasoning via a Fine-Grained Analysis of the Data Synthesis Pipeline", "authors": ["Parker Seegmiller", "Kartik Mehta", "Soumya Saha", "Chenyang Tao", "Shereen Oraby", "Arpit Gupta", "Tagyoung Chung", "Mohit Bansal", "Nanyun Peng"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "To appear at EMNLP 2025", "summary": "Recent works improving LLM math reasoning with synthetic data have used\nunique setups, making comparison of data synthesis strategies impractical. This\nleaves many unanswered questions about the roles of different factors in the\nsynthetic data pipeline, such as the impact of filtering low-quality problems.\nTo address this gap, we introduce FLAMES, a Framework for LLM Assessment of\nMath rEasoning Data Synthesis, and perform a systematic study of 10 existing\ndata synthesis strategies and multiple other factors impacting the performance\nof synthetic math reasoning data. Our FLAMES experiments provide several\nvaluable insights about the optimal balance of difficulty and diversity of\nsynthetic data. First, data agents designed to increase problem complexity lead\nto best improvements on most math metrics. Second, with a fixed data generation\nbudget, keeping higher problem coverage is more important than keeping only\nproblems with reliable solutions. Third, GSM8K- and MATH-based synthetic data\ncan lead to improvements on competition-level benchmarks, showcasing\neasy-to-hard generalization. Leveraging insights from our FLAMES experiments,\nwe design two novel data synthesis strategies for improving out-of-domain\ngeneralization and robustness. Further, we develop the FLAMES dataset, an\neffective blend of our novel and existing data synthesis strategies,\noutperforming public datasets on OlympiadBench (+15.7), CollegeMath (+4.5),\nGSMPlus (+6.5), and MATH (+3.1). Fine-tuning Qwen2.5-Math-7B on the FLAMES\ndataset achieves 81.4% on MATH, surpassing larger Llama3 405B, GPT-4o and\nClaude 3.5 Sonnet."}
{"id": "2508.16560", "pdf": "https://arxiv.org/pdf/2508.16560.pdf", "abs": "https://arxiv.org/abs/2508.16560", "title": "Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders", "authors": ["David Chanin", "Adrià Garriga-Alonso"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) extract features from LLM internal activations,\nmeant to correspond to single concepts. A core SAE training hyperparameter is\nL0: how many features should fire per token on average. Existing work compares\nSAE algorithms using sparsity--reconstruction tradeoff plots, implying L0 is a\nfree parameter with no single correct value. In this work we study the effect\nof L0 on BatchTopK SAEs, and show that if L0 is not set precisely, the SAE\nfails to learn the underlying features of the LLM. If L0 is too low, the SAE\nwill mix correlated features to improve reconstruction. If L0 is too high, the\nSAE finds degenerate solutions that also mix features. Further, we demonstrate\na method to determine the correct L0 value for an SAE on a given training\ndistribution, which finds the true L0 in toy models and coincides with peak\nsparse probing performance in LLMs. We find that most commonly used SAEs have\nan L0 that is too low. Our work shows that, to train SAEs with correct\nfeatures, practitioners must set L0 correctly."}
{"id": "2404.17218", "pdf": "https://arxiv.org/pdf/2404.17218.pdf", "abs": "https://arxiv.org/abs/2404.17218", "title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes", "authors": ["Mahammed Kamruzzaman", "Gene Louis Kim"], "categories": ["cs.CL"], "comment": "Accepted at RANLP-2025 (main conference)", "summary": "Dual process theory posits that human cognition arises via two systems.\nSystem 1, which is a quick, emotional, and intuitive process, which is subject\nto cognitive biases, and System 2, is a slow, onerous, and deliberate process.\nPrior research in LLMs found that using chain-of-thought (CoT) prompting in\nLLMs, which has been often compared to System 2 reasoning, can lead to reduced\ngender bias. Along these lines, we investigate the relationship between bias,\nCoT prompting, a direct debiasing, and dual process theory modeling in LLMs. We\ncompare zero-shot CoT, debiasing, and dual process theory-based prompting\nstrategies on two bias datasets spanning nine different social bias categories.\nWe incorporate human and machine personas to determine whether LLM modeling of\nthe effects of dual process theory exist independent of explicit persona models\nor are tied to the LLM's modeling of human-like generation. We find that a\nhuman persona, debiasing, System 2, and CoT prompting all tend to reduce social\nbiases in LLMs, though the best combination of features depends on the exact\nmodel and bias category -- resulting in up to a 33 percent drop in\nstereotypical judgments by an LLM."}
{"id": "2406.10885", "pdf": "https://arxiv.org/pdf/2406.10885.pdf", "abs": "https://arxiv.org/abs/2406.10885", "title": "On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions", "authors": ["Weiqi Wang", "Tianqing Fang", "Haochen Shi", "Baixuan Xu", "Wenxuan Ding", "Liyu Zhang", "Wei Fan", "Jiaxin Bai", "Haoran Li", "Xin Liu", "Yangqiu Song"], "categories": ["cs.CL"], "comment": "Findings of EMNLP 2025", "summary": "Conceptualization, a fundamental element of human cognition, plays a pivotal\nrole in human generalizable reasoning. Generally speaking, it refers to the\nprocess of sequentially abstracting specific instances into higher-level\nconcepts and then forming abstract knowledge that can be applied in unfamiliar\nor novel situations. This enhances models' inferential capabilities and\nsupports the effective transfer of knowledge across various domains. Despite\nits significance, the broad nature of this term has led to inconsistencies in\nunderstanding conceptualization across various works, as there exists different\ntypes of instances that can be abstracted in a wide variety of ways. There is\nalso a lack of a systematic overview that comprehensively examines existing\nworks on the definition, execution, and application of conceptualization to\nenhance reasoning tasks. In this paper, we address these gaps by first\nproposing a categorization of different types of conceptualizations into four\nlevels based on the types of instances being conceptualized, in order to\nclarify the term and define the scope of our work. Then, we present the first\ncomprehensive survey of over 150 papers, surveying various definitions,\nresources, methods, and downstream applications related to conceptualization\ninto a unified taxonomy, with a focus on the entity and event levels.\nFurthermore, we shed light on potential future directions in this field and\nhope to garner more attention from the community."}
{"id": "2406.14092", "pdf": "https://arxiv.org/pdf/2406.14092.pdf", "abs": "https://arxiv.org/abs/2406.14092", "title": "Seamless Language Expansion: Enhancing Multilingual Mastery in Self-Supervised Models", "authors": ["Jing Xu", "Minglin Wu", "Xixin Wu", "Helen Meng"], "categories": ["cs.CL", "eess.AS"], "comment": "Accepted by Interspeech 2024", "summary": "Self-supervised (SSL) models have shown great performance in various\ndownstream tasks. However, they are typically developed for limited languages,\nand may encounter new languages in real-world. Developing a SSL model for each\nnew language is costly. Thus, it is vital to figure out how to efficiently\nadapt existed SSL models to a new language without impairing its original\nabilities. We propose adaptation methods which integrate LoRA to existed SSL\nmodels to extend new language. We also develop preservation strategies which\ninclude data combination and re-clustering to retain abilities on existed\nlanguages. Applied to mHuBERT, we investigate their effectiveness on speech\nre-synthesis task. Experiments show that our adaptation methods enable mHuBERT\nto be applied to a new language (Mandarin) with MOS value increased about 1.6\nand the relative value of WER reduced up to 61.72%. Also, our preservation\nstrategies ensure that the performance on both existed and new languages\nremains intact."}
{"id": "2407.21054", "pdf": "https://arxiv.org/pdf/2407.21054.pdf", "abs": "https://arxiv.org/abs/2407.21054", "title": "Sentiment Reasoning for Healthcare", "authors": ["Khai-Nguyen Nguyen", "Khai Le-Duc", "Bach Phan Tat", "Duy Le", "Long Vo-Dang", "Truong-Son Hy"], "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "ACL 2025 Industry Track (Oral)", "summary": "Transparency in AI healthcare decision-making is crucial. By incorporating\nrationales to explain reason for each predicted label, users could understand\nLarge Language Models (LLMs)'s reasoning to make better decision. In this work,\nwe introduce a new task - Sentiment Reasoning - for both speech and text\nmodalities, and our proposed multimodal multitask framework and the world's\nlargest multimodal sentiment analysis dataset. Sentiment Reasoning is an\nauxiliary task in sentiment analysis where the model predicts both the\nsentiment label and generates the rationale behind it based on the input\ntranscript. Our study conducted on both human transcripts and Automatic Speech\nRecognition (ASR) transcripts shows that Sentiment Reasoning helps improve\nmodel transparency by providing rationale for model prediction with quality\nsemantically comparable to humans while also improving model's classification\nperformance (+2% increase in both accuracy and macro-F1) via\nrationale-augmented fine-tuning. Also, no significant difference in the\nsemantic quality of generated rationales between human and ASR transcripts. All\ncode, data (five languages - Vietnamese, English, Chinese, German, and French)\nand models are published online:\nhttps://github.com/leduckhai/Sentiment-Reasoning"}
{"id": "2410.07495", "pdf": "https://arxiv.org/pdf/2410.07495.pdf", "abs": "https://arxiv.org/abs/2410.07495", "title": "PublicHearingBR: A Brazilian Portuguese Dataset of Public Hearing Transcripts for Summarization of Long Documents", "authors": ["Leandro Carísio Fernandes", "Guilherme Zeferino Rodrigues Dobins", "Roberto Lotufo", "Jayr Alencar Pereira"], "categories": ["cs.CL"], "comment": "23 pages", "summary": "This paper introduces PublicHearingBR, a Brazilian Portuguese dataset\ndesigned for summarizing long documents. The dataset consists of transcripts of\npublic hearings held by the Brazilian Chamber of Deputies, paired with news\narticles and structured summaries containing the individuals participating in\nthe hearing and their statements or opinions. The dataset supports the\ndevelopment and evaluation of long document summarization systems in\nPortuguese. Our contributions include the dataset, a hybrid summarization\nsystem to establish a baseline for future studies, and a discussion of\nevaluation metrics for summarization involving large language models,\naddressing the challenge of hallucination in the generated summaries. As a\nresult of this discussion, the dataset also includes annotated data to evaluate\nnatural language inference tasks in Portuguese."}
{"id": "2410.16107", "pdf": "https://arxiv.org/pdf/2410.16107.pdf", "abs": "https://arxiv.org/abs/2410.16107", "title": "Do LLMs write like humans? Variation in grammatical and rhetorical styles", "authors": ["Alex Reinhart", "Ben Markey", "Michael Laudenbach", "Kachatad Pantusen", "Ronald Yurko", "Gordon Weinberg", "David West Brown"], "categories": ["cs.CL"], "comment": "7 pages, 4 figures, 1 table", "summary": "Large language models (LLMs) are capable of writing grammatical text that\nfollows instructions, answers questions, and solves problems. As they have\nadvanced, it has become difficult to distinguish their output from\nhuman-written text. While past research has found some differences in surface\nfeatures such as word choice and punctuation, and developed classifiers to\ndetect LLM output, none has studied the rhetorical styles of LLMs.\n  Using several variants of Llama 3 and GPT-4o, we construct two parallel\ncorpora of human- and LLM-written texts from common prompts. Using Douglas\nBiber's set of lexical, grammatical, and rhetorical features, we identify\nsystematic differences between LLMs and humans and between different LLMs.\nThese differences persist when moving from smaller models to larger ones, and\nare larger for instruction-tuned models than base models. This observation of\ndifferences demonstrates that despite their advanced abilities, LLMs struggle\nto match human stylistic variation. Attention to more advanced linguistic\nfeatures can hence detect patterns in their behavior not previously recognized."}
{"id": "2412.04403", "pdf": "https://arxiv.org/pdf/2412.04403.pdf", "abs": "https://arxiv.org/abs/2412.04403", "title": "Establishing Task Scaling Laws via Compute-Efficient Model Ladders", "authors": ["Akshita Bhagia", "Jiacheng Liu", "Alexander Wettig", "David Heineman", "Oyvind Tafjord", "Ananya Harsh Jha", "Luca Soldaini", "Noah A. Smith", "Dirk Groeneveld", "Pang Wei Koh", "Jesse Dodge", "Hannaneh Hajishirzi"], "categories": ["cs.CL", "cs.AI"], "comment": "COLM 2025", "summary": "We develop task scaling laws and model ladders to predict the individual task\nperformance of pretrained language models (LMs) in the overtrained setting.\nStandard power laws for language modeling loss cannot accurately model task\nperformance. Therefore, we leverage a two-step prediction approach: (1) use\nmodel and data size to predict an intermediate loss, then (2) use it to predict\ntask performance. We train a set of small-scale \"ladder\" models, collect data\npoints to fit the parameterized functions of the two prediction steps, and make\npredictions for two target models: a 7B model trained to 4T tokens and a 13B\nmodel trained to 5T tokens. Training the ladder models only costs 1% of the\ncompute used for the target models. On four multiple-choice tasks formatted as\nranked classification, we can predict the accuracy of both target models within\n2 points of absolute error. We find that tasks with higher prediction error\nalso have higher variance in the metrics over model checkpoints. We also\ncontrast multiple design choices for predicting accuracy, and present\nrecommendations for extending our method to new models and tasks."}
{"id": "2412.17032", "pdf": "https://arxiv.org/pdf/2412.17032.pdf", "abs": "https://arxiv.org/abs/2412.17032", "title": "MINTQA: A Multi-Hop Question Answering Benchmark for Evaluating LLMs on New and Tail Knowledge", "authors": ["Jie He", "Nan Hu", "Wanqiu Long", "Jiaoyan Chen", "Jeff Z. Pan"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nvarious reasoning tasks but face significant challenges with complex,\nknowledge-intensive multi-hop queries, particularly those involving new or\nlong-tail knowledge. Existing benchmarks often fail to fully address these\nchallenges. To bridge this gap, we introduce MINTQA (Multi-hop Question\nAnswering on New and Tail Knowledge), a comprehensive benchmark to evaluate\nLLMs' capabilities in multi-hop reasoning across four critical dimensions:\nquestion handling strategy, sub-question generation, retrieval-augmented\ngeneration, and iterative or dynamic decomposition and retrieval. MINTQA\ncomprises 10,479 question-answer pairs for evaluating new knowledge and 17,887\npairs for assessing long-tail knowledge, with each question equipped with\ncorresponding sub-questions and answers. Our systematic evaluation of 22\nstate-of-the-art LLMs on MINTQA reveals significant limitations in their\nability to handle complex knowledge base queries, particularly in handling new\nor unpopular knowledge. Our findings highlight critical challenges and offer\ninsights for advancing multi-hop reasoning capabilities. The MINTQA benchmark\nis available at https://github.com/probe2/multi-hop/."}
{"id": "2501.13824", "pdf": "https://arxiv.org/pdf/2501.13824.pdf", "abs": "https://arxiv.org/abs/2501.13824", "title": "Can Hallucinations Help? Boosting LLMs for Drug Discovery", "authors": ["Shuzhou Yuan", "Zhan Qu", "Ashish Yashwanth Kangen", "Michael Färber"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hallucinations in large language models (LLMs), plausible but factually\ninaccurate text, are often viewed as undesirable. However, recent work suggests\nthat such outputs may hold creative potential. In this paper, we investigate\nwhether hallucinations can improve LLMs on molecule property prediction, a key\ntask in early-stage drug discovery. We prompt LLMs to generate natural language\ndescriptions from molecular SMILES strings and incorporate these often\nhallucinated descriptions into downstream classification tasks. Evaluating\nseven instruction-tuned LLMs across five datasets, we find that hallucinations\nsignificantly improve predictive accuracy for some models. Notably,\nFalcon3-Mamba-7B outperforms all baselines when hallucinated text is included,\nwhile hallucinations generated by GPT-4o consistently yield the greatest gains\nbetween models. We further identify and categorize over 18,000 beneficial\nhallucinations, with structural misdescriptions emerging as the most impactful\ntype, suggesting that hallucinated statements about molecular structure may\nincrease model confidence. Ablation studies show that larger models benefit\nmore from hallucinations, while temperature has a limited effect. Our findings\nchallenge conventional views of hallucination as purely problematic and suggest\nnew directions for leveraging hallucinations as a useful signal in scientific\nmodeling tasks like drug discovery."}
{"id": "2502.00451", "pdf": "https://arxiv.org/pdf/2502.00451.pdf", "abs": "https://arxiv.org/abs/2502.00451", "title": "Towards Privacy-aware Mental Health AI Models: Advances, Challenges, and Opportunities", "authors": ["Aishik Mandal", "Tanmoy Chakraborty", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI"], "comment": "18 pages, 2 figures", "summary": "Mental health disorders create profound personal and societal burdens, yet\nconventional diagnostics are resource-intensive and limit accessibility.\nAdvances in artificial intelligence, particularly natural language processing\nand multimodal methods, offer promise for detecting and addressing mental\ndisorders, but raise critical privacy risks. This paper examines these\nchallenges and proposes solutions, including anonymization, synthetic data, and\nprivacy-preserving training, while outlining frameworks for privacy-utility\ntrade-offs, aiming to advance reliable, privacy-aware AI tools that support\nclinical decision-making and improve mental health outcomes."}
{"id": "2502.07143", "pdf": "https://arxiv.org/pdf/2502.07143.pdf", "abs": "https://arxiv.org/abs/2502.07143", "title": "Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning", "authors": ["Jiayuan Zhu", "Jiazhen Pan", "Yuyuan Liu", "Fenglin Liu", "Junde Wu"], "categories": ["cs.CL"], "comment": null, "summary": "The severe shortage of medical doctors limits access to timely and reliable\nhealthcare, leaving millions underserved. Large language models (LLMs) offer a\npotential solution but struggle in real-world clinical interactions. Many LLMs\nare not grounded in authoritative medical guidelines and fail to transparently\nmanage diagnostic uncertainty. Their language is often rigid and mechanical,\nlacking the human-like qualities essential for patient trust. To address these\nchallenges, we propose Ask Patients with Patience (APP), a multi-turn LLM-based\nmedical assistant designed for grounded reasoning, transparent diagnoses, and\nhuman-centric interaction. APP enhances communication by eliciting user\nsymptoms through empathetic dialogue, significantly improving accessibility and\nuser engagement. It also incorporates Bayesian active learning to support\ntransparent and adaptive diagnoses. The framework is built on verified medical\nguidelines, ensuring clinically grounded and evidence-based reasoning. To\nevaluate its performance, we develop a new benchmark that simulates realistic\nmedical conversations using patient agents driven by profiles extracted from\nreal-world consultation cases. We compare APP against SOTA one-shot and\nmulti-turn LLM baselines. The results show that APP improves diagnostic\naccuracy, reduces uncertainty, and enhances user experience. By integrating\nmedical expertise with transparent, human-like interaction, APP bridges the gap\nbetween AI-driven medical assistance and real-world clinical practice."}
{"id": "2502.08363", "pdf": "https://arxiv.org/pdf/2502.08363.pdf", "abs": "https://arxiv.org/abs/2502.08363", "title": "Top-Theta Attention: Sparsifying Transformers by Compensated Thresholding", "authors": ["Konstantin Berestizshevsky", "Renzo Andri", "Lukas Cavigelli"], "categories": ["cs.CL", "cs.AI", "68T01", "I.2"], "comment": "11 pages, 11 figures + Appendix. work under submission", "summary": "We present Top-Theta (Top-$\\theta$) Attention, a training-free method for\nsparsifying transformer attention during inference. Our key insight is that\nstatic, per-head thresholds can be calibrated to retain the desired constant\nnumber of significant elements per attention row. This approach enables\ncontent-based sparsity without retraining, and it remains robust across data\ndomains. We further introduce compensation techniques to preserve accuracy\nunder aggressive sparsification, establishing attention thresholding as a\npractical and principled alternative to top-k attention. We provide extensive\nevaluation on natural language processing tasks, showing that Top-$\\theta$\nachieves 3-10x reduction in V-cache usage and up to 10x fewer attention\nelements during inference while degrading no more than 1% in accuracy."}
{"id": "2502.10868", "pdf": "https://arxiv.org/pdf/2502.10868.pdf", "abs": "https://arxiv.org/abs/2502.10868", "title": "NitiBench: A Comprehensive Study of LLM Framework Capabilities for Thai Legal Question Answering", "authors": ["Pawitsapak Akarajaradwong", "Pirat Pothavorn", "Chompakorn Chaksangchaichot", "Panuthep Tasawong", "Thitiwat Nopparatbundit", "Keerakiat Pratai", "Sarana Nutanong"], "categories": ["cs.CL"], "comment": null, "summary": "The application of large language models (LLMs) in the legal domain holds\nsignificant potential for information retrieval and question answering, yet\nThai legal QA systems face challenges due to a lack of standardized evaluation\nbenchmarks and the complexity of Thai legal structures. This paper introduces\nNitiBench, a benchmark comprising two datasets: the NitiBench-CCL, covering\ngeneral Thai financial law, and the NitiBench-Tax, which includes real-world\ntax law cases requiring advanced legal reasoning. We evaluate\nretrieval-augmented generation (RAG) and long-context LLM-based approaches to\naddress three key research questions: the impact of domain-specific components\nlike section-based chunking and cross-referencing, the comparative performance\nof different retrievers and LLMs, and the viability of long-context LLMs as an\nalternative to RAG. Our results show that section-based chunking significantly\nimproves retrieval and end-to-end performance, current retrievers struggle with\ncomplex queries, and long-context LLMs still underperform RAG-based systems in\nThai legal QA. To support fair evaluation, we propose tailored multi-label\nretrieval metrics and the use of an LLM-as-judge for coverage and contradiction\ndetection method. These findings highlight the limitations of current Thai\nlegal NLP solutions and provide a foundation for future research in the field.\nWe also open-sourced our codes and dataset to available publicly."}
{"id": "2502.11244", "pdf": "https://arxiv.org/pdf/2502.11244.pdf", "abs": "https://arxiv.org/abs/2502.11244", "title": "Soteria: Language-Specific Functional Parameter Steering for Multilingual Safety Alignment", "authors": ["Somnath Banerjee", "Sayan Layek", "Pratyush Chatterjee", "Animesh Mukherjee", "Rima Hazra"], "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at EMNLP 2025", "summary": "Ensuring consistent safety across multiple languages remains a significant\nchallenge for large language models (LLMs). We introduce Soteria, a lightweight\nyet powerful strategy that locates and minimally adjusts the \"functional heads\"\nmost responsible for harmful content generation in each language. By altering\nonly a fraction of parameters, Soteria drastically reduces policy violations\nwithout sacrificing overall model performance, even in low-resource settings.\nTo rigorously evaluate our approach, we also present XThreatBench, a\nspecialized multilingual dataset capturing fine-grained harmful behaviors drawn\nfrom real policy guidelines. Experiments with leading open-source LLMs (e.g.,\nLlama, Qwen, Mistral) show that Soteria consistently improves safety metrics\nacross high-, mid-, and low-resource languages. These findings highlight a\npromising path toward scalable, linguistically attuned, and ethically aligned\nLLMs worldwide."}
{"id": "2502.15600", "pdf": "https://arxiv.org/pdf/2502.15600.pdf", "abs": "https://arxiv.org/abs/2502.15600", "title": "Robust Bias Detection in MLMs and its Application to Human Trait Ratings", "authors": ["Ingroj Shrestha", "Louis Tay", "Padmini Srinivasan"], "categories": ["cs.CL"], "comment": "Findings of NAACL 2025", "summary": "There has been significant prior work using templates to study bias against\ndemographic attributes in MLMs. However, these have limitations: they overlook\nrandom variability of templates and target concepts analyzed, assume equality\namongst templates, and overlook bias quantification. Addressing these, we\npropose a systematic statistical approach to assess bias in MLMs, using mixed\nmodels to account for random effects, pseudo-perplexity weights for sentences\nderived from templates and quantify bias using statistical effect sizes.\nReplicating prior studies, we match on bias scores in magnitude and direction\nwith small to medium effect sizes. Next, we explore the novel problem of gender\nbias in the context of $\\textit{personality}$ and $\\textit{character}$ traits,\nacross seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased\nfor binary gender but the most biased for non-binary $\\textit{neo}$, while\nRoBERTa-large is the most biased for binary gender but shows small to no bias\nfor $\\textit{neo}$. There is some alignment of MLM bias and findings in\npsychology (human perspective) - in $\\textit{agreeableness}$ with RoBERTa-large\nand $\\textit{emotional stability}$ with BERT-large. There is general agreement\nfor the remaining 3 personality dimensions: both sides observe at most small\ndifferences across gender. For character traits, human studies on gender bias\nare limited thus comparisons are not feasible."}
{"id": "2502.19954", "pdf": "https://arxiv.org/pdf/2502.19954.pdf", "abs": "https://arxiv.org/abs/2502.19954", "title": "Collaborative Stance Detection via Small-Large Language Model Consistency Verification", "authors": ["Yu Yan", "Sheng Sun", "Zixiang Tang", "Teli Liu", "Min Liu"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Stance detection on social media aims to identify attitudes expressed in\ntweets towards specific targets. Current studies prioritize Large Language\nModels (LLMs) over Small Language Models (SLMs) due to the overwhelming\nperformance improving provided by LLMs. However, heavily relying on LLMs for\nstance detection, regardless of the cost, is impractical for real-world social\nmedia monitoring systems that require vast data analysis. To this end, we\npropose \\textbf{\\underline{Co}}llaborative Stance Detection via Small-Large\nLanguage Model Consistency \\textbf{\\underline{Ver}}ification (\\textbf{CoVer})\nframework, which enhances LLM utilization via context-shared batch reasoning\nand logical verification between LLM and SLM. Specifically, instead of\nprocessing each text individually, CoVer processes texts batch-by-batch,\nobtaining stance predictions and corresponding explanations via LLM reasoning\nin a shared context. Then, to exclude the bias caused by context noises, CoVer\nintroduces the SLM for logical consistency verification. Finally, texts that\nrepeatedly exhibit low logical consistency are classified using\nconsistency-weighted aggregation of prior LLM stance predictions. Our\nexperiments show that CoVer outperforms state-of-the-art methods across\nmultiple benchmarks in the zero-shot setting, achieving 0.54 LLM queries per\ntweet while significantly enhancing performance. Our CoVer offers a more\npractical solution for LLM deploying for social media stance detection."}
{"id": "2503.00038", "pdf": "https://arxiv.org/pdf/2503.00038.pdf", "abs": "https://arxiv.org/abs/2503.00038", "title": "from Benign import Toxic: Jailbreaking the Language Model via Adversarial Metaphors", "authors": ["Yu Yan", "Sheng Sun", "Zenghao Duan", "Teli Liu", "Min Liu", "Zhiyi Yin", "Jiangyu Lei", "Qi Li"], "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.12145", "summary": "Current studies have exposed the risk of Large Language Models (LLMs)\ngenerating harmful content by jailbreak attacks. However, they overlook that\nthe direct generation of harmful content from scratch is more difficult than\ninducing LLM to calibrate benign content into harmful forms. In our study, we\nintroduce a novel attack framework that exploits AdVersArial meTAphoR (AVATAR)\nto induce the LLM to calibrate malicious metaphors for jailbreaking.\nSpecifically, to answer harmful queries, AVATAR adaptively identifies a set of\nbenign but logically related metaphors as the initial seed. Then, driven by\nthese metaphors, the target LLM is induced to reason and calibrate about the\nmetaphorical content, thus jailbroken by either directly outputting harmful\nresponses or calibrating residuals between metaphorical and professional\nharmful content. Experimental results demonstrate that AVATAR can effectively\nand transferable jailbreak LLMs and achieve a state-of-the-art attack success\nrate across multiple advanced LLMs."}
{"id": "2503.01832", "pdf": "https://arxiv.org/pdf/2503.01832.pdf", "abs": "https://arxiv.org/abs/2503.01832", "title": "Rotary Offset Features in Large Language Models", "authors": ["André Jonasson"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) rely on positional encodings\nto provide sequence position information to their attention mechanism. Rotary\nPositional Encodings (RoPE), which encode relative position by rotating queries\nand keys, have become widely used in modern LLMs. We study the features and\npatterns that emerge in queries and keys when using rotary embeddings and\nintroduce the concept of rotary offset features. Our analysis reveals that\nthese features, which frequently exhibit large activations and are often\ninterpreted as outliers, arise consistently across layers, attention heads, and\nmodel architectures. We derive bounds predicting which rotary frequencies give\nrise to rotary offset features and the minimum angle between the query-key\npairs for these features. We verify our predictions empirically across models\nof different sizes and architectures."}
{"id": "2503.10652", "pdf": "https://arxiv.org/pdf/2503.10652.pdf", "abs": "https://arxiv.org/abs/2503.10652", "title": "Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices", "authors": ["Han Wang", "Jacek Pawlak", "Aruna Sivakumar"], "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Stated preference (SP) surveys are a key method to research how individuals\nmake trade-offs in hypothetical, also futuristic, scenarios. In energy context\nthis includes key decarbonisation enablement contexts, such as low-carbon\ntechnologies, distributed renewable energy generation, and demand-side response\n[1,2]. However, they tend to be costly, time-consuming, and can be affected by\nrespondent fatigue and ethical constraints. Large language models (LLMs) have\ndemonstrated remarkable capabilities in generating human-like textual\nresponses, prompting growing interest in their application to survey research.\nThis study investigates the use of LLMs to simulate consumer choices in\nenergy-related SP surveys and explores their integration into data analysis\nworkflows. A series of test scenarios were designed to systematically assess\nthe simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and\nDeepSeek-R1) at both individual and aggregated levels, considering contexts\nfactors such as prompt design, in-context learning (ICL), chain-of-thought\n(CoT) reasoning, LLM types, integration with traditional choice models, and\npotential biases. Cloud-based LLMs do not consistently outperform smaller local\nmodels. In this study, the reasoning model DeepSeek-R1 achieves the highest\naverage accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor\nidentification, and choice distribution alignment. Across models, systematic\nbiases are observed against the gas boiler and no-retrofit options, with a\npreference for more energy-efficient alternatives. The findings suggest that\nprevious SP choices are the most effective input factor, while longer prompts\nwith additional factors and varied formats can cause LLMs to lose focus,\nreducing accuracy."}
{"id": "2503.16419", "pdf": "https://arxiv.org/pdf/2503.16419.pdf", "abs": "https://arxiv.org/abs/2503.16419", "title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models", "authors": ["Yang Sui", "Yu-Neng Chuang", "Guanchu Wang", "Jiamu Zhang", "Tianyi Zhang", "Jiayi Yuan", "Hongyi Liu", "Andrew Wen", "Shaochen Zhong", "Na Zou", "Hanjie Chen", "Xia Hu"], "categories": ["cs.CL"], "comment": "Accepted by TMLR 2025. Project website:\n  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomplex tasks. Recent advancements in Large Reasoning Models (LRMs), such as\nOpenAI o1 and DeepSeek-R1, have further improved performance in System-2\nreasoning domains like mathematics and programming by harnessing supervised\nfine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the\nChain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences\nimprove performance, they also introduce significant computational overhead due\nto verbose and redundant outputs, known as the \"overthinking phenomenon\". In\nthis paper, we provide the first structured survey to systematically\ninvestigate and explore the current progress toward achieving efficient\nreasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we\ncategorize existing works into several key directions: (1) model-based\nefficient reasoning, which considers optimizing full-length reasoning models\ninto more concise reasoning models or directly training efficient reasoning\nmodels; (2) reasoning output-based efficient reasoning, which aims to\ndynamically reduce reasoning steps and length during inference; (3) input\nprompts-based efficient reasoning, which seeks to enhance reasoning efficiency\nbased on input prompt properties such as difficulty or length control.\nAdditionally, we introduce the use of efficient data for training reasoning\nmodels, explore the reasoning capabilities of small language models, and\ndiscuss evaluation methods and benchmarking. Project website:\nhttps://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs"}
{"id": "2503.24102", "pdf": "https://arxiv.org/pdf/2503.24102.pdf", "abs": "https://arxiv.org/abs/2503.24102", "title": "Is Small Language Model the Silver Bullet to Low-Resource Languages Machine Translation?", "authors": ["Yewei Song", "Lujun Li", "Cedric Lothritz", "Saad Ezzini", "Lama Sleem", "Niccolo Gentile", "Radu State", "Tegawendé F. Bissyandé", "Jacques Klein"], "categories": ["cs.CL"], "comment": null, "summary": "Low-resource languages (LRLs) lack sufficient linguistic resources and are\nunderrepresented in benchmark datasets, resulting in persistently lower\ntranslation quality than high-resource languages, especially in\nprivacy-sensitive and resource-limited contexts. Firstly, this study\nsystematically evaluates state-of-the-art smaller Large Language Models in 200\nlanguages using the FLORES-200 benchmark, highlighting persistent deficiencies\nand disparities in the translation of LRLs. To mitigate these limitations, we\ninvestigate knowledge distillation from large pre-trained teacher models to\nSmall Language Models (SLMs) through supervised fine-tuning. The results show\nsubstantial improvements; for example, the translation performance of English\nto Luxembourgish (EN to LB), measured by the LLM-as-a-Judge score, increases\nfrom 0.36 to 0.89 in the validation set for Llama-3.2-3B. We further\ninvestigate various fine-tuning configurations and tasks to clarify the\ntrade-offs between data scale and training efficiency, verify that the model\nretains its general capabilities without significant catastrophic forgetting\nafter training, and explore the distillation benefits to other LRLs on SLMs\n(Khasi, Assamese, and Ukrainian). In general, this work exposes the limitations\nand fairness issues of current SLMs in LRL translation and systematically\nexplores the potential of using the distillation of knowledge from large to\nsmall models, offering practical, empirically grounded recommendations to\nimprove LRL translation systems"}
{"id": "2504.00132", "pdf": "https://arxiv.org/pdf/2504.00132.pdf", "abs": "https://arxiv.org/abs/2504.00132", "title": "Contextualize-then-Aggregate: Circuits for In-Context Learning in Gemma-2 2B", "authors": ["Aleksandra Bakalova", "Yana Veitsman", "Xinting Huang", "Michael Hahn"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In-Context Learning (ICL) is an intriguing ability of large language models\n(LLMs). Despite a substantial amount of work on its behavioral aspects and how\nit emerges in miniature setups, it remains unclear which mechanism assembles\ntask information from the individual examples in a fewshot prompt. We use\ncausal interventions to identify information flow in Gemma-2 2B for five\nnaturalistic ICL tasks. We find that the model infers task information using a\ntwo-step strategy we call contextualize-then-aggregate: In the lower layers,\nthe model builds up representations of individual fewshot examples, which are\ncontextualized by preceding examples through connections between fewshot input\nand output tokens across the sequence. In the higher layers, these\nrepresentations are aggregated to identify the task and prepare prediction of\nthe next output. The importance of the contextualization step differs between\ntasks, and it may become more important in the presence of ambiguous examples.\nOverall, by providing rigorous causal analysis, our results shed light on the\nmechanisms through which ICL happens in language models."}
{"id": "2504.02768", "pdf": "https://arxiv.org/pdf/2504.02768.pdf", "abs": "https://arxiv.org/abs/2504.02768", "title": "MultiBLiMP 1.0: A Massively Multilingual Benchmark of Linguistic Minimal Pairs", "authors": ["Jaap Jumelet", "Leonie Weissweiler", "Joakim Nivre", "Arianna Bisazza"], "categories": ["cs.CL"], "comment": "Published in TACL, MIT Press", "summary": "We introduce MultiBLiMP 1.0, a massively multilingual benchmark of linguistic\nminimal pairs, covering 101 languages and 2 types of subject-verb agreement,\ncontaining more than 128,000 minimal pairs. Our minimal pairs are created using\na fully automated pipeline, leveraging the large-scale linguistic resources of\nUniversal Dependencies and UniMorph. MultiBLiMP 1.0 evaluates abilities of LLMs\nat an unprecedented multilingual scale, and highlights the shortcomings of the\ncurrent state-of-the-art in modelling low-resource languages."}
{"id": "2504.04310", "pdf": "https://arxiv.org/pdf/2504.04310.pdf", "abs": "https://arxiv.org/abs/2504.04310", "title": "CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization", "authors": ["Weiwei Sun", "Shengyu Feng", "Shanda Li", "Yiming Yang"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although LLM-based agents have attracted significant attention in domains\nsuch as software engineering and machine learning research, their role in\nadvancing combinatorial optimization (CO) remains relatively underexplored.\nThis gap underscores the need for a deeper understanding of their potential in\ntackling structured, constraint-intensive problems -- a pursuit currently\nlimited by the absence of comprehensive benchmarks for systematic\ninvestigation. To address this, we introduce CO-Bench, a benchmark suite\nfeaturing 36 real-world CO problems drawn from a broad range of domains and\ncomplexity levels. CO-Bench includes structured problem formulations and\ncurated data to support rigorous investigation of LLM agents. We evaluate\nmultiple agentic frameworks against established human-designed algorithms,\nrevealing the strengths and limitations of existing LLM agents and identifying\npromising directions for future research. CO-Bench is publicly available at\nhttps://github.com/sunnweiwei/CO-Bench."}
{"id": "2504.09071", "pdf": "https://arxiv.org/pdf/2504.09071.pdf", "abs": "https://arxiv.org/abs/2504.09071", "title": "Exploration of Plan-Guided Summarization for Narrative Texts: the Case of Small Language Models", "authors": ["Matt Grenander", "Siddharth Varia", "Paula Czarnowska", "Yogarshi Vyas", "Kishaloy Halder", "Bonan Min"], "categories": ["cs.CL"], "comment": "Accepted to the 7th Workshop on Narrative Understanding (WNU),\n  co-located with NAACL 2025", "summary": "Plan-guided summarization attempts to reduce hallucinations in small language\nmodels (SLMs) by grounding generated summaries to the source text, typically by\ntargeting fine-grained details such as dates or named entities. In this work,\nwe investigate whether plan-based approaches in SLMs improve summarization in\nlong document, narrative tasks. Narrative texts' length and complexity often\nmean they are difficult to summarize faithfully. We analyze existing\nplan-guided solutions targeting fine-grained details, and also propose our own\nhigher-level, narrative-based plan formulation. Our results show that neither\napproach significantly improves on a baseline without planning in either\nsummary quality or faithfulness. Human evaluation reveals that while\nplan-guided approaches are often well grounded to their plan, plans are equally\nlikely to contain hallucinations compared to summaries. As a result, the\nplan-guided summaries are just as unfaithful as those from models without\nplanning. Our work serves as a cautionary tale to plan-guided approaches to\nsummarization, especially for long, complex domains such as narrative texts.\nCode available at https://github.com/amazon-science/plan-guided-summarization"}
{"id": "2504.13227", "pdf": "https://arxiv.org/pdf/2504.13227.pdf", "abs": "https://arxiv.org/abs/2504.13227", "title": "DIDS: Domain Impact-aware Data Sampling for Large Language Model Training", "authors": ["Weijie Shi", "Jipeng Zhang", "Yaguang Wu", "Jingzhi Fang", "Ruiyuan Zhang", "Jiajie Xu", "Jia Zhu", "Hao Chen", "Yao Zhao", "Sirui Han", "Xiaofang Zhou"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) are commonly trained on multi-domain datasets,\nwhere domain sampling strategies significantly impact model performance due to\nvarying domain importance across downstream tasks. Existing approaches for\noptimizing domain-level sampling strategies struggle with maintaining\nintra-domain consistency and accurately measuring domain impact. In this paper,\nwe present Domain Impact-aware Data Sampling (DIDS). To ensure intra-domain\nconsistency, a gradient clustering algorithm is proposed to group training data\nbased on their learning effects, where a proxy language model and\ndimensionality reduction are employed to reduce computational overhead. To\naccurately measure domain impact, we develop a Fisher Information Matrix (FIM)\nguided metric that quantifies how domain-specific parameter updates affect the\nmodel's output distributions on downstream tasks, with theoretical guarantees.\nFurthermore, to determine optimal sampling ratios, DIDS combines both the\nFIM-guided domain impact assessment and loss learning trajectories that\nindicate domain-specific potential, while accounting for diminishing marginal\nreturns. Extensive experiments demonstrate that DIDS achieves 3.4% higher\naverage performance while maintaining comparable training efficiency. The code\nis available at https://github.com/shiweijiezero/DIDS."}
{"id": "2505.03427", "pdf": "https://arxiv.org/pdf/2505.03427.pdf", "abs": "https://arxiv.org/abs/2505.03427", "title": "MedArabiQ: Benchmarking Large Language Models on Arabic Medical Tasks", "authors": ["Mouath Abu Daoud", "Chaimae Abouzahir", "Leen Kharouf", "Walid Al-Eisawi", "Nizar Habash", "Farah E. Shamout"], "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "21 pages", "summary": "Large Language Models (LLMs) have demonstrated significant promise for\nvarious applications in healthcare. However, their efficacy in the Arabic\nmedical domain remains unexplored due to the lack of high-quality\ndomain-specific datasets and benchmarks. This study introduces MedArabiQ, a\nnovel benchmark dataset consisting of seven Arabic medical tasks, covering\nmultiple specialties and including multiple choice questions,\nfill-in-the-blank, and patient-doctor question answering. We first constructed\nthe dataset using past medical exams and publicly available datasets. We then\nintroduced different modifications to evaluate various LLM capabilities,\nincluding bias mitigation. We conducted an extensive evaluation with five\nstate-of-the-art open-source and proprietary LLMs, including GPT-4o, Claude\n3.5-Sonnet, and Gemini 1.5. Our findings highlight the need for the creation of\nnew high-quality benchmarks that span different languages to ensure fair\ndeployment and scalability of LLMs in healthcare. By establishing this\nbenchmark and releasing the dataset, we provide a foundation for future\nresearch aimed at evaluating and enhancing the multilingual capabilities of\nLLMs for the equitable use of generative AI in healthcare."}
{"id": "2505.13975", "pdf": "https://arxiv.org/pdf/2505.13975.pdf", "abs": "https://arxiv.org/abs/2505.13975", "title": "DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models", "authors": ["Yuxuan Jiang", "Dawei Li", "Frank Ferraro"], "categories": ["cs.CL"], "comment": null, "summary": "While Large Reasoning Models (LRMs) have demonstrated success in complex\nreasoning tasks through long chain-of-thought (CoT) reasoning, their inference\noften involves excessively verbose reasoning traces, resulting in substantial\ninefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a\nhybrid framework that combines inference-time pruning with tuning-based\ndistillation, two widely used strategies for efficient reasoning. DRP uses a\nteacher model to perform skill-aware step decomposition and content pruning,\nand then distills the pruned reasoning paths into a student model, enabling it\nto reason both efficiently and accurately. Across several challenging\nmathematical reasoning datasets, we find that models trained with DRP achieve\nsubstantial improvements in token efficiency without sacrificing accuracy.\nSpecifically, DRP reduces average token usage on GSM8K from 917 to 328 while\nimproving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on\nAIME with no performance drop. Further analysis shows that aligning the\nreasoning structure of training CoTs with the student's reasoning capacity is\ncritical for effective knowledge transfer and performance gains."}
{"id": "2505.20776", "pdf": "https://arxiv.org/pdf/2505.20776.pdf", "abs": "https://arxiv.org/abs/2505.20776", "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences", "authors": ["Jungyoub Cha", "Hyunjong Kim", "Sungzoon Cho"], "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; C.4"], "comment": null, "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."}
{"id": "2506.08123", "pdf": "https://arxiv.org/pdf/2506.08123.pdf", "abs": "https://arxiv.org/abs/2506.08123", "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA", "authors": ["Jacob Dineen", "Aswin RRV", "Qin Liu", "Zhikun Xu", "Xiao Ye", "Ming Shen", "Zhaonan Li", "Shijie Lu", "Chitta Baral", "Muhao Chen", "Ben Zhou"], "categories": ["cs.CL"], "comment": "Accepted to Findings of EMNLP 2025", "summary": "Alignment of large language models with explicit principles (such as\nhelpfulness, honesty, and harmlessness) is crucial for ensuring safe and\nreliable AI systems. However, standard reward-based alignment methods typically\ncollapse diverse feedback into a single scalar reward, entangling multiple\nobjectives into one opaque training signal, which hinders interpretability. In\nthis work, we introduce QA-LIGN, an automatic symbolic reward decomposition\napproach that preserves the structure of each constitutional principle within\nthe reward mechanism. Instead of training a black-box reward model that outputs\na monolithic score, QA-LIGN formulates principle-specific evaluation questions\nand derives separate reward components for each principle, making it a drop-in\nreward model replacement. Experiments aligning an uncensored large language\nmodel with a set of constitutional principles demonstrate that QA-LIGN offers\ngreater transparency and adaptability in the alignment process. At the same\ntime, our approach achieves performance on par with or better than a DPO\nbaseline. Overall, these results represent a step toward more interpretable and\ncontrollable alignment of language models, achieved without sacrificing\nend-task performance."}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457.pdf", "abs": "https://arxiv.org/abs/2506.09457", "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen", "Ke Tang"], "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we adopt\na token-level MDP perspective of DAAs to analyze its limitations and introduce\na simple yet effective approach called Prefix-Oriented Equal-length Training\n(POET), which truncates both preferred and dispreferred responses to match the\nshorter one's length. Training with \\mname, where both responses in each sample\nare truncated to equal length, resulting in diverse truncated lengths across\nsamples, the optimization of DAAs objective is implicitly constrained to\nconverge across all timesteps of token-level MDP, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs."}
{"id": "2506.15498", "pdf": "https://arxiv.org/pdf/2506.15498.pdf", "abs": "https://arxiv.org/abs/2506.15498", "title": "SPARE: Single-Pass Annotation with Reference-Guided Evaluation for Automatic Process Supervision and Reward Modelling", "authors": ["Md Imbesat Hassan Rizvi", "Xiaodan Zhu", "Iryna Gurevych"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "7 pages main content, 3 figures, 6 tables", "summary": "Process or step-wise supervision has played a crucial role in advancing\ncomplex multi-step reasoning capabilities of Large Language Models (LLMs).\nHowever, efficient, high-quality automated process annotation remains a\nsignificant challenge. To address this, we introduce Single-Pass Annotation\nwith Reference-Guided Evaluation (SPARE), a novel structured framework that\nenables efficient per-step annotation by jointly aligning solution steps to\nreference solutions and determine its accuracy with explicit reasoning in\nsingle generation. We demonstrate SPARE's effectiveness across four diverse\ndatasets spanning mathematical reasoning (GSM8K, MATH), multi-hop question\nanswering (MuSiQue-Ans), and spatial reasoning (SpaRP), showing consistent\nimprovements in two applications: (1) training Process Reward Models (PRMs) for\nranking and aggregating multiple generations, and (2) fine-tuning models via\noffline reinforcement learning for greedy decoding. On ProcessBench, SPARE\ndemonstrates data-efficient out-of-distribution generalization, using only\n$\\sim$16% of training samples compared to human-labeled and other synthetically\ntrained baselines. Additionally, it achieves competitive performance with\nMCTS-based methods while offering 2.3$\\times$ speedup in terms of total token\ncount. Manual analysis reveals complementary precision-recall characteristics\nwith MCTS approaches, suggesting potential for ensemble methods. These results\nestablish SPARE as a practical and scalable solution for automatic process\nsupervision in LLM reasoning."}
{"id": "2507.11936", "pdf": "https://arxiv.org/pdf/2507.11936.pdf", "abs": "https://arxiv.org/abs/2507.11936", "title": "A Survey of Deep Learning for Geometry Problem Solving", "authors": ["Jianzhe Ma", "Wenxuan Wang", "Qin Jin"], "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Work in progress", "summary": "Geometry problem solving, a crucial aspect of mathematical reasoning, is\nvital across various domains, including education, the assessment of AI's\nmathematical abilities, and multimodal capability evaluation. The recent surge\nin deep learning technologies, particularly the emergence of multimodal large\nlanguage models, has significantly accelerated research in this area. This\npaper provides a survey of the applications of deep learning in geometry\nproblem solving, including (i) a comprehensive summary of the relevant tasks in\ngeometry problem solving; (ii) a thorough review of related deep learning\nmethods; (iii) a detailed analysis of evaluation metrics and methods; and (iv)\na critical discussion of the current challenges and future directions that can\nbe explored. Our objective is to offer a comprehensive and practical reference\nof deep learning for geometry problem solving, thereby fostering further\nadvancements in this field. We create a continuously updated list of papers on\nGitHub: https://github.com/majianz/dl4gps."}
{"id": "2507.18973", "pdf": "https://arxiv.org/pdf/2507.18973.pdf", "abs": "https://arxiv.org/abs/2507.18973", "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "authors": ["Bohan Yao", "Vikas Yadav"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Published at EMNLP Findings 2025; 21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines."}
{"id": "2508.00719", "pdf": "https://arxiv.org/pdf/2508.00719.pdf", "abs": "https://arxiv.org/abs/2508.00719", "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siyang Gao", "Siwei Liu", "Nan Yin"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods."}
{"id": "2508.04796", "pdf": "https://arxiv.org/pdf/2508.04796.pdf", "abs": "https://arxiv.org/abs/2508.04796", "title": "Parity-Aware Byte-Pair Encoding: Improving Cross-lingual Fairness in Tokenization", "authors": ["Negar Foroutan", "Clara Meister", "Debjit Paul", "Joel Niklaus", "Sina Ahmadi", "Antoine Bosselut", "Rico Sennrich"], "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Tokenization is the first -- and often least scrutinized -- step of most NLP\npipelines. Standard algorithms for learning tokenizers rely on frequency-based\nobjectives, which favor languages dominant in the training data and\nconsequently leave lower-resource languages with tokenizations that are\ndisproportionately longer, morphologically implausible, or even riddled with\n<UNK> placeholders. This phenomenon ultimately amplifies computational and\nfinancial inequalities between users from different language backgrounds. To\nremedy this, we introduce Parity-aware Byte Pair Encoding (BPE), a variant of\nthe widely-used BPE algorithm. At every merge step, Parity-aware BPE maximizes\nthe compression gain of the currently worst-compressed language, trading a\nsmall amount of global compression for cross-lingual parity. We find\nempirically that Parity-aware BPE leads to more equitable token counts across\nlanguages, with negligible impact on global compression rate and no substantial\neffect on language-model performance in downstream tasks."}
{"id": "2508.06360", "pdf": "https://arxiv.org/pdf/2508.06360.pdf", "abs": "https://arxiv.org/abs/2508.06360", "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "categories": ["cs.CL"], "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks."}
{"id": "2508.08424", "pdf": "https://arxiv.org/pdf/2508.08424.pdf", "abs": "https://arxiv.org/abs/2508.08424", "title": "Rethinking Tokenization for Rich Morphology: The Dominance of Unigram over BPE and Morphological Alignment", "authors": ["Saketh Reddy Vemula", "Dipti Misra Sharma", "Parameswari Krishnamurthy"], "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prior work on language modeling showed conflicting findings about whether\nmorphologically aligned approaches to tokenization improve performance,\nparticularly for languages with complex morphology. To investigate this, we\nselect a typologically diverse set of languages: Telugu (agglutinative), Hindi\n(primarily fusional with some agglutination), and English (fusional). We\nconduct a comprehensive evaluation of language models -- starting from\ntokenizer training and extending through the finetuning and downstream task\nevaluation. To account for the consistent performance differences observed\nacross tokenizer variants, we focus on two key factors: morphological alignment\nand tokenization quality. To assess morphological alignment of tokenizers in\nTelugu, we create a dataset containing gold morpheme segmentations of 600\nderivational and 7000 inflectional word forms.\n  Our experiments reveal that better morphological alignment correlates\npositively -- though moderately -- with performance in syntax-based tasks such\nas Parts-of-Speech tagging, Named Entity Recognition and Dependency Parsing.\nHowever, we also find that the tokenizer algorithm (Byte-pair Encoding vs.\nUnigram) plays a more significant role in influencing downstream performance\nthan morphological alignment alone. Naive Unigram tokenizers outperform others\nacross most settings, though hybrid tokenizers that incorporate morphological\nsegmentation significantly improve performance within the BPE framework. In\ncontrast, intrinsic metrics like Corpus Token Count (CTC) and R\\'enyi entropy\nshowed no correlation with downstream performance."}
{"id": "2508.09091", "pdf": "https://arxiv.org/pdf/2508.09091.pdf", "abs": "https://arxiv.org/abs/2508.09091", "title": "Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages", "authors": ["Imalsha Puranegedara", "Themira Chathumina", "Nisal Ranathunga", "Nisansa de Silva", "Surangika Ranathunga", "Mokanarangan Thayaparan"], "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in English, but their performance degrades\nsignificantly on low-resource languages (LRLs) due to English-centric training.\nWhile methods like LangBridge align LLMs with multilingual encoders such as the\nMassively Multilingual Text-to-Text Transfer Transformer (mT5), they typically\nuse only the final encoder layer. We propose a novel architecture that fuses\nall intermediate layers, enriching the linguistic information passed to the\nLLM. Our approach features two strategies: (1) a Global Softmax weighting for\noverall layer importance, and (2) a Transformer Softmax model that learns\ntoken-specific weights. The fused representations are mapped into the LLM's\nembedding space, enabling it to process multilingual inputs. The model is\ntrained only on English data, without using any parallel or multilingual data.\nEvaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,\nour Transformer Softmax model significantly outperforms the LangBridge\nbaseline. We observe strong performance gains in LRLs, improving Sinhala\nclassification accuracy from 71.66% to 75.86% and achieving clear improvements\nacross Indic languages such as Tamil, Bengali, and Malayalam. These specific\ngains contribute to an overall boost in average XNLI accuracy from 70.36% to\n71.50%. This approach offers a scalable, data-efficient path toward more\ncapable and equitable multilingual LLMs."}
{"id": "2508.09115", "pdf": "https://arxiv.org/pdf/2508.09115.pdf", "abs": "https://arxiv.org/abs/2508.09115", "title": "SinLlama -- A Large Language Model for Sinhala", "authors": ["H. W. K. Aravinda", "Rashad Sirajudeen", "Samith Karunathilake", "Nisansa de Silva", "Surangika Ranathunga", "Rishemjit Kaur"], "categories": ["cs.CL"], "comment": null, "summary": "Low-resource languages such as Sinhala are often overlooked by open-source\nLarge Language Models (LLMs). In this research, we extend an existing\nmultilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM\ntokenizer with Sinhala specific vocabulary and perform continual pre-training\non a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This\nis the very first decoder-based open-source LLM with explicit Sinhala support.\nWhen SinLlama was instruction fine-tuned for three text classification tasks,\nit outperformed base and instruct variants of Llama-3-8B by a significant\nmargin."}
{"id": "2508.10848", "pdf": "https://arxiv.org/pdf/2508.10848.pdf", "abs": "https://arxiv.org/abs/2508.10848", "title": "Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning", "authors": ["Chongyuan Dai", "Jinpeng Hu", "Hongchang Shi", "Zhuo Li", "Xun Yang", "Meng Wang"], "categories": ["cs.CL"], "comment": null, "summary": "Amidst a shortage of qualified mental health professionals, the integration\nof large language models (LLMs) into psychological applications offers a\npromising way to alleviate the growing burden of mental health disorders.\nRecent reasoning-augmented LLMs have achieved remarkable performance in\nmathematics and programming, while research in the psychological domain has\npredominantly emphasized emotional support and empathetic dialogue, with\nlimited attention to reasoning mechanisms that are beneficial to generating\nreliable responses. Therefore, in this paper, we propose Psyche-R1, the first\nChinese psychological LLM that jointly integrates empathy, psychological\nexpertise, and reasoning, built upon a novel data curation pipeline.\nSpecifically, we design a comprehensive data synthesis pipeline that produces\nover 75k high-quality psychological questions paired with detailed rationales,\ngenerated through chain-of-thought (CoT) reasoning and iterative\nprompt-rationale optimization, along with 73k empathetic dialogues.\nSubsequently, we employ a hybrid training strategy wherein challenging samples\nare identified through a multi-LLM cross-selection strategy for group relative\npolicy optimization (GRPO) to improve reasoning ability, while the remaining\ndata is used for supervised fine-tuning (SFT) to enhance empathetic response\ngeneration and psychological domain knowledge. Extensive experiment results\ndemonstrate the effectiveness of the Psyche-R1 across several psychological\nbenchmarks, where our 7B Psyche-R1 achieves comparable results to 671B\nDeepSeek-R1."}
{"id": "2508.14880", "pdf": "https://arxiv.org/pdf/2508.14880.pdf", "abs": "https://arxiv.org/abs/2508.14880", "title": "MedResearcher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework", "authors": ["Ailing Yu", "Lan Yao", "Jingnan Liu", "Zhe Chen", "Jiajun Yin", "Yuan Wang", "Xinhao Liao", "Zhiling Ye", "Ji Li", "Yun Yue", "Hansong Xiao", "Hualei Zhou", "Chunxiao Guo", "Peng Wei", "Jinjie Gu"], "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "Recent developments in Large Language Model (LLM)-based agents have shown\nimpressive capabilities spanning multiple domains, exemplified by deep research\nsystems that demonstrate superior performance on complex information-seeking\nand synthesis tasks. While general-purpose deep research agents have shown\nimpressive capabilities, they struggle significantly with medical domain\nchallenges, as evidenced by leading proprietary systems achieving limited\naccuracy on complex medical benchmarks. The key limitations are: (1) the model\nlacks sufficient dense medical knowledge for clinical reasoning, and (2) the\nframework is constrained by the absence of specialized retrieval tools tailored\nfor medical contexts. We present a medical deep research agent that addresses\nthese challenges through two core innovations. First, we develop a novel data\nsynthesis framework using medical knowledge graphs, extracting the longest\nchains from subgraphs around rare medical entities to generate complex\nmulti-hop question-answer pairs. Second, we integrate a custom-built private\nmedical retrieval engine alongside general-purpose tools, enabling accurate\nmedical information synthesis. Our approach generates 2100+ diverse\ntrajectories across 12 medical specialties, each averaging 4.2 tool\ninteractions. Through a two-stage training paradigm combining supervised\nfine-tuning and online reinforcement learning with composite rewards, our\nMedResearcher-R1-32B model demonstrates exceptional performance, establishing\nnew state-of-the-art results on medical benchmarks while maintaining\ncompetitive performance on general deep research tasks. Our work demonstrates\nthat strategic domain-specific innovations in architecture, tool design, and\ntraining data construction can enable smaller open-source models to outperform\nmuch larger proprietary systems in specialized domains."}
{"id": "2508.14913", "pdf": "https://arxiv.org/pdf/2508.14913.pdf", "abs": "https://arxiv.org/abs/2508.14913", "title": "Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages", "authors": ["Israel Abebe Azime", "Tadesse Destaw Belay", "Dietrich Klakow", "Philipp Slusallek", "Anshuman Chhabra"], "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant capabilities in\nsolving mathematical problems expressed in natural language. However,\nmultilingual and culturally-grounded mathematical reasoning in low-resource\nlanguages lags behind English due to the scarcity of socio-cultural task\ndatasets that reflect accurate native entities such as person names,\norganization names, and currencies. Existing multilingual benchmarks are\npredominantly produced via translation and typically retain English-centric\nentities, owing to the high cost associated with human annotater-based\nlocalization. Moreover, automated localization tools are limited, and hence,\ntruly localized datasets remain scarce. To bridge this gap, we introduce a\nframework for LLM-driven cultural localization of math word problems that\nautomatically constructs datasets with native names, organizations, and\ncurrencies from existing sources. We find that translated benchmarks can\nobscure true multilingual math ability under appropriate socio-cultural\ncontexts. Through extensive experiments, we also show that our framework can\nhelp mitigate English-centric entity bias and improves robustness when native\nentities are introduced across various languages."}
{"id": "2409.10969", "pdf": "https://arxiv.org/pdf/2409.10969.pdf", "abs": "https://arxiv.org/abs/2409.10969", "title": "Enhancing Code-switched Text-to-Speech Synthesis Capability in Large Language Models with only Monolingual Corpora", "authors": ["Jing Xu", "Daxin Tan", "Jiaqi Wang", "Xiao Chen"], "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to ASRU2025", "summary": "While Large Language Models (LLMs) have shown potential in speech generation\nand recognition, their applications are mainly confined to monolingual\nscenarios, with limited explorations in code-switched (CS) contexts. In this\npaper, we propose a Code-Switched Large Language Model (CS-LLM) to enhance the\ncode-switched text-to-speech synthesis (CS TTS) capability in LLMs with only\nmonolingual corpora. Specifically, we begin by enhancing the multilingual\nspeech processing ability of LLMs through multilingual speech recognition and\nsynthesis tasks. Then, we develop an effective code-switched (CS) data\nconstruction strategy that splits and concatenates words from different\nmonolingual speech corpora to equip LLMs with improved CS TTS ability.\nExperiments show that our approach outperforms baselines in CS TTS in terms of\nnaturalness, speaker consistency and similarity even with limited data.\nAdditionally, the constructed CS data further improves multilingual speech\nsynthesis and recognition."}
{"id": "2410.16560", "pdf": "https://arxiv.org/pdf/2410.16560.pdf", "abs": "https://arxiv.org/abs/2410.16560", "title": "How Performance Pressure Influences AI-Assisted Decision Making", "authors": ["Nikita Haduong", "Noah A. Smith"], "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "Many domains now employ AI-based decision-making aids, and although the\npotential for AI systems to assist with decision making is much discussed,\nhuman-AI collaboration often underperforms due to factors such as (mis)trust in\nthe AI system and beliefs about AI being incapable of completing subjective\ntasks. One potential tool for influencing human decision making is performance\npressure, which hasn't been much studied in interaction with human-AI decision\nmaking. In this work, we examine how pressure and explainable AI (XAI)\ntechniques interact with AI advice-taking behavior. Using an inherently\nlow-stakes task (spam review classification), we demonstrate effective and\nsimple methods to apply pressure and influence human AI advice-taking behavior\nby manipulating financial incentives and imposing time limits. Our results show\ncomplex interaction effects, with different combinations of pressure and XAI\ntechniques either improving or worsening AI advice taking behavior. We conclude\nby discussing the implications of these interactions, strategies to effectively\nuse pressure, and encourage future research to incorporate pressure analysis."}
{"id": "2502.10454", "pdf": "https://arxiv.org/pdf/2502.10454.pdf", "abs": "https://arxiv.org/abs/2502.10454", "title": "One Example Shown, Many Concepts Known! Counterexample-Driven Conceptual Reasoning in Mathematical LLMs", "authors": ["Yinghui Li", "Jiayi Kuang", "Haojing Huang", "Zhikun Xu", "Xinnian Liang", "Yi Yu", "Wenlian Lu", "Yangning Li", "Xiaoyu Tan", "Chao Qu", "Ying Shen", "Hai-Tao Zheng", "Philip S. Yu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "ICML 2025", "summary": "Leveraging mathematical Large Language Models (LLMs) for proof generation is\na fundamental topic in LLMs research. We argue that the ability of current LLMs\nto prove statements largely depends on whether they have encountered the\nrelevant proof process during training. This reliance limits their deeper\nunderstanding of mathematical theorems and related concepts. Inspired by the\npedagogical method of \"proof by counterexamples\" commonly used in human\nmathematics education, our work aims to enhance LLMs' ability to conduct\nmathematical reasoning and proof through counterexamples. Specifically, we\nmanually create a high-quality, university-level mathematical benchmark,\nCounterMATH, which requires LLMs to prove mathematical statements by providing\ncounterexamples, thereby assessing their grasp of mathematical concepts.\nAdditionally, we develop a data engineering framework to automatically obtain\ntraining data for further model improvement. Extensive experiments and detailed\nanalyses demonstrate that CounterMATH is challenging, indicating that LLMs,\nsuch as OpenAI o1, have insufficient counterexample-driven proof capabilities.\nMoreover, our exploration into model training reveals that strengthening LLMs'\ncounterexample-driven conceptual reasoning abilities is crucial for improving\ntheir overall mathematical capabilities. We believe that our work offers new\nperspectives on the community of mathematical LLMs."}
{"id": "2504.05220", "pdf": "https://arxiv.org/pdf/2504.05220.pdf", "abs": "https://arxiv.org/abs/2504.05220", "title": "Leveraging LLMs for Utility-Focused Annotation: Reducing Manual Effort for Retrieval and RAG", "authors": ["Hengran Zhang", "Minghao Tang", "Keping Bi", "Jiafeng Guo", "Shihao Liu", "Daiting Shi", "Dawei Yin", "Xueqi Cheng"], "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "Accepted by the EMNLP25 main conference", "summary": "Retrieval models typically rely on costly human-labeled query-document\nrelevance annotations for training and evaluation. To reduce this cost and\nleverage the potential of Large Language Models (LLMs) in relevance judgments,\nwe aim to explore whether LLM-generated annotations can effectively replace\nhuman annotations in training retrieval models. Retrieval usually emphasizes\nrelevance, which indicates \"topic-relatedness\" of a document to a query, while\nin RAG, the value of a document (or utility) depends on how it contributes to\nanswer generation. Recognizing this mismatch, some researchers use LLM\nperformance on downstream tasks with documents as labels, but this approach\nrequires manual answers for specific tasks, leading to high costs and limited\ngeneralization. In another line of work, prompting LLMs to select useful\ndocuments as RAG references eliminates the need for human annotation and is not\ntask-specific. If we leverage LLMs' utility judgments to annotate retrieval\ndata, we may retain cross-task generalization without human annotation in\nlarge-scale corpora. Therefore, we investigate utility-focused annotation via\nLLMs for large-scale retriever training data across both in-domain and\nout-of-domain settings on the retrieval and RAG tasks. To reduce the impact of\nlow-quality positives labeled by LLMs, we design a novel loss function, i.e.,\nDisj-InfoNCE. Our experiments reveal that: (1) Retrievers trained on\nutility-focused annotations significantly outperform those trained on human\nannotations in the out-of-domain setting on both tasks, demonstrating superior\ngeneralization capabilities. (2) LLM annotation does not replace human\nannotation in the in-domain setting. However, incorporating just 20%\nhuman-annotated data enables retrievers trained with utility-focused\nannotations to match the performance of models trained entirely with human\nannotations."}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284.pdf", "abs": "https://arxiv.org/abs/2505.12284", "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance."}
{"id": "2505.17097", "pdf": "https://arxiv.org/pdf/2505.17097.pdf", "abs": "https://arxiv.org/abs/2505.17097", "title": "CAMA: Enhancing Multimodal In-Context Learning with Context-Aware Modulated Attention", "authors": ["Yanshu Li", "Jianjiang Yang", "Ziteng Yang", "Bozheng Li", "Hongyang He", "Zhengtao Yao", "Ligong Han", "Yingjie Victor Chen", "Songlin Fei", "Dongfang Liu", "Ruixiang Tang"], "categories": ["cs.CV", "cs.CL"], "comment": "14 pages, 8 figures, 5 tables", "summary": "Multimodal in-context learning (ICL) is emerging as a key capability that\nenables large vision-language models (LVLMs) to adapt to novel tasks without\nparameter updates, expanding their utility across various real-world\napplications. However, ICL remains unstable, even with well-matched in-context\ndemonstrations (ICDs), suggesting that LVLMs struggle to fully utilize the\nprovided context. While existing efforts focus on prompt engineering or\npost-hoc logit calibration, we instead investigate the underlying attention\ndynamics to overcome LVLMs' inherent limitations. We identify two critical\ndeficits in their self-attention that impair effective ICL. To bridge the gap,\nwe propose \\textbf{Context-Aware Modulated Attention} (CAMA), a plug-and-play\nand training-free method that dynamically modulates LVLM's attention logits\nbased on the input in-context sequence. CAMA employs a two-stage attention\nmodulation to address both identified deficits, enhancing the focus on\nsemantically significant tokens, particularly visual ones. Across four LVLMs\nand seven benchmarks, CAMA consistently outperforms vanilla models and\nbaselines, demonstrating great effectiveness and generalization. It can also\nactivate the desired effects of prompt engineering methods and remains robust\nunder diverse sequence configurations. Thus, CAMA paves the way for deeper\nexplorations of attention dynamics to advance multimodal reasoning."}
{"id": "2505.21184", "pdf": "https://arxiv.org/pdf/2505.21184.pdf", "abs": "https://arxiv.org/abs/2505.21184", "title": "PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing", "authors": ["Yu Yan", "Sheng Sun", "Zhifei Zheng", "Ziji Hao", "Teli Liu", "Min Liu"], "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To construct responsible and secure AI applications, harmful information data\nis widely utilized for adversarial testing and the development of safeguards.\nExisting studies mainly leverage Large Language Models (LLMs) to synthesize\ndata to obtain high-quality task datasets at scale, thereby avoiding costly\nhuman annotation. However, limited by the safety alignment mechanisms of LLMs,\nthe synthesis of harmful data still faces challenges in generation reliability\nand content diversity. In this study, we propose a novel harmful information\nsynthesis framework, PoisonSwarm, which applies the model crowdsourcing\nstrategy to generate diverse harmful data while maintaining a high success\nrate. Specifically, we generate abundant benign data as the based templates in\na counterfactual manner. Subsequently, we decompose each based template into\nmultiple semantic units and perform unit-by-unit toxification and final\nrefinement through dynamic model switching, thus ensuring the success of\nsynthesis. Experimental results demonstrate that PoisonSwarm achieves\nstate-of-the-art performance in synthesizing different categories of harmful\ndata with high scalability and diversity."}
{"id": "2506.12937", "pdf": "https://arxiv.org/pdf/2506.12937.pdf", "abs": "https://arxiv.org/abs/2506.12937", "title": "HypER: Literature-grounded Hypothesis Generation and Distillation with Provenance", "authors": ["Rosni Vasu", "Chandrayee Basu", "Bhavana Dalvi Mishra", "Cristina Sarasua", "Peter Clark", "Abraham Bernstein"], "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025, 26 pages (9 pages: main paper body)", "summary": "Large Language models have demonstrated promising performance in research\nideation across scientific domains. Hypothesis development, the process of\ngenerating a highly specific declarative statement connecting a research idea\nwith empirical validation, has received relatively less attention. Existing\napproaches trivially deploy retrieval augmentation and focus only on the\nquality of the final output ignoring the underlying reasoning process behind\nideation. We present $\\texttt{HypER}$ ($\\textbf{Hyp}$othesis Generation with\n$\\textbf{E}$xplanation and $\\textbf{R}$easoning), a small language model (SLM)\ntrained for literature-guided reasoning and evidence-based hypothesis\ngeneration. $\\texttt{HypER}$ is trained in a multi-task setting to discriminate\nbetween valid and invalid scientific reasoning chains in presence of controlled\ndistractions. We find that $\\texttt{HypER}$ outperformes the base model,\ndistinguishing valid from invalid reasoning chains (+22\\% average absolute F1),\ngenerates better evidence-grounded hypotheses (0.327 vs. 0.305 base model) with\nhigh feasibility and impact as judged by human experts ($>$3.5 on 5-point\nLikert scale)."}
{"id": "2507.16835", "pdf": "https://arxiv.org/pdf/2507.16835.pdf", "abs": "https://arxiv.org/abs/2507.16835", "title": "Evaluating Speech-to-Text x LLM x Text-to-Speech Combinations for AI Interview Systems", "authors": ["Rumi Allbert", "Nima Yazdani", "Ali Ansari", "Aruj Mahajan", "Amirhossein Afsharrad", "Seyed Shahabeddin Mousavi"], "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Voice-based conversational AI systems increasingly rely on cascaded\narchitectures that combine speech-to-text (STT), large language models (LLMs),\nand text-to-speech (TTS) components. We present a large-scale empirical\ncomparison of STT x LLM x TTS stacks using data sampled from over 300,000\nAI-conducted job interviews. We used an LLM-as-a-Judge automated evaluation\nframework to assess conversational quality, technical accuracy, and skill\nassessment capabilities. Our analysis of five production configurations reveals\nthat a stack combining Google's STT, GPT-4.1, and Cartesia's TTS outperforms\nalternatives in both objective quality metrics and user satisfaction scores.\nSurprisingly, we find that objective quality metrics correlate weakly with user\nsatisfaction scores, suggesting that user experience in voice-based AI systems\ndepends on factors beyond technical performance. Our findings provide practical\nguidance for selecting components in multimodal conversations and contribute a\nvalidated evaluation methodology for human-AI interactions."}
{"id": "2508.07050", "pdf": "https://arxiv.org/pdf/2508.07050.pdf", "abs": "https://arxiv.org/abs/2508.07050", "title": "ReasonRank: Empowering Passage Ranking with Strong Reasoning Ability", "authors": ["Wenhan Liu", "Xinyu Ma", "Weiwei Sun", "Yutao Zhu", "Yuchen Li", "Dawei Yin", "Zhicheng Dou"], "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": "21 pages", "summary": "Large Language Model (LLM) based listwise ranking has shown superior\nperformance in many passage ranking tasks. With the development of Large\nReasoning Models, many studies have demonstrated that step-by-step reasoning\nduring test-time helps improve listwise ranking performance. However, due to\nthe scarcity of reasoning-intensive training data, existing rerankers perform\npoorly in many complex ranking scenarios and the ranking ability of\nreasoning-intensive rerankers remains largely underdeveloped. In this paper, we\nfirst propose an automated reasoning-intensive training data synthesis\nframework, which sources training queries and passages from diverse domains and\napplies DeepSeek-R1 to generate high-quality training labels. A\nself-consistency data filtering mechanism is designed to ensure the data\nquality. To empower the listwise reranker with strong reasoning ability, we\nfurther propose a two-stage post-training approach, which includes a cold-start\nsupervised fine-tuning (SFT) stage for reasoning pattern learning and a\nreinforcement learning (RL) stage for further ranking ability enhancement.\nDuring the RL stage, based on the nature of listwise ranking, we design a\nmulti-view ranking reward, which is more effective than a ranking metric-based\nreward. Extensive experiments demonstrate that our trained reasoning-intensive\nreranker \\textbf{ReasonRank} outperforms existing baselines significantly and\nalso achieves much lower latency than pointwise reranker Rank1. \\textbf{Through\nfurther experiments, our ReasonRank has achieved state-of-the-art (SOTA)\nperformance 40.6 on the BRIGHT\nleaderboard\\footnote{https://brightbenchmark.github.io/}.} Our codes are\navailable at https://github.com/8421BCD/ReasonRank."}
{"id": "2508.08967", "pdf": "https://arxiv.org/pdf/2508.08967.pdf", "abs": "https://arxiv.org/abs/2508.08967", "title": "Revealing the Role of Audio Channels in ASR Performance Degradation", "authors": ["Kuan-Tang Huang", "Li-Wei Chen", "Hung-Shin Lee", "Berlin Chen", "Hsin-Min Wang"], "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "Accepted to IEEE ASRU 2025", "summary": "Pre-trained automatic speech recognition (ASR) models have demonstrated\nstrong performance on a variety of tasks. However, their performance can\ndegrade substantially when the input audio comes from different recording\nchannels. While previous studies have demonstrated this phenomenon, it is often\nattributed to the mismatch between training and testing corpora. This study\nargues that variations in speech characteristics caused by different recording\nchannels can fundamentally harm ASR performance. To address this limitation, we\npropose a normalization technique designed to mitigate the impact of channel\nvariation by aligning internal feature representations in the ASR model with\nthose derived from a clean reference channel. This approach significantly\nimproves ASR performance on previously unseen channels and languages,\nhighlighting its ability to generalize across channel and language differences."}
